

信息熵的“好坏”取决于具体场景：在机器学习分类任务中，初始数据集的信息熵大，说明数据类别分布均匀、不确定性高，有较大的分类优化空间，这是“好”的；但划分后的子数据集信息熵小，说明子集内类别纯、不确定性低，分类效果好，这也是“好”的。而在密码学中，信息熵大意味着数据更无序、更难被破解，所以希望信息熵越大越好；在数据压缩中，信息熵大表示数据冗余少、压缩潜力大，同样希望熵大。  

信息增益的核心是“通过特征划分减少的不确定性”，所以无论在哪个领域（比如决策树分类、特征选择），信息增益都是越大越好——增益越大，说明用该特征划分后，数据的不确定性下降得越多，特征的分类价值或信息提取能力就越强。

[text](https://www.doubao.com/thread/wb148ce75e7885fc4)


精确率 Precision = TP / (TP + FP)，准确率 Accuracy = (TP + TN) / (TP + FP + TN + FN) 。其中 TP 是真正例，FP 是假正例，TN 是真反例，FN 是假反例。







好的，我们把信息增益公式IG(D,A)=H(D)-H(D|A)完全展开。首先H(D)是数据集D的信息熵，公式是H(D)=-Σ[P(Ck)·log₂P(Ck)]，其中Ck是D中第k类的样本（比如“购买”和“未购买”两类），P(Ck)=|Ck|/|D|，即第k类样本数占总样本数的比例。然后H(D|A)是用特征A划分后D的条件熵，假设特征A有V个不同取值{a1,a2,...,aV}，会把D分成V个子集{D1,D2,...,DV}，每个子集Di的样本数是|Di|，则H(D|A)=Σ[P(Di)·H(Di)]，其中P(Di)=|Di|/|D|是子集Di的占比，H(Di)是子集Di的信息熵，计算方式和H(D)一样：H(Di)=-Σ[P(Ck|Di)·log₂P(Ck|Di)]，这里P(Ck|Di)=|Ck∩Di|/|Di|，即子集中第k类样本数占子集样本数的比例。所以信息增益完整展开就是IG(D,A)=[-ΣP(Ck)log₂P(Ck)] - [ΣP(Di)·(-ΣP(Ck|Di)log₂P(Ck|Di))]，本质是总熵减去按特征A划分后的加权平均子熵。

[text](https://www.doubao.com/thread/w5b13086b19e0e06f)

