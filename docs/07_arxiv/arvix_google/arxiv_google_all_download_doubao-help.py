import asyncio
import aiohttp
import aiofiles
import arxiv
import os
import re
import logging
import time
import textwrap
from concurrent.futures import ThreadPoolExecutor
from functools import wraps
from collections import deque

# ================= é…ç½®åŒºåŸŸ =================
# æ ¸å¿ƒé¢‘ç‡é™åˆ¶ï¼šæ¯ç§’æœ€å¤š3æ¬¡è¯·æ±‚ï¼ˆæœç´¢+ä¸‹è½½åˆè®¡ï¼‰
MAX_REQUESTS_PER_SECOND = 2
DOWNLOAD_CONCURRENCY = 8    # ä¸‹è½½å¹¶å‘æ•°ï¼ˆç”±é¢‘ç‡æ§åˆ¶å™¨å…œåº•ï¼Œå¯é€‚å½“è°ƒé«˜ï¼‰
# SEARCH_CONCURRENCY = 3      # æœç´¢å¹¶å‘æ•°ï¼ˆâ‰¤MAX_REQUESTS_PER_SECONDï¼‰
MAX_RETRIES = 3             # é‡è¯•æ¬¡æ•°ï¼ˆé¢‘ç‡åˆè§„åå¯å‡å°‘ï¼‰
BASE_DELAY = 0.1            # åŸºç¡€å»¶è¿Ÿï¼ˆé¢‘ç‡æ§åˆ¶åæ— éœ€å¤§å»¶è¿Ÿï¼‰
DOWNLOAD_DIR = "Paper_Library_Async_Optimized"
LOG_FILE = "download_mission_optimized.log"

# ================= æ—¥å¿—é…ç½® =================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ================= é¢‘ç‡æ§åˆ¶å™¨ï¼ˆæ ¸å¿ƒï¼‰ =================
class RateLimiter:
    """ä»¤ç‰Œæ¡¶ç®—æ³•å®ç°ï¼šä¸¥æ ¼æ§åˆ¶æ¯ç§’è¯·æ±‚æ•°ä¸è¶…è¿‡æŒ‡å®šå€¼"""
    def __init__(self, max_requests_per_second):
        self.max_requests = max_requests_per_second
        self.tokens = max_requests_per_second  # åˆå§‹ä»¤ç‰Œæ•°
        self.last_refill_time = time.time()
        self.lock = asyncio.Lock()

    async def acquire(self):
        """è·å–ä»¤ç‰Œï¼ˆé˜»å¡ç›´åˆ°æœ‰å¯ç”¨ä»¤ç‰Œï¼‰"""
        async with self.lock:
            # 1. è®¡ç®—æ—¶é—´å·®ï¼Œè¡¥å……ä»¤ç‰Œï¼ˆæ¯ç§’è¡¥å……max_requestsä¸ªï¼‰
            now = time.time()
            time_passed = now - self.last_refill_time
            new_tokens = time_passed * self.max_requests
            if new_tokens > 0:
                self.tokens = min(self.max_requests, self.tokens + new_tokens)
                self.last_refill_time = now

            # 2. å¦‚æœä»¤ç‰Œä¸è¶³ï¼Œç­‰å¾…ç›´åˆ°æœ‰ä»¤ç‰Œ
            while self.tokens < 1:
                await asyncio.sleep(0.01)  # çŸ­è½®è¯¢
                now = time.time()
                time_passed = now - self.last_refill_time
                new_tokens = time_passed * self.max_requests
                if new_tokens > 0:
                    self.tokens += new_tokens
                    self.last_refill_time = now

            # 3. æ¶ˆè€—1ä¸ªä»¤ç‰Œ
            self.tokens -= 1
            return True

# ================= å·¥å…·å‡½æ•° =================
def sanitize_filename(filename):
    clean = re.sub(r'[\\/*?:"<>|]', "", filename)
    return clean.strip()[:150]

def generate_bibtex(result):
    """åŒæ­¥ç”Ÿæˆ BibTeX å­—ç¬¦ä¸²"""
    short_id = result.get_short_id()
    try:
        first_author = result.authors[0].name.split(' ')[-1].lower()
    except:
        first_author = "unknown"
    year = result.published.year
    first_word = result.title.split(' ')[0].lower()
    cite_key = f"{re.sub(r'[^a-z]', '', first_author)}{year}{re.sub(r'[^a-z]', '', first_word)}"
    authors = " and ".join([a.name for a in result.authors])
    
    return textwrap.dedent(f"""
    @misc{{{cite_key},
      title={{{result.title}}}, 
      author={{{authors}}},
      year={{{year}}},
      eprint={{{short_id}}},
      archivePrefix={{arXiv}},
      primaryClass={{{result.primary_category}}},
      url={{{result.entry_id}}}
    }}
    """).strip()

# ================= æ ¸å¿ƒå¼‚æ­¥ç±» =================
class OptimizedPaperDownloader:
    def __init__(self, download_dir):
        self.download_dir = download_dir
        if not os.path.exists(download_dir):
            os.makedirs(download_dir)
        
        self.processed_ids = set()
        self.sem_download = asyncio.Semaphore(DOWNLOAD_CONCURRENCY)
        self.rate_limiter = RateLimiter(MAX_REQUESTS_PER_SECOND)  # å…¨å±€é¢‘ç‡æ§åˆ¶
        
    async def download_file(self, session, url, file_path, file_type="File"):
        """å¸¦é¢‘ç‡æ§åˆ¶çš„å¼‚æ­¥ä¸‹è½½å‡½æ•°"""
        if os.path.exists(file_path):
            logger.info(f"â­ï¸  [Skip] {file_type} exists: {os.path.basename(file_path)}")
            return True

        async with self.sem_download:
            # ç¬¬ä¸€æ­¥ï¼šè·å–é¢‘ç‡ä»¤ç‰Œï¼ˆä¿è¯æ¯ç§’â‰¤3æ¬¡è¯·æ±‚ï¼‰
            await self.rate_limiter.acquire()
            
            for attempt in range(MAX_RETRIES):
                try:
                    headers = {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
                    }
                    async with session.get(url, headers=headers, timeout=60) as response:
                        if response.status == 200:
                            async with aiofiles.open(file_path, 'wb') as f:
                                while True:
                                    chunk = await response.content.read(1024 * 64)
                                    if not chunk:
                                        break
                                    await f.write(chunk)
                            logger.info(f"âœ… [Done] {file_type} downloaded: {os.path.basename(file_path)}")
                            return True
                        elif response.status in [403, 429]:
                            wait_time = BASE_DELAY * (2 ** attempt)
                            logger.warning(f"âš ï¸  [Rate Limit] {response.status} on {file_type}. Sleeping {wait_time}s...")
                            await asyncio.sleep(wait_time)
                        else:
                            logger.error(f"âŒ [Fail] {file_type} HTTP {response.status}: {url}")
                            return False
                except Exception as e:
                    logger.warning(f"âš ï¸  [Retry {attempt+1}/{MAX_RETRIES}] Error downloading {file_type}: {e}")
                    await asyncio.sleep(BASE_DELAY * (attempt + 1))
            
            logger.error(f"âŒ [GiveUp] Failed to download {file_type} after retries: {url}")
            return False

    async def async_search(self, query, max_results):
        """å¼‚æ­¥åŒ…è£…çš„æœç´¢å‡½æ•°ï¼ˆå¸¦é¢‘ç‡æ§åˆ¶ï¼‰"""
        # ç¬¬ä¸€æ­¥ï¼šè·å–é¢‘ç‡ä»¤ç‰Œï¼ˆä¿è¯æœç´¢è¯·æ±‚ä¹Ÿéµå®ˆæ¯ç§’â‰¤3æ¬¡ï¼‰
        await self.rate_limiter.acquire()
        
        logger.info(f"ğŸ” Searching: {query[:50]}...")
        client = arxiv.Client(
            page_size=50,
            delay_seconds=0.34,  # é¢‘ç‡æ§åˆ¶å™¨å·²å…œåº•ï¼Œæ— éœ€å¤§å»¶è¿Ÿ
            num_retries=2
        )
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.Relevance,
            sort_order=arxiv.SortOrder.Descending
        )
        
        loop = asyncio.get_running_loop()
        try:
            # æœç´¢æ˜¯é˜»å¡æ“ä½œï¼Œæ‰”åˆ°çº¿ç¨‹æ± 
            results = await loop.run_in_executor(
                None,  # ä½¿ç”¨é»˜è®¤çº¿ç¨‹æ± 
                lambda: list(client.results(search))
            )
            logger.info(f"ğŸ“„ Found {len(results)} papers for query: {query[:50]}...")
            return results
        except arxiv.HTTPError as e:
            logger.error(f"âŒ Search failed (HTTP {e.status_code}): {query[:50]}...")
            return []
        except Exception as e:
            logger.error(f"âŒ Search error: {query[:50]}... Error: {e}")
            return []

    async def process_paper(self, session, paper):
        """å¤„ç†å•ç¯‡è®ºæ–‡ï¼šåˆ›å»ºæ–‡ä»¶å¤¹ï¼Œç”ŸæˆBibï¼Œä¸‹è½½æ–‡ä»¶"""
        paper_id = paper.get_short_id()
        if paper_id in self.processed_ids:
            return
        self.processed_ids.add(paper_id)

        safe_title = sanitize_filename(paper.title)
        paper_dir = os.path.join(self.download_dir, safe_title)
        if not os.path.exists(paper_dir):
            os.makedirs(paper_dir)

        # ä¿å­˜ BibTeXï¼ˆåŒæ­¥IOï¼Œä¸å è¯·æ±‚é¢‘ç‡ï¼‰
        bib_path = os.path.join(paper_dir, "citation.bib")
        with open(bib_path, "w", encoding="utf-8") as f:
            f.write(generate_bibtex(paper))

        # å‡†å¤‡ä¸‹è½½ä»»åŠ¡
        pdf_url = paper.pdf_url
        source_url = f"https://export.arxiv.org/e-print/{paper_id}"
        # source_url = f"https://arxiv.org/e-print/{paper_id}"
        pdf_path = os.path.join(paper_dir, f"{safe_title}.pdf")
        source_path = os.path.join(paper_dir, "source.tar.gz")

        # å¹¶å‘ä¸‹è½½PDFå’Œæºç ï¼ˆä½†é¢‘ç‡æ§åˆ¶å™¨ä¼šä¿è¯æ€»è¯·æ±‚â‰¤3æ¬¡/ç§’ï¼‰
        await asyncio.gather(
            self.download_file(session, pdf_url, pdf_path, "PDF"),
            self.download_file(session, source_url, source_path, "Source")
        )

    async def main_pipeline(self, queries, max_results_per_query=20):
        """ä¼˜åŒ–åçš„ä¸»æµç¨‹ï¼šé¢‘ç‡æ§åˆ¶+å¼‚æ­¥å¹¶å‘"""
        connector = aiohttp.TCPConnector(limit=DOWNLOAD_CONCURRENCY + 5)
        async with aiohttp.ClientSession(connector=connector) as session:
            logger.info("ğŸš€ Starting Optimized Async Download Mission...")
            logger.info(f"ğŸ“Š Rate Limit: {MAX_REQUESTS_PER_SECOND} requests/second")
            
            # ç¬¬ä¸€é˜¶æ®µï¼šå¼‚æ­¥å¹¶å‘æœç´¢ï¼ˆå¸¦é¢‘ç‡æ§åˆ¶ï¼‰
            search_tasks = []
            for query in queries:
                task = asyncio.create_task(self.async_search(query, max_results_per_query))
                search_tasks.append(task)
            
            logger.info(f"ğŸ” Starting {len(search_tasks)} search tasks (rate-limited)...")
            all_search_results = await asyncio.gather(*search_tasks)
            
            # å±•å¹³ç»“æœå¹¶å»é‡ï¼ˆæŒ‰paper_idï¼‰
            paper_dict = {}
            for res in all_search_results:
                for paper in res:
                    paper_dict[paper.get_short_id()] = paper
            total_papers = list(paper_dict.values())
            logger.info(f"ğŸ“Š Total unique papers found: {len(total_papers)}")
            
            # ç¬¬äºŒé˜¶æ®µï¼šå¼‚æ­¥å¹¶å‘ä¸‹è½½ï¼ˆå¸¦é¢‘ç‡æ§åˆ¶ï¼‰
            download_tasks = []
            for paper in total_papers:
                task = asyncio.create_task(self.process_paper(session, paper))
                download_tasks.append(task)
            
            if download_tasks:
                logger.info(f"ğŸ”¥ Spawning {len(download_tasks)} download tasks (rate-limited)...")
                await asyncio.gather(*download_tasks)
            else:
                logger.warning("âš ï¸ No papers found to download.")
                    
        logger.info("âœ¨ Optimized Mission Complete! Check log for details.")

# ================= æ‰§è¡Œå…¥å£ =================
if __name__ == "__main__":
    # ä½ çš„æŸ¥è¯¢åˆ—è¡¨
    ALL_QUERIES = [
        'ti:"agent"',
        'ti:"agent"',
        # å¯æ·»åŠ æ›´å¤šæŸ¥è¯¢
    ]
    
    downloader = OptimizedPaperDownloader(DOWNLOAD_DIR)
    
    try:
        if os.name == 'nt':
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
            
        asyncio.run(downloader.main_pipeline(ALL_QUERIES, max_results_per_query=50))
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ User stopped the process.")
    except Exception as e:
        logger.error(f"ğŸ’¥ Fatal Error: {e}", exc_info=True)