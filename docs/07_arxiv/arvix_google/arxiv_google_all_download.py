import asyncio
import aiohttp
import aiofiles
import arxiv
import os
import re
import logging
import time
import textwrap
from concurrent.futures import ThreadPoolExecutor
from functools import wraps

# ================= é…ç½®åŒºåŸŸ =================
# æ³¨æ„ï¼šarXiv å¯¹å¹¶å‘æå…¶æ•æ„Ÿã€‚å»ºè®® DOWNLOAD_CONCURRENCY ä¸è¦è¶…è¿‡ 10ï¼Œå¦åˆ™ææ˜“è¢«å° IPã€‚
DOWNLOAD_CONCURRENCY = 8  
SEARCH_CONCURRENCY = 1    # æœç´¢æ”¹ä¸ºä¸²è¡Œï¼ˆ1ï¼‰ï¼Œå½»åº•é¿å…å¹¶å‘è§¦å‘é™æµ
MAX_RETRIES = 5           # ä¸‹è½½å¤±è´¥é‡è¯•æ¬¡æ•°
BASE_DELAY = 1.0          # åŸºç¡€å»¶è¿Ÿï¼ˆç§’ï¼‰
SEARCH_DELAY = 3.0        # æœç´¢è¯·æ±‚ä¹‹é—´çš„å›ºå®šå»¶è¿Ÿï¼ˆç§’ï¼‰
DOWNLOAD_DIR = "Paper_Library_Async"
LOG_FILE = "download_mission.log"

# ================= æ—¥å¿—é…ç½® =================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ================= å·¥å…·å‡½æ•° =================
def sanitize_filename(filename):
    clean = re.sub(r'[\\/*?:"<>|]', "", filename)
    return clean.strip()[:150]

def generate_bibtex(result):
    """åŒæ­¥ç”Ÿæˆ BibTeX å­—ç¬¦ä¸²"""
    short_id = result.get_short_id()
    try:
        first_author = result.authors[0].name.split(' ')[-1].lower()
    except:
        first_author = "unknown"
    year = result.published.year
    first_word = result.title.split(' ')[0].lower()
    cite_key = f"{re.sub(r'[^a-z]', '', first_author)}{year}{re.sub(r'[^a-z]', '', first_word)}"
    authors = " and ".join([a.name for a in result.authors])
    
    return textwrap.dedent(f"""
    @misc{{{cite_key},
      title={{{result.title}}}, 
      author={{{authors}}},
      year={{{year}}},
      eprint={{{short_id}}},
      archivePrefix={{arXiv}},
      primaryClass={{{result.primary_category}}},
      url={{{result.entry_id}}}
    }}
    """).strip()

# è£…é¥°å™¨ï¼šä¸ºå‡½æ•°æ·»åŠ æŒ‡æ•°é€€é¿é‡è¯•
def retry_on_429(max_retries=MAX_RETRIES, base_delay=BASE_DELAY):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except arxiv.HTTPError as e:
                    if e.status_code == 429:
                        # é‡åˆ°429ï¼ŒæŒ‡æ•°é€€é¿ç­‰å¾…
                        wait_time = base_delay * (2 ** attempt) * 10  # æ›´é•¿çš„ç­‰å¾…æ—¶é—´
                        logger.warning(f"âš ï¸  [Search Rate Limit] 429 error. Retry {attempt+1}/{max_retries} after {wait_time}s...")
                        time.sleep(wait_time)
                    else:
                        raise
                except Exception as e:
                    logger.warning(f"âš ï¸  [Search Retry {attempt+1}/{max_retries}] Error: {e}")
                    time.sleep(base_delay * (attempt + 1))
            raise Exception(f"âŒ Search failed after {max_retries} retries")
        return wrapper
    return decorator

# ================= æ ¸å¿ƒå¼‚æ­¥ç±» =================
class AsyncPaperDownloader:
    def __init__(self, download_dir):
        self.download_dir = download_dir
        if not os.path.exists(download_dir):
            os.makedirs(download_dir)
        
        self.processed_ids = set()
        # ä¿¡å·é‡ç”¨äºé™åˆ¶æœ€å¤§å¹¶å‘ï¼Œé˜²æ­¢è¢«å° IP
        self.sem_download = asyncio.Semaphore(DOWNLOAD_CONCURRENCY)
        
    async def download_file(self, session, url, file_path, file_type="File"):
        """é€šç”¨çš„å¼‚æ­¥ä¸‹è½½å‡½æ•°ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        if os.path.exists(file_path):
            logger.info(f"â­ï¸  [Skip] {file_type} exists: {os.path.basename(file_path)}")
            return True

        async with self.sem_download:
            for attempt in range(MAX_RETRIES):
                try:
                    # æ¨¡æ‹Ÿæµè§ˆå™¨ User-Agent å‡å°‘è¢«æ‹’æ¦‚ç‡
                    headers = {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
                    }
                    async with session.get(url, headers=headers, timeout=60) as response:
                        if response.status == 200:
                            async with aiofiles.open(file_path, 'wb') as f:
                                while True:
                                    chunk = await response.content.read(1024 * 64) # 64KB chunks
                                    if not chunk:
                                        break
                                    await f.write(chunk)
                            logger.info(f"âœ… [Done] {file_type} downloaded: {os.path.basename(file_path)}")
                            return True
                        elif response.status in [403, 429]:
                            # é‡åˆ°é™æµï¼ŒæŒ‡æ•°é€€é¿
                            wait_time = BASE_DELAY * (2 ** attempt) * 5
                            logger.warning(f"âš ï¸  [Rate Limit] {response.status} on {file_type}. Sleeping {wait_time}s...")
                            await asyncio.sleep(wait_time)
                        else:
                            logger.error(f"âŒ [Fail] {file_type} HTTP {response.status}: {url}")
                            return False
                except Exception as e:
                    logger.warning(f"âš ï¸  [Retry {attempt+1}/{MAX_RETRIES}] Error downloading {file_type}: {e}")
                    await asyncio.sleep(BASE_DELAY * (attempt + 1))
            
            logger.error(f"âŒ [GiveUp] Failed to download {file_type} after retries: {url}")
            return False

    @retry_on_429(max_retries=MAX_RETRIES, base_delay=BASE_DELAY)
    def sync_search(self, query, max_results):
        """åŒæ­¥æœç´¢å‡½æ•° (å¸¦429é‡è¯•æœºåˆ¶)"""
        logger.info(f"ğŸ” Searching: {query[:50]}...")
        # é…ç½®arxivå®¢æˆ·ç«¯ï¼Œå¢åŠ è¶…æ—¶
        client = arxiv.Client(
            page_size=50,
            delay_seconds=SEARCH_DELAY,
            num_retries=3
        )
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.Relevance,
            sort_order=arxiv.SortOrder.Descending
        )
        # å°† generator è½¬æ¢ä¸º list è¿”å›ï¼Œæ–¹ä¾¿ä¸»çº¿ç¨‹å¤„ç†
        results = list(client.results(search))
        logger.info(f"ğŸ“„ Found {len(results)} papers for query: {query[:50]}...")
        
        # æœç´¢åæ·»åŠ å›ºå®šå»¶è¿Ÿï¼Œé¿å…è¿ç»­è¯·æ±‚
        time.sleep(SEARCH_DELAY)
        return results

    async def process_paper(self, session, paper):
        """å¤„ç†å•ç¯‡è®ºæ–‡ï¼šåˆ›å»ºæ–‡ä»¶å¤¹ï¼Œç”ŸæˆBibï¼Œåˆ›å»ºä¸‹è½½ä»»åŠ¡"""
        paper_id = paper.get_short_id()
        
        # å»é‡æ£€æŸ¥
        if paper_id in self.processed_ids:
            return
        self.processed_ids.add(paper_id)

        safe_title = sanitize_filename(paper.title)
        paper_dir = os.path.join(self.download_dir, safe_title)
        
        if not os.path.exists(paper_dir):
            os.makedirs(paper_dir)

        # 1. ä¿å­˜ BibTeX (æœ¬åœ°IOï¼Œå¾ˆå¿«ï¼Œç›´æ¥åŒæ­¥åš)
        bib_path = os.path.join(paper_dir, "citation.bib")
        with open(bib_path, "w", encoding="utf-8") as f:
            f.write(generate_bibtex(paper))

        # 2. å‡†å¤‡ä¸‹è½½ä»»åŠ¡
        pdf_url = paper.pdf_url
        source_url = f"https://arxiv.org/e-print/{paper_id}"
        
        pdf_path = os.path.join(paper_dir, f"{safe_title}.pdf")
        source_path = os.path.join(paper_dir, "source.tar.gz")

        # å¹¶å‘æ‰§è¡Œ PDF å’Œ Source ä¸‹è½½
        await asyncio.gather(
            self.download_file(session, pdf_url, pdf_path, "PDF"),
            self.download_file(session, source_url, source_path, "Source")
        )

    async def main_pipeline(self, queries, max_results_per_query=20):
        # åˆ›å»ºä¸€ä¸ª TCP è¿æ¥æ± 
        connector = aiohttp.TCPConnector(limit=DOWNLOAD_CONCURRENCY + 5)
        async with aiohttp.ClientSession(connector=connector) as session:
            
            loop = asyncio.get_running_loop()
            tasks = []
            
            logger.info("ğŸš€ Starting Async Download Mission...")
            
            # ä½¿ç”¨ ThreadPoolExecutor æ¥è¿è¡Œé˜»å¡çš„ arxiv.Search
            with ThreadPoolExecutor(max_workers=SEARCH_CONCURRENCY) as pool:
                # 1. ç¬¬ä¸€é˜¶æ®µï¼šä¸²è¡Œæœç´¢å¹¶æ”¶é›†æ‰€æœ‰è®ºæ–‡å…ƒæ•°æ®ï¼ˆé¿å…å¹¶å‘è§¦å‘é™æµï¼‰
                all_search_results = []
                logger.info(f"ğŸ“‹ Starting search for {len(queries)} queries (serial mode)...")
                
                for idx, query in enumerate(queries):
                    try:
                        logger.info(f"ğŸ” Query {idx+1}/{len(queries)}: {query[:50]}...")
                        # ä¸²è¡Œæ‰§è¡Œæ¯ä¸ªæœç´¢è¯·æ±‚
                        future = loop.run_in_executor(pool, self.sync_search, query, max_results_per_query)
                        result = await future
                        all_search_results.append(result)
                    except Exception as e:
                        logger.error(f"âŒ Failed to search query {idx+1}: {query[:50]}... Error: {e}")
                        continue
                
                # å±•å¹³ç»“æœ
                total_papers = [p for res in all_search_results for p in res]
                logger.info(f"ğŸ“Š Total papers found (after dedup): {len(total_papers)}")
                
                # 2. ç¬¬äºŒé˜¶æ®µï¼šå¹¶å‘ä¸‹è½½
                # ä¸ºæ¯ç¯‡è®ºæ–‡åˆ›å»ºä¸€ä¸ªå¤„ç†ä»»åŠ¡
                for paper in total_papers:
                    task = asyncio.create_task(self.process_paper(session, paper))
                    tasks.append(task)
                
                if tasks:
                    logger.info(f"ğŸ”¥ Spawning {len(tasks)} download tasks with concurrency limit {DOWNLOAD_CONCURRENCY}...")
                    # æ˜¾ç¤ºè¿›åº¦æ¡ (å¯é€‰ï¼Œä½¿ç”¨ tqdm éœ€è¦ async é€‚é…ï¼Œè¿™é‡Œç”¨ç®€å•çš„æ—¥å¿—)
                    await asyncio.gather(*tasks)
                else:
                    logger.warning("âš ï¸ No papers found to download.")
                    
        logger.info("âœ¨ Mission Complete! Check log for details.")

# ================= æ‰§è¡Œå…¥å£ =================
if __name__ == "__main__":
    # ä½ çš„æŸ¥è¯¢åˆ—è¡¨
    ALL_QUERIES = [
        'all:"multi-agent path planning deadlock"',
        # 'abs:"distributed multi-agent" AND abs:"deadlock breaking"',
        # 'all:"Large Language Model" AND all:"multi-agent path planning"',
        # ... æŠŠä½ é‚£å‡ ç™¾æ¡ query æ”¾åœ¨è¿™é‡Œ ...
    ]
    
    downloader = AsyncPaperDownloader(DOWNLOAD_DIR)
    
    try:
        # Windows ä¸‹ asyncio çš„äº‹ä»¶å¾ªç¯ç­–ç•¥å¯èƒ½éœ€è¦è°ƒæ•´
        if os.name == 'nt':
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
            
        asyncio.run(downloader.main_pipeline(ALL_QUERIES, max_results_per_query=10))
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ User stopped the process.")
    except Exception as e:
        logger.error(f"ğŸ’¥ Fatal Error: {e}", exc_info=True)