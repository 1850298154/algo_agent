"""
US-N1
"""
import asyncio
import aiohttp
import aiofiles
# import arxiv
from src.retrieval.arXiv import arxiv_pydantic as arxiv
import os
import re
import logging
import time
import textwrap
from concurrent.futures import ThreadPoolExecutor
from functools import wraps
from collections import deque

# ================= é…ç½®åŒºåŸŸ =================
# æ ¸å¿ƒé¢‘ç‡é™åˆ¶ï¼šæ¯ç§’æœ€å¤š3æ¬¡è¯·æ±‚ï¼ˆæœç´¢+ä¸‹è½½åˆè®¡ï¼‰
MAX_REQUESTS_PER_SECOND = 2
DOWNLOAD_CONCURRENCY = 8    # ä¸‹è½½å¹¶å‘æ•°ï¼ˆç”±é¢‘ç‡æ§åˆ¶å™¨å…œåº•ï¼Œå¯é€‚å½“è°ƒé«˜ï¼‰
MAX_RETRIES = 3             # é‡è¯•æ¬¡æ•°ï¼ˆé¢‘ç‡åˆè§„åå¯å‡å°‘ï¼‰
BASE_DELAY = 0.1            # åŸºç¡€å»¶è¿Ÿï¼ˆé¢‘ç‡æ§åˆ¶åæ— éœ€å¤§å»¶è¿Ÿï¼‰
DOWNLOAD_DIR = "Paper_Library_Async_Optimized"
LOG_FILE = "download_mission_optimized.log"

# ================= æ—¥å¿—é…ç½® =================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ================= é¢‘ç‡æ§åˆ¶å™¨ï¼ˆæ ¸å¿ƒï¼‰ =================
class RateLimiter:
    """ä»¤ç‰Œæ¡¶ç®—æ³•å®ç°ï¼šä¸¥æ ¼æ§åˆ¶æ¯ç§’è¯·æ±‚æ•°ä¸è¶…è¿‡æŒ‡å®šå€¼"""
    def __init__(self, max_requests_per_second):
        self.max_requests = max_requests_per_second
        self.tokens = max_requests_per_second  # åˆå§‹ä»¤ç‰Œæ•°
        self.last_refill_time = time.time()
        self.lock = asyncio.Lock()

    async def acquire(self):
        """è·å–ä»¤ç‰Œï¼ˆé˜»å¡ç›´åˆ°æœ‰å¯ç”¨ä»¤ç‰Œï¼‰"""
        async with self.lock:
            # 1. è®¡ç®—æ—¶é—´å·®ï¼Œè¡¥å……ä»¤ç‰Œï¼ˆæ¯ç§’è¡¥å……max_requestsä¸ªï¼‰
            now = time.time()
            time_passed = now - self.last_refill_time
            new_tokens = time_passed * self.max_requests
            if new_tokens > 0:
                self.tokens = min(self.max_requests, self.tokens + new_tokens)
                self.last_refill_time = now

            # 2. å¦‚æœä»¤ç‰Œä¸è¶³ï¼Œç­‰å¾…ç›´åˆ°æœ‰ä»¤ç‰Œ
            while self.tokens < 1:
                await asyncio.sleep(0.01)  # çŸ­è½®è¯¢
                now = time.time()
                time_passed = now - self.last_refill_time
                new_tokens = time_passed * self.max_requests
                if new_tokens > 0:
                    self.tokens += new_tokens
                    self.last_refill_time = now

            # 3. æ¶ˆè€—1ä¸ªä»¤ç‰Œ
            self.tokens -= 1
            return True

# ================= å·¥å…·å‡½æ•° =================
def sanitize_filename(filename):
    clean = re.sub(r'[\\/*?:"<>|]', "", filename)
    return clean.strip()[:150]

def generate_bibtex(result):
    """åŒæ­¥ç”Ÿæˆ BibTeX å­—ç¬¦ä¸²"""
    short_id = result.get_short_id()
    try:
        first_author = result.authors[0].name.split(' ')[-1].lower()
    except:
        first_author = "unknown"
    year = result.published.year
    first_word = result.title.split(' ')[0].lower()
    cite_key = f"{re.sub(r'[^a-z]', '', first_author)}{year}{re.sub(r'[^a-z]', '', first_word)}"
    authors = " and ".join([a.name for a in result.authors])
    
    return textwrap.dedent(f"""
    @misc{{{cite_key},
      title={{{result.title}}}, 
      author={{{authors}}},
      year={{{year}}},
      eprint={{{short_id}}},
      archivePrefix={{arXiv}},
      primaryClass={{{result.primary_category}}},
      url={{{result.entry_id}}}
    }}
    """).strip()

# ================= æ ¸å¿ƒå¼‚æ­¥ç±» =================
class OptimizedPaperDownloader:
    def __init__(self, download_dir):
        self.download_dir = download_dir
        if not os.path.exists(download_dir):
            os.makedirs(download_dir)
        
        self.processed_ids = set()
        self.sem_download = asyncio.Semaphore(DOWNLOAD_CONCURRENCY)
        self.rate_limiter = RateLimiter(MAX_REQUESTS_PER_SECOND)  # å…¨å±€é¢‘ç‡æ§åˆ¶
        
    async def download_file(self, session, url, file_path, file_type="File", paper_id="", paper_title=""):
        """å¸¦é¢‘ç‡æ§åˆ¶çš„å¼‚æ­¥ä¸‹è½½å‡½æ•°ï¼ˆæ–°å¢è®ºæ–‡æ ‡è¯†å‚æ•°ï¼‰"""
        # ç®€åŒ–æ ‡é¢˜ç”¨äºæ—¥å¿—æ˜¾ç¤ºï¼ˆå¤ªé•¿å½±å“å¯è¯»æ€§ï¼‰
        short_title = paper_title[:80] + "..." if len(paper_title) > 80 else paper_title
        file_basename = os.path.basename(file_path)
        
        if os.path.exists(file_path):
            logger.info(f"â­ï¸  [Skip] [{paper_id}] {file_type} exists: {short_title} | {file_basename}")
            return True

        async with self.sem_download:
            # ç¬¬ä¸€æ­¥ï¼šè·å–é¢‘ç‡ä»¤ç‰Œï¼ˆä¿è¯æ¯ç§’â‰¤3æ¬¡è¯·æ±‚ï¼‰
            await self.rate_limiter.acquire()
            
            # æ–°å¢ï¼šè®°å½•å¼€å§‹ä¸‹è½½çš„æ—¥å¿—ï¼Œæ˜ç¡®æ ‡æ³¨è®ºæ–‡ä¿¡æ¯
            logger.info(f"ğŸš€ [Start Download] [{paper_id}] {file_type}: {short_title} | {file_basename}")
            
            for attempt in range(MAX_RETRIES):
                try:
                    headers = {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
                    }
                    async with session.get(url, headers=headers, timeout=60) as response:
                        if response.status == 200:
                            async with aiofiles.open(file_path, 'wb') as f:
                                while True:
                                    chunk = await response.content.read(1024 * 64)
                                    if not chunk:
                                        break
                                    await f.write(chunk)
                            logger.info(f"âœ… [Done] [{paper_id}] {file_type} downloaded: {short_title} | {file_basename}")
                            return True
                        elif response.status in [403, 429]:
                            wait_time = BASE_DELAY * (2 ** attempt)
                            logger.warning(f"âš ï¸  [Rate Limit] [{paper_id}] {response.status} on {file_type} ({short_title}). Sleeping {wait_time}s...", stack_info=True)
                            await asyncio.sleep(wait_time)
                        else:
                            logger.error(f"âŒ [Fail] [{paper_id}] {file_type} HTTP {response.status}: {short_title} | {url}", stack_info=True)
                            return False
                except Exception as e:
                    logger.warning(f"âš ï¸  [Retry {attempt+1}/{MAX_RETRIES}] [{paper_id}] Error downloading {file_type} ({short_title}): {e}", stack_info=True)
                    await asyncio.sleep(BASE_DELAY * (attempt + 1))
            
            logger.error(f"âŒ [GiveUp] [{paper_id}] Failed to download {file_type} after retries: {short_title} | {url}", stack_info=True)
            return False

    async def async_search(self, query, max_results):
        """å¼‚æ­¥åŒ…è£…çš„æœç´¢å‡½æ•°ï¼ˆå¸¦é¢‘ç‡æ§åˆ¶ï¼‰"""
        # ç¬¬ä¸€æ­¥ï¼šè·å–é¢‘ç‡ä»¤ç‰Œï¼ˆä¿è¯æœç´¢è¯·æ±‚ä¹Ÿéµå®ˆæ¯ç§’â‰¤3æ¬¡ï¼‰
        await self.rate_limiter.acquire()
        
        logger.info(f"ğŸ” Searching: {query[:50]}... (max results: {max_results})")
        client = arxiv.Client(
            page_size=50,
            delay_seconds=0.34,  # é¢‘ç‡æ§åˆ¶å™¨å·²å…œåº•ï¼Œæ— éœ€å¤§å»¶è¿Ÿ
            num_retries=2
        )
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.Relevance,
            sort_order=arxiv.SortOrder.Descending
        )
        
        loop = asyncio.get_running_loop()
        try:
            # æœç´¢æ˜¯é˜»å¡æ“ä½œï¼Œæ‰”åˆ°çº¿ç¨‹æ± 
            results = await loop.run_in_executor(
                None,  # ä½¿ç”¨é»˜è®¤çº¿ç¨‹æ± 
                lambda: list(client.results(search))
            )
            # æ–°å¢ï¼šè®°å½•æœç´¢åˆ°çš„è®ºæ–‡åˆ—è¡¨ï¼ˆID+æ ‡é¢˜ï¼‰
            if results:
                paper_info = [f"{r.get_short_id()}: {r.title[:60]}..." for r in results]
                logger.info(f"ğŸ“„ Found {len(results)} papers for query: {query[:50]}... | First 5 papers: {paper_info[:5]}")
            else:
                logger.info(f"ğŸ“„ Found 0 papers for query: {query[:50]}...")
            return results
        except arxiv.HTTPError as e:
            logger.error(f"âŒ Search failed (HTTP {e.status_code}): {query[:50]}...", stack_info=True)
            return []
        except Exception as e:
            logger.error(f"âŒ Search error: {query[:50]}... Error: {e}", stack_info=True)
            return []

    async def process_paper(self, session, paper):
        """å¤„ç†å•ç¯‡è®ºæ–‡ï¼šåˆ›å»ºæ–‡ä»¶å¤¹ï¼Œç”ŸæˆBibï¼Œä¸‹è½½æ–‡ä»¶"""
        paper_id = paper.get_short_id()
        paper_title = paper.title
        short_title = paper_title[:80] + "..." if len(paper_title) > 80 else paper_title
        
        if paper_id in self.processed_ids:
            logger.info(f"ğŸ”„ [Duplicate] Skip processing paper [{paper_id}]: {short_title}")
            return
        self.processed_ids.add(paper_id)

        # æ–°å¢ï¼šè®°å½•å¼€å§‹å¤„ç†è¯¥è®ºæ–‡çš„æ—¥å¿—
        logger.info(f"ğŸ“‹ [Process] Starting to handle paper [{paper_id}]: {short_title}")
        
        safe_title = sanitize_filename(paper.title)
        paper_dir = os.path.join(self.download_dir, safe_title)
        if not os.path.exists(paper_dir):
            os.makedirs(paper_dir)
            logger.info(f"ğŸ“‚ [Create Dir] [{paper_id}] Created folder: {paper_dir}")

        # ä¿å­˜ BibTeXï¼ˆåŒæ­¥IOï¼Œä¸å è¯·æ±‚é¢‘ç‡ï¼‰
        bib_path = os.path.join(paper_dir, "citation.bib")
        with open(bib_path, "w", encoding="utf-8") as f:
            f.write(generate_bibtex(paper))
        logger.info(f"ğŸ“„ [BibTeX] [{paper_id}] Saved citation file: {bib_path}")

        # å‡†å¤‡ä¸‹è½½ä»»åŠ¡
        pdf_url = paper.pdf_url
        source_url = f"https://export.arxiv.org/e-print/{paper_id}"
        pdf_path = os.path.join(paper_dir, f"{safe_title}.pdf")
        source_path = os.path.join(paper_dir, "source.tar.gz")

        # å¹¶å‘ä¸‹è½½PDFå’Œæºç ï¼ˆä¼ é€’è®ºæ–‡IDå’Œæ ‡é¢˜åˆ°ä¸‹è½½å‡½æ•°ï¼‰
        await asyncio.gather(
            self.download_file(session, pdf_url, pdf_path, "PDF", paper_id, paper_title),
            self.download_file(session, source_url, source_path, "Source", paper_id, paper_title)
        )
        
        # æ–°å¢ï¼šè®°å½•è¯¥è®ºæ–‡å¤„ç†å®Œæˆ
        logger.info(f"âœ… [Complete] Finished processing paper [{paper_id}]: {short_title}")

    async def main_pipeline(self, queries, max_results_per_query=20):
        """ä¼˜åŒ–åçš„ä¸»æµç¨‹ï¼šé¢‘ç‡æ§åˆ¶+å¼‚æ­¥å¹¶å‘"""
        connector = aiohttp.TCPConnector(limit=DOWNLOAD_CONCURRENCY + 5)
        async with aiohttp.ClientSession(connector=connector) as session:
            logger.info("ğŸš€ Starting Optimized Async Download Mission...")
            logger.info(f"ğŸ“Š Rate Limit: {MAX_REQUESTS_PER_SECOND} requests/second | Download Concurrency: {DOWNLOAD_CONCURRENCY}")
            
            # ç¬¬ä¸€é˜¶æ®µï¼šå¼‚æ­¥å¹¶å‘æœç´¢ï¼ˆå¸¦é¢‘ç‡æ§åˆ¶ï¼‰
            search_tasks = []
            for idx, query in enumerate(queries):
                logger.info(f"ğŸ” [Task {idx+1}/{len(queries)}] Preparing search for query: {query[:50]}...")
                task = asyncio.create_task(self.async_search(query, max_results_per_query))
                search_tasks.append(task)
            
            logger.info(f"ğŸ” Starting {len(search_tasks)} search tasks (rate-limited)...")
            all_search_results = await asyncio.gather(*search_tasks)
            
            # å±•å¹³ç»“æœå¹¶å»é‡ï¼ˆæŒ‰paper_idï¼‰
            paper_dict = {}
            for res in all_search_results:
                for paper in res:
                    paper_dict[paper.get_short_id()] = paper
            total_papers = list(paper_dict.values())
            # æ–°å¢ï¼šè®°å½•å»é‡åçš„è®ºæ–‡æ€»æ•°å’Œå‰10ç¯‡ä¿¡æ¯
            paper_summary = [f"{p.get_short_id()}: {p.title[:60]}..." for p in total_papers[:10]]
            logger.info(f"ğŸ“Š Total unique papers found: {len(total_papers)} | First 10 papers: {paper_summary}")
            
            # ç¬¬äºŒé˜¶æ®µï¼šå¼‚æ­¥å¹¶å‘ä¸‹è½½ï¼ˆå¸¦é¢‘ç‡æ§åˆ¶ï¼‰
            download_tasks = []
            for idx, paper in enumerate(total_papers):
                short_title = paper.title[:60] + "..." if len(paper.title) > 60 else paper.title
                logger.info(f"ğŸ”¥ [Download Task {idx+1}/{len(total_papers)}] Queueing paper [{paper.get_short_id()}]: {short_title}")
                task = asyncio.create_task(self.process_paper(session, paper))
                download_tasks.append(task)
            
            if download_tasks:
                logger.info(f"ğŸ”¥ Spawning {len(download_tasks)} download tasks (rate-limited)...")
                await asyncio.gather(*download_tasks)
            else:
                logger.warning("âš ï¸ No papers found to download.", stack_info=True)
                    
        logger.info("âœ¨ Optimized Mission Complete! Check log for details.")

# ================= æ‰§è¡Œå…¥å£ =================
if __name__ == "__main__":
    # ä½ çš„æŸ¥è¯¢åˆ—è¡¨
    ALL_QUERIES = [
        'ti:"agent"',
        # å¯æ·»åŠ æ›´å¤šæŸ¥è¯¢
    ]
    
    downloader = OptimizedPaperDownloader(DOWNLOAD_DIR)
    
    try:
        if os.name == 'nt':
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
            
        asyncio.run(downloader.main_pipeline(ALL_QUERIES, max_results_per_query=50))
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ User stopped the process.")
    except Exception as e:
        logger.error(f"ğŸ’¥ Fatal Error: {e}", stack_info=True)