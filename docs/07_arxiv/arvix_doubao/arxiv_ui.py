import streamlit as st
import feedparser
import requests
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import os
import tarfile
import json
from tqdm import tqdm
import time
from urllib.parse import quote

# --------------------------
# å…¨å±€é…ç½®ä¸å·¥å…·å‡½æ•°
# --------------------------
# åˆ›å»ºå¿…è¦çš„æ–‡ä»¶å¤¹
os.makedirs("arxiv_downloads/latex", exist_ok=True)
os.makedirs("arxiv_downloads/bibtex", exist_ok=True)
os.makedirs("arxiv_downloads/summaries", exist_ok=True)

# arXiv APIåŸºç¡€é…ç½®
ARXIV_API_BASE = "http://export.arxiv.org/api/query"
# é¿å…APIè°ƒç”¨é¢‘ç‡è¿‡é«˜ï¼ˆarXivå»ºè®®æ¯ç§’ä¸è¶…è¿‡1æ¬¡ï¼‰
API_DELAY = 1.0

def clean_filename(filename):
    """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
    return "".join(c for c in filename if c not in r'<>:"/\|?*').strip()

def parse_arxiv_date(date_str):
    """è§£æarXivçš„æ—¥æœŸå­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡"""
    try:
        return datetime.strptime(date_str, "%Y-%m-%dT%H:%M:%SZ")
    except:
        return None

def get_arxiv_paper_details(search_query, max_results=10):
    """
    ä»arXiv APIè·å–æ‰¹é‡è®ºæ–‡çš„å®Œæ•´ä¿¡æ¯ï¼ˆæ¦¨å¹²APIèƒ½åŠ›ï¼‰
    :param search_query: æœç´¢æ¡ä»¶ï¼ˆæ ‡é¢˜/å…³é”®è¯/åˆ†ç±»ï¼‰
    :param max_results: è¿”å›ç»“æœæ•°é‡
    :return: åŒ…å«æ‰€æœ‰è®ºæ–‡è¯¦ç»†ä¿¡æ¯çš„åˆ—è¡¨
    """
    # æ„é€ APIè¯·æ±‚å‚æ•°ï¼ˆæœ€å¤§åŒ–æå–å­—æ®µï¼‰
    encoded_query = quote(search_query)
    url = f"{ARXIV_API_BASE}?search_query={encoded_query}&max_results={max_results}&sort=submittedDate&order=desc"
    
    try:
        # é¿å…é«˜é¢‘è°ƒç”¨
        time.sleep(API_DELAY)
        feed = feedparser.parse(url)
        
        if feed.bozo != 0:
            st.error(f"APIè¯·æ±‚é”™è¯¯: {feed.bozo_exception}")
            return []
        
        papers = []
        for entry in tqdm(feed.entries, desc="è§£æè®ºæ–‡ä¿¡æ¯"):
            # æ ¸å¿ƒåŸºç¡€ä¿¡æ¯
            arxiv_id = entry.id.split('/')[-1] if 'id' in entry else ""
            title = entry.title.strip().replace('\n', ' ') if 'title' in entry else ""
            authors = [author.name for author in entry.authors] if 'authors' in entry else []
            abstract = entry.summary.strip() if 'summary' in entry else ""
            
            # åˆ†ç±»ä¿¡æ¯ï¼ˆarXivæ ¸å¿ƒåˆ†ç±»ä½“ç³»ï¼‰
            primary_category = entry.arxiv_primary_category['term'] if 'arxiv_primary_category' in entry else ""
            categories = entry.tags[0]['term'].split(',') if 'tags' in entry else []
            # è§£æåˆ†ç±»å¯¹åº”çš„å­¦ç§‘åç§°ï¼ˆç®€åŒ–æ˜ å°„ï¼‰
            category_mapping = {
                'cs': 'Computer Science', 'physics': 'Physics', 'math': 'Mathematics',
                'q-bio': 'Quantitative Biology', 'q-fin': 'Quantitative Finance',
                'stat': 'Statistics', 'econ': 'Economics', 'astro-ph': 'Astrophysics'
            }
            primary_discipline = next((v for k, v in category_mapping.items() if primary_category.startswith(k)), "Other")
            
            # æ—¥æœŸä¿¡æ¯ï¼ˆæäº¤/æ›´æ–°ï¼‰
            submitted_date = parse_arxiv_date(entry.published) if 'published' in entry else None
            updated_date = parse_arxiv_date(entry.updated) if 'updated' in entry else None
            
            # é“¾æ¥ä¿¡æ¯ï¼ˆæœ€å¤§åŒ–æå–ï¼‰
            pdf_url = next((link.href for link in entry.links if link.rel == 'alternate' and link.type == 'application/pdf'), "")
            latex_url = f"https://arxiv.org/e-print/{arxiv_id}" if arxiv_id else ""
            bibtex_url = f"{ARXIV_API_BASE}?search_query=id:{arxiv_id}&max_results=1&format=bibtex"
            
            # é™„åŠ ä¿¡æ¯
            comment = entry.comment.strip() if 'comment' in entry else ""
            journal_ref = entry.journal_ref.strip() if 'journal_ref' in entry else ""
            doi = entry.doi if 'doi' in entry else ""
            license = entry.rights if 'rights' in entry else ""
            
            # è·å–BibTeXå†…å®¹
            bibtex_content = ""
            if bibtex_url:
                try:
                    time.sleep(API_DELAY)
                    bibtex_resp = requests.get(bibtex_url, timeout=10)
                    if bibtex_resp.status_code == 200:
                        bibtex_content = bibtex_resp.text
                except Exception as e:
                    st.warning(f"è·å–{arxiv_id}çš„BibTeXå¤±è´¥: {str(e)}")
            
            # æ„å»ºè®ºæ–‡ä¿¡æ¯å­—å…¸
            paper = {
                'arxiv_id': arxiv_id,
                'title': title,
                'authors': ', '.join(authors),
                'author_count': len(authors),
                'primary_category': primary_category,
                'primary_discipline': primary_discipline,
                'all_categories': ', '.join(categories),
                'submitted_date': submitted_date,
                'updated_date': updated_date,
                'abstract': abstract,
                'pdf_url': pdf_url,
                'latex_url': latex_url,
                'bibtex_content': bibtex_content,
                'comment': comment,
                'journal_ref': journal_ref,
                'doi': doi,
                'license': license,
                'latex_download_status': 'pending',
                'bibtex_save_status': 'pending'
            }
            papers.append(paper)
        
        return papers
    
    except Exception as e:
        st.error(f"è·å–è®ºæ–‡ä¿¡æ¯å¤±è´¥: {str(e)}")
        return []

def download_latex_batch(papers):
    """æ‰¹é‡ä¸‹è½½LaTeXæºç ï¼ˆtar.gzæ ¼å¼ï¼‰"""
    for paper in tqdm(papers, desc="ä¸‹è½½LaTeXæºç "):
        arxiv_id = paper['arxiv_id']
        title = clean_filename(paper['title'][:50])  # æˆªæ–­é•¿æ ‡é¢˜
        latex_url = paper['latex_url']
        
        if not latex_url or not arxiv_id:
            paper['latex_download_status'] = 'failed (no URL)'
            continue
        
        try:
            time.sleep(API_DELAY)
            response = requests.get(latex_url, timeout=30, stream=True)
            if response.status_code == 200:
                # ä¿å­˜tar.gzæ–‡ä»¶
                file_path = f"arxiv_downloads/latex/{arxiv_id}_{title}.tar.gz"
                with open(file_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                # å¯é€‰ï¼šè§£å‹æ–‡ä»¶ï¼ˆå¦‚éœ€ç›´æ¥è·å–.texæ–‡ä»¶ï¼‰
                # with tarfile.open(file_path, 'r:gz') as tar:
                #     tar.extractall(f"arxiv_downloads/latex/{arxiv_id}_{title}")
                paper['latex_download_status'] = 'success'
            else:
                paper['latex_download_status'] = f"failed (status: {response.status_code})"
        except Exception as e:
            paper['latex_download_status'] = f"failed: {str(e)[:50]}"
    
    return papers

def save_bibtex_batch(papers):
    """æ‰¹é‡ä¿å­˜BibTeXæ–‡ä»¶"""
    for paper in tqdm(papers, desc="ä¿å­˜BibTeX"):
        arxiv_id = paper['arxiv_id']
        title = clean_filename(paper['title'][:50])
        bibtex_content = paper['bibtex_content']
        
        if not bibtex_content or not arxiv_id:
            paper['bibtex_save_status'] = 'failed (no content)'
            continue
        
        try:
            # ä¿å­˜å•ä¸ªBibTeXæ–‡ä»¶
            file_path = f"arxiv_downloads/bibtex/{arxiv_id}_{title}.bib"
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(bibtex_content)
            paper['bibtex_save_status'] = 'success'
        except Exception as e:
            paper['bibtex_save_status'] = f"failed: {str(e)[:50]}"
    
    # ç”Ÿæˆæ±‡æ€»BibTeXæ–‡ä»¶
    all_bibtex = "\n\n".join([p['bibtex_content'] for p in papers if p['bibtex_content']])
    summary_bib_path = f"arxiv_downloads/bibtex/all_papers_{datetime.now().strftime('%Y%m%d_%H%M%S')}.bib"
    with open(summary_bib_path, 'w', encoding='utf-8') as f:
        f.write(all_bibtex)
    
    return papers

def generate_summary_stats(papers_df):
    """ç”Ÿæˆç»Ÿè®¡åˆ†æç»“æœ"""
    stats = {}
    
    # 1. åŸºç¡€ç»Ÿè®¡
    stats['total_papers'] = len(papers_df)
    stats['success_latex'] = len(papers_df[papers_df['latex_download_status'] == 'success'])
    stats['success_bibtex'] = len(papers_df[papers_df['bibtex_save_status'] == 'success'])
    stats['avg_authors'] = papers_df['author_count'].mean()
    
    # 2. åˆ†ç±»åˆ†å¸ƒ
    category_counts = papers_df['primary_discipline'].value_counts()
    stats['category_distribution'] = category_counts
    
    # 3. æ—¶é—´åˆ†å¸ƒï¼ˆæŒ‰æœˆï¼‰
    papers_df['submitted_month'] = papers_df['submitted_date'].dt.to_period('M')
    monthly_counts = papers_df['submitted_month'].value_counts().sort_index()
    stats['monthly_submissions'] = monthly_counts
    
    return stats

# --------------------------
# Streamlitç•Œé¢æ„å»º
# --------------------------
def main():
    st.set_page_config(
        page_title="arXivæ‰¹é‡ä¸‹è½½ä¸åˆ†æå·¥å…·",
        page_icon="ğŸ“š",
        layout="wide"
    )
    
    st.title("ğŸ“š arXivæ‰¹é‡è®ºæ–‡å¤„ç†å·¥å…·")
    st.subheader("æœ€å¤§åŒ–åˆ©ç”¨arXiv API - æ‰¹é‡ä¸‹è½½LaTeX/BibTeX + å…¨ç»´åº¦ä¿¡æ¯åˆ†æ")
    
    # ä¾§è¾¹æ ï¼šå‚æ•°é…ç½®
    with st.sidebar:
        st.header("ğŸ”§ æœç´¢é…ç½®")
        search_type = st.radio("æœç´¢ç±»å‹", ["æŒ‰æ ‡é¢˜æ‰¹é‡æœç´¢", "æŒ‰å…³é”®è¯/åˆ†ç±»æœç´¢"])
        
        if search_type == "æŒ‰æ ‡é¢˜æ‰¹é‡æœç´¢":
            paper_titles = st.text_area(
                "è¾“å…¥è®ºæ–‡æ ‡é¢˜ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
                placeholder="Attention Is All You Need\nLanguage Models are Few-Shot Learners"
            )
            search_query = " OR ".join([f"title:{quote(title.strip())}" for title in paper_titles.split('\n') if title.strip()])
        else:
            search_keywords = st.text_input(
                "è¾“å…¥å…³é”®è¯/åˆ†ç±»ï¼ˆå¦‚cs.AI, transformerï¼‰",
                value="cs.AI"
            )
            search_query = search_keywords
        
        max_results = st.slider("æœ€å¤§ç»“æœæ•°é‡", 1, 50, 10)
        
        st.divider()
        st.header("ğŸ“¥ ä¸‹è½½é…ç½®")
        auto_download_latex = st.checkbox("è‡ªåŠ¨ä¸‹è½½LaTeXæºç ", value=True)
        auto_save_bibtex = st.checkbox("è‡ªåŠ¨ä¿å­˜BibTeXæ–‡ä»¶", value=True)
        
        st.divider()
        st.info(
            "ğŸ“ æ³¨æ„äº‹é¡¹ï¼š\n"
            "1. arXiv APIé™åˆ¶æ¯ç§’1æ¬¡è°ƒç”¨ï¼Œè¯·è€å¿ƒç­‰å¾…\n"
            "2. LaTeXæ–‡ä»¶ä¿å­˜è‡³arxiv_downloads/latex\n"
            "3. BibTeXæ–‡ä»¶ä¿å­˜è‡³arxiv_downloads/bibtex\n"
            "4. ä»…å¼€æºè®ºæ–‡æä¾›LaTeXæºç "
        )
    
    # ä¸»ç•Œé¢ï¼šæ“ä½œåŒº
    col1, col2, col3 = st.columns(3)
    with col1:
        get_papers_btn = st.button("ğŸš€ è·å–è®ºæ–‡ä¿¡æ¯", type="primary")
    with col2:
        export_excel_btn = st.button("ğŸ“¤ å¯¼å‡ºæ±‡æ€»è¡¨æ ¼")
    with col3:
        export_stats_btn = st.button("ğŸ“Š å¯¼å‡ºç»Ÿè®¡æŠ¥å‘Š")
    
    # æ ¸å¿ƒé€»è¾‘æ‰§è¡Œ
    if get_papers_btn and search_query:
        with st.spinner("æ­£åœ¨è°ƒç”¨arXiv APIè·å–è®ºæ–‡ä¿¡æ¯..."):
            # 1. è·å–è®ºæ–‡è¯¦ç»†ä¿¡æ¯
            papers = get_arxiv_paper_details(search_query, max_results)
            
            if not papers:
                st.warning("æœªæ‰¾åˆ°åŒ¹é…çš„è®ºæ–‡ï¼Œè¯·æ£€æŸ¥æœç´¢æ¡ä»¶")
                return
            
            # 2. æ‰¹é‡ä¸‹è½½/ä¿å­˜
            if auto_download_latex:
                with st.spinner("æ‰¹é‡ä¸‹è½½LaTeXæºç ..."):
                    papers = download_latex_batch(papers)
            
            if auto_save_bibtex:
                with st.spinner("æ‰¹é‡ä¿å­˜BibTeXæ–‡ä»¶..."):
                    papers = save_bibtex_batch(papers)
            
            # 3. è½¬æ¢ä¸ºDataFrameä¾¿äºå±•ç¤ºå’Œåˆ†æ
            papers_df = pd.DataFrame(papers)
            # å¤„ç†æ—¥æœŸæ ¼å¼ï¼ˆä¾¿äºå±•ç¤ºï¼‰
            papers_df['submitted_date'] = papers_df['submitted_date'].dt.strftime('%Y-%m-%d %H:%M')
            papers_df['updated_date'] = papers_df['updated_date'].dt.strftime('%Y-%m-%d %H:%M')
            
            # 4. ç¼“å­˜æ•°æ®
            st.session_state['papers_df'] = papers_df
            
            # 5. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            stats = generate_summary_stats(papers_df)
            st.session_state['stats'] = stats
            
            st.success(f"âœ… æˆåŠŸè·å– {len(papers_df)} ç¯‡è®ºæ–‡ä¿¡æ¯ï¼")
    
    # å±•ç¤ºåŒº
    if 'papers_df' in st.session_state:
        papers_df = st.session_state['papers_df']
        stats = st.session_state.get('stats', {})
        
        # ç¬¬ä¸€éƒ¨åˆ†ï¼šæ ¸å¿ƒç»Ÿè®¡å¡ç‰‡
        st.subheader("ğŸ“ˆ æ ¸å¿ƒç»Ÿè®¡")
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("æ€»è®ºæ–‡æ•°", stats.get('total_papers', 0))
        with col2:
            st.metric("LaTeXä¸‹è½½æˆåŠŸ", stats.get('success_latex', 0))
        with col3:
            st.metric("BibTeXä¿å­˜æˆåŠŸ", stats.get('success_bibtex', 0))
        with col4:
            st.metric("å¹³å‡ä½œè€…æ•°", f"{stats.get('avg_authors', 0):.1f}")
        
        # ç¬¬äºŒéƒ¨åˆ†ï¼šè®ºæ–‡ä¿¡æ¯è¡¨æ ¼
        st.subheader("ğŸ“‹ è®ºæ–‡è¯¦ç»†ä¿¡æ¯")
        # é€‰æ‹©å±•ç¤ºçš„åˆ—ï¼ˆé¿å…è¡¨æ ¼è¿‡å®½ï¼‰
        display_columns = [
            'arxiv_id', 'title', 'primary_discipline', 'authors', 
            'submitted_date', 'latex_download_status', 'bibtex_save_status'
        ]
        st.dataframe(
            papers_df[display_columns],
            use_container_width=True,
            hide_index=True,
            column_config={
                "arxiv_id": st.column_config.TextColumn("arXiv ID", width="small"),
                "title": st.column_config.TextColumn("æ ‡é¢˜", width="large"),
                "primary_discipline": st.column_config.TextColumn("ä¸»å­¦ç§‘", width="medium"),
                "authors": st.column_config.TextColumn("ä½œè€…", width="large"),
                "submitted_date": st.column_config.TextColumn("æäº¤æ—¥æœŸ", width="medium"),
                "latex_download_status": st.column_config.TextColumn("LaTeXçŠ¶æ€", width="small"),
                "bibtex_save_status": st.column_config.TextColumn("BibTeXçŠ¶æ€", width="small")
            }
        )
        
        # ç¬¬ä¸‰éƒ¨åˆ†ï¼šç»Ÿè®¡å¯è§†åŒ–
        st.subheader("ğŸ“Š ç»Ÿè®¡åˆ†æ")
        tab1, tab2, tab3 = st.tabs(["å­¦ç§‘åˆ†å¸ƒ", "æœˆåº¦æäº¤", "ä½œè€…æ•°é‡åˆ†å¸ƒ"])
        
        with tab1:
            if 'category_distribution' in stats:
                fig = px.pie(
                    values=stats['category_distribution'].values,
                    names=stats['category_distribution'].index,
                    title="è®ºæ–‡ä¸»å­¦ç§‘åˆ†å¸ƒ",
                    hole=0.3
                )
                st.plotly_chart(fig, use_container_width=True)
        
        with tab2:
            if 'monthly_submissions' in stats:
                fig = px.bar(
                    x=stats['monthly_submissions'].index.astype(str),
                    y=stats['monthly_submissions'].values,
                    title="æœˆåº¦æäº¤æ•°é‡",
                    labels={"x": "æœˆä»½", "y": "è®ºæ–‡æ•°é‡"}
                )
                st.plotly_chart(fig, use_container_width=True)
        
        with tab3:
            fig = px.histogram(
                papers_df,
                x="author_count",
                title="ä½œè€…æ•°é‡åˆ†å¸ƒ",
                labels={"author_count": "ä½œè€…æ•°é‡", "count": "è®ºæ–‡æ•°"}
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # ç¬¬å››éƒ¨åˆ†ï¼šè¯¦ç»†ä¿¡æ¯å±•å¼€
        st.subheader("ğŸ” å•ç¯‡è®ºæ–‡è¯¦æƒ…")
        selected_arxiv_id = st.selectbox(
            "é€‰æ‹©è®ºæ–‡IDæŸ¥çœ‹è¯¦æƒ…",
            papers_df['arxiv_id'].tolist()
        )
        selected_paper = papers_df[papers_df['arxiv_id'] == selected_arxiv_id].iloc[0]
        
        with st.expander(f"ğŸ“„ {selected_paper['title']}", expanded=True):
            col1, col2 = st.columns(2)
            with col1:
                st.write(f"**arXiv ID**: {selected_paper['arxiv_id']}")
                st.write(f"**ä¸»åˆ†ç±»**: {selected_paper['primary_category']}")
                st.write(f"**ä¸»å­¦ç§‘**: {selected_paper['primary_discipline']}")
                st.write(f"**æ‰€æœ‰åˆ†ç±»**: {selected_paper['all_categories']}")
                st.write(f"**æäº¤æ—¥æœŸ**: {selected_paper['submitted_date']}")
                st.write(f"**æ›´æ–°æ—¥æœŸ**: {selected_paper['updated_date']}")
                st.write(f"**ä½œè€…æ•°**: {selected_paper['author_count']}")
                st.write(f"**DOI**: {selected_paper['doi']}")
                st.write(f"**æœŸåˆŠå¼•ç”¨**: {selected_paper['journal_ref']}")
                
                # é“¾æ¥æŒ‰é’®
                st.markdown(f"[ğŸ“„ PDFé“¾æ¥]({selected_paper['pdf_url']})")
                st.markdown(f"[ğŸ“¥ LaTeXä¸‹è½½]({selected_paper['latex_url']})")
            
            with col2:
                st.write("**æ‘˜è¦**:")
                st.write(selected_paper['abstract'][:500] + "..." if len(selected_paper['abstract']) > 500 else selected_paper['abstract'])
                
                st.write("**BibTeX**:")
                st.code(selected_paper['bibtex_content'], language='bibtex')
        
        # å¯¼å‡ºåŠŸèƒ½
        if export_excel_btn:
            # å¯¼å‡ºå®Œæ•´æ•°æ®åˆ°Excel
            excel_path = f"arxiv_downloads/summaries/papers_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
            papers_df.to_excel(excel_path, index=False)
            st.success(f"âœ… æ±‡æ€»è¡¨æ ¼å·²å¯¼å‡ºè‡³: {excel_path}")
        
        if export_stats_btn:
            # å¯¼å‡ºç»Ÿè®¡æŠ¥å‘Š
            stats_report = {
                "ç”Ÿæˆæ—¶é—´": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "æœç´¢æ¡ä»¶": search_query,
                "æ ¸å¿ƒç»Ÿè®¡": stats,
                "ä¸‹è½½è·¯å¾„": {
                    "latex": "./arxiv_downloads/latex",
                    "bibtex": "./arxiv_downloads/bibtex"
                }
            }
            stats_path = f"arxiv_downloads/summaries/stats_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(stats_path, 'w', encoding='utf-8') as f:
                json.dump(stats_report, f, ensure_ascii=False, indent=4)
            st.success(f"âœ… ç»Ÿè®¡æŠ¥å‘Šå·²å¯¼å‡ºè‡³: {stats_path}")

if __name__ == "__main__":
    main()