{
    "jsonrpc": "2.0",
    "id": 1,
    "result": {
        "content": [
            {
                "type": "text",
                "text": "# Page: Overview\n\n# Overview\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.vscode/settings.json](.vscode/settings.json)\n- [docs/sql.md](docs/sql.md)\n- [pyproject.toml](pyproject.toml)\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/agent/llm.py](src/agent/llm.py)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n- [uv.lock](uv.lock)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document provides a high-level introduction to the **algo_agent** system, an AI agent framework that combines Large Language Model (LLM) reasoning with safe Python code execution capabilities. The system enables users to submit natural language queries that are automatically translated into executable Python code, run in isolated environments, and iteratively refined based on execution results.\n\n**For detailed information on specific subsystems:**\n- Agent query processing and decision loops: see [Query Processing Loop](#3.1)\n- LLM integration details: see [LLM Integration](#3.2)\n- Code execution strategies: see [Execution Runtime](#5)\n- Tool system architecture: see [Tool System](#4)\n- Logging and debugging: see [Observability and Logging](#6)\n\n**Sources:** [src/agent/deep_research.py:1-129](), [pyproject.toml:1-21]()\n\n---\n\n## System Architecture Overview\n\nThe algo_agent system is organized into five primary layers that work together to process user queries and execute code:\n\n```mermaid\ngraph TB\n    User[\"User Query<br/>(Natural Language)\"]\n    \n    Agent[\"deep_research.py<br/>user_query function<br/>Agent Orchestration\"]\n    \n    LLM[\"llm.py<br/>qwen-plus model<br/>DashScope API\"]\n    \n    Memory[\"memory.py<br/>init_messages_with_system_prompt<br/>Message History\"]\n    \n    Action[\"action.py<br/>call_tools_safely<br/>Tool Dispatcher\"]\n    \n    ToolInterface[\"tool/base.py<br/>BaseTool<br/>Abstract Interface\"]\n    \n    PythonTool[\"tool/python_tool.py<br/>ExecutePythonCodeTool\"]\n    \n    TodoTool[\"tool/todo_tool.py<br/>RecursivePlanTreeTodoTool\"]\n    \n    SubprocessExec[\"executor/run_in_subprocess.py<br/>run_structured_in_subprocess\"]\n    \n    SubthreadExec[\"executor/run_in_subthread.py<br/>run_structured_in_subthread\"]\n    \n    DirectExec[\"executor/run_in_direct.py<br/>run_structured_direct_exec\"]\n    \n    Workspace[\"executor/workspace.py<br/>arg_globals_list<br/>out_globals_list\"]\n    \n    Logger[\"utils/log_decorator.py<br/>global_logger<br/>traceable\"]\n    \n    User --> Agent\n    Agent --> Memory\n    Agent --> LLM\n    LLM --> Action\n    Action --> ToolInterface\n    ToolInterface --> PythonTool\n    ToolInterface --> TodoTool\n    PythonTool --> SubprocessExec\n    PythonTool --> SubthreadExec\n    PythonTool --> DirectExec\n    PythonTool --> Workspace\n    SubprocessExec --> Workspace\n    SubthreadExec --> Workspace\n    DirectExec --> Workspace\n    \n    Agent -.logs.-> Logger\n    Action -.logs.-> Logger\n    PythonTool -.logs.-> Logger\n```\n\n**Core Design Principles:**\n\n| Principle | Implementation |\n|-----------|---------------|\n| **Isolation** | Python code runs in subprocess/subthread environments with timeout enforcement |\n| **Safety** | Workspace globals are filtered and serialized; crash detection prevents system failures |\n| **Observability** | Comprehensive logging via `@traceable` and `@log_function` decorators |\n| **Extensibility** | Tool system uses `BaseTool` interface for easy addition of new capabilities |\n| **Iterative Refinement** | Agent loops until LLM produces a final answer without tool calls |\n\n**Sources:** [src/agent/deep_research.py:1-74](), [src/agent/llm.py:1-51](), [src/utils/log_decorator.py:19-64]()\n\n---\n\n## Key Components\n\n### Agent Orchestration Layer\n\nThe **agent orchestration layer** manages the end-to-end query processing workflow through the `user_query` function in [src/agent/deep_research.py:15-73]().\n\n```mermaid\ngraph LR\n    UserQuery[\"user_query(user_input)\"]\n    InitMem[\"memory.init_messages_with_system_prompt\"]\n    GetSchema[\"tool.schema.get_tools_schema\"]\n    GenOutput[\"llm.generate_assistant_output_append\"]\n    CallTools[\"action.call_tools_safely\"]\n    \n    UserQuery --> InitMem\n    UserQuery --> GetSchema\n    InitMem --> GenOutput\n    GetSchema --> GenOutput\n    GenOutput --> CallTools\n    CallTools --> GenOutput\n```\n\n**Key functions:**\n- `user_query(user_input)` - Entry point for all queries [src/agent/deep_research.py:15]()\n- `init_messages_with_system_prompt(user_input)` - Initializes conversation with system prompt\n- `generate_assistant_output_append(messages, tools_schema_list)` - Queries LLM and appends response [src/agent/llm.py:34-41]()\n- `call_tools_safely(tool_info)` - Dispatches tool execution with error handling\n\n**Sources:** [src/agent/deep_research.py:15-73](), [src/agent/llm.py:27-50]()\n\n---\n\n### LLM Integration\n\nThe system integrates with **Alibaba's qwen-plus model** via the DashScope API using OpenAI-compatible client.\n\n| Configuration | Value |\n|--------------|-------|\n| **Model** | `qwen-plus` |\n| **API Base URL** | `https://dashscope.aliyuncs.com/compatible-mode/v1` |\n| **Client Library** | `openai>=2.7.2` |\n| **Tool Calling** | Parallel tool calls enabled |\n\n**Core functions:**\n- `_generate_chat_completion(messages, tools_schema_list)` - Creates OpenAI client and calls API [src/agent/llm.py:14-24]()\n- `has_tool_call(assistant_output)` - Checks if response contains tool calls [src/agent/llm.py:44-45]()\n- `has_function_call(assistant_output)` - Checks for legacy function call format [src/agent/llm.py:48-49]()\n\n**Message format:** Follows OpenAI chat completion format with roles: `system`, `user`, `assistant`, `tool`, `function`.\n\n**Sources:** [src/agent/llm.py:1-51](), [pyproject.toml:14]()\n\n---\n\n### Tool Execution System\n\nThe **tool system** provides extensible capabilities through two concrete implementations:\n\n```mermaid\ngraph TB\n    BaseTool[\"tool/base.py<br/>BaseTool<br/>Abstract Base Class\"]\n    \n    PythonTool[\"tool/python_tool.py<br/>ExecutePythonCodeTool<br/>Code Execution\"]\n    \n    TodoTool[\"tool/todo_tool.py<br/>RecursivePlanTreeTodoTool<br/>Task Management\"]\n    \n    RunMethod[\"run(tool_call_arguments)<br/>Execute tool logic\"]\n    \n    Schema[\"get_tool_schema()<br/>JSON schema for LLM\"]\n    \n    BaseTool --> PythonTool\n    BaseTool --> TodoTool\n    BaseTool --> RunMethod\n    BaseTool --> Schema\n```\n\n**BaseTool interface requirements:**\n- `tool_name` - String identifier\n- `tool_description` - Natural language description for LLM\n- `get_parameter_schema()` - JSON schema of input parameters\n- `get_tool_schema()` - Complete tool schema in OpenAI format\n- `run(tool_call_arguments)` - Execution logic returning string result\n\n**Available tools (configured in [src/agent/deep_research.py:20-23]()):**\n- `ExecutePythonCodeTool` - Executes Python code snippets with state persistence\n- `RecursivePlanTreeTodoTool` - Manages hierarchical task structures (currently commented out)\n\n**Sources:** [src/agent/deep_research.py:20-23]()\n\n---\n\n### Python Execution Runtime\n\nThe **execution runtime** provides three strategies for running Python code with varying isolation levels:\n\n| Strategy | Module | Isolation Level | Timeout Support | Use Case |\n|----------|--------|-----------------|-----------------|----------|\n| **Subprocess** | `executor/run_in_subprocess.py` | Full process isolation | â | Production code execution |\n| **Subthread** | `executor/run_in_subthread.py` | Thread-based isolation | â | Faster execution, shared memory |\n| **Direct** | `executor/run_in_direct.py` | In-process | â | Testing, minimal overhead |\n\n**Common execution flow:**\n\n```mermaid\ngraph LR\n    Input[\"Python Code<br/>+ arg_globals_list<br/>+ timeout\"]\n    \n    Filter[\"workspace.filter_and_deepcopy_globals<br/>Serialize safe objects\"]\n    \n    Execute[\"Executor Strategy<br/>subprocess/subthread/direct\"]\n    \n    Result[\"ExecutionResult<br/>status + stdout + stderr<br/>+ exception + out_globals\"]\n    \n    Append[\"workspace.append_out_globals<br/>Persist variables\"]\n    \n    Input --> Filter\n    Filter --> Execute\n    Execute --> Result\n    Result --> Append\n```\n\n**ExecutionResult schema:**\n- `status` - Enum: `SUCCESS`, `FAILURE`, `TIMEOUT`, `CRASHED`\n- `stdout` - Captured standard output\n- `stderr` - Captured standard error\n- `exception_type`, `exception_value`, `traceback` - Error details\n- `out_globals` - Dictionary of output variables\n\n**Sources:** [src/agent/deep_research.py:1-129]()\n\n---\n\n### Observability Layer\n\nThe **logging system** provides comprehensive tracing through decorators and multiple log files:\n\n```mermaid\ngraph TB\n    Decorator[\"@traceable<br/>@log_function<br/>utils/log_decorator.py\"]\n    \n    Setup[\"setup_logger<br/>Configure handlers\"]\n    \n    GlobalLogger[\"global_logger<br/>logs/print.log\"]\n    \n    TraceLogger[\"traceable<br/>logs/trace.log\"]\n    \n    AllLogger[\"all_logger<br/>logs/all.log\"]\n    \n    Decorator --> Setup\n    Setup --> GlobalLogger\n    Setup --> TraceLogger\n    Setup --> AllLogger\n```\n\n**Logger instances:**\n- `global_logger` - User-facing operations and query processing [src/utils/log_decorator.py:305-306]()\n- `traceable` - Function call tracing with timing [src/utils/log_decorator.py:297-302]()\n- `all_logger` - Comprehensive system logs [src/utils/log_decorator.py:293-294]()\n\n**Logging features:**\n- Automatic parameter/return value capture\n- Execution timing (milliseconds)\n- Stack trace recording with project-only filtering [src/utils/log_decorator.py:208-217]()\n- Exception handling with traceback [src/utils/log_decorator.py:233-247]()\n- Rotating file handlers (10MB max, 5 backups) [src/utils/log_decorator.py:53-61]()\n\n**Sources:** [src/utils/log_decorator.py:19-307]()\n\n---\n\n## Execution Flow: Query to Response\n\nThe following sequence diagram shows how a user query flows through the system:\n\n```mermaid\nsequenceDiagram\n    participant U as User\n    participant DRA as deep_research.py<br/>user_query\n    participant MEM as memory.py<br/>init_messages\n    participant LLM as llm.py<br/>qwen-plus\n    participant ACT as action.py<br/>call_tools_safely\n    participant PYT as ExecutePythonCodeTool\n    participant EXE as Subprocess Executor\n    participant LOG as global_logger\n    \n    U->>DRA: user_query(user_input)\n    DRA->>LOG: Log user input\n    DRA->>MEM: init_messages_with_system_prompt\n    MEM-->>DRA: messages list\n    \n    loop Until no tool calls\n        DRA->>LLM: generate_assistant_output_append\n        LLM-->>DRA: ChatCompletionMessage\n        \n        alt has tool_calls or function_call\n            DRA->>ACT: call_tools_safely(tool_info)\n            ACT->>PYT: run(tool_call_arguments)\n            PYT->>EXE: run_structured_in_subprocess\n            EXE-->>PYT: ExecutionResult\n            PYT-->>ACT: formatted result\n            ACT->>LOG: Log tool output\n            ACT-->>DRA: tool result\n            DRA->>MEM: Append tool result to messages\n        else No tool call\n            DRA->>LOG: Log final answer\n            DRA-->>U: Return response\n        end\n    end\n```\n\n**Key decision points:**\n1. **Line 27**: Check `has_tool_call()` or `has_function_call()` [src/agent/deep_research.py:27]()\n2. **Line 32**: Loop continues while tools are called [src/agent/deep_research.py:32]()\n3. **Line 33-46**: Handle legacy `function_call` format [src/agent/deep_research.py:33-46]()\n4. **Line 47-64**: Handle modern `tool_calls` array [src/agent/deep_research.py:47-64]()\n5. **Line 73**: Final answer logged and returned [src/agent/deep_research.py:73]()\n\n**Sources:** [src/agent/deep_research.py:15-73](), [src/agent/llm.py:34-50]()\n\n---\n\n## Technology Stack\n\nThe system is built on the following dependencies:\n\n| Category | Package | Version | Purpose |\n|----------|---------|---------|---------|\n| **LLM Client** | `openai` | â¥2.7.2 | DashScope API communication |\n| **Data Processing** | `pandas` | â¥2.3.3 | DataFrame operations |\n| **Data Processing** | `numpy` | â¥2.3.4 | Numerical computing |\n| **Visualization** | `matplotlib` | â¥3.10.7 | Plotting and charts |\n| **Graph Processing** | `networkx` | â¥3.5 | Network/graph algorithms |\n| **ML/Analysis** | `scikit-learn` | â¥1.7.2 | Machine learning utilities |\n| **Utilities** | `deepdiff` | â¥8.6.1 | Deep comparison of data structures |\n| **Utilities** | `decorator` | â¥5.2.1 | Decorator utilities |\n| **Utilities** | `inflection` | â¥0.5.1 | String transformations |\n| **Web Interface** | `streamlit` | â¥1.51.0 | Interactive web UI (optional) |\n| **HTTP** | `requests` | â¥2.32.5 | HTTP client |\n| **Visualization** | `wordcloud` | â¥1.9.4 | Word cloud generation |\n\n**Python version requirement:** â¥3.12\n\n**Configuration files:**\n- [pyproject.toml:1-21]() - Project metadata and dependencies\n- [uv.lock:1-302]() - Locked dependency versions\n- [.vscode/settings.json:1-34]() - IDE configuration\n\n**Sources:** [pyproject.toml:1-21](), [uv.lock:1-302]()\n\n---\n\n## System Characteristics\n\nThe algo_agent system is characterized by:\n\n**Strengths:**\n- **Safety-first design**: Multiple isolation levels prevent code crashes from affecting the system\n- **Comprehensive observability**: Every function call, execution, and error is logged\n- **Extensible architecture**: New tools can be added by implementing `BaseTool` interface\n- **State persistence**: Global variables maintained across executions via workspace management\n- **Flexible execution**: Three strategies (subprocess/subthread/direct) for different use cases\n\n**Typical use cases (from logs):**\n- Emergency response planning with data schema validation [src/agent/deep_research.py:77-128]()\n- Geographic data processing and visualization\n- Multi-day travel route optimization with metro/walking constraints\n- Pydantic data modeling and validation\n- Algorithm development and testing\n\n**For more detailed information:**\n- Installation and setup: see [Getting Started](#2)\n- Adding new tools: see [Creating New Tools](#8.2)\n- Debugging execution issues: see [Troubleshooting](#10)\n\n**Sources:** [src/agent/deep_research.py:76-129](), [pyproject.toml:1-21]()\n\n---\n\n# Page: Getting Started\n\n# Getting Started\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.vscode/settings.json](.vscode/settings.json)\n- [docs/sql.md](docs/sql.md)\n- [pyproject.toml](pyproject.toml)\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n- [uv.lock](uv.lock)\n\n</details>\n\n\n\nThis page provides step-by-step instructions for installing, configuring, and running your first query with the algo_agent system. It covers dependency installation, API configuration, and executing a basic code generation task. For detailed information about the agent's decision loop and tool execution mechanics, see [Agent Orchestration](#3). For information about the execution strategies and isolation modes, see [Execution Runtime](#5).\n\n---\n\n## Prerequisites\n\nThe algo_agent system requires:\n\n- **Python 3.12 or higher** - The project specifies `requires-python = \">=3.12\"` in its configuration\n- **Git** - For cloning the repository\n- **uv** (recommended) or pip - For dependency management\n- **DashScope API access** - The system uses Alibaba's qwen-plus LLM model\n\nSources: [pyproject.toml:6]()\n\n---\n\n## Installation\n\n### Step 1: Clone the Repository\n\n```bash\ngit clone https://github.com/1850298154/algo_agent\ncd algo_agent\n```\n\n### Step 2: Install Dependencies\n\nThe project uses `uv` for dependency management. Install all required packages:\n\n```bash\n# Using uv (recommended)\nuv sync\n\n# Or using pip\npip install -e .\n```\n\nThis installs the following core dependencies:\n\n| Package | Version | Purpose |\n|---------|---------|---------|\n| `openai` | â¥2.7.2 | LLM client interface |\n| `matplotlib` | â¥3.10.7 | Data visualization |\n| `pandas` | â¥2.3.3 | Data manipulation |\n| `numpy` | â¥2.3.4 | Numerical computing |\n| `networkx` | â¥3.5 | Graph algorithms |\n| `streamlit` | â¥1.51.0 | Web interface |\n| `deepdiff` | â¥8.6.1 | Deep object comparison |\n| `scikit-learn` | â¥1.7.2 | Machine learning utilities |\n\nSources: [pyproject.toml:7-20](), [uv.lock:1-43]()\n\n### Step 3: Verify Installation\n\nVerify the installation by checking the Python environment:\n\n```bash\npython -c \"import openai, matplotlib, pandas; print('Dependencies OK')\"\n```\n\n---\n\n## Configuration\n\n### API Key Setup\n\nThe system uses Alibaba's DashScope API for LLM access. You need to configure your API key:\n\n**Option 1: Environment Variable**\n```bash\nexport DASHSCOPE_API_KEY=\"your-api-key-here\"\n```\n\n**Option 2: Configuration File**\nCreate a configuration file in your project with the API credentials. The LLM client will read from the environment.\n\n### Working Directory Configuration\n\nThe system creates a `logs/` directory for storing execution logs. This is automatically created by the logging system:\n\n```mermaid\ngraph LR\n    START[\"System Startup\"] --> CHECK_DIR[\"setup_logger checks log directory\"]\n    CHECK_DIR --> CREATE[\"os.makedirs(log_dir, exist_ok=True)\"]\n    CREATE --> LOGGERS[\"Initialize loggers:<br/>- global.log<br/>- trace.log<br/>- print.log<br/>- all.log\"]\n    LOGGERS --> READY[\"System Ready\"]\n```\n\n**Diagram: Log Directory Initialization**\n\nThe logging system automatically creates necessary directories on first run.\n\nSources: [src/utils/log_decorator.py:19-28](), [src/utils/log_decorator.py:288-306]()\n\n### VS Code Configuration (Optional)\n\nIf using VS Code, the repository includes workspace settings that set `PYTHONPATH`:\n\n```json\n{\n    \"terminal.integrated.env.windows\": {\n        \"PYTHONPATH\": \"${workspaceFolder};${env:PYTHONPATH}\"\n    },\n    \"terminal.integrated.env.linux\": {\n        \"PYTHONPATH\": \"${workspaceFolder}:${env:PYTHONPATH}\"\n    }\n}\n```\n\nThis ensures the `src/` module can be imported correctly.\n\nSources: [.vscode/settings.json:2-7]()\n\n---\n\n## Running Your First Query\n\n### Basic Query Example\n\nThe primary entry point is the `user_query()` function in the deep research agent module:\n\n```python\nfrom src.agent.deep_research import user_query\n\n# Simple data analysis query\nquery = \"\"\"\nGenerate a list of prime numbers up to 100 using Python,\nthen create a histogram showing their distribution.\n\"\"\"\n\nuser_query(query)\n```\n\n### Query Execution Flow\n\nWhen you call `user_query()`, the following sequence occurs:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant user_query[\"user_query()<br/>(deep_research.py:15)\"]\n    participant memory[\"init_messages_with_system_prompt()<br/>(memory module)\"]\n    participant llm[\"generate_assistant_output_append()<br/>(llm module)\"]\n    participant tools[\"get_tools_schema()<br/>(tool.schema)\"]\n    participant action[\"call_tools_safely()<br/>(action module)\"]\n    participant ExecutePythonCodeTool[\"ExecutePythonCodeTool.run()\"]\n    participant global_logger[\"global_logger<br/>(logs/print.log)\"]\n\n    User->>user_query: \"query string\"\n    user_query->>global_logger: Log user input\n    user_query->>memory: Initialize message history\n    memory-->>user_query: messages list\n    \n    user_query->>tools: Get tool schemas\n    tools-->>user_query: [ExecutePythonCodeTool schema]\n    \n    user_query->>llm: Call qwen-plus with messages + tools\n    llm-->>user_query: assistant_output with tool_calls\n    \n    user_query->>action: call_tools_safely(tool_info)\n    action->>ExecutePythonCodeTool: Execute Python code\n    ExecutePythonCodeTool-->>action: ExecutionResult\n    action-->>user_query: tool output\n    \n    user_query->>global_logger: Log tool output\n    user_query->>llm: Call LLM with tool results\n    llm-->>user_query: Final answer\n    user_query->>global_logger: Log final answer\n    user_query-->>User: Complete\n```\n\n**Diagram: First Query Execution Sequence**\n\nThis shows how a user query flows through the agent, LLM, and tool execution system.\n\nSources: [src/agent/deep_research.py:15-73]()\n\n### Understanding the Output\n\nThe system produces multiple types of output:\n\n#### 1. Console Output\n\nThe agent logs its progress to the console via `global_logger`:\n\n```\n[2025-01-15 10:30:15]  ç¨æ·è¾å¥ï¼ Generate prime numbers...\n[2025-01-15 10:30:16]  å·¥å· tool call è¾åºä¿¡æ¯ï¼ Execution completed successfully\n[2025-01-15 10:30:17]  æç»ç­æ¡ï¼ Here are the prime numbers...\n```\n\nSources: [src/agent/deep_research.py:17](), [src/agent/deep_research.py:62](), [src/agent/deep_research.py:73]()\n\n#### 2. Log Files\n\nThe system writes detailed logs to multiple files in the `logs/` directory:\n\n| Log File | Content | Logger Name |\n|----------|---------|-------------|\n| `print.log` | User-facing messages and results | `root.all.print` |\n| `trace.log` | Detailed function traces with timing | `root.all.trace` |\n| `global.log` | System-level operation logs | `root.all` |\n| `all.log` | Comprehensive combined logs | `root.all` |\n\nSources: [src/utils/log_decorator.py:292-306]()\n\n#### 3. Execution Artifacts\n\nIf the query involves code execution with file output (plots, data files), these are saved to the working directory. The system outputs absolute paths:\n\n```\nå¾ççç»å¯¹è·¯å¾ï¼/path/to/workspace/prime_histogram_1.png\n```\n\nSources: [src/agent/deep_research.py:108]()\n\n---\n\n## Directory Structure After Setup\n\nAfter installation and first run, your directory structure will look like:\n\n```\nalgo_agent/\nâââ src/\nâ   âââ agent/\nâ   â   âââ deep_research.py      # Main entry point\nâ   âââ llm/                       # LLM integration\nâ   âââ action/                    # Tool dispatcher\nâ   âââ memory/                    # Context management\nâ   âââ tool/                      # Tool implementations\nâ   âââ utils/                     # Logging and utilities\nâââ logs/                          # Auto-created log directory\nâ   âââ print.log\nâ   âââ trace.log\nâ   âââ global.log\nâ   âââ all.log\nâââ pyproject.toml                 # Project dependencies\nâââ uv.lock                        # Locked dependencies\n```\n\n---\n\n## Tool Configuration\n\nBy default, the system loads the following tools:\n\n```python\ntools_schema_list = tool.schema.get_tools_schema([\n    tool.python_tool.ExecutePythonCodeTool,\n    # tool.todo_tool.RecursivePlanTreeTodoTool,  # Commented out\n])\n```\n\nThe `ExecutePythonCodeTool` is enabled by default, allowing the agent to write and execute Python code. The task planning tool (`RecursivePlanTreeTodoTool`) is commented out in the example but can be enabled for hierarchical task management.\n\nSources: [src/agent/deep_research.py:20-23]()\n\n---\n\n## Execution Mode Selection\n\nThe Python code execution tool supports three isolation strategies. The system automatically selects the appropriate mode based on the execution requirements:\n\n```mermaid\ngraph TB\n    CODE[\"Python Code Snippet\"] --> TOOL[\"ExecutePythonCodeTool\"]\n    TOOL --> DECISION{\"Select Execution Mode\"}\n    \n    DECISION -->|\"Full Isolation\"| SUBPROCESS[\"run_structured_in_subprocess()<br/>Process isolation<br/>Pipe-based IPC<br/>Crash protection\"]\n    \n    DECISION -->|\"Balanced\"| SUBTHREAD[\"run_structured_in_subthread()<br/>Thread isolation<br/>Shared memory<br/>Timeout support\"]\n    \n    DECISION -->|\"Fast\"| DIRECT[\"Direct execution<br/>In-process<br/>No isolation<br/>Fastest\"]\n    \n    SUBPROCESS --> RESULT[\"ExecutionResult\"]\n    SUBTHREAD --> RESULT\n    DIRECT --> RESULT\n    \n    RESULT --> STATUS[\"status: SUCCESS/FAILURE/TIMEOUT/CRASHED\"]\n    RESULT --> OUTPUT[\"stdout/stderr capture\"]\n    RESULT --> GLOBALS[\"out_globals_list<br/>Persisted variables\"]\n```\n\n**Diagram: Execution Mode Selection**\n\nThe tool selects execution modes based on isolation requirements and performance needs.\n\nSources: Inferred from architecture diagrams in the provided context\n\n---\n\n## Testing Your Setup\n\n### Test 1: Basic Code Execution\n\n```python\nfrom src.agent.deep_research import user_query\n\nuser_query(\"Write Python code to print 'Hello, algo_agent!'\")\n```\n\nExpected output:\n```\nç¨æ·è¾å¥ï¼ Write Python code to print 'Hello, algo_agent!'\nå·¥å· tool call è¾åºä¿¡æ¯ï¼ Execution completed successfully\næç»ç­æ¡ï¼ The code has been executed successfully...\n```\n\n### Test 2: Data Processing\n\n```python\nfrom src.agent.deep_research import user_query\n\nuser_query(\"\"\"\nCreate a pandas DataFrame with 10 random numbers,\ncalculate their mean and standard deviation,\nand display the results.\n\"\"\")\n```\n\nThis tests:\n- Python code generation\n- Library imports (pandas, numpy)\n- Statistical computation\n- Result formatting\n\n---\n\n## Common Setup Issues\n\n### Issue 1: Import Errors\n\n**Problem**: `ModuleNotFoundError: No module named 'src'`\n\n**Solution**: Ensure `PYTHONPATH` includes the project root:\n```bash\nexport PYTHONPATH=\"${PWD}:${PYTHONPATH}\"\n```\n\nOr install the package in development mode:\n```bash\npip install -e .\n```\n\n### Issue 2: API Key Not Found\n\n**Problem**: LLM calls fail with authentication errors\n\n**Solution**: Verify the `DASHSCOPE_API_KEY` environment variable is set:\n```bash\necho $DASHSCOPE_API_KEY\n```\n\n### Issue 3: Log Directory Permissions\n\n**Problem**: `PermissionError` when creating log files\n\n**Solution**: The logging system automatically creates the directory with `exist_ok=True`, but ensure write permissions:\n```bash\nchmod -R u+w logs/\n```\n\nSources: [src/utils/log_decorator.py:19-28]()\n\n---\n\n## Next Steps\n\nAfter successfully running your first query, you can:\n\n1. **Understand Agent Behavior** - Read [Agent Orchestration](#3) to learn how the agent processes queries and makes tool decisions\n2. **Explore Tool System** - See [Tool System](#4) for details on creating custom tools\n3. **Review Execution Modes** - Study [Execution Runtime](#5) to understand code isolation strategies\n4. **Analyze Logs** - Check [Observability and Logging](#6) for debugging and performance analysis\n5. **Try Complex Examples** - Explore [Use Cases and Examples](#7) for real-world scenarios\n\n---\n\n## Quick Reference: Key Code Entities\n\n| Entity | Location | Purpose |\n|--------|----------|---------|\n| `user_query()` | [src/agent/deep_research.py:15]() | Main entry point for queries |\n| `init_messages_with_system_prompt()` | memory module | Initialize conversation context |\n| `generate_assistant_output_append()` | llm module | Call LLM with tool schemas |\n| `call_tools_safely()` | action module | Dispatch and execute tools |\n| `ExecutePythonCodeTool` | tool.python_tool module | Execute Python code snippets |\n| `get_tools_schema()` | tool.schema module | Generate tool schemas for LLM |\n| `global_logger` | [src/utils/log_decorator.py:305]() | User-facing log output |\n| `setup_logger()` | [src/utils/log_decorator.py:19]() | Configure logging system |\n\nSources: [src/agent/deep_research.py:15-73](), [src/utils/log_decorator.py:19-63](), [src/utils/log_decorator.py:292-306]()\n\n---\n\n# Page: Agent Orchestration\n\n# Agent Orchestration\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/functino_call_err.design.md](docs/functino_call_err.design.md)\n- [src/agent/action.py](src/agent/action.py)\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/agent/llm.py](src/agent/llm.py)\n- [src/agent/memory.py](src/agent/memory.py)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document describes the core agent orchestration system that coordinates the execution of user queries. The orchestration layer is responsible for managing the conversation loop between the user, the LLM (Large Language Model), and available tools. It handles message flow, tool dispatch, context management, and error recovery.\n\nFor detailed information about specific subsystems:\n- Query processing mechanics: see [Query Processing Loop](#3.1)\n- LLM API integration details: see [LLM Integration](#3.2)\n- Conversation history management: see [Memory and Context Management](#3.3)\n- Tool execution coordination: see [Action Coordination](#3.4)\n- Tool implementations: see [Tool System](#4)\n\n## Architecture Overview\n\nThe agent orchestration system is centered around the `user_query` function in [src/agent/deep_research.py:15-74](), which coordinates four primary components: memory management, LLM integration, action coordination, and logging.\n\n```mermaid\ngraph TB\n    UserInput[\"User Input<br/>(user_query parameter)\"]\n    \n    subgraph \"Agent Orchestration Layer\"\n        UserQuery[\"user_query()<br/>src/agent/deep_research.py:15\"]\n        DecisionLoop[\"Agent Decision Loop<br/>lines 32-65\"]\n    end\n    \n    subgraph \"Core Components\"\n        Memory[\"memory.init_messages_with_system_prompt()<br/>src/agent/memory.py:5\"]\n        LLM[\"llm.generate_assistant_output_append()<br/>src/agent/llm.py:34\"]\n        Action[\"action.call_tools_safely()<br/>src/agent/action.py:10\"]\n        ToolSchema[\"tool.schema.get_tools_schema()<br/>src/agent/tool/schema.py\"]\n    end\n    \n    subgraph \"State Management\"\n        Messages[\"messages: list[dict]<br/>OpenAI format\"]\n        ToolInfo[\"tool_info: dict<br/>role, content, tool_call_id\"]\n    end\n    \n    subgraph \"Tools\"\n        PythonTool[\"ExecutePythonCodeTool\"]\n        TodoTool[\"RecursivePlanTreeTodoTool\"]\n    end\n    \n    subgraph \"Observability\"\n        GlobalLogger[\"global_logger<br/>src/utils/log_decorator.py:305\"]\n    end\n    \n    UserInput --> UserQuery\n    UserQuery --> Memory\n    Memory --> Messages\n    UserQuery --> ToolSchema\n    \n    UserQuery --> DecisionLoop\n    DecisionLoop --> LLM\n    LLM --> Messages\n    Messages --> LLM\n    \n    DecisionLoop --> Action\n    Action --> ToolInfo\n    ToolInfo --> PythonTool\n    ToolInfo --> TodoTool\n    PythonTool --> ToolInfo\n    TodoTool --> ToolInfo\n    ToolInfo --> Messages\n    \n    UserQuery --> GlobalLogger\n    DecisionLoop --> GlobalLogger\n    Action --> GlobalLogger\n    \n    style UserQuery fill:#f9f9f9\n    style DecisionLoop fill:#f9f9f9\n    style Messages fill:#f0f0f0\n```\n\n**Diagram: Agent Orchestration Architecture and Code Entities**\n\nSources: [src/agent/deep_research.py:1-74](), [src/agent/memory.py:1-17](), [src/agent/llm.py:1-51](), [src/agent/action.py:1-49]()\n\n## Core Components\n\nThe orchestration layer integrates four primary components:\n\n| Component | Module | Primary Function | Key Functions |\n|-----------|--------|------------------|---------------|\n| **Deep Research Agent** | `src/agent/deep_research.py` | Entry point and decision loop | `user_query()` |\n| **Memory Management** | `src/agent/memory.py` | Initialize and maintain conversation context | `init_messages_with_system_prompt()` |\n| **LLM Integration** | `src/agent/llm.py` | Interface with qwen-plus model via DashScope | `generate_assistant_output_append()`, `has_tool_call()`, `has_function_call()` |\n| **Action Coordinator** | `src/agent/action.py` | Dispatch tool calls and handle errors | `call_tools_safely()` |\n\n### Deep Research Agent\n\nThe `user_query` function at [src/agent/deep_research.py:15]() serves as the orchestration entry point. It initializes the conversation, manages the agent decision loop, and coordinates all interactions between the LLM and tools.\n\n### Memory Management\n\nMemory management is handled by `init_messages_with_system_prompt()` at [src/agent/memory.py:5-16](). It creates a message list with the system prompt from `prompt.react_system_prompt` and the initial user input, following the OpenAI message format.\n\n### LLM Integration\n\nThe LLM integration layer at [src/agent/llm.py:1-51]() provides three critical functions:\n- `generate_assistant_output_append()`: Calls the LLM and appends the response to messages\n- `has_tool_call()`: Checks if the response contains tool_calls\n- `has_function_call()`: Checks if the response contains function_call (legacy format)\n\n### Action Coordinator\n\nThe action coordinator at [src/agent/action.py:10-48]() provides safe tool execution through `call_tools_safely()`, which wraps tool dispatch in error handling and logs all execution attempts.\n\nSources: [src/agent/deep_research.py:15-74](), [src/agent/memory.py:5-16](), [src/agent/llm.py:34-49](), [src/agent/action.py:10-48]()\n\n## Orchestration Flow\n\nThe agent follows a continuous decision loop until the LLM produces a final answer without requesting tool execution:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant user_query as \"user_query()<br/>deep_research.py:15\"\n    participant memory as \"memory.init_messages_with_system_prompt()<br/>memory.py:5\"\n    participant messages as \"messages: list[dict]\"\n    participant llm as \"llm.generate_assistant_output_append()<br/>llm.py:34\"\n    participant has_tool_call as \"llm.has_tool_call()<br/>llm.py:44\"\n    participant action as \"action.call_tools_safely()<br/>action.py:10\"\n    participant tools as \"Tool Implementations\"\n    \n    User->>user_query: \"user_input string\"\n    user_query->>memory: \"user_input\"\n    memory->>messages: \"[system_prompt, user_input]\"\n    memory-->>user_query: \"messages\"\n    \n    user_query->>llm: \"(messages, tools_schema_list)\"\n    llm-->>messages: \"append assistant_output\"\n    llm-->>user_query: \"assistant_output: ChatCompletionMessage\"\n    \n    user_query->>has_tool_call: \"assistant_output\"\n    \n    alt No tool calls\n        has_tool_call-->>user_query: \"False\"\n        user_query->>User: \"assistant_output.content\"\n    else Has tool calls\n        has_tool_call-->>user_query: \"True\"\n        \n        loop For each tool_call\n            user_query->>action: \"tool_info dict\"\n            action->>tools: \"Dispatch by tool_call_name\"\n            tools-->>action: \"result or error\"\n            action-->>user_query: \"tool_info with content\"\n            user_query->>messages: \"append tool_info\"\n        end\n        \n        user_query->>llm: \"(messages, tools_schema_list)\"\n        Note over user_query,llm: Loop continues until no tool calls\n    end\n```\n\n**Diagram: Agent Decision Loop Execution Flow**\n\nThe flow consists of these stages:\n\n1. **Initialization** [src/agent/deep_research.py:19-23](): Initialize messages with system prompt and obtain tool schemas\n2. **First LLM Call** [src/agent/deep_research.py:26-29](): Generate initial assistant response\n3. **Decision Loop** [src/agent/deep_research.py:32-65](): \n   - Check for `tool_calls` or `function_call` in assistant output\n   - If present, execute tools and append results to messages\n   - Call LLM again with updated context\n   - Repeat until no tool calls are requested\n4. **Final Response** [src/agent/deep_research.py:73](): Log and return the final answer\n\nSources: [src/agent/deep_research.py:15-74](), [src/agent/llm.py:34-49](), [src/agent/action.py:10-21]()\n\n## Message Protocol\n\nThe agent uses OpenAI's message format to maintain conversation state. Messages are stored in a `list[dict]` structure where each message has specific required fields based on its role.\n\n### Message Structure\n\n```mermaid\ngraph LR\n    subgraph \"Message Types\"\n        System[\"role: 'system'<br/>content: react_system_prompt\"]\n        User[\"role: 'user'<br/>content: user_input\"]\n        Assistant[\"role: 'assistant'<br/>content: text response<br/>tool_calls: list or None\"]\n        Tool[\"role: 'tool'<br/>content: tool output<br/>tool_call_id: tracking id\"]\n        Function[\"role: 'function'<br/>content: tool output<br/>(legacy format)\"]\n    end\n    \n    Messages[\"messages: list[dict]\"]\n    \n    Messages --> System\n    Messages --> User\n    Messages --> Assistant\n    Messages --> Tool\n    Messages --> Function\n```\n\n**Diagram: Message Format and Role Types**\n\n### Message Roles\n\n| Role | Fields | Purpose | Code Location |\n|------|--------|---------|---------------|\n| `system` | `content`, `role` | System prompt defining agent behavior | [src/agent/memory.py:7-10]() |\n| `user` | `content`, `role` | User input or query | [src/agent/memory.py:11-14]() |\n| `assistant` | `content`, `role`, `tool_calls` (optional) | LLM response with potential tool requests | [src/agent/llm.py:36-40]() |\n| `tool` | `content`, `role`, `tool_call_id` | Tool execution result | [src/agent/deep_research.py:50-57]() |\n| `function` | `content`, `role` | Legacy function call result | [src/agent/deep_research.py:34-41]() |\n\n### Tool Call Structure\n\nWhen the LLM requests tool execution, the assistant message contains a `tool_calls` field. The agent extracts this information into a `tool_info` dictionary at [src/agent/deep_research.py:50-57]():\n\n```python\ntool_info = {\n    \"content\": \"\",  # Populated by tool execution\n    \"role\": \"tool\",\n    \"tool_call_id\": assistant_output.tool_calls[i].id,\n    # Non-required fields for dispatch\n    \"tool_call_name\": assistant_output.tool_calls[i].function.name,\n    \"tool_call_arguments\": assistant_output.tool_calls[i].function.arguments,\n}\n```\n\nThe system also supports the legacy `function_call` format at [src/agent/deep_research.py:34-41]() for backward compatibility.\n\nSources: [src/agent/deep_research.py:34-64](), [src/agent/memory.py:5-16](), [src/agent/llm.py:36-40]()\n\n## Tool Dispatch Mechanism\n\nThe agent determines which tool to execute by examining the `tool_call_name` field in the `tool_info` dictionary. The dispatch logic at [src/agent/action.py:11-21]() uses a conditional chain:\n\n```mermaid\ngraph TB\n    CallToolsSafely[\"call_tools_safely(tool_info)<br/>action.py:10\"]\n    ParseArgs[\"json.loads(tool_call_arguments)<br/>action.py:13\"]\n    CheckName[\"Check tool_call_name\"]\n    \n    ExecutePythonCode[\"ExecutePythonCodeTool(**arguments)<br/>action.py:15-17\"]\n    RecursiveTodo[\"RecursivePlanTreeTodoTool(**arguments)<br/>action.py:18-20\"]\n    \n    SetContent[\"tool_info['content'] = result\"]\n    ErrorHandler[\"Exception Handler<br/>traceback.format_exc()<br/>action.py:42-47\"]\n    \n    CallToolsSafely --> ParseArgs\n    ParseArgs --> CheckName\n    CheckName -->|\"== 'execute_python_code'\"| ExecutePythonCode\n    CheckName -->|\"== 'recursive_plan_tree_todo'\"| RecursiveTodo\n    \n    ExecutePythonCode --> SetContent\n    RecursiveTodo --> SetContent\n    \n    SetContent -.Exception.-> ErrorHandler\n    ErrorHandler --> SetContent\n```\n\n**Diagram: Tool Dispatch and Error Handling Logic**\n\nThe dispatch mechanism:\n1. Parses JSON arguments from `tool_call_arguments` [src/agent/action.py:13]()\n2. Matches `tool_call_name` against known tool names [src/agent/action.py:15-20]()\n3. Instantiates the tool class with parsed arguments\n4. Calls the tool's `run()` method and stores the result in `tool_info[\"content\"]`\n5. Returns the modified `tool_info` dictionary\n\nTool registration is performed at [src/agent/deep_research.py:20-23]() where tool schemas are provided to the LLM:\n\n```python\ntools_schema_list = tool.schema.get_tools_schema([\n    tool.python_tool.ExecutePythonCodeTool,\n    # tool.todo_tool.RecursivePlanTreeTodoTool,  # Can be enabled\n])\n```\n\nSources: [src/agent/action.py:10-48](), [src/agent/deep_research.py:20-23]()\n\n## Error Handling Strategy\n\nThe orchestration layer implements multiple levels of error handling to ensure robustness:\n\n### Tool Execution Errors\n\nThe `call_tools_safely()` function at [src/agent/action.py:40-47]() wraps all tool dispatch in a try-except block:\n\n```python\ntry:\n    return call_tools(tool_info)\nexcept Exception as e:\n    error_msg = traceback.format_exc()\n    global_logger.error(f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\", exc_info=True)\n    tool_info[\"content\"] = f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\"\n    return tool_info\n```\n\nWhen a tool fails:\n1. The full traceback is captured using `traceback.format_exc()`\n2. The error is logged to `global_logger`\n3. The error message is placed in `tool_info[\"content\"]`\n4. The modified `tool_info` is returned to the agent\n5. The error message becomes part of the conversation context\n6. The LLM receives the error and can attempt recovery\n\n### LLM Integration Errors\n\nLLM API errors can occur due to invalid message formats, as documented in [docs/functino_call_err.design.md:8-27](). The most common error is:\n\n```\nBadRequestError: An assistant message with \"tool_calls\" must be followed by \ntool messages responding to each \"tool_call_id\"\n```\n\nThis occurs when the message sequence is malformed. The agent ensures correctness by:\n1. Always appending tool results immediately after tool calls [src/agent/deep_research.py:46, 64]()\n2. Maintaining proper role sequencing (assistant â tool â assistant)\n3. Including the correct `tool_call_id` for each tool response [src/agent/deep_research.py:53]()\n\n### Logging Infrastructure\n\nAll orchestration activities are logged through `global_logger` defined at [src/utils/log_decorator.py:305-306](). Key logging points:\n\n| Event | Location | Log Level |\n|-------|----------|-----------|\n| User input received | [src/agent/deep_research.py:17]() | INFO |\n| No tool call needed | [src/agent/deep_research.py:28]() | INFO |\n| Tool output received | [src/agent/deep_research.py:44, 62]() | INFO |\n| LLM round output | [src/agent/deep_research.py:68-72]() | INFO |\n| Final answer | [src/agent/deep_research.py:73]() | INFO |\n\nThe `@traceable` decorator from [src/utils/log_decorator.py:297-302]() is applied to key functions (`init_messages_with_system_prompt`, `generate_assistant_output_append`, `call_tools_safely`) to capture detailed execution traces including arguments, return values, timing, and stack traces.\n\nSources: [src/agent/action.py:40-47](), [src/agent/deep_research.py:17-73](), [src/utils/log_decorator.py:297-306](), [docs/functino_call_err.design.md:8-27]()\n\n## Configuration\n\nThe orchestration system is configured through two primary mechanisms:\n\n### Tool Registration\n\nTools are registered at [src/agent/deep_research.py:20-23]() by passing tool classes to `get_tools_schema()`:\n\n```python\ntools_schema_list = tool.schema.get_tools_schema([\n    tool.python_tool.ExecutePythonCodeTool,\n    # tool.todo_tool.RecursivePlanTreeTodoTool,\n])\n```\n\nTools can be enabled or disabled by commenting/uncommenting lines in this list.\n\n### LLM Configuration\n\nThe LLM client is configured at [src/agent/llm.py:8-11]():\n\n```python\nclient = openai.OpenAI(\n    api_key=model_api_key,\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\n```\n\nModel selection occurs at [src/agent/llm.py:16]():\n\n```python\nmodel=\"qwen-plus\"\n```\n\nAdditional LLM parameters are set at [src/agent/llm.py:19-22]():\n- `tools`: Tool schema list for function calling\n- `function_call`: None (disabled)\n- `parallel_tool_calls`: True (enables parallel tool execution)\n\nSources: [src/agent/deep_research.py:20-23](), [src/agent/llm.py:8-22]()\n\n---\n\n# Page: Query Processing Loop\n\n# Query Processing Loop\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document explains the core agent decision loop implemented in the `user_query` function that processes user queries from start to finish. The loop orchestrates the interaction between the user, the LLM, and available tools until a final answer is produced.\n\nFor information about how the LLM is called and responses are parsed, see [LLM Integration](#3.2). For details on memory and context management, see [Memory and Context Management](#3.3). For tool execution specifics, see [Action Coordination](#3.4).\n\n## Architecture Overview\n\nThe query processing loop operates as a state machine that alternates between LLM reasoning and tool execution phases. The loop maintains conversational state in a `messages` list and continues until the LLM decides no further tool calls are needed.\n\n```mermaid\ngraph TB\n    START[\"user_query(user_input)\"]\n    INIT[\"Initialize messages<br/>memory.init_messages_with_system_prompt()\"]\n    SCHEMA[\"Get tool schemas<br/>tool.schema.get_tools_schema()\"]\n    LLM[\"Call LLM<br/>llm.generate_assistant_output_append()\"]\n    CHECK{\"Check tool calls<br/>llm.has_tool_call() or<br/>llm.has_function_call()\"}\n    DISPATCH[\"Extract tool_info dict<br/>action.call_tools_safely()\"]\n    APPEND[\"Append tool result<br/>to messages list\"]\n    RETURN[\"Return final answer<br/>assistant_output.content\"]\n    \n    START --> INIT\n    INIT --> SCHEMA\n    SCHEMA --> LLM\n    LLM --> CHECK\n    CHECK -->|\"No tool calls\"| RETURN\n    CHECK -->|\"Has tool calls\"| DISPATCH\n    DISPATCH --> APPEND\n    APPEND --> LLM\n```\n\n**Diagram: Main Query Processing Flow**\n\nSources: [src/agent/deep_research.py:15-74]()\n\n## Entry Point and Initialization\n\nThe `user_query` function serves as the main entry point for all query processing. It accepts a `user_input` string and orchestrates the entire interaction lifecycle.\n\n### Function Signature\n\n```python\ndef user_query(user_input):\n```\n\n### Initialization Steps\n\nThe function performs three critical initialization steps before entering the main loop:\n\n| Step | Function Call | Purpose |\n|------|--------------|---------|\n| 1. Log user input | `global_logger.info()` | Record incoming query for observability |\n| 2. Initialize messages | `memory.init_messages_with_system_prompt(user_input)` | Create message list with system prompt and user query |\n| 3. Prepare tool schemas | `tool.schema.get_tools_schema([...])` | Generate JSON schemas for available tools |\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant user_query\n    participant memory\n    participant tool_schema\n    participant global_logger\n    \n    User->>user_query: user_input string\n    user_query->>global_logger: Log user input\n    user_query->>memory: init_messages_with_system_prompt(user_input)\n    memory-->>user_query: messages list\n    user_query->>tool_schema: get_tools_schema([ExecutePythonCodeTool, ...])\n    tool_schema-->>user_query: tools_schema_list\n```\n\n**Diagram: Initialization Sequence**\n\nThe `messages` list follows OpenAI's chat format with roles: `system`, `user`, `assistant`, `tool`, and `function`. The initial list contains two messages:\n\n1. **System message**: Contains `react_system_prompt` defining agent behavior\n2. **User message**: Contains the actual user query\n\nSources: [src/agent/deep_research.py:15-24](), [logs/global.log:57-181]()\n\n## Main Decision Loop Structure\n\nThe core of the query processing system is a `while` loop that continues executing until the LLM produces a response without tool calls. This represents the agent's iterative reasoning process.\n\n### Loop Condition\n\n```python\nwhile llm.has_tool_call(assistant_output) or llm.has_function_call(assistant_output):\n```\n\nThe loop checks two conditions because the system supports both:\n- **tool_calls** format (OpenAI's newer function calling format)\n- **function_call** format (legacy format for backward compatibility)\n\n### First LLM Call (Outside Loop)\n\nBefore entering the loop, one LLM call is made to determine if tools are needed at all:\n\n```python\nassistant_output: ChatCompletionMessage = llm.generate_assistant_output_append(messages, tools_schema_list)\nif not llm.has_tool_call(assistant_output) and not llm.has_function_call(assistant_output):\n    global_logger.info(f\"æ éè°ç¨å·¥å·ï¼æå¯ä»¥ç´æ¥åå¤ï¼{assistant_output.content}\")\n    return\n```\n\nThis early exit optimization avoids entering the loop for queries that don't require tool usage.\n\n```mermaid\nstateDiagram-v2\n    [*] --> InitialLLMCall\n    InitialLLMCall --> CheckToolCalls\n    CheckToolCalls --> DirectAnswer: No tool calls\n    CheckToolCalls --> EnterLoop: Has tool calls\n    DirectAnswer --> [*]\n    EnterLoop --> ProcessToolCall\n    ProcessToolCall --> SubsequentLLMCall\n    SubsequentLLMCall --> CheckLoopCondition\n    CheckLoopCondition --> ProcessToolCall: Still has tool calls\n    CheckLoopCondition --> FinalAnswer: No more tool calls\n    FinalAnswer --> [*]\n```\n\n**Diagram: Loop State Machine**\n\nSources: [src/agent/deep_research.py:25-32](), [logs/global.log:893-900]()\n\n## Tool Call Detection and Extraction\n\nThe system handles two different tool calling formats from the LLM. Each format requires different extraction logic.\n\n### Format 1: function_call (Legacy)\n\n```python\nif llm.has_function_call(assistant_output):\n    tool_info = {\n        \"content\": \"\",\n        \"role\": \"function\",\n        \"tool_call_id\": \"\",\n        \"tool_call_name\": assistant_output.function_call.name,\n        \"tool_call_arguments\": assistant_output.function_call.arguments,\n    }\n```\n\n### Format 2: tool_calls (Current)\n\n```python\nif llm.has_tool_call(assistant_output):\n    for i in range(len(assistant_output.tool_calls)):\n        tool_info = {\n            \"content\": \"\",\n            \"role\": \"tool\",\n            \"tool_call_id\": assistant_output.tool_calls[i].id,\n            \"tool_call_name\": assistant_output.tool_calls[i].function.name,\n            \"tool_call_arguments\": assistant_output.tool_calls[i].function.arguments,\n        }\n```\n\n### tool_info Dictionary Structure\n\n| Field | Type | Purpose |\n|-------|------|---------|\n| `content` | string | Initially empty, filled with tool output after execution |\n| `role` | string | Either `\"tool\"` or `\"function\"` depending on format |\n| `tool_call_id` | string | Unique identifier for tracking tool calls |\n| `tool_call_name` | string | Name of the tool to invoke (e.g., `\"execute_python_code\"`) |\n| `tool_call_arguments` | string | JSON string containing tool parameters |\n\n```mermaid\ngraph LR\n    ASST[\"assistant_output<br/>ChatCompletionMessage\"]\n    CHECK_FC{\"has_function_call()\"}\n    CHECK_TC{\"has_tool_call()\"}\n    EXTRACT_FC[\"Extract from<br/>function_call field\"]\n    EXTRACT_TC[\"Extract from<br/>tool_calls list\"]\n    TOOL_INFO[\"tool_info dict\"]\n    \n    ASST --> CHECK_FC\n    ASST --> CHECK_TC\n    CHECK_FC -->|True| EXTRACT_FC\n    CHECK_TC -->|True| EXTRACT_TC\n    EXTRACT_FC --> TOOL_INFO\n    EXTRACT_TC --> TOOL_INFO\n```\n\n**Diagram: Tool Call Extraction Logic**\n\nSources: [src/agent/deep_research.py:33-57](), [logs/global.log:908-1125]()\n\n## Tool Execution Path\n\nOnce a `tool_info` dictionary is constructed, it is passed to the action coordinator for safe execution. The coordinator handles dispatch, error handling, and result capture.\n\n### Execution Flow\n\n```mermaid\nsequenceDiagram\n    participant Loop as \"Query Loop\"\n    participant action as \"action.call_tools_safely()\"\n    participant tool as \"Tool Instance\"\n    participant global_logger\n    \n    Loop->>action: call_tools_safely(tool_info)\n    action->>tool: Dispatch to specific tool.run()\n    tool-->>action: Execution result\n    action->>tool_info: Update content field\n    action-->>Loop: Modified tool_info\n    Loop->>global_logger: Log tool output\n    Loop->>messages: Append tool_info to list\n```\n\n**Diagram: Tool Execution Sequence**\n\n### Code Implementation\n\nThe loop processes both `function_call` and `tool_calls` formats:\n\n**For function_call format:**\n```python\naction.call_tools_safely(tool_info)\ntool_output = tool_info[\"content\"]\nglobal_logger.info(f\"å·¥å· function call è¾åºä¿¡æ¯ï¼ {tool_output}\\n\")\nglobal_logger.info(\"-\" * 60)\nmessages.append(tool_info)\n```\n\n**For tool_calls format:**\n```python\nfor i in range(len(assistant_output.tool_calls)):\n    # ... extract tool_info ...\n    action.call_tools_safely(tool_info)\n    tool_output = tool_info[\"content\"]\n    global_logger.info(f\"å·¥å· tool call è¾åºä¿¡æ¯ï¼ {tool_output}\\n\")\n    global_logger.info(\"-\" * 60)\n    messages.append(tool_info)\n```\n\nThe key insight is that `action.call_tools_safely()` **mutates** the `tool_info` dictionary by filling its `\"content\"` field with the execution result. This modified dictionary is then appended to the `messages` list.\n\nSources: [src/agent/deep_research.py:42-64](), [logs/utils.log:59-62]()\n\n## Message Accumulation and Context\n\nThe `messages` list serves as the cumulative conversation history that provides context to the LLM on each iteration. Understanding its growth pattern is crucial.\n\n### Message List Growth Pattern\n\n```mermaid\ngraph TD\n    INIT[\"messages = [<br/>  {role: 'system', content: ...},<br/>  {role: 'user', content: user_input}<br/>]\"]\n    \n    LLM1[\"LLM Call 1\"]\n    ASST1[\"Append assistant message<br/>(via generate_assistant_output_append)\"]\n    TOOL1[\"Append tool result message<br/>{role: 'tool', content: result, ...}\"]\n    \n    LLM2[\"LLM Call 2\"]\n    ASST2[\"Append assistant message\"]\n    TOOL2[\"Append tool result message\"]\n    \n    LLMN[\"LLM Call N\"]\n    ASSTN[\"Append assistant message<br/>(no tool_calls)\"]\n    FINAL[\"Final messages list\"]\n    \n    INIT --> LLM1\n    LLM1 --> ASST1\n    ASST1 --> TOOL1\n    TOOL1 --> LLM2\n    LLM2 --> ASST2\n    ASST2 --> TOOL2\n    TOOL2 --> LLLM[\"...\"]\n    LLLM --> LLMN\n    LLMN --> ASSTN\n    ASSTN --> FINAL\n```\n\n**Diagram: Message List Evolution**\n\n### Message Roles and Structure\n\nEach message in the list has a specific role that determines its purpose:\n\n| Role | When Added | Content |\n|------|-----------|---------|\n| `system` | Initialization | System prompt defining agent behavior |\n| `user` | Initialization | User's original query |\n| `assistant` | After each LLM call | LLM response (may include `tool_calls` field) |\n| `tool` | After tool execution | Tool output and metadata |\n| `function` | After tool execution (legacy) | Tool output for function_call format |\n\n### Example Message Sequence\n\nFrom the logs, we can see a typical message sequence:\n\n**Iteration 1:**\n```\nRound 1:\n- messages[0]: {role: \"system\", content: \"ä½ æ¯ä¸ä¸ªèµæ·±çæ·±åº¦ç ç©¶ä¸å®¶...\"}\n- messages[1]: {role: \"user\", content: \"ä½ æå¨çå·¥ä½è·¯å¾ä¸é¢...\"}\n- messages[2]: {role: \"assistant\", tool_calls: [...]}  # Added by generate_assistant_output_append\n- messages[3]: {role: \"tool\", content: \"...\", tool_call_id: \"...\"}  # Added after tool execution\n```\n\n**Iteration 2:**\n```\nRound 2:\n- messages[0-3]: (previous messages)\n- messages[4]: {role: \"assistant\", content: \"...\", tool_calls: [...]}  # Next LLM call\n- messages[5]: {role: \"tool\", content: \"...\", tool_call_id: \"...\"}  # Next tool result\n```\n\nThe function `llm.generate_assistant_output_append()` automatically appends the assistant's message to the list before returning it.\n\nSources: [src/agent/deep_research.py:19-65](), [logs/global.log:115-181](), [logs/utils.log:56-63]()\n\n## Loop Termination and Final Answer\n\nThe loop terminates when the LLM produces an `assistant_output` that contains no `tool_calls` and no `function_call`. This signals that the LLM has determined it has sufficient information to provide a final answer.\n\n### Termination Logic\n\n```python\n# After each LLM call in the loop\nassistant_output = llm.generate_assistant_output_append(messages, tools_schema_list)\n\n# Null content safety check\nif assistant_output.content is None:\n    assistant_output.content = \"\"\n\n# Loop condition check happens automatically\n# If no tool calls, loop exits\n\n# After loop exits\nglobal_logger.info(f\"æç»ç­æ¡ï¼ {assistant_output.content}\")\n```\n\n### Logging and Output\n\nThroughout the loop, the system logs extensive information for debugging:\n\n**During loop iterations:**\n```python\nglobal_logger.info(\n    f\"\"\"ç¬¬{len(messages) // 2}è½®å¤§æ¨¡åè¾åºä¿¡æ¯ï¼ \n\\n\\nassistant_output.content:: \\n\\n {pprint.pformat(assistant_output.content)}\n\\n\\nassistant_output.tool_calls::\\n\\n {pprint.pformat([toolcall.model_dump() for toolcall in assistant_output.tool_calls] if assistant_output.tool_calls else [])}\\n\"\"\"\n)\n```\n\nThis log entry includes:\n- Round number (estimated as `len(messages) // 2`)\n- Full assistant content\n- Any tool calls present\n\n```mermaid\nflowchart TD\n    LOOP_ITER[\"Loop Iteration\"]\n    CALL_LLM[\"llm.generate_assistant_output_append()\"]\n    CHECK_NULL{\"assistant_output.content<br/>is None?\"}\n    SET_EMPTY[\"Set content = ''\"]\n    LOG_ROUND[\"Log round number and output\"]\n    CHECK_TOOLS{\"has_tool_call() or<br/>has_function_call()?\"}\n    EXEC_TOOLS[\"Execute tools\"]\n    LOG_FINAL[\"Log final answer\"]\n    EXIT[\"Return from user_query()\"]\n    \n    LOOP_ITER --> CALL_LLM\n    CALL_LLM --> CHECK_NULL\n    CHECK_NULL -->|Yes| SET_EMPTY\n    CHECK_NULL -->|No| LOG_ROUND\n    SET_EMPTY --> LOG_ROUND\n    LOG_ROUND --> CHECK_TOOLS\n    CHECK_TOOLS -->|Yes| EXEC_TOOLS\n    CHECK_TOOLS -->|No| LOG_FINAL\n    EXEC_TOOLS --> LOOP_ITER\n    LOG_FINAL --> EXIT\n```\n\n**Diagram: Loop Termination Flow**\n\nSources: [src/agent/deep_research.py:65-74](), [logs/utils.log:62-64]()\n\n## Complete Loop Walkthrough\n\nTo solidify understanding, here's a complete trace of the loop with actual code references:\n\n### Phase 1: Setup\n- [Line 15-17](): Log user input\n- [Line 19](): Call `memory.init_messages_with_system_prompt(user_input)` â returns `messages` list\n- [Line 20-23](): Call `tool.schema.get_tools_schema([tool classes])` â returns `tools_schema_list`\n\n### Phase 2: First LLM Call\n- [Line 26](): Call `llm.generate_assistant_output_append(messages, tools_schema_list)` â returns `assistant_output`\n  - This function appends the assistant's message to `messages` automatically\n- [Line 27-29](): Check if no tool calls, if true, log and return early\n\n### Phase 3: Tool Execution Loop\n- [Line 32](): While `has_tool_call()` or `has_function_call()`:\n  - [Line 33-46](): If `function_call` format:\n    - Extract `tool_info` dictionary\n    - Call `action.call_tools_safely(tool_info)` (mutates `tool_info[\"content\"]`)\n    - Log tool output\n    - Append `tool_info` to `messages`\n  - [Line 47-64](): If `tool_calls` format:\n    - Iterate through `assistant_output.tool_calls` list\n    - For each tool call, extract `tool_info` dictionary\n    - Call `action.call_tools_safely(tool_info)` (mutates `tool_info[\"content\"]`)\n    - Log tool output\n    - Append `tool_info` to `messages`\n  - [Line 65](): Call `llm.generate_assistant_output_append(messages, tools_schema_list)` again\n  - [Line 66-67](): Safety check for null content\n  - [Line 68-72](): Log this round's output\n  - Loop continues or exits based on tool call presence\n\n### Phase 4: Completion\n- [Line 73](): Log final answer from `assistant_output.content`\n- Function returns (implicitly)\n\n```mermaid\ngraph TB\n    subgraph \"Phase 1: Setup\"\n        S1[\"Log input\"]\n        S2[\"init_messages_with_system_prompt()\"]\n        S3[\"get_tools_schema()\"]\n        S1 --> S2 --> S3\n    end\n    \n    subgraph \"Phase 2: First LLM Call\"\n        L1[\"generate_assistant_output_append()\"]\n        L2{\"No tool calls?\"}\n        L3[\"Return early\"]\n        L1 --> L2\n        L2 -->|Yes| L3\n    end\n    \n    subgraph \"Phase 3: Loop (while tool calls exist)\"\n        LOOP1[\"Extract tool_info<br/>(function_call or tool_calls)\"]\n        LOOP2[\"call_tools_safely()<br/>(mutates tool_info)\"]\n        LOOP3[\"Append tool_info to messages\"]\n        LOOP4[\"generate_assistant_output_append()\"]\n        LOOP5[\"Log round output\"]\n        LOOP1 --> LOOP2 --> LOOP3 --> LOOP4 --> LOOP5\n        LOOP5 -.->|has tool calls| LOOP1\n    end\n    \n    subgraph \"Phase 4: Completion\"\n        F1[\"Log final answer\"]\n        F2[\"Return\"]\n        F1 --> F2\n    end\n    \n    S3 --> L1\n    L2 -->|No| LOOP1\n    LOOP5 -.->|no tool calls| F1\n```\n\n**Diagram: Complete Loop Phases**\n\nSources: [src/agent/deep_research.py:15-74]()\n\n## Error Handling and Edge Cases\n\nWhile the main loop structure is straightforward, several edge cases are handled:\n\n### Null Content Handling\n\n```python\nif assistant_output.content is None:\n    assistant_output.content = \"\"\n```\n\nThis prevents errors when the LLM returns no textual content (e.g., when only tool calls are present).\n\n### Both Formats Support\n\nThe loop explicitly checks and handles both `function_call` and `tool_calls` formats within the same iteration, supporting mixed-format responses if the LLM provider sends them.\n\n### Multiple Tool Calls per Round\n\nThe `tool_calls` format allows multiple tools to be called in a single LLM response. The loop handles this with:\n\n```python\nfor i in range(len(assistant_output.tool_calls)):\n```\n\nEach tool result is appended to `messages` separately, maintaining proper ordering for the LLM to understand.\n\nSources: [src/agent/deep_research.py:33-74]()\n\n## Integration with Other Components\n\nThe query processing loop integrates with several other system components:\n\n| Component | Integration Point | Purpose |\n|-----------|-------------------|---------|\n| Memory module | `memory.init_messages_with_system_prompt()` | Initialize conversation context |\n| LLM module | `llm.generate_assistant_output_append()` | Generate responses |\n| LLM module | `llm.has_tool_call()` / `llm.has_function_call()` | Detect tool usage |\n| Tool schema | `tool.schema.get_tools_schema()` | Provide tool definitions to LLM |\n| Action coordinator | `action.call_tools_safely()` | Execute tools with error handling |\n| Logging | `global_logger.info()` | Record execution trace |\n\nFor detailed information about each integration:\n- LLM integration specifics: [LLM Integration](#3.2)\n- Message initialization and context: [Memory and Context Management](#3.3)\n- Tool execution and error handling: [Action Coordination](#3.4)\n- Individual tool implementations: [Tool System](#4)\n\nSources: [src/agent/deep_research.py:1-13](), [src/agent/deep_research.py:15-74]()\n\n---\n\n# Page: LLM Integration\n\n# LLM Integration\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/agent/llm.py](src/agent/llm.py)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document describes how the algo_agent system integrates with Large Language Models (LLMs) to generate intelligent responses and coordinate tool executions. The LLM serves as the reasoning engine that interprets user queries, decides which tools to invoke, and synthesizes final answers.\n\nThe system uses the **qwen-plus** model hosted on Alibaba's DashScope platform, accessed through OpenAI-compatible APIs. This integration handles message formatting, tool schema injection, response parsing, and function calling protocols.\n\nFor information about how the agent processes queries and manages the decision loop, see [Query Processing Loop](#3.1). For details on conversation state management, see [Memory and Context Management](#3.3). For tool execution orchestration, see [Action Coordination](#3.4).\n\n---\n\n## API Configuration\n\nThe LLM client is initialized in [src/agent/llm.py:8-11]() using the OpenAI SDK with DashScope-specific configuration:\n\n```python\nclient = openai.OpenAI(\n    api_key=model_api_key,    \n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\n```\n\n| Configuration | Value | Purpose |\n|--------------|-------|---------|\n| **Model** | `qwen-plus` | Primary reasoning model |\n| **Base URL** | `https://dashscope.aliyuncs.com/compatible-mode/v1` | DashScope OpenAI-compatible endpoint |\n| **API Key** | From `secret.model_api_key` | Authentication token |\n| **Parallel Tool Calls** | `True` | Enables multiple tool invocations in single response |\n\nThe OpenAI-compatible interface allows seamless integration while using Alibaba's infrastructure.\n\n**Sources:** [src/agent/llm.py:1-11]()\n\n---\n\n## Core LLM Functions\n\nThe LLM integration module provides a layered abstraction with three primary functions:\n\n```mermaid\ngraph TB\n    Agent[\"Deep Research Agent<br/>deep_research.py\"]\n    \n    GenAppend[\"generate_assistant_output_append()<br/>Public Interface\"]\n    Extract[\"_extract_assistant_output_from_chat()<br/>Response Extraction\"]\n    GenCompletion[\"_generate_chat_completion()<br/>API Call\"]\n    \n    CheckTool[\"has_tool_call()<br/>Tool Detection\"]\n    CheckFunc[\"has_function_call()<br/>Function Detection\"]\n    \n    API[\"DashScope API<br/>qwen-plus model\"]\n    \n    Agent --> GenAppend\n    GenAppend --> Extract\n    Extract --> GenCompletion\n    GenCompletion --> API\n    \n    GenAppend --> CheckTool\n    GenAppend --> CheckFunc\n    \n    style GenAppend fill:#f9f9f9\n    style GenCompletion fill:#e8e8e8\n```\n\n### Function Hierarchy\n\n1. **`_generate_chat_completion(messages, tools_schema_list)`** [src/agent/llm.py:14-24]()\n   - Lowest level: Makes actual API call to DashScope\n   - Returns full `ChatCompletion` object\n   - Decorated with `@traceable` for detailed logging\n\n2. **`_extract_assistant_output_from_chat(messages, tools_schema_list)`** [src/agent/llm.py:27-31]()\n   - Extracts `ChatCompletionMessage` from completion response\n   - Returns only the assistant's message content\n\n3. **`generate_assistant_output_append(messages, tools_schema_list)`** [src/agent/llm.py:34-41]()\n   - Public interface used by agent\n   - Appends assistant response to messages list\n   - Ensures `content` field is never `None` (defaults to empty string)\n\n4. **`has_tool_call(assistant_output)`** [src/agent/llm.py:44-45]()\n   - Checks if response contains `tool_calls` field\n\n5. **`has_function_call(assistant_output)`** [src/agent/llm.py:48-49]()\n   - Checks if response contains legacy `function_call` field\n\n**Sources:** [src/agent/llm.py:14-49]()\n\n---\n\n## Message Format and Structure\n\nThe system uses the OpenAI chat completion message format. Messages maintain conversation state and include multiple roles:\n\n```mermaid\ngraph LR\n    subgraph \"Message Roles\"\n        System[\"system<br/>System prompt and instructions\"]\n        User[\"user<br/>User queries\"]\n        Assistant[\"assistant<br/>LLM responses + tool_calls\"]\n        Tool[\"tool<br/>Tool execution results\"]\n        Function[\"function<br/>Legacy function results\"]\n    end\n    \n    subgraph \"Message Flow\"\n        Init[\"init_messages_with_system_prompt()\"]\n        Messages[\"messages: list[dict]\"]\n        LLM[\"LLM Processing\"]\n        Append[\"append to messages\"]\n    end\n    \n    System --> Messages\n    User --> Messages\n    Messages --> LLM\n    LLM --> Assistant\n    Assistant --> Append\n    Tool --> Append\n    Function --> Append\n    Append --> Messages\n```\n\n### Message Structure\n\nEach message in the list follows this schema:\n\n| Field | Type | Description | Example Roles |\n|-------|------|-------------|---------------|\n| `role` | `str` | Message sender | `system`, `user`, `assistant`, `tool`, `function` |\n| `content` | `str` | Text content | User queries, assistant responses, tool outputs |\n| `tool_calls` | `list` | Tool invocation requests (optional) | Only in `assistant` messages |\n| `tool_call_id` | `str` | Unique identifier linking tool results (optional) | Only in `tool` messages |\n| `function_call` | `dict` | Legacy function call format (optional) | Only in `assistant` messages |\n\n### Example Message Sequence\n\nFrom [src/agent/deep_research.py:19-46](), a typical conversation flow:\n\n1. **System Message** - Initialized via `memory.init_messages_with_system_prompt(user_input)`\n2. **User Message** - Contains the original query\n3. **Assistant Message** - LLM response, may contain `tool_calls`\n4. **Tool Messages** - Results from tool executions, linked via `tool_call_id`\n5. **Assistant Message** - Final response after tool use\n\n**Sources:** [src/agent/deep_research.py:19-46](), [src/agent/llm.py:14-41]()\n\n---\n\n## LLM Call Flow\n\nThe following diagram shows the complete flow from agent query to LLM response:\n\n```mermaid\nsequenceDiagram\n    participant Agent as \"deep_research.py<br/>user_query()\"\n    participant GenAppend as \"llm.py<br/>generate_assistant_output_append()\"\n    participant Extract as \"llm.py<br/>_extract_assistant_output_from_chat()\"\n    participant GenComp as \"llm.py<br/>_generate_chat_completion()\"\n    participant API as \"DashScope API<br/>qwen-plus\"\n    \n    Agent->>Agent: \"init_messages_with_system_prompt()\"\n    Agent->>Agent: \"get_tools_schema()\"\n    \n    Agent->>GenAppend: \"generate_assistant_output_append(messages, tools_schema_list)\"\n    GenAppend->>GenAppend: \"Log separator (60 dashes)\"\n    GenAppend->>Extract: \"_extract_assistant_output_from_chat(messages, tools_schema_list)\"\n    \n    Extract->>GenComp: \"_generate_chat_completion(messages, tools_schema_list)\"\n    Note over GenComp: \"@traceable decorator<br/>logs all parameters\"\n    \n    GenComp->>API: \"client.chat.completions.create()\"\n    Note over GenComp,API: \"model='qwen-plus'<br/>tools=tools_schema_list<br/>parallel_tool_calls=True\"\n    \n    API-->>GenComp: \"ChatCompletion object\"\n    GenComp-->>Extract: \"completion\"\n    \n    Extract->>Extract: \"completion.choices[0].message\"\n    Extract-->>GenAppend: \"ChatCompletionMessage\"\n    \n    GenAppend->>GenAppend: \"Ensure content != None\"\n    GenAppend->>GenAppend: \"messages.append(assistant_output)\"\n    GenAppend-->>Agent: \"assistant_output\"\n    \n    Agent->>Agent: \"Check has_tool_call() or has_function_call()\"\n```\n\n### Key Parameters\n\nFrom [src/agent/llm.py:15-23](), the `_generate_chat_completion` function passes these parameters:\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `model` | `\"qwen-plus\"` | Specifies the LLM model to use |\n| `messages` | `list[dict]` | Complete conversation history |\n| `tools` | `tools_schema_list` | Available tool schemas in OpenAI format |\n| `function_call` | `None` | Disables legacy function calling |\n| `parallel_tool_calls` | `True` | Allows multiple simultaneous tool invocations |\n\n**Sources:** [src/agent/llm.py:14-24](), [src/agent/deep_research.py:26-29]()\n\n---\n\n## Tool Calling Mechanism\n\nThe LLM can request tool executions through two formats: **modern tool_calls** and **legacy function_call**.\n\n### Tool Calls Format (Modern)\n\nWhen the LLM decides to use tools, it returns a `ChatCompletionMessage` with a `tool_calls` field:\n\n```mermaid\ngraph TB\n    subgraph \"Assistant Response\"\n        AssistMsg[\"ChatCompletionMessage<br/>role='assistant'\"]\n        ToolCalls[\"tool_calls: list\"]\n        Content[\"content: str or None\"]\n    end\n    \n    subgraph \"Tool Call Structure\"\n        TC1[\"ToolCall[0]\"]\n        TC2[\"ToolCall[1]\"]\n        TCN[\"ToolCall[n]\"]\n    end\n    \n    subgraph \"ToolCall Fields\"\n        ID[\"id: str<br/>'call_abc123'\"]\n        Type[\"type: 'function'\"]\n        Func[\"function\"]\n        FuncName[\"name: 'ExecutePythonCodeTool'\"]\n        FuncArgs[\"arguments: JSON string\"]\n    end\n    \n    AssistMsg --> ToolCalls\n    AssistMsg --> Content\n    \n    ToolCalls --> TC1\n    ToolCalls --> TC2\n    ToolCalls --> TCN\n    \n    TC1 --> ID\n    TC1 --> Type\n    TC1 --> Func\n    \n    Func --> FuncName\n    Func --> FuncArgs\n```\n\nFrom [src/agent/deep_research.py:47-64](), the agent processes tool_calls:\n\n```python\nif llm.has_tool_call(assistant_output):\n    for i in range(len(assistant_output.tool_calls)):\n        tool_info = {\n            \"content\": \"\",\n            \"role\": \"tool\",\n            \"tool_call_id\": assistant_output.tool_calls[i].id,\n            # Non-required fields\n            \"tool_call_name\": assistant_output.tool_calls[i].function.name,\n            \"tool_call_arguments\": assistant_output.tool_calls[i].function.arguments,\n        }\n        action.call_tools_safely(tool_info)\n        messages.append(tool_info)\n```\n\n### Function Call Format (Legacy)\n\nThe system also supports the older `function_call` format [src/agent/deep_research.py:33-46]():\n\n```python\nif llm.has_function_call(assistant_output):\n    tool_info = {\n        \"content\": \"\",\n        \"role\": \"function\",\n        \"tool_call_id\": \"\",\n        # Non-required fields\n        \"tool_call_name\": assistant_output.function_call.name,\n        \"tool_call_arguments\": assistant_output.function_call.arguments,\n    }\n```\n\n### Tool Invocation Decision Logic\n\nThe agent checks both formats in its decision loop [src/agent/deep_research.py:32-65]():\n\n```mermaid\ngraph TD\n    Start[\"Assistant Response Received\"]\n    \n    CheckToolCall{\"has_tool_call()?\"}\n    CheckFuncCall{\"has_function_call()?\"}\n    \n    ProcessTool[\"Process tool_calls array<br/>Multiple tools possible\"]\n    ProcessFunc[\"Process function_call<br/>Single function\"]\n    \n    NoTools[\"No tool execution needed<br/>Return final answer\"]\n    \n    CallAction[\"action.call_tools_safely(tool_info)\"]\n    AppendMsg[\"messages.append(tool_info)\"]\n    NextRound[\"Next LLM round\"]\n    \n    Start --> CheckToolCall\n    Start --> CheckFuncCall\n    \n    CheckToolCall -->|Yes| ProcessTool\n    CheckFuncCall -->|Yes| ProcessFunc\n    \n    CheckToolCall -->|No| CheckFuncCall\n    CheckFuncCall -->|No| NoTools\n    \n    ProcessTool --> CallAction\n    ProcessFunc --> CallAction\n    \n    CallAction --> AppendMsg\n    AppendMsg --> NextRound\n```\n\n**Sources:** [src/agent/deep_research.py:27-74](), [src/agent/llm.py:44-49]()\n\n---\n\n## Response Handling and Content Management\n\n### Null Content Handling\n\nThe system ensures that `assistant_output.content` is never `None` to prevent downstream errors:\n\nFrom [src/agent/llm.py:38-40]():\n```python\nif assistant_output.content is None:\n    assistant_output.content = \"\"\nmessages.append(assistant_output)\n```\n\nThis is also enforced in the agent loop [src/agent/deep_research.py:66-68]():\n```python\nif assistant_output.content is None:\n    assistant_output.content = \"\"\n```\n\n### Message Appending Strategy\n\nThe `generate_assistant_output_append` function automatically appends the assistant's response to the messages list [src/agent/llm.py:40](). This maintains conversation continuity and ensures the LLM has full context for subsequent rounds.\n\n### Multi-Round Conversation Flow\n\n```mermaid\ngraph TB\n    Start[\"User Query\"]\n    InitMsg[\"Initialize messages with<br/>system prompt + user query\"]\n    \n    LLMCall[\"LLM generates response\"]\n    \n    CheckDone{\"has_tool_call OR<br/>has_function_call?\"}\n    \n    ExecuteTools[\"Execute requested tools<br/>Append tool results to messages\"]\n    \n    FinalAnswer[\"Return final answer to user\"]\n    \n    Start --> InitMsg\n    InitMsg --> LLMCall\n    \n    LLMCall --> CheckDone\n    \n    CheckDone -->|Yes| ExecuteTools\n    CheckDone -->|No| FinalAnswer\n    \n    ExecuteTools --> LLMCall\n```\n\nThe loop continues [src/agent/deep_research.py:32-65]() until the LLM generates a response without tool calls, indicated by the final log statement [src/agent/deep_research.py:73]():\n```python\nglobal_logger.info(f\"æç»ç­æ¡ï¼ {assistant_output.content}\")\n```\n\n**Sources:** [src/agent/llm.py:34-41](), [src/agent/deep_research.py:26-73]()\n\n---\n\n## Observability and Tracing\n\n### Function Tracing\n\nThe `_generate_chat_completion` function is decorated with `@traceable` [src/agent/llm.py:13](), which provides comprehensive logging via the log_function decorator system [src/utils/log_decorator.py:297-302]().\n\nThis captures:\n- Function call parameters (messages, tools_schema_list)\n- Execution timing\n- Return values\n- Exception traces\n\nTraces are written to `logs/trace.log` [src/utils/log_decorator.py:296]().\n\n### Agent-Level Logging\n\nThe agent logs key events using `global_logger` [src/agent/deep_research.py:4]():\n\n| Log Point | Location | Purpose |\n|-----------|----------|---------|\n| User input | [src/agent/deep_research.py:17]() | Records incoming query |\n| No tool needed | [src/agent/deep_research.py:28]() | Direct response without tools |\n| Tool output | [src/agent/deep_research.py:44,62]() | Tool execution results |\n| Round info | [src/agent/deep_research.py:68-72]() | Assistant response + tool_calls details |\n| Final answer | [src/agent/deep_research.py:73]() | Conversation conclusion |\n\n### Log Separator Pattern\n\nThe LLM module uses visual separators [src/agent/llm.py:35]():\n```python\nglobal_logger.info(\"-\" * 60)\n```\n\nThis creates clear boundaries in log files, making it easier to trace individual LLM calls.\n\n**Sources:** [src/agent/llm.py:13-35](), [src/agent/deep_research.py:4,17,28,44,62,68-73](), [src/utils/log_decorator.py:296-306]()\n\n---\n\n## Tool Schema Injection\n\nThe agent provides available tools to the LLM via the `tools_schema_list` parameter. From [src/agent/deep_research.py:20-23]():\n\n```python\ntools_schema_list = tool.schema.get_tools_schema([\n    tool.python_tool.ExecutePythonCodeTool,\n    # tool.todo_tool.RecursivePlanTreeTodoTool,\n])\n```\n\nThis schema list is passed to every LLM call [src/agent/deep_research.py:26,65](), enabling the model to understand available capabilities and invoke them appropriately.\n\nThe schema format follows OpenAI's function calling specification, including:\n- Tool name\n- Tool description\n- Parameter schemas with types and descriptions\n- Required vs. optional parameters\n\nFor details on schema generation, see [Tool Schema Format](#9.3).\n\n**Sources:** [src/agent/deep_research.py:20-26]()\n\n---\n\n## Error Handling\n\n### API Call Tracing\n\nThe `@traceable` decorator on `_generate_chat_completion` [src/agent/llm.py:13]() automatically captures and logs exceptions from the DashScope API, including:\n- Network errors\n- Authentication failures\n- Rate limiting\n- Invalid request formats\n\nFrom [src/utils/log_decorator.py:233-257](), when exceptions occur:\n```python\nexcept Exception as e:\n    # Record exception with full traceback\n    logger.error(\n        f\"ãè°ç¨å¤±è´¥ã æ è·¯å¾ï¼ {stack_full_path} | èæ¶ï¼ {elapsed_time:.3f}ms \"\n        f\"| å¼å¸¸ä½ç½®ï¼ {module_name}.{class_name}.{func_name}:{exc_lineno} \"\n        f\"| å¼å¸¸ç±»åï¼ {exception_type} | å¼å¸¸ä¿¡æ¯ï¼ {exception_msg} | å æ ä¿¡æ¯ï¼ {traceback_str}\",\n        exc_info=True\n    )\n    raise  # Exception is re-raised\n```\n\n### Null Safety\n\nThe system implements defensive null checking:\n- `assistant_output.content` is never `None` [src/agent/llm.py:38-39]()\n- This prevents `NoneType` errors when processing responses\n- Empty string `\"\"` is used as the default value\n\n### Tool Call Validation\n\nBefore processing tool calls, the agent validates their presence:\n- `has_tool_call(assistant_output)` [src/agent/llm.py:44-45]()\n- `has_function_call(assistant_output)` [src/agent/llm.py:48-49]()\n\nThese simple boolean checks prevent attribute errors when accessing `tool_calls` or `function_call` fields that may be `None`.\n\n**Sources:** [src/agent/llm.py:38-49](), [src/utils/log_decorator.py:233-257]()\n\n---\n\n## Integration with Agent Components\n\nThe LLM module serves as the central reasoning component, interacting with multiple subsystems:\n\n```mermaid\ngraph TB\n    subgraph \"llm.py Module\"\n        LLMGen[\"generate_assistant_output_append()\"]\n        LLMCheck[\"has_tool_call()<br/>has_function_call()\"]\n    end\n    \n    subgraph \"Agent Components\"\n        Agent[\"deep_research.py<br/>user_query()\"]\n        Memory[\"memory.py<br/>init_messages_with_system_prompt()\"]\n        Action[\"action.py<br/>call_tools_safely()\"]\n        Schema[\"tool.schema.py<br/>get_tools_schema()\"]\n    end\n    \n    subgraph \"External Services\"\n        DashScope[\"DashScope API<br/>qwen-plus model\"]\n    end\n    \n    subgraph \"Logging\"\n        GlobalLog[\"global_logger<br/>logs/print.log\"]\n        TraceLog[\"traceable_logger<br/>logs/trace.log\"]\n    end\n    \n    Agent --> Memory\n    Agent --> Schema\n    \n    Memory --> Agent\n    Schema --> Agent\n    \n    Agent --> LLMGen\n    LLMGen --> DashScope\n    DashScope --> LLMGen\n    \n    LLMGen --> Agent\n    Agent --> LLMCheck\n    LLMCheck --> Agent\n    \n    Agent --> Action\n    \n    LLMGen --> GlobalLog\n    LLMGen --> TraceLog\n```\n\n### Component Interactions\n\n1. **Memory Management** - `memory.init_messages_with_system_prompt()` initializes conversation state (see [Memory and Context Management](#3.3))\n\n2. **Tool Schema** - `tool.schema.get_tools_schema()` provides tool definitions to the LLM (see [Tool Schema Format](#9.3))\n\n3. **Action Coordination** - `action.call_tools_safely()` executes tools requested by the LLM (see [Action Coordination](#3.4))\n\n4. **Logging** - Both `global_logger` and `@traceable` decorator provide comprehensive observability (see [Logging System Architecture](#6.1))\n\n**Sources:** [src/agent/deep_research.py:1-74](), [src/agent/llm.py:1-49]()\n\n---\n\n## Summary\n\nThe LLM Integration layer provides a clean abstraction over the DashScope API with these key features:\n\n| Feature | Implementation | Benefit |\n|---------|---------------|---------|\n| **OpenAI Compatibility** | Uses OpenAI SDK with DashScope endpoint | Standard interface, easy to swap models |\n| **Tool Calling** | Supports both `tool_calls` and `function_call` | Flexible integration, backward compatible |\n| **Parallel Execution** | `parallel_tool_calls=True` | Multiple tools in single round |\n| **Null Safety** | Automatic `None` to `\"\"` conversion | Prevents downstream errors |\n| **Comprehensive Logging** | `@traceable` + `global_logger` | Full observability of LLM interactions |\n| **Stateless Design** | Messages passed explicitly | Clear data flow, easy to debug |\n\nThe module's simplicity and clear separation of concerns make it easy to understand, maintain, and extend.\n\n**Sources:** [src/agent/llm.py:1-49](), [src/agent/deep_research.py:1-74]()\n\n---\n\n# Page: Memory and Context Management\n\n# Memory and Context Management\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/functino_call_err.design.md](docs/functino_call_err.design.md)\n- [src/agent/action.py](src/agent/action.py)\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/agent/memory.py](src/agent/memory.py)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page describes how the algo_agent system maintains conversation history, initializes system prompts, and manages context that is passed to the LLM. The memory system uses a messages list following OpenAI's chat completion format to track the entire conversation flow including user queries, assistant responses, and tool execution results.\n\nFor information about the broader query processing loop that uses this memory system, see [Query Processing Loop](#3.1). For details on how the LLM consumes these messages, see [LLM Integration](#3.2). For tool execution result handling, see [Action Coordination](#3.4).\n\n---\n\n## Core Concepts\n\nThe memory system is built around a single, append-only **messages list** that serves as the complete conversation history. This list is initialized at the start of each query and grows as the agent interacts with the LLM and tools. The system follows OpenAI's message format specification to ensure compatibility with the underlying LLM API.\n\n### Message List Structure\n\nThe `messages` list is a Python list of dictionaries, where each dictionary represents a single message in the conversation. The list maintains strict orderingâmessages appear in chronological order, and the LLM processes them sequentially to build context.\n\n**Core Message Roles:**\n\n| Role | Purpose | Required Fields | Created By |\n|------|---------|----------------|------------|\n| `system` | Sets the agent's behavior and instructions | `role`, `content` | `init_messages_with_system_prompt` |\n| `user` | Contains the user's query or input | `role`, `content` | `init_messages_with_system_prompt`, user input |\n| `assistant` | LLM's response, may include tool calls | `role`, `content`, `tool_calls` (optional) | `generate_assistant_output_append` |\n| `tool` | Result from tool execution (modern format) | `role`, `content`, `tool_call_id` | `call_tools_safely` |\n| `function` | Result from tool execution (legacy format) | `role`, `content`, `tool_call_id` | `call_tools_safely` |\n\nSources: [src/agent/deep_research.py:1-74](), [src/agent/memory.py:1-17]()\n\n---\n\n## System Initialization\n\n### Message Initialization with System Prompt\n\nThe entry point for memory management is the `init_messages_with_system_prompt` function, which creates the initial messages list with exactly two messages: a system message and a user message.\n\n```mermaid\ngraph TB\n    INPUT[\"user_input<br/>(user query string)\"]\n    FUNC[\"init_messages_with_system_prompt()\"]\n    SYSTEMPROMPT[\"react_system_prompt<br/>(imported from prompt module)\"]\n    \n    MSG1[\"Message 1<br/>role: system<br/>content: react_system_prompt\"]\n    MSG2[\"Message 2<br/>role: user<br/>content: user_input\"]\n    \n    OUTPUT[\"messages list<br/>[system_msg, user_msg]\"]\n    \n    INPUT --> FUNC\n    SYSTEMPROMPT --> FUNC\n    FUNC --> MSG1\n    FUNC --> MSG2\n    MSG1 --> OUTPUT\n    MSG2 --> OUTPUT\n    \n    style FUNC fill:#f9f9f9\n    style OUTPUT fill:#f9f9f9\n```\n\n**Function Implementation:**\n\nThe `init_messages_with_system_prompt` function is defined in [src/agent/memory.py:4-16]() as:\n\n```python\ndef init_messages_with_system_prompt(user_input: str) -> list[dict[str, str]]:\n    messages = [\n        {\n            \"content\": react_system_prompt,\n            \"role\": \"system\",\n        },\n        {\n            \"content\": user_input,\n            \"role\": \"user\",\n        }\n    ]\n    return messages\n```\n\n**Key Characteristics:**\n\n- **System Prompt Source**: Imported from `prompt.react_system_prompt` module (not shown in provided files)\n- **Return Type**: Returns a list of dictionaries with string keys and values\n- **Immutability**: Creates a new list each time; does not maintain global state\n- **Logging**: Decorated with `@traceable` for execution tracking\n\nSources: [src/agent/memory.py:1-17]()\n\n---\n\n## Message Flow and Lifecycle\n\n### Message Appending Pattern\n\nThroughout the agent's execution loop, messages are appended to the list using Python's `list.append()` method. The system follows a strict alternating pattern to maintain LLM API compatibility.\n\n```mermaid\nsequenceDiagram\n    participant Init as init_messages_with_system_prompt\n    participant MsgList as messages list\n    participant LLM as generate_assistant_output_append\n    participant Action as call_tools_safely\n    \n    Note over Init,MsgList: Initialization Phase\n    Init->>MsgList: Append system message (role=system)\n    Init->>MsgList: Append user message (role=user)\n    \n    Note over MsgList,LLM: First LLM Call\n    LLM->>MsgList: Append assistant message<br/>(role=assistant, may have tool_calls)\n    \n    Note over MsgList,Action: Tool Execution Phase (if tool_calls present)\n    loop For each tool_call\n        Action->>MsgList: Append tool result<br/>(role=tool, tool_call_id=xxx)\n    end\n    \n    Note over MsgList,LLM: Subsequent LLM Calls\n    LLM->>MsgList: Append assistant message\n    \n    alt More tool calls\n        loop For each tool_call\n            Action->>MsgList: Append tool result\n        end\n        LLM->>MsgList: Append assistant message\n    else No tool calls\n        Note over LLM: Loop terminates\n    end\n```\n\n**Append Locations in Code:**\n\n1. **Assistant Messages**: Appended within `generate_assistant_output_append` in the LLM module (not shown in provided files, but called at [src/agent/deep_research.py:26]())\n\n2. **Tool Result Messages**: Appended in the main loop at [src/agent/deep_research.py:46]() (function call format) and [src/agent/deep_research.py:64]() (tool call format)\n\nSources: [src/agent/deep_research.py:15-74]()\n\n---\n\n## Message Schema and Structure\n\n### Standard Message Fields\n\nEach message dictionary in the list follows a specific schema based on its role. Here's the detailed structure:\n\n#### System Message\n```python\n{\n    \"role\": \"system\",\n    \"content\": \"<system prompt text>\"\n}\n```\n\n#### User Message\n```python\n{\n    \"role\": \"user\",\n    \"content\": \"<user query text>\"\n}\n```\n\n#### Assistant Message (without tool calls)\n```python\n{\n    \"role\": \"assistant\",\n    \"content\": \"<assistant response text>\",\n    \"tool_calls\": None  # or not present\n}\n```\n\n#### Assistant Message (with tool calls)\n```python\n{\n    \"role\": \"assistant\",\n    \"content\": \"<optional reasoning text>\",\n    \"tool_calls\": [\n        {\n            \"id\": \"<tool_call_id>\",\n            \"function\": {\n                \"name\": \"<tool_name>\",\n                \"arguments\": \"<json_string>\"\n            }\n        }\n    ]\n}\n```\n\n#### Tool Result Message\n```python\n{\n    \"role\": \"tool\",\n    \"content\": \"<tool execution result>\",\n    \"tool_call_id\": \"<matching_id_from_assistant_message>\",\n    # Additional fields (not required by LLM API):\n    \"tool_call_name\": \"<tool_name>\",\n    \"tool_call_arguments\": \"<json_string>\"\n}\n```\n\nSources: [src/agent/deep_research.py:34-64](), [src/agent/action.py:10-21]()\n\n---\n\n## Tool Information Dictionary\n\n### Tool Info Structure\n\nDuring tool execution, the system uses a `tool_info` dictionary to track tool execution metadata. This structure bridges between the assistant's tool call and the resulting tool message.\n\n```mermaid\ngraph LR\n    subgraph \"Assistant Message\"\n        TOOLCALL[\"tool_calls[i]<br/>id, function.name, function.arguments\"]\n    end\n    \n    subgraph \"Tool Info Dictionary\"\n        TOOLINFO[\"tool_info dict<br/>role, tool_call_id, content<br/>tool_call_name, tool_call_arguments\"]\n    end\n    \n    subgraph \"Action Module\"\n        CALLTOOL[\"call_tools_safely()<br/>Executes tool, updates content\"]\n    end\n    \n    subgraph \"Messages List\"\n        TOOLMSG[\"Tool result message<br/>Appended to messages\"]\n    end\n    \n    TOOLCALL -->|\"Extract fields\"| TOOLINFO\n    TOOLINFO --> CALLTOOL\n    CALLTOOL -->|\"Update content field\"| TOOLINFO\n    TOOLINFO -->|\"Append\"| TOOLMSG\n```\n\n**Tool Info Creation for Modern Format (tool_calls):**\n\nAt [src/agent/deep_research.py:50-57](), the system creates:\n\n```python\ntool_info = {\n    \"content\": \"\",  # Empty initially, filled by tool execution\n    \"role\": \"tool\",\n    \"tool_call_id\": assistant_output.tool_calls[i].id,\n    # Additional tracking fields:\n    \"tool_call_name\": assistant_output.tool_calls[i].function.name,\n    \"tool_call_arguments\": assistant_output.tool_calls[i].function.arguments,\n}\n```\n\n**Tool Info Creation for Legacy Format (function_call):**\n\nAt [src/agent/deep_research.py:34-41](), for backward compatibility:\n\n```python\ntool_info = {\n    \"content\": \"\",\n    \"role\": \"function\",\n    \"tool_call_id\": \"\",  # Empty for function_call format\n    \"tool_call_name\": assistant_output.function_call.name,\n    \"tool_call_arguments\": assistant_output.function_call.arguments,\n}\n```\n\nSources: [src/agent/deep_research.py:32-64](), [src/agent/action.py:1-48]()\n\n---\n\n## Context Management Flow\n\n### Complete Context Pipeline\n\nThe following diagram shows how context flows from initialization through multiple agent loops:\n\n```mermaid\ngraph TB\n    subgraph \"Initialization\"\n        USERINPUT[\"User Input String\"]\n        INITMSG[\"init_messages_with_system_prompt()\"]\n        INITLIST[\"Initial messages list<br/>[system, user]\"]\n    end\n    \n    subgraph \"LLM Context Building\"\n        MSGLIST[\"messages list<br/>(growing over time)\"]\n        TOOLSCHEMA[\"tools_schema_list<br/>(available tools)\"]\n        LLMCONTEXT[\"LLM Context Package<br/>messages + tools\"]\n    end\n    \n    subgraph \"LLM Processing\"\n        LLM[\"generate_assistant_output_append()\"]\n        ASSISTMSG[\"assistant_output<br/>(ChatCompletionMessage)\"]\n    end\n    \n    subgraph \"Tool Execution Context\"\n        TOOLCALL[\"tool_calls extraction\"]\n        TOOLINFO[\"tool_info dict\"]\n        TOOLEXEC[\"call_tools_safely()\"]\n        TOOLRESULT[\"Tool execution result\"]\n    end\n    \n    subgraph \"Context Update\"\n        APPEND[\"messages.append()\"]\n        UPDATEDLIST[\"Updated messages list<br/>(includes new assistant & tool messages)\"]\n    end\n    \n    USERINPUT --> INITMSG\n    INITMSG --> INITLIST\n    INITLIST --> MSGLIST\n    \n    MSGLIST --> LLMCONTEXT\n    TOOLSCHEMA --> LLMCONTEXT\n    LLMCONTEXT --> LLM\n    LLM --> ASSISTMSG\n    \n    ASSISTMSG --> APPEND\n    ASSISTMSG --> TOOLCALL\n    TOOLCALL --> TOOLINFO\n    TOOLINFO --> TOOLEXEC\n    TOOLEXEC --> TOOLRESULT\n    TOOLRESULT --> TOOLINFO\n    TOOLINFO --> APPEND\n    \n    APPEND --> UPDATEDLIST\n    UPDATEDLIST --> MSGLIST\n    \n    style MSGLIST fill:#f9f9f9\n    style LLMCONTEXT fill:#f9f9f9\n```\n\n**Key Flow Points:**\n\n1. **Initialization**: [src/agent/deep_research.py:19]() - `messages = memory.init_messages_with_system_prompt(user_input)`\n2. **Tool Schema Addition**: [src/agent/deep_research.py:20-23]() - Tools schema is passed alongside messages but not stored in messages\n3. **LLM Call**: [src/agent/deep_research.py:26]() - `llm.generate_assistant_output_append(messages, tools_schema_list)`\n4. **Tool Result Append**: [src/agent/deep_research.py:46]() and [src/agent/deep_research.py:64]() - `messages.append(tool_info)`\n5. **Loop Continuation**: [src/agent/deep_research.py:65]() - Next LLM call uses updated messages list\n\nSources: [src/agent/deep_research.py:15-74]()\n\n---\n\n## Context Preservation Across Iterations\n\n### Stateless Function with Stateful Data\n\nThe memory management design follows a **stateless function, stateful data** pattern:\n\n- **Stateless**: The `init_messages_with_system_prompt` function and `call_tools_safely` functions don't maintain internal state\n- **Stateful**: The `messages` list accumulates all conversation history and is passed by reference\n\n```mermaid\ngraph LR\n    subgraph \"Iteration 1\"\n        MSG1[\"messages<br/>[system, user]\"]\n        LLM1[\"LLM Call 1\"]\n        MSG1A[\"messages<br/>[system, user, assistant1]\"]\n        TOOL1[\"Tool Call\"]\n        MSG1B[\"messages<br/>[..., assistant1, tool1]\"]\n    end\n    \n    subgraph \"Iteration 2\"\n        LLM2[\"LLM Call 2\"]\n        MSG2A[\"messages<br/>[..., tool1, assistant2]\"]\n        TOOL2[\"Tool Call\"]\n        MSG2B[\"messages<br/>[..., assistant2, tool2]\"]\n    end\n    \n    subgraph \"Iteration N\"\n        LLMN[\"LLM Call N\"]\n        MSGN[\"messages<br/>[..., assistantN]\"]\n        DONE[\"No tool calls<br/>Done\"]\n    end\n    \n    MSG1 --> LLM1\n    LLM1 --> MSG1A\n    MSG1A --> TOOL1\n    TOOL1 --> MSG1B\n    MSG1B --> LLM2\n    LLM2 --> MSG2A\n    MSG2A --> TOOL2\n    TOOL2 --> MSG2B\n    MSG2B -->|\"Loop continues\"| LLMN\n    LLMN --> MSGN\n    MSGN --> DONE\n    \n    style MSG1 fill:#f9f9f9\n    style MSG1B fill:#f9f9f9\n    style MSG2B fill:#f9f9f9\n    style MSGN fill:#f9f9f9\n```\n\n**Reference Passing:**\n\nThe messages list is passed by reference throughout the execution:\n- LLM module receives the list and appends assistant messages\n- Tool execution results are appended in the main loop\n- Each subsequent LLM call sees the accumulated history\n- No explicit state management or persistence layer is needed\n\nSources: [src/agent/deep_research.py:15-74]()\n\n---\n\n## Error Handling and Context Validation\n\n### Message Format Validation\n\nThe system relies on LLM API validation for message format correctness. When messages are malformed, the API returns specific error codes.\n\n**Common Error: Tool Call Response Mismatch**\n\nA critical error occurs when assistant messages with `tool_calls` are not followed by corresponding tool response messages. This is documented in [docs/functino_call_err.design.md:1-51]():\n\n```\nError code: 400 - An assistant message with \"tool_calls\" must be followed by \ntool messages responding to each \"tool_call_id\". The following tool_call_ids \ndid not have response messages: message[7].role\n```\n\n**Root Cause**: At [src/agent/deep_research.py:32-46](), if a message has `function_call` (legacy format), the system creates a role `\"function\"` message. However, if the LLM uses modern `tool_calls` format, the API expects role `\"tool\"` messages with matching `tool_call_id` values.\n\n**Prevention Strategy:**\n\nThe current code handles both formats:\n- Modern format: [src/agent/deep_research.py:47-64]() properly sets `role: \"tool\"` and `tool_call_id`\n- Legacy format: [src/agent/deep_research.py:32-46]() sets `role: \"function\"` (may cause issues if LLM sends `tool_calls`)\n\nSources: [docs/functino_call_err.design.md:1-51](), [src/agent/deep_research.py:32-64]()\n\n---\n\n## Logging and Observability\n\n### Memory Operations Tracing\n\nThe memory system integrates with the logging architecture to track all memory operations.\n\n**Logged Operations:**\n\n1. **Initialization**: The `init_messages_with_system_prompt` function is decorated with `@traceable` at [src/agent/memory.py:4]()\n2. **User Input**: Logged at [src/agent/deep_research.py:17]() with `global_logger.info`\n3. **Tool Outputs**: Logged at [src/agent/deep_research.py:44]() and [src/agent/deep_research.py:62]()\n4. **Assistant Outputs**: Logged at [src/agent/deep_research.py:68-72]() with detailed content and tool_calls\n\n**Log Decorator Behavior:**\n\nThe `@traceable` decorator (defined at [src/utils/log_decorator.py:297-302]()) automatically logs:\n- Function entry with all arguments (the `user_input` string)\n- Function exit with return value (the initialized messages list)\n- Execution time\n- Call stack for debugging\n\n**Log File Locations:**\n\nPer [src/utils/log_decorator.py:288-306]():\n- `logs/trace.log` - Detailed execution traces including `init_messages_with_system_prompt` calls\n- `logs/print.log` - User-facing logs including tool outputs and final answers\n- `logs/all.log` - Comprehensive logs of all operations\n\nSources: [src/agent/memory.py:1-17](), [src/agent/deep_research.py:15-74](), [src/utils/log_decorator.py:287-307]()\n\n---\n\n## Context Size and Memory Limits\n\n### Unbounded Growth Pattern\n\nThe current implementation uses an **unbounded append-only list**, meaning:\n\n- No automatic truncation or summarization of old messages\n- Full conversation history is sent to the LLM on every call\n- Context window limits are enforced by the LLM API, not the agent\n\n**Implications:**\n\n| Aspect | Behavior |\n|--------|----------|\n| Memory Usage | Grows linearly with conversation length |\n| Context Window | Limited by LLM API (model-specific, typically 8k-32k tokens) |\n| Token Costs | Increases with each iteration (full history resent) |\n| Truncation | Not implemented; will fail when exceeding API limits |\n\n**Potential Improvements** (not currently implemented):\n\n- Implement sliding window context retention\n- Add message summarization for long conversations\n- Implement token counting and automatic truncation\n- Use context compression techniques\n\nSources: [src/agent/deep_research.py:15-74](), [src/agent/memory.py:1-17]()\n\n---\n\n## Integration with Execution State\n\n### Relationship to Workspace State\n\nThe conversation memory (messages list) is separate from but complementary to the execution state (workspace globals). See [Workspace State Management](#5.5) for details on execution state.\n\n**Key Differences:**\n\n| Aspect | Conversation Memory | Execution State |\n|--------|---------------------|-----------------|\n| **Data Structure** | `messages` list (dicts) | `arg_globals_list` / `out_globals_list` |\n| **Purpose** | Track conversation history | Persist Python variables across executions |\n| **Scope** | Entire agent session | Individual tool executions |\n| **Serialization** | JSON-serializable strings | Pickleable Python objects |\n| **Location** | [src/agent/memory.py]() | [src/agent/tool/python_tool.py]() |\n\n**Interaction Points:**\n\n1. Tool execution results (from workspace state) are converted to strings and added to conversation memory\n2. The LLM's reasoning (in conversation memory) determines which code to execute (which updates workspace state)\n3. Both are maintained in parallel but independently\n\nSources: [src/agent/memory.py:1-17](), [src/agent/deep_research.py:15-74]()\n\n---\n\n## Summary\n\nThe memory and context management system in algo_agent is designed around a simple, append-only messages list that follows OpenAI's chat completion format. Key characteristics:\n\n- **Initialization**: `init_messages_with_system_prompt` creates the initial `[system, user]` message pair\n- **Growth Pattern**: Messages are appended using `messages.append()` throughout the agent loop\n- **Message Roles**: System, user, assistant, tool (modern), and function (legacy)\n- **Context Passing**: The full messages list is passed to the LLM on each iteration\n- **State Management**: Stateless functions operating on a stateful data structure (the messages list)\n- **Integration**: Works alongside tool schema and workspace state systems\n- **Observability**: Full logging via `@traceable` decorator and `global_logger`\n\nThe system prioritizes simplicity and API compatibility over optimization, making it easy to understand and debug but potentially inefficient for very long conversations.\n\nSources: [src/agent/memory.py:1-17](), [src/agent/deep_research.py:1-74](), [src/agent/action.py:1-48]()\n\n---\n\n# Page: Action Coordination\n\n# Action Coordination\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/functino_call_err.design.md](docs/functino_call_err.design.md)\n- [src/agent/action.py](src/agent/action.py)\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/agent/memory.py](src/agent/memory.py)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThe Action Coordination subsystem is responsible for safely dispatching tool calls from the LLM to concrete tool implementations. It serves as the bridge between the agent's decision loop and the tool execution layer, providing error handling, logging, and result formatting.\n\nThis page covers the `call_tools_safely` function and the tool dispatch mechanism. For information about the agent's decision loop and how tool calls are generated, see [Query Processing Loop](#3.1). For details on individual tool implementations, see [Tool System](#4).\n\n---\n\n## Core Components\n\n### The `call_tools_safely` Function\n\nThe action coordinator is implemented primarily through the `call_tools_safely` function in [src/agent/action.py:10-47](), which acts as the central dispatch point for all tool executions in the system.\n\n**Function Signature:**\n```python\n@traceable\ndef call_tools_safely(tool_info: dict)\n```\n\nThe function accepts a `tool_info` dictionary that is mutated in-place to store the execution result in the `\"content\"` field. This design allows the agent to pass tool results directly back to the LLM without additional data structure transformations.\n\nSources: [src/agent/action.py:1-49]()\n\n---\n\n### The tool_info Dictionary Structure\n\nThe `tool_info` dictionary serves as the primary data structure for tool coordination, containing both input parameters and execution results:\n\n| Field | Type | Purpose | Required |\n|-------|------|---------|----------|\n| `tool_call_id` | str | Unique identifier for tracking tool calls in conversation | Yes |\n| `role` | str | Either `\"tool\"` or `\"function\"` depending on API format | Yes |\n| `tool_call_name` | str | Name of the tool to invoke (e.g., `\"execute_python_code\"`) | Yes |\n| `tool_call_arguments` | str | JSON-encoded string of tool parameters | Yes |\n| `content` | str | Tool execution result or error message (populated by dispatcher) | Modified |\n\n**Example tool_info Dictionary:**\n```python\ntool_info = {\n    \"content\": \"\",  # Will be populated with result\n    \"role\": \"tool\",\n    \"tool_call_id\": \"call_abc123\",\n    \"tool_call_name\": \"execute_python_code\",\n    \"tool_call_arguments\": '{\"python_code_snippet\": \"print(1+1)\", ...}'\n}\n```\n\nSources: [src/agent/deep_research.py:34-41](), [src/agent/deep_research.py:50-57]()\n\n---\n\n## Tool Dispatch Flow\n\n### Dispatch Mechanism\n\nThe dispatcher uses a simple if-elif chain to route tool calls based on the `tool_call_name` field. This design prioritizes simplicity and debuggability over extensibility.\n\n```mermaid\ngraph TD\n    A[\"call_tools_safely(tool_info)\"] --> B[\"Extract function_name<br/>from tool_info\"]\n    B --> C[\"Parse arguments JSON\"]\n    C --> D{\"Match function_name\"}\n    \n    D -->|\"execute_python_code\"| E[\"Instantiate<br/>ExecutePythonCodeTool\"]\n    D -->|\"update_recursive_plan_tree\"| F[\"Instantiate<br/>RecursivePlanTreeTodoTool\"]\n    D -->|\"Unknown\"| G[\"No match<br/>(falls through)\"]\n    \n    E --> H[\"Call tool.run()\"]\n    F --> H\n    G --> I[\"tool_info['content']<br/>remains empty\"]\n    \n    H --> J[\"Store result in<br/>tool_info['content']\"]\n    J --> K[\"Return tool_info\"]\n    I --> K\n```\n\n**Dispatch Implementation:**\n```python\ndef call_tools(tool_info: dict):\n    function_name = tool_info[\"tool_call_name\"]\n    arguments = json.loads(tool_info[\"tool_call_arguments\"])\n    \n    if function_name == ExecutePythonCodeTool.tool_name():\n        execute_python_code_tool = ExecutePythonCodeTool(**arguments)\n        tool_info[\"content\"] = execute_python_code_tool.run()\n    elif function_name == RecursivePlanTreeTodoTool.tool_name():\n        recursive_plan_tree_todo_tool = RecursivePlanTreeTodoTool(**arguments)\n        tool_info[\"content\"] = recursive_plan_tree_todo_tool.run()\n    \n    return tool_info\n```\n\nSources: [src/agent/action.py:11-21]()\n\n---\n\n### Tool Instantiation Pattern\n\nEach tool is instantiated dynamically using the parsed arguments dictionary. This leverages Python's `**kwargs` syntax to map JSON parameters to tool constructor arguments:\n\n```mermaid\nsequenceDiagram\n    participant Dispatcher as \"call_tools\"\n    participant JSON as \"json.loads\"\n    participant Tool as \"Tool Class\"\n    participant Instance as \"Tool Instance\"\n    \n    Dispatcher->>JSON: Parse tool_call_arguments\n    JSON-->>Dispatcher: arguments dict\n    Dispatcher->>Tool: Tool(**arguments)\n    Tool->>Instance: __init__ with kwargs\n    Instance-->>Dispatcher: tool instance\n    Dispatcher->>Instance: run()\n    Instance-->>Dispatcher: result string\n    Dispatcher->>Dispatcher: tool_info[\"content\"] = result\n```\n\nThis pattern requires that tool classes implement a constructor that accepts all parameters defined in their tool schema. See [BaseTool Interface](#4.1) for details on tool schema definitions.\n\nSources: [src/agent/action.py:15-20]()\n\n---\n\n## Error Handling and Recovery\n\n### Exception Capture Strategy\n\nThe `call_tools_safely` function wraps the entire dispatch logic in a try-except block that catches all exceptions, ensuring the agent loop never crashes due to tool failures:\n\n```mermaid\ngraph TB\n    A[\"call_tools_safely(tool_info)\"] --> B[\"try:\"]\n    B --> C[\"call_tools(tool_info)\"]\n    C --> D{\"Exception?\"}\n    \n    D -->|\"No Exception\"| E[\"Return tool_info<br/>with success result\"]\n    \n    D -->|\"Exception Raised\"| F[\"traceback.format_exc()\"]\n    F --> G[\"Log error with<br/>global_logger.error()\"]\n    G --> H[\"Set tool_info['content']<br/>to error message\"]\n    H --> I[\"Return tool_info<br/>with error\"]\n    \n    style F fill:#ffebee\n    style G fill:#ffebee\n    style H fill:#ffebee\n    style I fill:#ffebee\n```\n\n**Error Handling Implementation:**\n```python\ntry:\n    return call_tools(tool_info)\nexcept Exception as e:\n    error_msg = traceback.format_exc()\n    global_logger.error(\n        f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\", \n        exc_info=True\n    )\n    tool_info[\"content\"] = f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\"\n    return tool_info\n```\n\nSources: [src/agent/action.py:40-47]()\n\n---\n\n### Error Message Format\n\nWhen an exception occurs, the error message is formatted to include:\n\n1. **Contextual prefix**: `\"å·¥å·å½æ°è°ç¨å¤±è´¥\"` (Tool function call failed)\n2. **Original content**: The partial content from `tool_info['content']` (usually empty)\n3. **Full traceback**: Complete stack trace from `traceback.format_exc()`\n\nThis comprehensive error format ensures that:\n- The LLM receives actionable error information to adjust its approach\n- Developers can debug tool failures without accessing logs\n- The conversation history maintains a complete audit trail\n\n**Example Error Message in tool_info:**\n```\nå·¥å·å½æ°è°ç¨å¤±è´¥, éè¯¯ä¿¡æ¯: Traceback (most recent call last):\n  File \"src/agent/action.py\", line 41, in call_tools_safely\n    return call_tools(tool_info)\n  File \"src/agent/action.py\", line 13, in call_tools\n    arguments = json.loads(tool_info[\"tool_call_arguments\"])\n  File \"json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n```\n\nSources: [src/agent/action.py:43-46]()\n\n---\n\n## Integration with Agent Loop\n\n### Call Sites in Agent Decision Loop\n\nThe action coordinator is invoked at two points in the agent's decision loop to handle both legacy `function_call` and modern `tool_calls` API formats:\n\n```mermaid\nsequenceDiagram\n    participant Agent as \"user_query\"\n    participant LLM as \"generate_assistant_output_append\"\n    participant Action as \"call_tools_safely\"\n    participant Memory as \"messages list\"\n    \n    Agent->>LLM: Generate response with tools\n    LLM-->>Agent: assistant_output\n    \n    alt has_function_call (legacy)\n        Agent->>Agent: Build tool_info with role=\"function\"\n        Agent->>Action: call_tools_safely(tool_info)\n        Action-->>Agent: tool_info with content\n        Agent->>Memory: Append tool_info\n    end\n    \n    alt has_tool_call (modern)\n        loop For each tool_call\n            Agent->>Agent: Build tool_info with role=\"tool\"\n            Agent->>Action: call_tools_safely(tool_info)\n            Action-->>Agent: tool_info with content\n            Agent->>Memory: Append tool_info\n        end\n    end\n    \n    Agent->>LLM: Continue with updated messages\n```\n\n**Function Call Handler** [src/agent/deep_research.py:33-46]():\n```python\nif llm.has_function_call(assistant_output):\n    tool_info = {\n        \"content\": \"\",\n        \"role\": \"function\",\n        \"tool_call_id\": \"\",\n        \"tool_call_name\": assistant_output.function_call.name,\n        \"tool_call_arguments\": assistant_output.function_call.arguments,\n    }\n    action.call_tools_safely(tool_info)\n    messages.append(tool_info)\n```\n\n**Tool Calls Handler** [src/agent/deep_research.py:47-64]():\n```python\nif llm.has_tool_call(assistant_output):\n    for i in range(len(assistant_output.tool_calls)):\n        tool_info = {\n            \"content\": \"\",\n            \"role\": \"tool\",\n            \"tool_call_id\": assistant_output.tool_calls[i].id,\n            \"tool_call_name\": assistant_output.tool_calls[i].function.name,\n            \"tool_call_arguments\": assistant_output.tool_calls[i].function.arguments,\n        }\n        action.call_tools_safely(tool_info)\n        messages.append(tool_info)\n```\n\nSources: [src/agent/deep_research.py:32-64]()\n\n---\n\n### Message Flow and State Mutation\n\nThe action coordinator follows a **mutation-based design** where the `tool_info` dictionary is modified in-place rather than returning a new object. This design choice has several implications:\n\n```mermaid\ngraph LR\n    A[\"Agent creates<br/>empty tool_info\"] --> B[\"call_tools_safely\"]\n    B --> C[\"Mutate content field\"]\n    C --> D[\"Return same object\"]\n    D --> E[\"Agent appends<br/>to messages\"]\n    E --> F[\"LLM receives<br/>tool result\"]\n    \n    style C fill:#fff3e0\n```\n\n**Benefits:**\n- Simplifies agent code (no need to reassign variables)\n- Preserves object identity for debugging\n- Maintains all metadata fields unchanged\n\n**Drawbacks:**\n- Less functional/immutable\n- Potential confusion about ownership semantics\n- No explicit return type in function signature\n\nSources: [src/agent/action.py:10-21](), [src/agent/deep_research.py:42-46](), [src/agent/deep_research.py:59-64]()\n\n---\n\n## Observability and Tracing\n\n### The @traceable Decorator\n\nThe `call_tools_safely` function is decorated with `@traceable`, which provides comprehensive execution tracing:\n\n```mermaid\ngraph TB\n    A[\"@traceable decorator\"] --> B[\"Log call start<br/>with parameters\"]\n    B --> C[\"Execute<br/>call_tools_safely\"]\n    C --> D{\"Exception?\"}\n    \n    D -->|\"Success\"| E[\"Log completion<br/>with timing & result\"]\n    D -->|\"Exception\"| F[\"Log failure<br/>with stack trace\"]\n    \n    E --> G[\"Write to trace.log\"]\n    F --> G\n    \n    C --> H[\"Inner exception<br/>handling\"]\n    H --> I[\"Log to global.log\"]\n```\n\n**Traceable Decorator Configuration** [src/utils/log_decorator.py:297-302]():\n```python\ntraceable = lambda func: log_function(\n    logger_name=\"root.all.trace\",\n    log_file=traceable_logger_file_name,  # logs/trace.log\n    exclude_args=[\"password\", \"token\", \"secret\"],\n    level=logging.DEBUG\n)(func)\n```\n\nThis dual-logging approach provides:\n- **trace.log**: Complete parameter and return value logging\n- **global.log**: High-level operation logging with error details\n\nSources: [src/agent/action.py:9-10](), [src/utils/log_decorator.py:287-306]()\n\n---\n\n### Logged Information\n\nFor each tool call, the tracing system captures:\n\n| Information | Source | Log Level |\n|------------|--------|-----------|\n| Function arguments (tool_info dict) | @traceable | DEBUG |\n| Call stack trace | @traceable | DEBUG |\n| Execution start time | @traceable | DEBUG |\n| Tool execution result | Inner logging | INFO |\n| Execution duration | @traceable | DEBUG |\n| Exception type and message | Both | ERROR |\n| Full traceback | Both | ERROR |\n\n**Example Log Entry:**\n```\n[2025-11-26 03:39:31,682] ãè°ç¨å¼å§ã æ è·¯å¾ï¼ action.None.call_tools_safely \n| å¼å§æ¶é´ï¼ 2025-11-26 03:39:31.682000 \n| ä½ç½®åæ°ï¼ {'tool_call_name': 'execute_python_code', ...}\n| å³é®å­åæ°ï¼ {}\n```\n\nSources: [src/utils/log_decorator.py:199-217](), [src/agent/action.py:45]()\n\n---\n\n## Tool Registration Pattern\n\n### Current Implementation\n\nThe current implementation uses **hardcoded if-elif chains** for tool dispatch. This approach is simple but requires code changes to add new tools:\n\n```python\nif function_name == ExecutePythonCodeTool.tool_name():\n    # ExecutePythonCodeTool dispatch\nelif function_name == RecursivePlanTreeTodoTool.tool_name():\n    # RecursivePlanTreeTodoTool dispatch\n```\n\n**Advantages:**\n- Explicit and easy to understand\n- No magic registration mechanisms\n- Excellent IDE support (jump to definition works)\n- Easy to debug (straightforward control flow)\n\n**Disadvantages:**\n- Requires modifying action.py for each new tool\n- Cannot dynamically discover tools\n- Violates Open-Closed Principle\n\nSources: [src/agent/action.py:14-20]()\n\n---\n\n### Tool Discovery\n\nTools are explicitly registered in the agent's initialization [src/agent/deep_research.py:20-23]():\n\n```python\ntools_schema_list = tool.schema.get_tools_schema([\n    tool.python_tool.ExecutePythonCodeTool,\n    # tool.todo_tool.RecursivePlanTreeTodoTool,  # Commented out\n])\n```\n\nThis schema list is passed to the LLM to inform it which tools are available. The action coordinator must maintain consistency with this list, ensuring every tool in the schema has a corresponding dispatch branch.\n\n**Consistency Requirements:**\n1. Tool name in dispatch must match `Tool.tool_name()`\n2. Tool parameters must match schema definition\n3. Tools in schema list must have dispatch handlers\n4. Commented tools should also be commented in dispatcher\n\nSources: [src/agent/deep_research.py:20-23](), [src/agent/action.py:15-20]()\n\n---\n\n## Error Scenarios and Recovery\n\n### Common Error Patterns\n\nThe action coordinator handles several classes of errors:\n\n```mermaid\ngraph TB\n    A[\"Tool Execution Errors\"] --> B[\"JSON Parsing Errors\"]\n    A --> C[\"Tool Instantiation Errors\"]\n    A --> D[\"Tool Runtime Errors\"]\n    \n    B --> B1[\"Invalid JSON in<br/>tool_call_arguments\"]\n    C --> C1[\"Missing required parameters\"]\n    C --> C2[\"Type validation failures\"]\n    D --> D1[\"Code execution timeouts\"]\n    D --> D2[\"Serialization errors\"]\n    D --> D3[\"External dependency failures\"]\n    \n    B1 --> E[\"Captured by<br/>call_tools_safely\"]\n    C1 --> E\n    C2 --> E\n    D1 --> E\n    D2 --> E\n    D3 --> E\n    \n    E --> F[\"Error message returned<br/>to LLM in content field\"]\n```\n\n**JSON Parsing Error Example:**\n```python\n# If tool_call_arguments contains invalid JSON:\narguments = json.loads(tool_info[\"tool_call_arguments\"])\n# Raises: json.decoder.JSONDecodeError\n# Caught by: try-except in call_tools_safely\n# Result: Error message in tool_info[\"content\"]\n```\n\nSources: [src/agent/action.py:13](), [src/agent/action.py:40-47]()\n\n---\n\n### Function Call vs Tool Calls API Mismatch\n\nA critical error pattern occurs when the LLM uses `function_call` format but no handler processes it. This is documented in [docs/functino_call_err.design.md:2-3]():\n\n```\n{'content': 'æ²¡æå®ä¹function_callå·¥å·è°ç¨ï¼æ æ³æ§è¡function_callï¼è¯·ä½¿ç¨tool_callsè°ç¨å·¥å·ã',\n 'role': 'user'}\n```\n\nThis error occurs when:\n1. LLM generates a response with `function_call` field\n2. Agent detects `has_function_call()` but doesn't properly handle it\n3. Empty or malformed message is appended to conversation\n4. LLM API rejects the malformed message sequence\n\n**Mitigation:**\nThe agent properly handles both formats [src/agent/deep_research.py:33-64]() by checking for both `has_function_call()` and `has_tool_call()` in the decision loop.\n\nSources: [docs/functino_call_err.design.md:1-51](), [src/agent/deep_research.py:33-64]()\n\n---\n\n## Adding New Tools\n\nTo add a new tool to the action coordinator:\n\n### Step 1: Implement Tool Class\nCreate a new tool class extending `BaseTool` (see [Creating New Tools](#8.2))\n\n### Step 2: Add Dispatch Branch\nModify [src/agent/action.py:14-20]() to add a new elif branch:\n\n```python\nelif function_name == NewTool.tool_name():\n    new_tool = NewTool(**arguments)\n    tool_info[\"content\"] = new_tool.run()\n```\n\n### Step 3: Register Tool Schema\nAdd tool to schema list in [src/agent/deep_research.py:20-23]():\n\n```python\ntools_schema_list = tool.schema.get_tools_schema([\n    tool.python_tool.ExecutePythonCodeTool,\n    tool.new_tool.NewTool,  # Add here\n])\n```\n\n### Step 4: Test Error Handling\nVerify that:\n- Invalid arguments raise appropriate errors\n- Errors are caught by `call_tools_safely`\n- Error messages are informative for the LLM\n\nSources: [src/agent/action.py:14-20](), [src/agent/deep_research.py:20-23]()\n\n---\n\n## Performance Considerations\n\n### Synchronous Execution Model\n\nThe action coordinator executes tools **synchronously** within the agent loop. This means:\n\n- Each tool call blocks until completion\n- Multiple tool calls are executed sequentially\n- Long-running tools delay the agent response\n\n```mermaid\nsequenceDiagram\n    participant Agent\n    participant Coordinator as \"call_tools_safely\"\n    participant Tool1 as \"Tool 1\"\n    participant Tool2 as \"Tool 2\"\n    \n    Agent->>Coordinator: Call Tool 1\n    Coordinator->>Tool1: run()\n    Note over Tool1: Executes (may take seconds)\n    Tool1-->>Coordinator: Result\n    Coordinator-->>Agent: Updated tool_info\n    \n    Agent->>Coordinator: Call Tool 2\n    Coordinator->>Tool2: run()\n    Note over Tool2: Executes (may take seconds)\n    Tool2-->>Coordinator: Result\n    Coordinator-->>Agent: Updated tool_info\n```\n\nFor Python code execution specifically, timeouts are enforced at the tool level (see [Python Code Execution Tool](#4.2)), not by the action coordinator.\n\nSources: [src/agent/action.py:10-47](), [src/agent/deep_research.py:47-64]()\n\n---\n\n### Memory Overhead\n\nThe action coordinator has minimal memory overhead:\n- No caching of tool instances (created per call)\n- No buffering of results (stored directly in messages list)\n- tool_info dictionaries are small (typically < 1KB)\n\nThe primary memory concerns are:\n1. **Large tool results**: Python code output stored in `content` field\n2. **Message history growth**: All tool calls remain in conversation\n3. **Tool internal state**: Managed by individual tools (see [Workspace State Management](#5.5))\n\nSources: [src/agent/action.py:10-21](), [src/agent/deep_research.py:46](), [src/agent/deep_research.py:64]()\n\n---\n\n## Summary\n\nThe Action Coordination subsystem provides a simple, robust dispatching layer with the following characteristics:\n\n**Design Principles:**\n- **Safety First**: Comprehensive exception handling prevents crashes\n- **Observability**: Dual logging (trace + global) for debugging\n- **Simplicity**: Direct if-elif dispatch over complex registration\n- **LLM-Friendly Errors**: Detailed error messages help the model recover\n\n**Key Components:**\n- `call_tools_safely`: Error-handling wrapper\n- `call_tools`: Core dispatch logic  \n- `tool_info`: Mutable state container\n- `@traceable`: Execution tracing decorator\n\n**Integration Points:**\n- Called by agent loop for each tool invocation\n- Instantiates tool classes dynamically from schemas\n- Returns results via in-place mutation\n- Logs to both trace.log and global.log\n\nSources: [src/agent/action.py:1-49](), [src/agent/deep_research.py:1-129]()\n\n---\n\n# Page: Tool System\n\n# Tool System\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/functino_call_err.design.md](docs/functino_call_err.design.md)\n- [src/agent/action.py](src/agent/action.py)\n- [src/agent/memory.py](src/agent/memory.py)\n- [src/agent/tool/base_tool.py](src/agent/tool/base_tool.py)\n- [src/agent/tool/python_tool.py](src/agent/tool/python_tool.py)\n- [src/agent/tool/todo_tool.py](src/agent/tool/todo_tool.py)\n- [src/memory/tree_todo/schemas.py](src/memory/tree_todo/schemas.py)\n- [src/memory/tree_todo/todo_track.py](src/memory/tree_todo/todo_track.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThe Tool System provides the interface layer between the LLM agent and executable actions. It defines how tools are structured, discovered, invoked, and integrated into the agent's decision loop. This system enables the agent to interact with Python execution environments and task management structures through a unified, type-safe interface.\n\nFor details on how the agent orchestrates tool invocations, see [Agent Orchestration](#3). For the base interface specification, see [BaseTool Interface](#4.1). For individual tool implementations, see [Python Code Execution Tool](#4.2) and [Recursive Task Planning Tool](#4.3).\n\n**Sources:** [src/agent/tool/base_tool.py:1-76](), [src/agent/tool/python_tool.py:1-51](), [src/agent/tool/todo_tool.py:1-37]()\n\n---\n\n## Architecture Overview\n\nThe tool system consists of four primary components: the `BaseTool` abstract interface, concrete tool implementations, the action dispatcher, and the schema generation system that bridges tools to LLM function calling.\n\n### System Component Diagram\n\n```mermaid\ngraph TB\n    subgraph \"LLM Integration Layer\"\n        LLM[\"LLM<br/>(qwen-plus)\"]\n        SCHEMA[\"get_tool_schema()<br/>JSON Schema Generator\"]\n    end\n    \n    subgraph \"Tool Interface Layer\"\n        BASE[\"BaseTool<br/>(Abstract Base Class)\"]\n        METHODS[\"Interface Methods:<br/>- tool_name()<br/>- tool_description()<br/>- get_parameter_schema()<br/>- get_tool_schema()<br/>- run()\"]\n    end\n    \n    subgraph \"Concrete Tool Implementations\"\n        PYTOOL[\"ExecutePythonCodeTool<br/>python_code_snippet: str<br/>timeout: int\"]\n        TODOTOOL[\"RecursivePlanTreeTodoTool<br/>recursive_plan_tree: RecursivePlanTree\"]\n    end\n    \n    subgraph \"Dispatch Layer\"\n        DISPATCHER[\"call_tools_safely()<br/>action.py\"]\n        ERROR[\"Exception Handler<br/>traceback.format_exc()\"]\n    end\n    \n    subgraph \"Execution Backends\"\n        RUNTIME[\"Python Execution<br/>Runtime System\"]\n        TRACK[\"Task Tree<br/>Tracking System\"]\n    end\n    \n    LLM --> SCHEMA\n    SCHEMA --> BASE\n    BASE --> METHODS\n    METHODS --> PYTOOL\n    METHODS --> TODOTOOL\n    \n    LLM --> DISPATCHER\n    DISPATCHER --> PYTOOL\n    DISPATCHER --> TODOTOOL\n    DISPATCHER --> ERROR\n    \n    PYTOOL --> RUNTIME\n    TODOTOOL --> TRACK\n```\n\nThe architecture follows a standard plugin pattern where:\n- `BaseTool` defines the contract all tools must implement\n- Concrete tools extend `BaseTool` and provide specific functionality\n- The dispatcher routes tool calls from the LLM to the appropriate tool instance\n- Schema generation produces JSON schemas compatible with OpenAI's function calling format\n\n**Sources:** [src/agent/tool/base_tool.py:7-76](), [src/agent/action.py:1-48]()\n\n---\n\n## Tool Definition and Schema Generation\n\n### Pydantic-Based Tool Definition\n\nAll tools inherit from `BaseTool`, which is itself a Pydantic `BaseModel`. This provides automatic validation, type checking, and schema generation. Each tool defines its parameters as Pydantic fields with descriptions and type annotations.\n\n| Component | Purpose | Implementation |\n|-----------|---------|----------------|\n| `tool_name()` | Generates unique identifier from class name | [base_tool.py:14-18]() |\n| `tool_description()` | Extracts docstring for LLM context | [base_tool.py:20-23]() |\n| `get_parameter_schema()` | Generates JSON Schema from Pydantic model | [base_tool.py:26-28]() |\n| `get_tool_schema()` | Produces complete function calling schema | [base_tool.py:31-71]() |\n| `run()` | Abstract method for tool execution logic | [base_tool.py:73-75]() |\n\n### Schema Generation Flow\n\n```mermaid\nsequenceDiagram\n    participant Agent as \"Agent System\"\n    participant Tool as \"Tool Class<br/>(ExecutePythonCodeTool)\"\n    participant Pydantic as \"Pydantic<br/>Schema Engine\"\n    participant LLM as \"LLM Service\"\n    \n    Agent->>Tool: get_tool_schema()\n    Tool->>Tool: tool_name()<br/>\"execute_python_code\"\n    Tool->>Tool: tool_description()<br/>Extract docstring\n    Tool->>Pydantic: model_json_schema()\n    Pydantic-->>Tool: Parameter schema dict\n    Tool->>Tool: Construct OpenAI format\n    Tool-->>Agent: Complete tool schema\n    Agent->>LLM: Pass tools parameter\n    LLM-->>Agent: Response with tool_calls\n```\n\nThe `get_tool_schema()` method [base_tool.py:31-71]() produces a structure conforming to OpenAI's function calling format:\n\n```json\n{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"execute_python_code\",\n    \"description\": \"å¿é¡»è°ç¨å¨æ¯ä¸è½®æ¨çä¸­...\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"tool_call_purpose\": {...},\n        \"python_code_snippet\": {...},\n        \"timeout\": {...}\n      },\n      \"required\": [\"tool_call_purpose\", \"python_code_snippet\"]\n    },\n    \"strict\": true\n  }\n}\n```\n\n**Sources:** [src/agent/tool/base_tool.py:26-71](), [src/agent/tool/python_tool.py:13-40]()\n\n---\n\n## Tool Invocation Pipeline\n\n### Dispatcher Architecture\n\nThe `call_tools_safely()` function [action.py:10-47]() serves as the central dispatcher that routes tool calls from the LLM to the appropriate tool implementation. It provides comprehensive error handling and logging.\n\n```mermaid\ngraph LR\n    subgraph \"Input\"\n        TOOLINFO[\"tool_info dict<br/>{tool_call_name,<br/>tool_call_arguments,<br/>content}\"]\n    end\n    \n    subgraph \"call_tools_safely Function\"\n        PARSE[\"Parse Arguments<br/>json.loads()\"]\n        ROUTE[\"Route by Name<br/>function_name match\"]\n        INSTANTIATE[\"Instantiate Tool<br/>**arguments\"]\n        EXECUTE[\"Execute run()<br/>Get result\"]\n        CATCH[\"Exception Handler<br/>traceback.format_exc()\"]\n    end\n    \n    subgraph \"Tool Implementations\"\n        EXEC[\"ExecutePythonCodeTool.run()\"]\n        TODO[\"RecursivePlanTreeTodoTool.run()\"]\n    end\n    \n    subgraph \"Output\"\n        SUCCESS[\"tool_info with content\"]\n        ERROR[\"tool_info with error message\"]\n    end\n    \n    TOOLINFO --> PARSE\n    PARSE --> ROUTE\n    ROUTE --> INSTANTIATE\n    INSTANTIATE --> EXEC\n    INSTANTIATE --> TODO\n    EXEC --> EXECUTE\n    TODO --> EXECUTE\n    EXECUTE --> SUCCESS\n    EXECUTE --> CATCH\n    CATCH --> ERROR\n```\n\n### Call Flow with Code References\n\nThe dispatcher follows this execution path:\n\n1. **Extract function name and arguments** [action.py:12-13]()\n   - `function_name = tool_info[\"tool_call_name\"]`\n   - `arguments = json.loads(tool_info[\"tool_call_arguments\"])`\n\n2. **Route to tool implementation** [action.py:15-20]()\n   - Match against `ExecutePythonCodeTool.tool_name()`\n   - Match against `RecursivePlanTreeTodoTool.tool_name()`\n\n3. **Instantiate and execute** [action.py:16-20]()\n   - Create tool instance with validated parameters\n   - Call `run()` method\n   - Store result in `tool_info[\"content\"]`\n\n4. **Handle exceptions** [action.py:40-47]()\n   - Capture full traceback with `traceback.format_exc()`\n   - Log error with `global_logger.error()`\n   - Return error message in `tool_info[\"content\"]`\n\n**Sources:** [src/agent/action.py:10-47]()\n\n---\n\n## Available Tools\n\nThe system provides two concrete tool implementations:\n\n### ExecutePythonCodeTool\n\nExecutes Python code snippets in a stateful environment with variable persistence across calls. This tool is the primary computational interface for the agent.\n\n**Key Parameters:**\n- `python_code_snippet: str` - The Python code to execute [python_tool.py:29-35]()\n- `timeout: int` - Maximum execution time in seconds (default: 30) [python_tool.py:36-39]()\n\n**Execution Flow:**\n1. Retrieves execution context from `workspace.get_arg_globals()` [python_tool.py:42]()\n2. Runs code using `subthread_python_executor.run_structured_in_thread()` [python_tool.py:44-48]()\n3. Persists output globals with `workspace.append_out_globals()` [python_tool.py:49]()\n4. Returns formatted result to LLM [python_tool.py:50]()\n\nFor detailed implementation, see [Python Code Execution Tool](#4.2).\n\n### RecursivePlanTreeTodoTool\n\nManages hierarchical task structures with status tracking, version history, and change analysis. This tool enables the agent to maintain complex planning state.\n\n**Key Parameters:**\n- `recursive_plan_tree: RecursivePlanTree` - The task tree structure [todo_tool.py:19-25]()\n\n**Execution Flow:**\n1. Delegates to `todo_track.run()` with tree structure [todo_tool.py:31]()\n2. Compares against previous version for change detection\n3. Renders Markdown representation of task tree\n4. Returns change summary and statistics [todo_tool.py:32-36]()\n\nFor detailed implementation, see [Recursive Task Planning Tool](#4.3).\n\n**Sources:** [src/agent/tool/python_tool.py:13-51](), [src/agent/tool/todo_tool.py:10-37]()\n\n---\n\n## Tool Registration and Discovery\n\n### Static Registration\n\nTools are currently registered through explicit imports and conditional logic in the dispatcher. The registration happens in two locations:\n\n1. **Import statements** [action.py:7-8]()\n   ```python\n   from src.agent.tool.python_tool import ExecutePythonCodeTool \n   from src.agent.tool.todo_tool import RecursivePlanTreeTodoTool\n   ```\n\n2. **Dispatch logic** [action.py:15-20]()\n   ```python\n   if function_name == ExecutePythonCodeTool.tool_name():\n       execute_python_code_tool = ExecutePythonCodeTool(**arguments)\n       tool_info[\"content\"] = execute_python_code_tool.run()\n   elif function_name == RecursivePlanTreeTodoTool.tool_name():\n       recursive_plan_tree_todo_tool = RecursivePlanTreeTodoTool(**arguments)\n       tool_info[\"content\"] = recursive_plan_tree_todo_tool.run()\n   ```\n\n### Tool Name Convention\n\nTool names are automatically generated from class names using the `inflection` library [base_tool.py:16-18]():\n- `ExecutePythonCodeTool` â `\"execute_python_code\"`\n- `RecursivePlanTreeTodoTool` â `\"recursive_plan_tree_todo\"`\n\nThis transformation:\n1. Removes \"Tool\" suffix from class name\n2. Converts from CamelCase to snake_case using `inflection.underscore()`\n\n**Sources:** [src/agent/action.py:7-20](), [src/agent/tool/base_tool.py:14-18]()\n\n---\n\n## Integration with Agent System\n\n### Schema Provisioning to LLM\n\nTools are made available to the LLM through the `tools` parameter in chat completion requests. The agent must collect all tool schemas before each LLM call.\n\n```mermaid\nsequenceDiagram\n    participant Agent as \"user_query()<br/>deep_research.py\"\n    participant Tools as \"Tool Classes\"\n    participant Memory as \"Message List\"\n    participant LLM as \"generate_chat_completion()\"\n    \n    Agent->>Tools: ExecutePythonCodeTool.get_tool_schema()\n    Tools-->>Agent: Python tool schema\n    Agent->>Tools: RecursivePlanTreeTodoTool.get_tool_schema()\n    Tools-->>Agent: Todo tool schema\n    Agent->>LLM: messages + tools=[schemas]\n    LLM-->>Agent: assistant_output with tool_calls\n    Agent->>Agent: call_tools_safely(tool_info)\n    Agent->>Memory: Append tool result\n```\n\n### Response Handling\n\nWhen the LLM decides to invoke a tool, it returns a response with `tool_calls` attribute. The agent must:\n\n1. **Extract tool call information** from `assistant_output.tool_calls[0]`\n2. **Build tool_info dictionary**:\n   ```python\n   tool_info = {\n       \"tool_call_name\": function.name,\n       \"tool_call_arguments\": function.arguments,\n       \"content\": \"\"\n   }\n   ```\n3. **Invoke dispatcher** via `call_tools_safely(tool_info)`\n4. **Append result to message history** with role `\"tool\"` and appropriate `tool_call_id`\n\nFor the complete integration flow, see [Action Coordination](#3.4).\n\n**Sources:** [src/agent/action.py:10-47](), [src/agent/tool/base_tool.py:31-71]()\n\n---\n\n## Error Handling and Resilience\n\n### Multi-Layer Error Protection\n\nThe tool system implements error handling at multiple levels:\n\n| Level | Implementation | Purpose |\n|-------|---------------|---------|\n| Pydantic Validation | Automatic on tool instantiation | Type checking and required field validation |\n| Dispatcher Try-Catch | [action.py:40-47]() | Catches tool execution failures |\n| Execution Runtime | See [Execution Runtime](#5) | Handles code crashes, timeouts, and subprocess failures |\n| Logging | `global_logger.error()` [action.py:45]() | Records full exception details with stack traces |\n\n### Error Response Format\n\nWhen a tool fails, the dispatcher captures the complete traceback and returns it in the tool response:\n\n```python\nerror_msg = traceback.format_exc()\nglobal_logger.error(f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\", exc_info=True)\ntool_info[\"content\"] = f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\"\n```\n\nThis allows the LLM to:\n- Understand what went wrong\n- Attempt self-correction\n- Retry with modified parameters\n- Choose alternative approaches\n\n**Sources:** [src/agent/action.py:40-47]()\n\n---\n\n## Tool Execution Context\n\n### State Persistence Across Calls\n\nThe `ExecutePythonCodeTool` maintains execution state through the workspace system:\n\n```mermaid\ngraph LR\n    subgraph \"Call N\"\n        GET1[\"workspace.get_arg_globals()\"]\n        EXEC1[\"Execute Code<br/>Variables: x, y, func()\"]\n        STORE1[\"workspace.append_out_globals()\"]\n    end\n    \n    subgraph \"Global Workspace\"\n        GLOBALS[\"arg_globals_list<br/>Persistent storage\"]\n    end\n    \n    subgraph \"Call N+1\"\n        GET2[\"workspace.get_arg_globals()\"]\n        EXEC2[\"Execute Code<br/>Uses: x, y, func()\"]\n        STORE2[\"workspace.append_out_globals()\"]\n    end\n    \n    GET1 --> GLOBALS\n    EXEC1 --> STORE1\n    STORE1 --> GLOBALS\n    GLOBALS --> GET2\n    GET2 --> EXEC2\n    EXEC2 --> STORE2\n    STORE2 --> GLOBALS\n```\n\n### Context Flow\n\n1. **Context Retrieval** [python_tool.py:42]()\n   - `execution_context = workspace.get_arg_globals()`\n   - Returns the most recent execution state\n\n2. **Execution with Context** [python_tool.py:44-48]()\n   - Passes `_globals=execution_context` to executor\n   - Code runs with previously defined variables available\n\n3. **State Update** [python_tool.py:49]()\n   - `workspace.append_out_globals(exec_result.arg_globals)`\n   - New or modified variables persist for next call\n\nFor detailed workspace management, see [Workspace State Management](#5.5).\n\n**Sources:** [src/agent/tool/python_tool.py:41-50](), [src/runtime/workspace.py]()\n\n---\n\n## Tool Schema Examples\n\n### ExecutePythonCodeTool Schema\n\nThe Python execution tool exposes this interface to the LLM:\n\n```json\n{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"execute_python_code\",\n    \"description\": \"å¿é¡»è°ç¨å¨æ¯ä¸è½®æ¨çä¸­ï¼ä½ä¸ºè®¡ç®å·¥å·ãå¨æç¶æçç¯å¢ä¸­æ§è¡Pythonä»£ç çæ®µ...\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"tool_call_purpose\": {\n          \"type\": \"string\",\n          \"description\": \"å·¥å·è°ç¨çç®çï¼å³è°ç¨è¯¥å·¥å·çå·ä½åºæ¯æé®é¢ã\"\n        },\n        \"python_code_snippet\": {\n          \"type\": \"string\",\n          \"description\": \"è¦æ§è¡çææ Python ä»£ç çæ®µãä¸å¾åå«æ¶æä»£ç ...\",\n          \"examples\": [\"print('Hello, World!')\"]\n        },\n        \"timeout\": {\n          \"type\": \"integer\",\n          \"default\": 30,\n          \"description\": \"æ§è¡ä»£ç çæå¤§æ¶é´ï¼ç§ï¼...\"\n        }\n      },\n      \"required\": [\"tool_call_purpose\", \"python_code_snippet\"]\n    },\n    \"strict\": true\n  }\n}\n```\n\n### RecursivePlanTreeTodoTool Schema\n\nThe task management tool exposes this interface:\n\n```json\n{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"recursive_plan_tree_todo\",\n    \"description\": \"å¿é¡»è°ç¨ä½ä¸ºè®¡åæ¨çè¿ç¨çæèä¸è®°å½...\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"tool_call_purpose\": {\n          \"type\": \"string\",\n          \"description\": \"å·¥å·è°ç¨çç®ç...\"\n        },\n        \"recursive_plan_tree\": {\n          \"type\": \"object\",\n          \"description\": \"è¦ç®¡ççéå½è®¡åæ å¯¹è±¡...\",\n          \"properties\": {\n            \"core_goal\": {\"type\": \"string\"},\n            \"tree_nodes\": {\n              \"type\": \"array\",\n              \"items\": {\"$ref\": \"#/definitions/RecursivePlanTreeNode\"}\n            },\n            \"next_action\": {\"type\": \"object\"},\n            \"references\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n          }\n        }\n      },\n      \"required\": [\"tool_call_purpose\", \"recursive_plan_tree\"]\n    },\n    \"strict\": true\n  }\n}\n```\n\n**Sources:** [src/agent/tool/python_tool.py:13-40](), [src/agent/tool/todo_tool.py:10-26](), [src/memory/tree_todo/schemas.py:1-80]()\n\n---\n\n## Adding New Tools\n\nTo add a new tool to the system:\n\n1. **Create tool class** extending `BaseTool` in `src/agent/tool/`\n2. **Define Pydantic fields** with descriptions and type hints\n3. **Implement `run()` method** with execution logic\n4. **Import in dispatcher** [action.py:7-8]()\n5. **Add dispatch case** in `call_tools_safely()` [action.py:15-20]()\n\nFor a step-by-step guide, see [Creating New Tools](#8.2).\n\n**Sources:** [src/agent/tool/base_tool.py:7-76](), [src/agent/action.py:7-20]()\n\n---\n\n# Page: BaseTool Interface\n\n# BaseTool Interface\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/functino_call_err.design.md](docs/functino_call_err.design.md)\n- [src/agent/action.py](src/agent/action.py)\n- [src/agent/memory.py](src/agent/memory.py)\n- [src/agent/tool/base_tool.py](src/agent/tool/base_tool.py)\n- [src/memory/tree_todo/schemas.py](src/memory/tree_todo/schemas.py)\n- [src/memory/tree_todo/todo_track.py](src/memory/tree_todo/todo_track.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThe `BaseTool` interface defines the abstract contract that all tools in the algo_agent system must implement. It provides a standardized way to define tools that can be invoked by the LLM agent, including automatic schema generation for function calling, parameter validation, and execution logic.\n\nThis document covers the base tool interface, its required methods, and how tools integrate with the agent orchestration system. For details on specific tool implementations, see [Python Code Execution Tool](#4.2) and [Recursive Task Planning Tool](#4.3). For information on how tools are invoked during agent execution, see [Action Coordination](#3.4).\n\n**Sources:** [src/agent/tool/base_tool.py:1-76]()\n\n---\n\n## Abstract Class Structure\n\nThe `BaseTool` class is implemented as a Pydantic `BaseModel`, providing automatic parameter validation and JSON schema generation. All tools must inherit from this base class and implement its abstract methods.\n\n### Core Definition\n\n```python\nclass BaseTool(BaseModel):\n    \"\"\"ææå·¥å·çåºç±»ï¼å®ä¹ç»ä¸æ¥å£\"\"\"\n    tool_call_purpose: str = Field(\n        ..., \n        description=\"å·¥å·è°ç¨çç®çï¼å³è°ç¨è¯¥å·¥å·çå·ä½åºæ¯æé®é¢ã\"\n    )\n```\n\nEvery tool instance requires a `tool_call_purpose` parameter that describes why the tool is being invoked for a particular request. This helps maintain context and improves logging/debugging.\n\n**Sources:** [src/agent/tool/base_tool.py:7-12]()\n\n---\n\n## Tool Identification Methods\n\n### Class Method: `tool_name()`\n\nGenerates a unique identifier for the tool based on its class name. The naming convention automatically converts the class name to snake_case and removes \"Tool\" or \"tool\" suffixes.\n\n| Method | Return Type | Purpose |\n|--------|-------------|---------|\n| `tool_name()` | `str` | Unique tool identifier used for routing and dispatch |\n\n**Implementation:**\n```python\n@classmethod\ndef tool_name(cls) -> str:\n    \"\"\"å·¥å·å¯ä¸æ è¯åï¼ç¨äºè·¯ç±å¹éï¼å¦ \"weatherquery\"ï¼\"\"\"\n    return inflection.underscore(\n        cls.__name__.replace(\"Tool\", \"\").replace(\"tool\", \"\")\n    )\n```\n\n**Example:** A class named `ExecutePythonCodeTool` automatically generates the tool name `\"execute_python_code\"`.\n\n**Sources:** [src/agent/tool/base_tool.py:13-18]()\n\n---\n\n### Class Method: `tool_description()`\n\nExtracts the tool's docstring to provide a natural language description of its purpose and capabilities. This description is used by the LLM to determine when to invoke the tool.\n\n```python\n@classmethod\ndef tool_description(cls) -> str:\n    \"\"\"å·¥å·æè¿°ï¼ä¾ Agent çè§£ç¨éï¼\"\"\"\n    return inspect.getdoc(cls) or \"æ å·¥å·æè¿°\"\n```\n\n**Sources:** [src/agent/tool/base_tool.py:20-23]()\n\n---\n\n## Schema Generation System\n\n### Diagram: Tool Schema Generation Flow\n\n```mermaid\ngraph TB\n    BaseTool[\"BaseTool<br/>(base_tool.py)\"]\n    \n    subgraph \"Schema Methods\"\n        GetParam[\"get_parameter_schema()<br/>Returns Pydantic JSON Schema\"]\n        GetTool[\"get_tool_schema()<br/>Returns OpenAI Function Format\"]\n        ToolName[\"tool_name()<br/>Generates identifier\"]\n        ToolDesc[\"tool_description()<br/>Extracts docstring\"]\n    end\n    \n    subgraph \"Generated Schema Structure\"\n        Schema[\"Tool Schema Object\"]\n        FuncDef[\"function definition\"]\n        Params[\"parameters (JSON Schema)\"]\n        Required[\"required fields\"]\n    end\n    \n    subgraph \"LLM Integration\"\n        AgentTools[\"Agent Tools List\"]\n        LLMContext[\"LLM Context<br/>(tools parameter)\"]\n        FuncCall[\"Function Call Response\"]\n    end\n    \n    BaseTool --> GetParam\n    BaseTool --> GetTool\n    BaseTool --> ToolName\n    BaseTool --> ToolDesc\n    \n    GetTool --> ToolName\n    GetTool --> ToolDesc\n    GetTool --> GetParam\n    \n    GetTool --> Schema\n    Schema --> FuncDef\n    Schema --> Params\n    Schema --> Required\n    \n    Schema --> AgentTools\n    AgentTools --> LLMContext\n    LLMContext --> FuncCall\n```\n\n**Sources:** [src/agent/tool/base_tool.py:26-71](), [src/agent/action.py:1-49]()\n\n---\n\n### Method: `get_parameter_schema()`\n\nLeverages Pydantic's built-in JSON schema generation to produce a JSON Schema representation of the tool's parameters. This schema defines the types, descriptions, and validation rules for each parameter.\n\n```python\n@classmethod\ndef get_parameter_schema(cls) -> dict:\n    \"\"\"è·ååæ° JSON Schemaï¼ä¾ Agent æé åæ°ï¼\"\"\"\n    return cls.model_json_schema()\n```\n\nThe generated schema includes:\n- Parameter names and types\n- Field descriptions from `Field()` definitions\n- Validation constraints (enums, min/max values, etc.)\n- Required vs. optional fields\n\n**Sources:** [src/agent/tool/base_tool.py:25-28]()\n\n---\n\n### Method: `get_tool_schema()`\n\nCombines the tool identification methods and parameter schema into a complete OpenAI-compatible function calling schema. This format allows the LLM to understand when and how to invoke the tool.\n\n**Schema Structure:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `type` | `\"function\"` | Indicates this is a function tool |\n| `function.name` | `str` | Tool identifier from `tool_name()` |\n| `function.description` | `str` | Tool description from `tool_description()` |\n| `function.parameters` | `dict` | JSON Schema from `get_parameter_schema()` |\n| `function.strict` | `bool` | Enforces strict parameter validation |\n\n**Implementation:**\n```python\n@classmethod\ndef get_tool_schema(cls) -> str:\n    tool_schema = {\n        \"type\": \"function\",\n        \"function\": {\n            \"type\": \"function\",\n            \"name\": cls.tool_name(),\n            \"description\": cls.tool_description(),\n            \"parameters\": cls.get_parameter_schema(),\n            \"strict\": True,\n        },\n    }\n    return tool_schema\n```\n\n**Sources:** [src/agent/tool/base_tool.py:30-71]()\n\n---\n\n## Execution Contract\n\n### Abstract Method: `run()`\n\nThe `run()` method is the core execution logic that every tool must implement. It contains the actual functionality that gets invoked when the agent calls the tool.\n\n```python\ndef run(self) -> str:\n    \"\"\"å·¥å·æ ¸å¿æ§è¡é»è¾ï¼å­ç±»å¿é¡»å®ç°ï¼\"\"\"\n    raise NotImplementedError(\"ææå·¥å·å¿é¡»å®ç° run æ¹æ³\")\n```\n\n**Contract Requirements:**\n- **Return Type:** Must return a `str` containing the execution result\n- **Implementation:** Subclasses must override this method\n- **Error Handling:** Should handle exceptions gracefully and return meaningful error messages\n- **Side Effects:** Can modify global state (e.g., workspace variables) but must document this\n\n**Sources:** [src/agent/tool/base_tool.py:73-75]()\n\n---\n\n## Tool Registration and Dispatch\n\n### Diagram: Tool Dispatch Flow\n\n```mermaid\nsequenceDiagram\n    participant LLM as \"LLM<br/>(qwen-plus)\"\n    participant Agent as \"Agent<br/>(deep_research.py)\"\n    participant Dispatcher as \"call_tools_safely<br/>(action.py)\"\n    participant ToolClass as \"Concrete Tool<br/>(e.g., ExecutePythonCodeTool)\"\n    participant Run as \"tool.run()\"\n    \n    LLM->>Agent: \"assistant_output with tool_calls\"\n    Agent->>Agent: \"Extract tool_call_name and arguments\"\n    Agent->>Dispatcher: \"call_tools_safely(tool_info)\"\n    \n    Note over Dispatcher: \"tool_info = {<br/>tool_call_name,<br/>tool_call_arguments,<br/>content}\"\n    \n    Dispatcher->>Dispatcher: \"Parse JSON arguments\"\n    Dispatcher->>Dispatcher: \"Match function_name to tool class\"\n    \n    alt \"function_name == ExecutePythonCodeTool.tool_name()\"\n        Dispatcher->>ToolClass: \"ExecutePythonCodeTool(**arguments)\"\n        Note over ToolClass: \"Pydantic validates parameters\"\n    else \"function_name == RecursivePlanTreeTodoTool.tool_name()\"\n        Dispatcher->>ToolClass: \"RecursivePlanTreeTodoTool(**arguments)\"\n    end\n    \n    Dispatcher->>Run: \"execute_python_code_tool.run()\"\n    Run-->>Dispatcher: \"result_string\"\n    Dispatcher->>Dispatcher: \"tool_info['content'] = result\"\n    Dispatcher-->>Agent: \"tool_info with result\"\n    \n    alt \"Exception during execution\"\n        Dispatcher->>Dispatcher: \"Catch exception, format traceback\"\n        Dispatcher-->>Agent: \"tool_info with error message\"\n    end\n```\n\n**Sources:** [src/agent/action.py:10-47]()\n\n---\n\n### Implementation in `call_tools_safely()`\n\nThe action coordinator uses a conditional dispatch pattern to route tool calls to the appropriate implementation:\n\n```python\n@traceable\ndef call_tools_safely(tool_info: dict):\n    def call_tools(tool_info: dict):\n        function_name = tool_info[\"tool_call_name\"]\n        arguments = json.loads(tool_info[\"tool_call_arguments\"])\n        \n        if function_name == ExecutePythonCodeTool.tool_name():\n            execute_python_code_tool = ExecutePythonCodeTool(**arguments)\n            tool_info[\"content\"] = execute_python_code_tool.run()\n        elif function_name == RecursivePlanTreeTodoTool.tool_name():\n            recursive_plan_tree_todo_tool = RecursivePlanTreeTodoTool(**arguments)\n            tool_info[\"content\"] = recursive_plan_tree_todo_tool.run()\n        \n        return tool_info\n```\n\n**Dispatch Process:**\n1. Extract `function_name` from `tool_info[\"tool_call_name\"]`\n2. Parse JSON `arguments` from `tool_info[\"tool_call_arguments\"]`\n3. Match `function_name` against `ToolClass.tool_name()` for each registered tool\n4. Instantiate the tool class with unpacked arguments (Pydantic validates parameters)\n5. Call `tool.run()` and capture the result\n6. Store result in `tool_info[\"content\"]`\n7. Return updated `tool_info` to agent\n\n**Sources:** [src/agent/action.py:10-21]()\n\n---\n\n### Error Handling Wrapper\n\nThe outer `call_tools_safely()` function wraps the dispatch logic with comprehensive exception handling:\n\n```python\ntry:\n    return call_tools(tool_info)\nexcept Exception as e:\n    error_msg = traceback.format_exc()\n    global_logger.error(f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\", exc_info=True)\n    tool_info[\"content\"] = f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\"\n    return tool_info\n```\n\nThis ensures that tool failures don't crash the agent and provide detailed error information for debugging.\n\n**Sources:** [src/agent/action.py:40-47]()\n\n---\n\n## Tool Definition Pattern\n\n### Standard Tool Implementation Template\n\n```python\nfrom src.agent.tool.base_tool import BaseTool\nfrom pydantic import Field\n\nclass MyCustomTool(BaseTool):\n    \"\"\"\n    Brief description of what this tool does.\n    \n    Used when the agent needs to perform specific actions.\n    \"\"\"\n    \n    # Define tool-specific parameters\n    parameter_one: str = Field(\n        ..., \n        description=\"Description of parameter_one\"\n    )\n    parameter_two: int = Field(\n        default=10,\n        description=\"Description of parameter_two\"\n    )\n    \n    def run(self) -> str:\n        \"\"\"\n        Execute the tool's core logic.\n        \n        Returns:\n            str: Result message or error description\n        \"\"\"\n        try:\n            # Tool implementation\n            result = self._perform_action()\n            return f\"Success: {result}\"\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n```\n\n**Sources:** [src/agent/tool/base_tool.py:1-76]()\n\n---\n\n## Integration with Pydantic V2\n\nThe `BaseTool` class leverages Pydantic V2 features for robust parameter validation and schema generation:\n\n### Key Features Used\n\n| Feature | Purpose | Implementation |\n|---------|---------|---------------|\n| `BaseModel` | Base class providing validation | `class BaseTool(BaseModel)` |\n| `Field()` | Parameter metadata and constraints | Used in `tool_call_purpose` and subclass fields |\n| `model_json_schema()` | Automatic JSON Schema generation | Called by `get_parameter_schema()` |\n| Parameter validation | Runtime type checking | Automatic during tool instantiation |\n| `model_copy()` | Deep copying instances | Used in tool state management |\n\n### Validation Benefits\n\nWhen the dispatcher instantiates a tool with `ToolClass(**arguments)`, Pydantic automatically:\n- Validates parameter types match schema definitions\n- Enforces required fields are present\n- Applies field constraints (enums, ranges, patterns)\n- Coerces compatible types (e.g., string to int)\n- Raises `ValidationError` for invalid inputs\n\n**Sources:** [src/agent/tool/base_tool.py:1-6](), [src/memory/tree_todo/schemas.py:1-3]()\n\n---\n\n## Concrete Tool Examples\n\n### Tool Schema Comparison Table\n\n| Tool Class | Generated Name | Primary Purpose | Key Parameters |\n|------------|----------------|-----------------|----------------|\n| `ExecutePythonCodeTool` | `execute_python_code` | Execute Python code snippets | `python_code_snippet` |\n| `RecursivePlanTreeTodoTool` | `recursive_plan_tree_todo` | Manage hierarchical task trees | `current_plan_tree` |\n\n**Sources:** [src/agent/action.py:7-8](), [src/agent/action.py:15-20]()\n\n---\n\n## Relationship to Agent Architecture\n\n### Diagram: BaseTool in System Context\n\n```mermaid\ngraph TB\n    subgraph \"Tool System Layer\"\n        BaseTool[\"BaseTool<br/>(base_tool.py:7-75)\"]\n        Schema[\"Tool Schema Methods\"]\n        Run[\"run() Abstract Method\"]\n    end\n    \n    subgraph \"Concrete Tools\"\n        PythonTool[\"ExecutePythonCodeTool<br/>(python_tool.py)\"]\n        TodoTool[\"RecursivePlanTreeTodoTool<br/>(todo_tool.py)\"]\n    end\n    \n    subgraph \"Agent Orchestration\"\n        ActionCoord[\"call_tools_safely<br/>(action.py:10-47)\"]\n        AgentLoop[\"user_query<br/>(deep_research.py)\"]\n    end\n    \n    subgraph \"LLM Integration\"\n        ToolsParam[\"tools parameter<br/>(list of tool schemas)\"]\n        FunctionCall[\"function_call / tool_calls<br/>in LLM response\"]\n    end\n    \n    BaseTool --> Schema\n    BaseTool --> Run\n    \n    BaseTool -.inherits.-> PythonTool\n    BaseTool -.inherits.-> TodoTool\n    \n    PythonTool --> ActionCoord\n    TodoTool --> ActionCoord\n    \n    Schema --> ToolsParam\n    ToolsParam --> AgentLoop\n    AgentLoop --> FunctionCall\n    FunctionCall --> ActionCoord\n    \n    ActionCoord -.invokes.-> Run\n```\n\n**Sources:** [src/agent/tool/base_tool.py:1-76](), [src/agent/action.py:1-49]()\n\n---\n\n## Tool Name Conventions\n\nThe `tool_name()` method uses a specific transformation algorithm:\n\n### Transformation Process\n\n1. **Input:** Python class name (e.g., `ExecutePythonCodeTool`)\n2. **Remove suffix:** Strip \"Tool\" or \"tool\" â `ExecutePythonCode`\n3. **Convert to snake_case:** Apply `inflection.underscore()` â `execute_python_code`\n4. **Output:** Tool identifier for function calling\n\n### Example Transformations\n\n| Class Name | Generated Tool Name |\n|------------|---------------------|\n| `ExecutePythonCodeTool` | `execute_python_code` |\n| `RecursivePlanTreeTodoTool` | `recursive_plan_tree_todo` |\n| `WeatherQueryTool` | `weather_query` |\n| `SearchTool` | `search` |\n\nThe library `inflection` handles the camelCase to snake_case conversion reliably.\n\n**Sources:** [src/agent/tool/base_tool.py:4](), [src/agent/tool/base_tool.py:13-18]()\n\n---\n\n## Adding New Tools to the System\n\nTo add a new tool to the agent system:\n\n### Step 1: Create Tool Class\n\nCreate a new file in `src/agent/tool/` that inherits from `BaseTool`:\n\n```python\nfrom src.agent.tool.base_tool import BaseTool\nfrom pydantic import Field\n\nclass MyNewTool(BaseTool):\n    \"\"\"Description of what this tool does.\"\"\"\n    \n    param1: str = Field(..., description=\"Parameter description\")\n    \n    def run(self) -> str:\n        # Implementation\n        return \"result\"\n```\n\n### Step 2: Register in Dispatcher\n\nAdd a conditional branch in `call_tools_safely()` at [src/agent/action.py:14-20]():\n\n```python\nelif function_name == MyNewTool.tool_name():\n    my_new_tool = MyNewTool(**arguments)\n    tool_info[\"content\"] = my_new_tool.run()\n```\n\n### Step 3: Import Tool\n\nAdd the import statement at the top of [src/agent/action.py]():\n\n```python\nfrom src.agent.tool.my_new_tool import MyNewTool\n```\n\nThe tool is now automatically available to the agent through the schema generation system.\n\n**Sources:** [src/agent/action.py:7-8](), [src/agent/action.py:14-20]()\n\n---\n\n## Design Rationale\n\n### Why Pydantic BaseModel?\n\nUsing Pydantic as the foundation provides several advantages:\n\n1. **Automatic Validation:** Parameters are validated at instantiation time\n2. **Schema Generation:** JSON Schema is auto-generated from type hints\n3. **Documentation:** Field descriptions become part of the schema\n4. **Type Safety:** IDE support for autocomplete and type checking\n5. **Serialization:** Built-in JSON serialization for logging and debugging\n\n### Why Class Methods for Metadata?\n\nTool identification methods (`tool_name()`, `tool_description()`, `get_tool_schema()`) are class methods because:\n\n1. They describe the tool class, not a specific instance\n2. They can be called without instantiation\n3. They're needed before parameters are available\n4. They enable tool discovery and schema generation at startup\n\n### Why String Return Type?\n\nThe `run()` method returns `str` rather than structured data because:\n\n1. **LLM Compatibility:** String results integrate seamlessly into conversation context\n2. **Flexibility:** Tools can format output for readability\n3. **Error Messages:** Failures can be described naturally\n4. **Consistency:** Uniform interface regardless of tool complexity\n\n**Sources:** [src/agent/tool/base_tool.py:1-76]()\n\n---\n\n## Summary\n\nThe `BaseTool` interface provides a standardized contract for all tools in the algo_agent system:\n\n- **Identification:** Automatic tool naming and description extraction\n- **Schema Generation:** OpenAI-compatible function calling schemas\n- **Parameter Validation:** Pydantic-based type checking and constraints\n- **Execution Contract:** Abstract `run()` method for tool logic\n- **Error Handling:** Wrapped in `call_tools_safely()` for robustness\n- **Extensibility:** Simple pattern for adding new tools\n\nThis architecture enables the LLM to discover, understand, and invoke tools dynamically while maintaining type safety and comprehensive error handling.\n\n**Sources:** [src/agent/tool/base_tool.py:1-76](), [src/agent/action.py:1-49]()\n\n---\n\n# Page: Python Code Execution Tool\n\n# Python Code Execution Tool\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [README.md](README.md)\n- [docs/error correction.design.md](docs/error correction.design.md)\n- [docs/log.md](docs/log.md)\n- [src/agent/tool/python_tool.py](src/agent/tool/python_tool.py)\n- [src/agent/tool/todo_tool.py](src/agent/tool/todo_tool.py)\n- [src/runtime/cwd.py](src/runtime/cwd.py)\n- [src/runtime/subprocess_python_executor.py](src/runtime/subprocess_python_executor.py)\n- [src/runtime/workspace.py](src/runtime/workspace.py)\n- [src/utils/__pycache__/__init__.cpython-312.pyc](src/utils/__pycache__/__init__.cpython-312.pyc)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThe `ExecutePythonCodeTool` is a core agent capability that enables the execution of arbitrary Python code snippets in an isolated, stateful environment. This tool serves as the agent's computational engine, allowing it to perform data processing, algorithm implementation, calculations, and file operations during its reasoning process.\n\n**Scope of this document:**\n- Tool interface, parameters, and schema definition\n- Execution workflow from tool invocation to result delivery\n- State persistence mechanisms across multiple executions\n- Working directory management and environment setup\n- Error handling and timeout enforcement\n- Return format and LLM feedback\n\n**Related documentation:**\n- For the abstract tool interface: see [BaseTool Interface](#4.1)\n- For detailed execution strategies (subprocess, subthread, direct): see [Execution Runtime](#5)\n- For global variable management: see [Workspace State Management](#5.5)\n- For execution result schemas: see [ExecutionResult and Status Handling](#5.4)\n\n---\n\n## Tool Definition and Schema\n\nThe `ExecutePythonCodeTool` class extends `BaseTool` and defines two primary parameters that the LLM must provide when invoking the tool:\n\n| Parameter | Type | Required | Default | Description |\n|-----------|------|----------|---------|-------------|\n| `python_code_snippet` | `str` | Yes | - | Valid Python code to execute. Must not contain malicious operations. |\n| `timeout` | `int` | No | 30 | Maximum execution time in seconds before termination. |\n\n### Tool Description for LLM Context\n\nThe tool provides a detailed description that guides the LLM on proper usage:\n\n```\nå¿é¡»è°ç¨å¨æ¯ä¸è½®æ¨çä¸­ï¼ä½ä¸ºè®¡ç®å·¥å·ã\nå¨æç¶æçç¯å¢ä¸­æ§è¡Pythonä»£ç çæ®µï¼ç±»ä¼¼äºå¨Jupyter Notebookä¸­è¿è¡ååæ ¼ã\n\nåè½ï¼\n- ä½¿ç¨ Python 3.12 æ§è¡è®¡ç®ãæ°æ®å¤çåé»è¾æ§è¡\n- ç¶ææä¹åï¼å¨ä¸æ¬¡è°ç¨ä¸­å®ä¹çåéåå½æ°ä¼ä¸ºåç»­è°ç¨ä¿ç\n- éè¯¯åé¦ï¼å¦æä»£ç è¿è¡å¤±è´¥ï¼å°è¿åè¯¦ç»çåæº¯ä¿¡æ¯\n\nå³é®è§åï¼\n1. è¾åºå¯è§æ§ï¼å¿é¡»ä½¿ç¨ print(...) æè½æ¥çä»»ä½ç»æ\n2. å¯¼å¥æ¨¡åï¼ç±äºåºååéå¶ï¼æ¨¡åä¸ä¼æç»­å­å¨ï¼å¨æ¯ä¸ªä»£ç çæ®µä¸­å§ç»éæ°å¯¼å¥å¿è¦çæ¨¡å\n3. å®å¨æ§ï¼æ éå¾ªç¯æè¿è¡æ¶é´æé¿çä»£ç å°è¢«è¶æ¶æºå¶ç»æ­¢\n4. ä¾èµç®¡çï¼å¯ä½¿ç¨ subprocess.check_call([\"uv\", \"add\", package_name]) å®è£ä¾èµ\n```\n\nSources: [src/agent/tool/python_tool.py:13-40]()\n\n---\n\n## Execution Flow Architecture\n\n### High-Level Workflow\n\nThe following diagram illustrates how code execution flows from tool invocation through the execution runtime and workspace management back to the LLM:\n\n```mermaid\ngraph TB\n    LLM[\"LLM (qwen-plus)\"]\n    AgentLoop[\"Agent Decision Loop\"]\n    PythonTool[\"ExecutePythonCodeTool\"]\n    WorkspaceGet[\"workspace.get_arg_globals()\"]\n    Executor[\"subthread_python_executor<br/>run_structured_in_thread()\"]\n    WorkspaceAppend[\"workspace.append_out_globals()\"]\n    \n    LLM -->|\"tool_call with<br/>python_code_snippet<br/>timeout\"| AgentLoop\n    AgentLoop -->|\"dispatch to tool\"| PythonTool\n    PythonTool -->|\"1. Retrieve execution context\"| WorkspaceGet\n    WorkspaceGet -->|\"Returns filtered globals dict\"| PythonTool\n    PythonTool -->|\"2. Execute code with context\"| Executor\n    Executor -->|\"Returns ExecutionResult<br/>(status, stdout, globals)\"| PythonTool\n    PythonTool -->|\"3. Persist output globals\"| WorkspaceAppend\n    PythonTool -->|\"4. Return ret_tool2llm\"| AgentLoop\n    AgentLoop -->|\"tool result message\"| LLM\n    \n    subgraph \"State Management\"\n        WorkspaceGet\n        WorkspaceAppend\n        GlobalsList[\"arg_globals_list<br/>out_globals_list\"]\n        WorkspaceGet -.->|\"reads from\"| GlobalsList\n        WorkspaceAppend -.->|\"writes to\"| GlobalsList\n    end\n```\n\nSources: [src/agent/tool/python_tool.py:41-50](), [src/runtime/workspace.py:81-98]()\n\n### Code Execution Method\n\nThe `run()` method orchestrates the entire execution process:\n\n```python\ndef run(self) -> str:\n    # 1. Retrieve the current execution context (global variables from previous executions)\n    execution_context: Optional[Dict[str, Any]] = workspace.get_arg_globals()\n    \n    # 2. Log the code snippet for debugging\n    global_logger.info(f\"æ§è¡Pythonä»£ç çæ®µï¼{pprint.pformat(self.python_code_snippet)}\")\n    \n    # 3. Execute code in subthread executor with timeout\n    exec_result: subthread_python_executor.ExecutionResult = subthread_python_executor.run_structured_in_thread(\n        command=self.python_code_snippet, \n        _globals=execution_context,\n        timeout=self.timeout\n    )\n    \n    # 4. Persist modified globals for next execution\n    workspace.append_out_globals(exec_result.arg_globals)\n    \n    # 5. Return formatted result to LLM\n    return exec_result.ret_tool2llm\n```\n\nSources: [src/agent/tool/python_tool.py:41-50]()\n\n---\n\n## State Persistence Mechanism\n\nOne of the tool's most important features is **stateful execution** - variables and functions defined in one execution persist to subsequent executions, mimicking a Jupyter notebook environment.\n\n### Workspace Global Variables\n\nThe workspace module maintains two global lists that track execution state:\n\n| List | Purpose | Content |\n|------|---------|---------|\n| `arg_globals_list` | Input globals for each execution | Filtered globals dict passed to executor |\n| `out_globals_list` | Output globals after each execution | Modified globals returned from executor |\n\n### State Flow Diagram\n\n```mermaid\nsequenceDiagram\n    participant Tool as ExecutePythonCodeTool\n    participant WSGet as workspace.get_arg_globals()\n    participant WSFilter as filter_and_deepcopy_globals()\n    participant Executor as Execution Runtime\n    participant WSAppend as workspace.append_out_globals()\n    \n    Note over Tool: Execution 1: x = 10\n    Tool->>WSGet: Retrieve context\n    WSGet->>WSGet: arg_globals_list empty,<br/>initialize_workspace()\n    WSGet->>WSFilter: Filter builtins & modules\n    WSFilter-->>Tool: {'__name__': '__main__'}\n    Tool->>Executor: exec(\"x = 10\", globals)\n    Executor-->>Tool: ExecutionResult with<br/>arg_globals={'x': 10}\n    Tool->>WSAppend: Persist output\n    WSAppend->>WSFilter: Filter & deepcopy\n    WSFilter->>WSAppend: Add to out_globals_list\n    \n    Note over Tool: Execution 2: print(x + 5)\n    Tool->>WSGet: Retrieve context\n    WSGet->>WSGet: Read from out_globals_list[-1]\n    WSGet->>WSFilter: Filter & deepcopy\n    WSFilter-->>Tool: {'x': 10, '__name__': '__main__'}\n    Tool->>Executor: exec(\"print(x + 5)\", globals)\n    Executor-->>Tool: ExecutionResult with stdout=\"15\"\n    Tool->>WSAppend: Persist output\n```\n\nSources: [src/runtime/workspace.py:81-98](), [src/runtime/workspace.py:38-78]()\n\n### Global Variable Filtering\n\nTo ensure cross-process serialization, the workspace filters globals before persistence:\n\n**Filtering Rules:**\n1. **Exclude `__builtins__`** - Standard Python builtins are not serialized\n2. **Exclude module objects** - Module references cannot be pickled reliably\n3. **Verify pickle serialization** - Only objects that can be serialized via `pickle.dumps()` are retained\n4. **Deep copy values** - All retained values are deep-copied to prevent reference issues\n\n```python\ndef filter_and_deepcopy_globals(original_globals: Dict[str, Any]) -> Dict[str, Any]:\n    filtered_dict = {}\n    for key, value in original_globals.items():\n        # Exclude __builtins__\n        if key == '__builtins__':\n            continue\n        # Exclude module types\n        import sys\n        if isinstance(value, type(sys)):\n            continue\n        # Verify picklability\n        if _is_serializable(value) is False:\n            continue\n        # Deep copy valid values\n        filtered_dict[key] = copy.deepcopy(value)\n    return filtered_dict\n```\n\nSources: [src/runtime/workspace.py:38-78]()\n\n---\n\n## Execution Runtime Integration\n\nWhile the `ExecutePythonCodeTool` defaults to using `subthread_python_executor`, it can be configured to use different execution strategies. The tool abstracts the execution backend, focusing on the interface contract rather than implementation details.\n\n### Executor Selection\n\nThe tool currently uses the **subthread executor** for a balance of isolation and performance:\n\n```python\nexec_result: subthread_python_executor.ExecutionResult = subthread_python_executor.run_structured_in_thread(\n    command=self.python_code_snippet, \n    _globals=execution_context,\n    timeout=self.timeout\n)\n```\n\n**Rationale for subthread executor:**\n- **Moderate isolation**: Runs in separate thread, preventing blocking of main agent loop\n- **Shared memory**: Can access and modify globals dictionary without IPC overhead\n- **Timeout support**: Can enforce execution time limits\n- **Stdout capture**: Redirects output to buffer for collection\n\nFor detailed information on all execution strategies (subprocess, subthread, direct), see [Execution Runtime](#5).\n\nSources: [src/agent/tool/python_tool.py:44-48]()\n\n---\n\n## Timeout and Error Handling\n\n### Timeout Enforcement\n\nThe tool accepts a `timeout` parameter (default: 30 seconds) that limits execution time:\n\n```python\ntimeout: int = Field(\n    30, \n    description=\"æ§è¡ä»£ç çæå¤§æ¶é´ï¼ç§ï¼ãå¦æä»£ç è¿è¡æ¶é´è¶è¿æ­¤å¼ï¼å°è¢«ç»æ­¢å¹¶è¿åéè¯¯æ¶æ¯ã\"\n)\n```\n\nWhen a timeout occurs:\n1. The executor terminates the running code\n2. An `ExecutionResult` is created with `exit_status=ExecutionStatus.TIMEOUT`\n3. The LLM receives a formatted error message indicating the timeout\n\n### Error Categories\n\nThe execution system captures four types of execution outcomes:\n\n| Status | Enum Value | Cause | Example |\n|--------|-----------|-------|---------|\n| Success | `ExecutionStatus.SUCCESS` | Code executed without exceptions | `print(\"hello\")` |\n| Failure | `ExecutionStatus.FAILURE` | Python exception raised | `1 / 0` (ZeroDivisionError) |\n| Timeout | `ExecutionStatus.TIMEOUT` | Execution exceeded timeout limit | `while True: pass` |\n| Crashed | `ExecutionStatus.CRASHED` | Process/thread crashed unexpectedly | Segmentation fault |\n\n### Exception Information\n\nWhen execution fails, the `ExecutionResult` captures comprehensive error details:\n\n```python\nExecutionResult(\n    exit_status=ExecutionStatus.FAILURE,\n    exception_repr=repr(e),              # \"ZeroDivisionError('division by zero')\"\n    exception_type=type(e).__name__,     # \"ZeroDivisionError\"\n    exception_value=str(e),              # \"division by zero\"\n    exception_traceback=traceback.format_exc()  # Full stack trace\n)\n```\n\nThis detailed feedback allows the LLM to diagnose and correct errors in subsequent tool calls.\n\nSources: [src/runtime/subprocess_python_executor.py:52-69](), [src/runtime/schemas.py]()\n\n---\n\n## Working Directory Management\n\nEach execution occurs in a dedicated working directory to isolate file operations and prevent conflicts between concurrent executions.\n\n### Directory Context Setup\n\nThe `cwd` module provides working directory management:\n\n```python\n# In subprocess executor\ndef _worker_with_pipe(...):\n    # Set working directory for this execution\n    cwd.create_cwd('./wsm/2/g7-2')\n    # ... execute code ...\n```\n\n**Directory structure:**\n```\nalgo_agent/\nâââ wsm/          # Workspace root\nâ   âââ 1/        # Execution group 1\nâ   â   âââ g4-1/ # Specific execution workspace\nâ   âââ 2/        # Execution group 2\nâ   â   âââ g7-2/ # Specific execution workspace\nâ   âââ 3/        # Execution group 3\n```\n\n### Context Manager Pattern\n\nFor temporary directory changes, the `ChangeDirectory` context manager ensures proper cleanup:\n\n```python\nwith ChangeDirectory('./wsm/3/g8-1'):\n    # Execute operations in this directory\n    # File I/O, data processing, etc.\n    pass\n# Automatically restored to original directory\n```\n\nThis pattern is useful when:\n- Code needs to read/write files in a specific location\n- Multiple executions must be isolated from each other\n- Testing different working directory scenarios\n\nSources: [src/runtime/cwd.py:9-28](), [src/runtime/cwd.py:31-54](), [src/runtime/subprocess_python_executor.py:27-28]()\n\n---\n\n## Return Format and LLM Feedback\n\n### ExecutionResult Schema\n\nThe executor returns a structured `ExecutionResult` object containing all execution details:\n\n```mermaid\nclassDiagram\n    class ExecutionResult {\n        +str arg_command\n        +dict arg_globals\n        +int arg_timeout\n        +ExecutionStatus exit_status\n        +int exit_code\n        +str ret_stdout\n        +str ret_tool2llm\n        +str exception_repr\n        +str exception_type\n        +str exception_value\n        +str exception_traceback\n    }\n    \n    class ExecutionStatus {\n        <<enumeration>>\n        SUCCESS\n        FAILURE\n        TIMEOUT\n        CRASHED\n    }\n    \n    ExecutionResult --> ExecutionStatus\n```\n\n### LLM-Formatted Response\n\nThe `ret_tool2llm` field provides a formatted message tailored for LLM consumption:\n\n**Success Example:**\n```\næ§è¡æåï¼\næ åè¾åºï¼\nHello, World!\nResult: 42\n```\n\n**Failure Example:**\n```\næ§è¡å¤±è´¥ï¼\néè¯¯ç±»åï¼ZeroDivisionError\néè¯¯ä¿¡æ¯ï¼division by zero\nå¼å¸¸å æ ï¼\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nZeroDivisionError: division by zero\n```\n\n**Timeout Example:**\n```\næ§è¡è¶æ¶ï¼\nè¶æ¶è®¾ç½®ï¼30ç§\nä»£ç å¯è½åå«æ éå¾ªç¯æèæ¶è¿é¿çæä½ã\n```\n\nThis formatted feedback enables the LLM to:\n1. Understand whether execution succeeded\n2. Read captured output via `print()` statements\n3. Diagnose errors from exception details\n4. Adjust code and retry with corrections\n\nSources: [src/runtime/schemas.py](), [src/agent/tool/python_tool.py:50]()\n\n---\n\n## Execution Flow Example\n\n### Natural Language to Code Entity Mapping\n\n```mermaid\ngraph LR\n    subgraph \"Natural Language Space\"\n        UserIntent[\"User: 'Calculate<br/>sum of 1 to 100'\"]\n        LLMDecision[\"LLM decides to<br/>use Python tool\"]\n    end\n    \n    subgraph \"Tool Layer\"\n        ToolCall[\"tool_call with<br/>python_code_snippet:<br/>'sum(range(1, 101))'\"]\n        ExecutePython[\"ExecutePythonCodeTool.run()\"]\n    end\n    \n    subgraph \"Execution Layer\"\n        GetGlobals[\"workspace.get_arg_globals()\"]\n        RunThread[\"run_structured_in_thread()\"]\n        AppendGlobals[\"workspace.append_out_globals()\"]\n    end\n    \n    subgraph \"Result Space\"\n        ExecResult[\"ExecutionResult<br/>exit_status=SUCCESS<br/>ret_stdout='5050'\"]\n        ToolResult[\"tool result message<br/>to LLM\"]\n        LLMResponse[\"LLM: 'The sum<br/>is 5050'\"]\n    end\n    \n    UserIntent --> LLMDecision\n    LLMDecision --> ToolCall\n    ToolCall --> ExecutePython\n    ExecutePython --> GetGlobals\n    GetGlobals --> RunThread\n    RunThread --> ExecResult\n    ExecResult --> AppendGlobals\n    AppendGlobals --> ToolResult\n    ToolResult --> LLMResponse\n```\n\nSources: [src/agent/tool/python_tool.py:41-50]()\n\n### Multi-Execution State Example\n\nThe following example demonstrates state persistence across multiple tool invocations:\n\n**Execution 1: Define data structure**\n```python\n# LLM calls tool with:\npython_code_snippet = \"\"\"\nimport json\ndata = {'users': [{'id': 1, 'name': 'Alice'}, {'id': 2, 'name': 'Bob'}]}\nprint(\"Data initialized\")\n\"\"\"\n# Result: \"Data initialized\"\n# State: data variable persisted\n```\n\n**Execution 2: Process data**\n```python\n# LLM calls tool with:\npython_code_snippet = \"\"\"\nimport json  # Must re-import (modules don't persist)\nuser_names = [u['name'] for u in data['users']]\nprint(\"User names:\", user_names)\n\"\"\"\n# Result: \"User names: ['Alice', 'Bob']\"\n# State: data and user_names both persisted\n```\n\n**Execution 3: Compute statistics**\n```python\n# LLM calls tool with:\npython_code_snippet = \"\"\"\ncount = len(user_names)\nprint(f\"Total users: {count}\")\n\"\"\"\n# Result: \"Total users: 2\"\n# State: All variables (data, user_names, count) persisted\n```\n\nThis stateful execution model allows the agent to:\n- Break complex operations into logical steps\n- Inspect intermediate results between operations\n- Build up complex data structures incrementally\n- Reuse computed values across multiple reasoning steps\n\nSources: [src/agent/tool/python_tool.py:14-40](), [src/runtime/workspace.py:81-98]()\n\n---\n\n## Module Import Considerations\n\nDue to serialization constraints in the state persistence mechanism, **imported modules do not persist across executions**. The tool description explicitly guides the LLM on this behavior:\n\n**Rule:** Always re-import necessary modules in each code snippet.\n\n**Correct Pattern:**\n```python\n# Execution 1\nimport numpy as np\narr = np.array([1, 2, 3])\nprint(arr)\n\n# Execution 2\nimport numpy as np  # Re-import required\nresult = np.sum(arr)  # arr persists, np does not\nprint(result)\n```\n\n**Incorrect Pattern:**\n```python\n# Execution 1\nimport numpy as np\narr = np.array([1, 2, 3])\n\n# Execution 2\nresult = np.sum(arr)  # Error: 'np' is not defined\n```\n\n**Why modules don't persist:**\n- Module objects fail the `_is_serializable()` check in `filter_and_deepcopy_globals()`\n- They are excluded from `out_globals_list`\n- This prevents pickle serialization errors and ensures clean cross-process boundaries\n\nSources: [src/agent/tool/python_tool.py:24-26](), [src/runtime/workspace.py:68-73]()\n\n---\n\n## Dependency Installation\n\nThe tool supports dynamic dependency installation during execution:\n\n```python\npython_code_snippet = \"\"\"\nimport subprocess\nsubprocess.check_call([\"uv\", \"add\", \"pandas\"])\nimport pandas as pd\ndf = pd.DataFrame({'a': [1, 2, 3]})\nprint(df)\n\"\"\"\n```\n\nThis capability enables the agent to:\n1. Detect missing dependencies from `ImportError` exceptions\n2. Install required packages using the `uv` package manager\n3. Retry execution with the newly installed dependency\n4. Continue with data processing or computation\n\n**Note:** Installation executes in the same subprocess/thread as the code, so installed packages are immediately available.\n\nSources: [src/agent/tool/python_tool.py:27]()\n\n---\n\n## Integration with Agent Loop\n\nThe Python execution tool integrates seamlessly with the agent's decision loop:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Agent as Deep Research Agent\n    participant LLM\n    participant Action as Action Coordinator\n    participant PyTool as ExecutePythonCodeTool\n    participant Executor\n    \n    User->>Agent: \"Process this dataset\"\n    Agent->>LLM: Generate response with tools\n    LLM-->>Agent: tool_call: ExecutePythonCodeTool\n    Agent->>Action: call_tools_safely()\n    Action->>PyTool: Instantiate with parameters\n    PyTool->>PyTool: workspace.get_arg_globals()\n    PyTool->>Executor: run_structured_in_thread()\n    Executor-->>PyTool: ExecutionResult\n    PyTool->>PyTool: workspace.append_out_globals()\n    PyTool-->>Action: ret_tool2llm\n    Action-->>Agent: Tool result\n    Agent->>LLM: Continue with tool result\n    LLM-->>Agent: Next action or final answer\n    Agent-->>User: Response\n```\n\n**Key integration points:**\n1. **Tool discovery**: Agent includes tool schema in LLM context\n2. **Parameter validation**: Pydantic validates `python_code_snippet` and `timeout`\n3. **Safe execution**: `call_tools_safely()` wraps execution with exception handling\n4. **Result appending**: Tool result added to message history for LLM context\n5. **Iterative refinement**: LLM can call tool multiple times to refine solutions\n\nFor details on the action coordination mechanism, see [Action Coordination](#3.4).\n\nSources: [src/agent/tool/python_tool.py:13-50]()\n\n---\n\n## Summary\n\nThe `ExecutePythonCodeTool` provides the agent with powerful computational capabilities through:\n\n| Feature | Implementation | Benefit |\n|---------|----------------|---------|\n| **Stateful Execution** | `workspace.get_arg_globals()` / `append_out_globals()` | Variables persist like Jupyter notebooks |\n| **Isolation** | Subthread executor with timeout | Safe execution without blocking agent |\n| **Error Feedback** | `ExecutionResult` with traceback | LLM can diagnose and fix errors |\n| **Output Capture** | Redirected stdout/stderr | LLM sees print() output |\n| **Timeout Protection** | Enforced time limits | Prevents infinite loops |\n| **Working Directory** | `cwd.create_cwd()` | Isolated file operations |\n| **Dependency Management** | `uv add` support | Dynamic package installation |\n\nThis tool serves as the primary mechanism for the agent to perform data analysis, algorithm implementation, file processing, and computational tasks during its research and problem-solving activities.\n\nSources: [src/agent/tool/python_tool.py:13-50](), [src/runtime/workspace.py](), [src/runtime/subprocess_python_executor.py](), [src/runtime/cwd.py]()\n\n---\n\n# Page: Recursive Task Planning Tool\n\n# Recursive Task Planning Tool\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/agent/tool/base_tool.py](src/agent/tool/base_tool.py)\n- [src/agent/tool/python_tool.py](src/agent/tool/python_tool.py)\n- [src/agent/tool/todo_tool.py](src/agent/tool/todo_tool.py)\n- [src/memory/tree_todo/schemas.py](src/memory/tree_todo/schemas.py)\n- [src/memory/tree_todo/todo_track.py](src/memory/tree_todo/todo_track.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThe Recursive Task Planning Tool (`RecursivePlanTreeTodoTool`) enables the agent to maintain hierarchical task structures with nested subtasks, status tracking, and automated change detection. This tool allows the LLM to decompose complex research or problem-solving goals into manageable task trees, track progress across multiple reasoning steps, and analyze what has changed between planning iterations.\n\nThis document covers the tool's implementation, data schemas, version tracking mechanism, and integration with the agent loop. For information about the base tool interface, see [BaseTool Interface](#4.1). For code execution capabilities, see [Python Code Execution Tool](#4.2).\n\n**Sources:** [src/agent/tool/todo_tool.py:1-37](), [src/memory/tree_todo/schemas.py:1-81]()\n\n---\n\n## Tool Interface\n\nThe `RecursivePlanTreeTodoTool` class extends `BaseTool` and provides a structured interface for managing recursive task trees.\n\n### Class Definition\n\n```\nRecursivePlanTreeTodoTool(BaseTool)\nâââ tool_call_purpose: str          # Required: purpose of this tool invocation\nâââ recursive_plan_tree: RecursivePlanTree  # The task tree to process\n```\n\nThe tool's `run()` method delegates to `todo_track.run()`, which:\n1. Stores the current plan tree as a new version\n2. Compares with the previous version to detect changes\n3. Renders a Markdown visualization of the tree\n4. Calculates status statistics\n\nThe tool's docstring specifies that it **must be called** before other tools as part of the agent's thinking and recording process, then other tools can be invoked for execution.\n\n**Sources:** [src/agent/tool/todo_tool.py:10-36]()\n\n### Tool Schema Generation\n\nLike all tools, `RecursivePlanTreeTodoTool` automatically generates a JSON schema for LLM consumption through the inherited `get_tool_schema()` method. The schema includes:\n\n- `tool_call_purpose`: Description of why the tool is being invoked\n- `recursive_plan_tree`: A nested structure with `RecursivePlanTree` definition including all node properties\n\nThe schema uses JSON Schema `$defs` to define recursive references for `RecursivePlanTreeNode`, allowing unlimited nesting depth.\n\n**Sources:** [src/agent/tool/base_tool.py:30-71](), [logs/global.log:214-308]()\n\n---\n\n## Data Model\n\nThe task planning system uses three core Pydantic models to represent hierarchical task structures.\n\n### Task Status Enumeration\n\nThe `TaskStatus` enum defines six possible states for any task node:\n\n| Status | Value | Symbol | Description |\n|--------|-------|--------|-------------|\n| PENDING | `\"pending\"` | â³ | Task is waiting to be executed |\n| PROCESSING | `\"processing\"` | â¡ï¸ | Task is currently being executed |\n| COMPLETED | `\"completed\"` | â | Task finished successfully |\n| FAILED | `\"failed\"` | â | Task execution failed |\n| RETRY | `\"retry\"` | â»ï¸ | Task needs to be retried |\n| SKIPPED | `\"skipped\"` | â | Task was intentionally skipped |\n\nEach status includes display properties:\n- `display_symbol`: Visual emoji indicator for Markdown rendering\n- `display_desc`: Human-readable description in Chinese\n\n**Sources:** [src/memory/tree_todo/schemas.py:7-41]()\n\n### RecursivePlanTreeNode\n\nThe core task unit that can contain child tasks recursively:\n\n```python\nRecursivePlanTreeNode:\n    task_id: str                          # Auto-generated UUID (e.g., \"TASK-123e4567\")\n    task_name: str                        # Unique descriptive name (required)\n    description: str = \"\"                 # Detailed explanation (optional)\n    status: TaskStatus = PENDING          # Current execution state\n    output: str = \"\"                      # Results when completed/failed\n    dependencies: List[str] | None        # List of task_name strings this depends on\n    research_directions: List[str] | None # Research paths for complex tasks\n    children: List[RecursivePlanTreeNode] | None  # Nested subtasks\n```\n\n**Key behaviors:**\n- `task_id` is auto-generated using UUID if not provided\n- `task_name` must be globally unique as it's referenced by `dependencies` lists\n- `children` is set to `None` if empty (validated by `empty_children_to_none`)\n- Self-referential structure resolved via `model_rebuild()`\n\n**Sources:** [src/memory/tree_todo/schemas.py:44-64]()\n\n### RecursivePlanTree\n\nThe top-level container representing the entire planning structure:\n\n```python\nRecursivePlanTree:\n    core_goal: str                        # Overall objective (required)\n    tree_nodes: List[RecursivePlanTreeNode]  # Root-level tasks\n    next_action: Dict[str, Any] = {}      # Suggested next steps (optional)\n    references: List[str] | None          # Resource links/citations (optional)\n```\n\nThe `tree_nodes` list contains all root-level tasks, each of which may have nested children forming a hierarchical structure.\n\n**Sources:** [src/memory/tree_todo/schemas.py:68-80]()\n\n### Data Model Diagram\n\n```mermaid\nclassDiagram\n    class TaskStatus {\n        <<enumeration>>\n        PENDING\n        PROCESSING\n        COMPLETED\n        FAILED\n        RETRY\n        SKIPPED\n        +display_symbol() str\n        +display_desc() str\n    }\n    \n    class RecursivePlanTreeNode {\n        +task_id: str\n        +task_name: str\n        +description: str\n        +status: TaskStatus\n        +output: str\n        +dependencies: List[str]\n        +research_directions: List[str]\n        +children: List[RecursivePlanTreeNode]\n    }\n    \n    class RecursivePlanTree {\n        +core_goal: str\n        +tree_nodes: List[RecursivePlanTreeNode]\n        +next_action: Dict\n        +references: List[str]\n    }\n    \n    class RecursivePlanTreeTodoTool {\n        +tool_call_purpose: str\n        +recursive_plan_tree: RecursivePlanTree\n        +run() str\n    }\n    \n    RecursivePlanTreeNode --> TaskStatus\n    RecursivePlanTreeNode --> RecursivePlanTreeNode : children\n    RecursivePlanTree --> RecursivePlanTreeNode : tree_nodes\n    RecursivePlanTreeTodoTool --> RecursivePlanTree\n```\n\n**Sources:** [src/memory/tree_todo/schemas.py:1-81](), [src/agent/tool/todo_tool.py:10-26]()\n\n---\n\n## Version Tracking System\n\nThe tool maintains a history of plan tree versions to enable change detection and incremental updates.\n\n### Global State Variables\n\nThe tracking system maintains three global lists in `todo_track.py`:\n\n```python\narg_todo_list: List[RecursivePlanTree]  # Input history (all submitted versions)\nout_todo_list: List[RecursivePlanTree]  # Output history (processed versions)\ntrack_diff_result_list: List[str]       # Change summaries\n```\n\nEach list is initialized with an empty placeholder tree with `core_goal=\"ç©ºè®¡åæ ç­å¾åå§å\"`.\n\n**Sources:** [src/memory/tree_todo/todo_track.py:6-18]()\n\n### Version Comparison Workflow\n\n```mermaid\nsequenceDiagram\n    participant LLM as \"LLM (Agent)\"\n    participant Tool as \"RecursivePlanTreeTodoTool\"\n    participant Track as \"todo_track.run()\"\n    participant Analyze as \"_analyze_changes()\"\n    participant Render as \"_render_plan_tree_markdown()\"\n    \n    LLM->>Tool: \"Call with recursive_plan_tree\"\n    Tool->>Track: \"run(current_plan_tree)\"\n    \n    Track->>Track: \"Retrieve last_plan from arg_todo_list[-1]\"\n    Track->>Track: \"Append current_plan_tree to arg_todo_list\"\n    \n    alt \"First invocation (no history)\"\n        Track->>Track: \"changes_summary = 'â é¦æ¬¡åå»ºè®¡åæ '\"\n    else \"Subsequent invocations\"\n        Track->>Analyze: \"_analyze_changes(last_plan, current_plan)\"\n        Analyze->>Analyze: \"collect_all_task_ids() for both trees\"\n        Analyze->>Analyze: \"Detect new, deleted, status-changed tasks\"\n        Analyze->>Analyze: \"Detect hierarchy changes (parent shifts)\"\n        Analyze-->>Track: \"Formatted change summary\"\n    end\n    \n    Track->>Render: \"_render_plan_tree_markdown(tree_nodes, 0)\"\n    Render->>Render: \"Recursively format with indentation\"\n    Render-->>Track: \"Markdown string\"\n    \n    Track->>Track: \"_calculate_status_statistics()\"\n    Track-->>Tool: \"Dict with changes_summary, markdown, stats\"\n    Tool-->>LLM: \"Formatted string output\"\n```\n\n**Sources:** [src/memory/tree_todo/todo_track.py:21-49]()\n\n### Change Detection Logic\n\nThe `_analyze_changes()` function identifies four types of modifications:\n\n1. **New Tasks** (ð): Tasks present in current tree but not in last version\n   - Collected by comparing `task_id` sets\n   - Reported with task names\n\n2. **Deleted Tasks** (ðï¸): Tasks in last version but removed from current\n   - Typically indicates scope changes or task completion consolidation\n\n3. **Status Changes** (ð): Tasks with different `status` values between versions\n   - Displays transitions like \"pending â completed\"\n   - Shows old and new status descriptions\n\n4. **Hierarchy Adjustments** (ð): Tasks moved to different parent nodes\n   - Detected by comparing parent task names via `_find_parent_task()`\n   - Indicates task reorganization\n\n**Sources:** [src/memory/tree_todo/todo_track.py:62-121]()\n\n### Task Lookup Functions\n\n```python\n_get_task_by_id(nodes, task_id) -> RecursivePlanTreeNode | None\n    # Recursively searches tree for node with matching task_id\n    # Depth-first traversal through children\n\n_find_parent_task(nodes, target_task_id) -> RecursivePlanTreeNode | None\n    # Recursively finds parent node containing target_task_id in children\n    # Returns None if task is at root level\n```\n\nThese utility functions enable efficient tree traversal for change detection.\n\n**Sources:** [src/memory/tree_todo/todo_track.py:51-135]()\n\n---\n\n## Markdown Rendering\n\nThe tool generates human-readable Markdown representations of the task tree for LLM consumption and debugging.\n\n### Rendering Algorithm\n\n```mermaid\ngraph TD\n    Start[\"_render_plan_tree_markdown(nodes, indent_level)\"] --> Loop[\"For each node in nodes\"]\n    Loop --> GetStatus[\"Get node.status.display_symbol\"]\n    GetStatus --> BuildLine[\"Build task line:<br/>indent + '- ' + symbol + task_name + task_id\"]\n    \n    BuildLine --> CheckStatus{\"node.status in<br/>[FAILED, RETRY, SKIPPED]?\"}\n    CheckStatus -->|Yes| AddStatus[\"Append: '| ç¶æï¼' + status_desc\"]\n    CheckStatus -->|No| CheckDesc\n    AddStatus --> CheckDesc\n    \n    CheckDesc{\"node.description<br/>not empty?\"}\n    CheckDesc -->|Yes| AddDesc[\"Append: '  > è¯´æï¼' + description\"]\n    CheckDesc -->|No| CheckOutput\n    AddDesc --> CheckOutput\n    \n    CheckOutput{\"node.output<br/>not empty?\"}\n    CheckOutput -->|Yes| AddOutput[\"Append: '  > ç»æï¼' + output\"]\n    CheckOutput -->|No| CheckChildren\n    AddOutput --> CheckChildren\n    \n    CheckChildren{\"node.children<br/>exists?\"}\n    CheckChildren -->|Yes| Recurse[\"Recursively call with<br/>children, indent_level + 1\"]\n    CheckChildren -->|No| AppendLine\n    Recurse --> AppendLine[\"Append formatted line\"]\n    \n    AppendLine --> Loop\n    Loop --> Return[\"Return joined lines\"]\n```\n\n**Sources:** [src/memory/tree_todo/todo_track.py:137-171]()\n\n### Example Markdown Output\n\nFor a task tree with nested structure, the output format is:\n\n```markdown\n- [â³] **è¯»åå¹¶è§£æ schema.json æä»¶**ï¼IDï¼TASK-abc123ï¼\n  > è¯´æï¼å è½½ schema.json æä»¶åå®¹ï¼è§£æå¶ JSON ç»æ\n  - [â] **è¯å«å³é®å®ä½**ï¼IDï¼TASK-def456ï¼\n    > ç»æï¼å·²è¯å« Task, Carrier, Resource ç­æ ¸å¿å®ä½\n  - [â¡ï¸] **åæå®ä½é´å³ç³»**ï¼IDï¼TASK-ghi789ï¼ | ç¶æï¼æ­£å¨æ§è¡\n- [â³] **è¯»åå¹¶æ£æ¥åºæ¥ååºæ°æ®æä»¶**ï¼IDï¼TASK-jkl012ï¼\n```\n\nEach task displays:\n- Indentation level (2 spaces per level)\n- Status symbol\n- Bold task name\n- Task ID in parentheses\n- Optional description prefixed with `> è¯´æï¼`\n- Optional output prefixed with `> ç»æï¼`\n- Nested children with increased indentation\n\n**Sources:** [src/memory/tree_todo/todo_track.py:137-171]()\n\n---\n\n## Status Statistics\n\nThe tool calculates aggregate statistics across all tasks in the tree.\n\n### Statistics Structure\n\nThe `_calculate_status_statistics()` function returns a dictionary with:\n\n```python\n{\n    \"pending\": int,      # Count of PENDING tasks\n    \"processing\": int,   # Count of PROCESSING tasks\n    \"completed\": int,    # Count of COMPLETED tasks\n    \"failed\": int,       # Count of FAILED tasks\n    \"retry\": int,        # Count of RETRY tasks\n    \"skipped\": int,      # Count of SKIPPED tasks\n    \"__total\": int,      # Total task count\n    \"__completion_rate\": float,  # completed / total (rounded to 2 decimals)\n    \"__pending_rate\": float      # pending / total (rounded to 2 decimals)\n}\n```\n\n### Calculation Process\n\n1. Initialize all status counts to 0\n2. Recursively traverse `tree_nodes` and all nested `children`\n3. Increment counter for each node's `status.value`\n4. Calculate derived metrics (total, completion rate, pending rate)\n\n**Sources:** [src/memory/tree_todo/todo_track.py:173-201]()\n\n---\n\n## Integration with Agent Loop\n\nThe recursive task planning tool plays a critical role in the agent's reasoning process.\n\n### Agent Workflow with Todo Tool\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Agent as \"user_query()\"\n    participant LLM as \"LLM Service\"\n    participant TodoTool as \"RecursivePlanTreeTodoTool\"\n    participant PyTool as \"ExecutePythonCodeTool\"\n    \n    User->>Agent: \"Initial query with complex goal\"\n    Agent->>Agent: \"init_messages_with_system_prompt()\"\n    \n    Note over Agent,LLM: \"System prompt instructs:<br/>ç»´æ¤ä¸é¢åæè®¡åæ \"\n    \n    Agent->>LLM: \"Generate with tool schemas\"\n    LLM->>LLM: \"Decompose goal into task tree\"\n    LLM-->>Agent: \"tool_calls: [recursive_plan_tree_todo]\"\n    \n    Agent->>TodoTool: \"Call with initial tree structure\"\n    TodoTool->>TodoTool: \"Store in arg_todo_list[0]\"\n    TodoTool->>TodoTool: \"Render markdown + stats\"\n    TodoTool-->>Agent: \"â é¦æ¬¡åå»ºè®¡åæ <br/>Markdown visualization\"\n    \n    Agent->>Agent: \"Append tool result to messages\"\n    Agent->>LLM: \"Continue with updated context\"\n    LLM-->>Agent: \"tool_calls: [execute_python_code]\"\n    \n    Agent->>PyTool: \"Execute task code\"\n    PyTool-->>Agent: \"Execution results\"\n    \n    Agent->>LLM: \"Continue with execution results\"\n    LLM->>LLM: \"Update task statuses based on results\"\n    LLM-->>Agent: \"tool_calls: [recursive_plan_tree_todo]\"\n    \n    Agent->>TodoTool: \"Call with updated tree\"\n    TodoTool->>TodoTool: \"Compare with arg_todo_list[-1]\"\n    TodoTool->>TodoTool: \"Detect: ð ç¶æåæ´<br/>pending â completed\"\n    TodoTool-->>Agent: \"Change summary + updated markdown\"\n    \n    Agent->>Agent: \"Loop until LLM returns final answer\"\n```\n\n**Sources:** [logs/utils.log:1-58](), [logs/global.log:1-308]()\n\n### System Prompt Integration\n\nThe agent's system prompt explicitly mentions maintaining a task tree:\n\n```\nä½ çä¸»å¾ªç¯æ¯ç»´æ¤ä¸é¢åæè®¡åæ ï¼\n1. ç½åå¼å¾è®¡ç®åæ¢ç´¢çæ¹åï¼å½¢æåæè®¡åæ çèç¹ã\n2. æ ¹æ®åæè®¡åæ çèç¹ï¼å³å®æ¯å¦éè¦è°ç¨å·¥å·æ¥è·åä¿¡æ¯ã\n3. å¨å¿è¦æ¶ï¼è°ç¨å·¥å·å¹¶å¤çè¿åçä¿¡æ¯ï¼ä»¥ä¾¿æä¾åç¡®çç­æ¡ã\n4. æ ¹æ®è·åçä¿¡æ¯ï¼æ´æ°åæè®¡åæ ï¼ç»§ç»­è¿è¡åæï¼ç´å°å¾åºæç»ç»è®ºã\n```\n\nThis instructs the LLM to use the todo tool as a thinking and planning mechanism throughout the reasoning process.\n\n**Sources:** [logs/global.log:115-126]()\n\n---\n\n## Usage Example from Logs\n\nThe following demonstrates actual tool usage from the emergency response planning scenario.\n\n### Initial Tree Creation\n\nThe LLM called `recursive_plan_tree_todo` with a comprehensive research plan:\n\n```json\n{\n  \"tool_call_purpose\": \"å»ºç«åå§åæè®¡åæ ï¼æç¡®ç ç©¶ç®æ åä»»å¡åè§£æ¹å\",\n  \"recursive_plan_tree\": {\n    \"core_goal\": \"åºäºæä¾çåºæ¥ææ´æ°æ®ï¼æ·±å¥çè§£æ°æ®ç»æå¹¶è®¾è®¡å¯æµè¯çå¤æºè½ä½ååè°åº¦ç®æ³ã\",\n    \"tree_nodes\": [\n      {\n        \"task_id\": \"T1\",\n        \"task_name\": \"è¯»åå¹¶è§£æ schema.json æä»¶\",\n        \"description\": \"å è½½ schema.json æä»¶åå®¹ï¼è§£æå¶ JSON ç»æ...\",\n        \"status\": \"pending\",\n        \"research_directions\": [\n          \"è¯å«å³é®å®ä½ï¼Task, Carrier, Resource, Location...\",\n          \"åæå®ä½é´å³ç³»ï¼ä»»å¡ä¸è½½ä½çåéå³ç³»...\"\n        ],\n        \"dependencies\": null,\n        \"children\": null,\n        \"output\": \"\",\n        \"references\": [\"./schema.json\"]\n      },\n      {\n        \"task_id\": \"T2\",\n        \"task_name\": \"è¯»åå¹¶æ£æ¥åºæ¥ååºæ°æ®æä»¶\",\n        \"description\": \"æ¹éè¯»å emergency_response_data_01.json è³ 05.json...\",\n        \"status\": \"pending\",\n        \"dependencies\": [\"è¯»åå¹¶è§£æ schema.json æä»¶\"],\n        ...\n      },\n      ...\n    ]\n  }\n}\n```\n\nThe tree contained 12 tasks (T1-T12) organized hierarchically with dependencies and research directions. Tasks ranged from data structure analysis to algorithm implementation and performance evaluation.\n\n**Sources:** [logs/utils.log:57-58]()\n\n### Task Dependencies\n\nNotice that `T2` specifies:\n```json\n\"dependencies\": [\"è¯»åå¹¶è§£æ schema.json æä»¶\"]\n```\n\nThis references the `task_name` of `T1`, establishing an execution order constraint. The LLM uses these dependencies to sequence tool calls appropriately.\n\n**Sources:** [logs/utils.log:57-58]()\n\n### Multiple Tool Calls Pattern\n\nAfter establishing the plan tree, the LLM made multiple `execute_python_code` tool calls in a single response to execute tasks in parallel:\n\n```python\ntool_calls=[\n    ChatCompletionMessageFunctionToolCall(\n        id='call_3085f1f75d534390a7c2b7',\n        function=Function(\n            name='execute_python_code',\n            arguments='{\"tool_call_purpose\": \"è¯»åå¹¶è§£æ schema.json æä»¶\",...}'\n        )\n    ),\n    ChatCompletionMessageFunctionToolCall(\n        id='call_ab025bb2bb3e430aaefd92',\n        function=Function(\n            name='execute_python_code',\n            arguments='{\"tool_call_purpose\": \"è¯»åå¹¶è§£æ emergency_response_data_01.json...\"}'\n        )\n    ),\n    ...\n]\n```\n\nThis demonstrates the tool's role in coordinating complex multi-step research processes.\n\n**Sources:** [logs/utils.log:127-129]()\n\n---\n\n## Implementation Notes\n\n### Empty Return Value\n\nThe current implementation of `RecursivePlanTreeTodoTool.run()` returns an empty string:\n\n```python\ndef run(self, ) -> Dict[str, str]:\n    result = todo_track.run(self.recursive_plan_tree)\n    s  = (f\"åæ´æ»ç»ï¼\\n{result['changes_summary']}\")\n    s += (f\"Markdownæ¸åï¼\\n{result['markdown_todo_list']}\\n\")\n    s += (f\"status_statistics: {pprint.pformat(result['status_statistics'])}\")\n    s = \"\"  # Overwritten to empty string\n    return s\n```\n\nThe function constructs a formatted string with change summary, markdown, and statistics, but then overwrites it with an empty string before returning. This appears to be intentional to avoid cluttering the conversation history, with the actual state management happening through the global `arg_todo_list` storage.\n\n**Sources:** [src/agent/tool/todo_tool.py:28-36]()\n\n### Model Serialization\n\nThe Pydantic v2 migration is evident in the code:\n- `model_copy(deep=True)` instead of `copy()`\n- `model_dump()` instead of `dict()`\n- `model_rebuild()` for self-referential models\n\nThese changes maintain compatibility with Pydantic v2's stricter validation and improved performance.\n\n**Sources:** [src/memory/tree_todo/todo_track.py:32](), [src/agent/tool/todo_tool.py:70]()\n\n### Task ID Generation\n\nTask IDs are automatically generated using UUID4 if not provided:\n\n```python\ntask_id: str = Field(\n    default_factory=lambda: f\"TASK-{str(uuid.uuid4())}\", \n    description=\"ä»»å¡å¯ä¸ID...\"\n)\n```\n\nHowever, the LLM can also provide explicit IDs (like \"T1\", \"T2\") for easier reference in subsequent calls.\n\n**Sources:** [src/memory/tree_todo/schemas.py:46]()\n\n---\n\n## Summary\n\nThe Recursive Task Planning Tool provides:\n\n| Capability | Implementation |\n|------------|----------------|\n| **Hierarchical Planning** | Recursive `RecursivePlanTreeNode` with unlimited nesting |\n| **Status Tracking** | Six-state `TaskStatus` enum with visual symbols |\n| **Version History** | Global `arg_todo_list` storing all submitted versions |\n| **Change Detection** | Automated comparison of task IDs, statuses, and hierarchy |\n| **Visualization** | Markdown rendering with indentation and status symbols |\n| **Statistics** | Aggregate counts and completion rates |\n| **LLM Integration** | JSON schema generation for function calling |\n\nThis tool enables the agent to maintain complex research plans across multiple reasoning iterations, providing structure and memory to the otherwise stateless LLM conversation.\n\n**Sources:** [src/agent/tool/todo_tool.py:1-133](), [src/memory/tree_todo/schemas.py:1-81](), [src/memory/tree_todo/todo_track.py:1-201]()\n\n---\n\n# Page: Execution Runtime\n\n# Execution Runtime\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [docs/crashed.design.md](docs/crashed.design.md)\n- [src/runtime/before_thread/plt_back_chinese.py](src/runtime/before_thread/plt_back_chinese.py)\n- [src/runtime/cwd.py](src/runtime/cwd.py)\n- [src/runtime/python_executor.py](src/runtime/python_executor.py)\n- [src/runtime/schemas.py](src/runtime/schemas.py)\n- [src/runtime/subprocess_python_executor.py](src/runtime/subprocess_python_executor.py)\n- [src/runtime/subthread_python_executor.py](src/runtime/subthread_python_executor.py)\n- [tests/playground/crashed.py](tests/playground/crashed.py)\n- [tests/playground/gen/g9/beijing_scenic_spot_schema_with_play_hours.json](tests/playground/gen/g9/beijing_scenic_spot_schema_with_play_hours.json)\n- [tests/playground/gen/g9/beijing_scenic_spot_validated_data_with_play_hours.json](tests/playground/gen/g9/beijing_scenic_spot_validated_data_with_play_hours.json)\n- [tests/playground/gen/g9/g9.py](tests/playground/gen/g9/g9.py)\n- [tests/playground/subprocess_output.py](tests/playground/subprocess_output.py)\n- [tests/unit/runtime/test_exec_runner.py](tests/unit/runtime/test_exec_runner.py)\n- [tests/unit/runtime/test_python_executor.py](tests/unit/runtime/test_python_executor.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThe Execution Runtime system provides safe, isolated execution of arbitrary Python code snippets with comprehensive error handling and state management. This page provides an overview of the multi-strategy execution architecture that enables the agent to run user-provided code with varying levels of isolation and performance characteristics.\n\nFor detailed information about specific execution strategies, see:\n- [Subprocess Execution](#5.1) - Process-isolated execution with pipe-based IPC\n- [Subthread Execution](#5.2) - Thread-based execution with buffer capture\n- [Direct Execution](#5.3) - In-process execution for maximum performance\n- [ExecutionResult Schema](#5.4) - Structured result format and status codes\n- [Workspace State Management](#5.5) - Global variable persistence across executions\n- [Working Directory Management](#5.6) - Environment setup and output redirection\n\n## Multi-Strategy Execution Architecture\n\nThe system implements three distinct execution strategies, each optimized for different use cases. The choice of strategy balances isolation strength, resource overhead, and execution performance.\n\n| Strategy | Isolation Level | Process Model | IPC Mechanism | Timeout Support | Crash Detection | Primary Use Case |\n|----------|----------------|---------------|---------------|-----------------|-----------------|------------------|\n| **Subprocess** | Full process isolation | `multiprocessing.Process` | Pipe-based bidirectional | Yes (terminate) | Yes (exitcode) | Production, untrusted code |\n| **Subthread** | Shared memory space | `threading.Thread` | In-memory buffer | Yes (non-blocking) | Limited | Matplotlib, GUI libraries |\n| **Direct** | No isolation | Same process | Queue | Yes (terminate worker) | No | Testing, trusted code |\n\nSources: [src/runtime/subprocess_python_executor.py](), [src/runtime/subthread_python_executor.py](), [src/runtime/python_executor.py]()\n\n## System Architecture\n\nThe following diagram illustrates how the three execution strategies interact with the core runtime components:\n\n```mermaid\ngraph TB\n    subgraph \"Execution Request\"\n        INPUT[\"Python Code String<br/>+ _globals dict<br/>+ timeout int\"]\n    end\n    \n    subgraph \"Strategy Dispatcher\"\n        TOOL[\"ExecutePythonCodeTool\"]\n        SELECTOR[\"Strategy Selection Logic\"]\n    end\n    \n    subgraph \"Execution Strategies\"\n        SUBPROCESS[\"run_structured_in_subprocess<br/>subprocess_python_executor.py\"]\n        SUBTHREAD[\"run_structured_in_thread<br/>subthread_python_executor.py\"]\n        DIRECT[\"run_structured<br/>python_executor.py\"]\n    end\n    \n    subgraph \"Execution Workers\"\n        SUBPROC_WORKER[\"_worker_with_pipe<br/>Separate Process\"]\n        THREAD_WORKER[\"_worker_with_buffer<br/>Separate Thread\"]\n        DIRECT_WORKER[\"worker_with_globals_capture<br/>Same Process\"]\n    end\n    \n    subgraph \"Runtime Infrastructure\"\n        PIPE[\"multiprocessing.Pipe<br/>_PipeType.STDOUT/RESULT\"]\n        BUFFER[\"list[str] buffer<br/>shared memory\"]\n        QUEUE[\"multiprocessing.Queue\"]\n        \n        CWD[\"cwd.create_cwd<br/>cwd.ChangeDirectory\"]\n        STDOUT[\"sys.stdout/stderr<br/>redirection\"]\n    end\n    \n    subgraph \"Result Processing\"\n        SCHEMA[\"ExecutionResult<br/>schemas.py\"]\n        STATUS[\"ExecutionStatus<br/>SUCCESS/FAILURE/TIMEOUT/CRASHED\"]\n        WORKSPACE[\"workspace.filter_and_deepcopy_globals\"]\n    end\n    \n    INPUT --> TOOL\n    TOOL --> SELECTOR\n    \n    SELECTOR -->|\"process isolation\"| SUBPROCESS\n    SELECTOR -->|\"GUI libraries\"| SUBTHREAD\n    SELECTOR -->|\"testing\"| DIRECT\n    \n    SUBPROCESS --> SUBPROC_WORKER\n    SUBTHREAD --> THREAD_WORKER\n    DIRECT --> DIRECT_WORKER\n    \n    SUBPROC_WORKER -.uses.-> PIPE\n    SUBPROC_WORKER -.uses.-> CWD\n    SUBPROC_WORKER -.redirects.-> STDOUT\n    \n    THREAD_WORKER -.uses.-> BUFFER\n    THREAD_WORKER -.uses.-> CWD\n    THREAD_WORKER -.redirects.-> STDOUT\n    \n    DIRECT_WORKER -.uses.-> QUEUE\n    DIRECT_WORKER -.redirects.-> STDOUT\n    \n    SUBPROC_WORKER --> SCHEMA\n    THREAD_WORKER --> SCHEMA\n    DIRECT_WORKER --> SCHEMA\n    \n    SCHEMA --> STATUS\n    SCHEMA --> WORKSPACE\n```\n\n**Diagram: Execution Runtime Architecture** - Shows the relationship between execution strategies, worker processes/threads, and the shared result schema.\n\nSources: [src/runtime/subprocess_python_executor.py:76-163](), [src/runtime/subthread_python_executor.py:86-128](), [src/runtime/python_executor.py:92-130]()\n\n## Execution Flow\n\nAll three strategies follow a common execution pattern with strategy-specific implementations:\n\n```mermaid\nsequenceDiagram\n    participant Tool as ExecutePythonCodeTool\n    participant Executor as Execution Strategy\n    participant Worker as Worker Process/Thread\n    participant Runtime as exec() Runtime\n    participant Result as ExecutionResult\n    \n    Tool->>Executor: run_structured_in_X(command, _globals, timeout)\n    \n    Note over Executor: Setup IPC channel<br/>(Pipe/Buffer/Queue)\n    \n    Executor->>Worker: Start worker process/thread\n    \n    Note over Worker: Redirect stdout/stderr<br/>Change working directory\n    \n    Worker->>Runtime: exec(command, _globals, _locals)\n    \n    alt Code Executes Successfully\n        Runtime-->>Worker: Execution completes\n        Worker->>Result: Create SUCCESS result\n    else Exception Raised\n        Runtime-->>Worker: Exception caught\n        Worker->>Result: Create FAILURE result<br/>+ traceback\n    else Process/Thread Crashes\n        Runtime--xWorker: Unexpected termination\n        Note over Executor: No result received\n        Executor->>Result: Create CRASHED result\n    end\n    \n    par Timeout Monitoring\n        Executor->>Executor: join(timeout)\n        alt Timeout Exceeded\n            Executor->>Worker: terminate/kill\n            Executor->>Result: Create TIMEOUT result\n        end\n    end\n    \n    Worker->>Executor: Send result via IPC\n    Executor->>Executor: Collect stdout buffer\n    Executor->>Executor: Set exit_code\n    Executor->>Executor: Generate ret_tool2llm\n    Executor-->>Tool: Return ExecutionResult\n```\n\n**Diagram: Common Execution Flow** - Sequence diagram showing the standardized execution lifecycle across all strategies.\n\nSources: [src/runtime/subprocess_python_executor.py:76-163](), [src/runtime/subthread_python_executor.py:86-128]()\n\n## ExecutionResult Schema\n\nAll execution strategies return a unified `ExecutionResult` object that encapsulates the execution outcome:\n\n### Core Structure\n\nThe `ExecutionResult` model defined in [src/runtime/schemas.py:56-71]() contains:\n\n| Field Category | Field Name | Type | Description |\n|---------------|------------|------|-------------|\n| **Input Arguments** | `arg_command` | `str` | The executed Python code |\n| | `arg_timeout` | `int` | Configured timeout in seconds |\n| | `arg_globals` | `Dict[str, Any]` | Filtered global variables (post-execution) |\n| **Execution Status** | `exit_status` | `ExecutionStatus` | Enum: SUCCESS/FAILURE/TIMEOUT/CRASHED |\n| | `exit_code` | `Optional[int]` | Process exit code (subprocess only) |\n| **Error Information** | `exception_repr` | `Optional[str]` | Python repr() of exception |\n| | `exception_type` | `Optional[str]` | Exception class name |\n| | `exception_value` | `Optional[str]` | Exception message |\n| | `exception_traceback` | `Optional[str]` | Full traceback string |\n| **Output Capture** | `ret_stdout` | `str` | Combined stdout/stderr output |\n| | `ret_tool2llm` | `Optional[str]` | Formatted message for LLM consumption |\n\nSources: [src/runtime/schemas.py:56-71]()\n\n### ExecutionStatus Enum\n\nThe `ExecutionStatus` enum [src/runtime/schemas.py:10-47]() defines four possible outcomes:\n\n```python\nclass ExecutionStatus(str, Enum):\n    SUCCESS = \"success\"   # Code executed without exceptions\n    FAILURE = \"failure\"   # Python exception raised during execution\n    TIMEOUT = \"timeout\"   # Execution exceeded timeout limit\n    CRASHED = \"crashed\"   # Process terminated unexpectedly (SegFault, OOM)\n```\n\nEach status has an associated LLM-friendly message template generated by `ExecutionStatus.get_return_llm()` [src/runtime/schemas.py:18-47]() that formats the result for the agent to understand and act upon.\n\nSources: [src/runtime/schemas.py:10-47]()\n\n## Subprocess Execution Strategy\n\nThe **subprocess executor** provides the strongest isolation by executing code in a separate OS process using `multiprocessing.Process`.\n\n### Key Characteristics\n\n- **Full Process Isolation**: Child process has its own memory space, preventing memory corruption in the parent\n- **Pipe-Based IPC**: Bidirectional communication using `multiprocessing.Pipe()` for stdout streaming and result transmission\n- **Crash Detection**: Can detect segmentation faults, memory errors, and other process crashes via exit codes\n- **Timeout Enforcement**: Uses `Process.terminate()` to forcefully kill runaway processes\n\n### Implementation Entry Point\n\n```python\n@traceable\ndef run_structured_in_subprocess(\n    command: str,\n    _globals: dict[str, Any] | None = None,\n    _locals: Optional[Dict] = None,\n    timeout: Optional[int] = None,\n) -> ExecutionResult:\n```\n\nLocation: [src/runtime/subprocess_python_executor.py:76-163]()\n\nFor detailed implementation analysis, see [Subprocess Execution](#5.1).\n\nSources: [src/runtime/subprocess_python_executor.py:19-163]()\n\n## Subthread Execution Strategy\n\nThe **subthread executor** runs code in a separate thread within the same process, sharing the memory space with the parent.\n\n### Key Characteristics\n\n- **Shared Memory Space**: All threads share the same Python interpreter and global state\n- **Buffer-Based Output**: Captures stdout/stderr using in-memory list buffer\n- **GUI Library Support**: Required for libraries like `matplotlib` that have GUI backend restrictions\n- **Limited Crash Protection**: Cannot fully isolate from crashes like SegFaults\n\n### Implementation Entry Point\n\n```python\n@traceable\ndef run_structured_in_thread(\n    command: str,\n    _globals: dict[str, Any] | None = None,\n    _locals: Optional[Dict] = None,\n    timeout: Optional[int] = None,\n) -> ExecutionResult:\n```\n\nLocation: [src/runtime/subthread_python_executor.py:86-128]()\n\n### Special Initialization\n\nBefore any matplotlib code execution, the system imports [src/runtime/before_thread/plt_back_chinese.py:1-13]() which:\n1. Sets matplotlib backend to `Agg` (non-GUI) via `matplotlib.use(\"Agg\")`\n2. Configures Chinese font support with `SimHei`\n\nThis prevents GUI-related warnings and errors when running plotting code in threads.\n\nSources: [src/runtime/subthread_python_executor.py:13-128](), [src/runtime/before_thread/plt_back_chinese.py:1-13]()\n\n## Direct Execution Strategy\n\nThe **direct executor** runs code in the same process with minimal isolation, used primarily for testing and trusted code.\n\n### Key Characteristics\n\n- **In-Process Execution**: No separate process or thread created\n- **Queue-Based Communication**: Uses `multiprocessing.Queue` for result passing\n- **Fastest Execution**: No process/thread startup overhead\n- **No Crash Protection**: Cannot recover from process crashes\n\n### Implementation Entry Point\n\n```python\n@traceable\ndef run_structured(\n    command: str, \n    _globals: dict[str, Any] | None = None, \n    _locals: Optional[Dict] = None, \n    timeout: Optional[int] = None\n) -> ExecutionResult:\n```\n\nLocation: [src/runtime/python_executor.py:92-130]()\n\nFor detailed implementation, see [Direct Execution](#5.3).\n\nSources: [src/runtime/python_executor.py:28-130]()\n\n## Working Directory and Output Management\n\n### Directory Context Management\n\nThe `cwd` module provides context managers for safely changing working directories and redirecting output streams:\n\n#### ChangeDirectory Context Manager\n\n```python\nclass ChangeDirectory:\n    \"\"\"Automatically restores original directory on exit\"\"\"\n    def __init__(self, target_dir):\n        self.target_dir = target_dir\n        self.original_dir = None\n```\n\nLocation: [src/runtime/cwd.py:31-54]()\n\nUsage pattern:\n```python\nwith cwd.ChangeDirectory('./wsm/4/g9-1'):\n    # Code executes in this directory\n    exec(command, _globals, _locals)\n# Directory automatically restored\n```\n\n#### Output Redirection Context Manager\n\n```python\nclass Change_STDOUT_STDERR:\n    \"\"\"Captures stdout/stderr to custom writer\"\"\"\n    def __init__(self, new_stdout, new_stderr=None):\n        self.original_stdout = sys.stdout\n        self.original_stderr = sys.stderr\n```\n\nLocation: [src/runtime/cwd.py:56-77]()\n\nFor more details, see [Working Directory and Environment](#5.6).\n\nSources: [src/runtime/cwd.py:9-92]()\n\n## Workspace State Management\n\nThe execution runtime integrates with the workspace management system to persist variables across multiple code executions.\n\n### Global Variable Filtering\n\nThe `ExecutionResult` model automatically filters and validates global variables using a field validator [src/runtime/schemas.py:77-83]():\n\n```python\n@field_validator('arg_globals')\n@classmethod        \ndef field_validate_globals(cls, value: Dict[str, Any]) -> Dict[str, Any]:\n    if value is None:\n        return {}\n    return workspace.filter_and_deepcopy_globals(value)\n```\n\nThis ensures that only serializable, non-builtin objects are preserved between executions.\n\nFor complete details on workspace management, see [Workspace State Management](#5.5).\n\nSources: [src/runtime/schemas.py:77-83]()\n\n## Strategy Selection Guidelines\n\nThe following table provides guidance on when to use each execution strategy:\n\n| Use Case | Recommended Strategy | Reasoning |\n|----------|---------------------|-----------|\n| Untrusted user code | Subprocess | Full isolation prevents memory corruption |\n| Long-running computations | Subprocess | Timeout enforcement via process termination |\n| Code that might crash | Subprocess | Exit code detection for crash analysis |\n| Matplotlib/plotting | Subthread | GUI backend compatibility |\n| Data visualization | Subthread | Shared memory for efficient data access |\n| Quick calculations | Direct | Minimal overhead for simple operations |\n| Unit testing | Direct | Fast iteration, controlled environment |\n| I/O heavy operations | Subprocess | Prevents blocking main process |\n\n## Implementation Details by Strategy\n\n### Subprocess: Pipe-Based Communication\n\nThe subprocess strategy uses a two-channel protocol defined by `_PipeType` enum [src/runtime/subprocess_python_executor.py:13-16]():\n\n```python\nclass _PipeType(str, Enum):\n    STDOUT = \"stdout\"   # Real-time output streaming\n    RESULT = \"result\"   # Final ExecutionResult object\n```\n\nA dedicated reader thread [src/runtime/subprocess_python_executor.py:88-112]() continuously reads from the pipe and dispatches messages to appropriate buffers.\n\n### Subthread: Buffer Writer\n\nThe subthread strategy uses a custom writer class [src/runtime/subthread_python_executor.py:23-32]() that appends output to a shared list:\n\n```python\nclass _BufferWriter:\n    def __init__(self, buffer: list[str]):\n        self.buffer = buffer\n    \n    def write(self, msg: str):\n        if msg:\n            self.buffer.append(msg)\n```\n\n### Traceback Filtering\n\nThe subthread executor implements specialized traceback filtering [src/runtime/subthread_python_executor.py:47-72]() to extract only the relevant `<string>` (exec'd code) portions, removing infrastructure noise.\n\nSources: [src/runtime/subprocess_python_executor.py:13-112](), [src/runtime/subthread_python_executor.py:23-72]()\n\n## Exit Code Semantics\n\nProcess exit codes follow standard conventions [src/runtime/schemas.py:50-54]():\n\n| Exit Code Range | Meaning | Detection |\n|----------------|---------|-----------|\n| `0` | Normal exit | SUCCESS or FAILURE (exception caught) |\n| `> 0` | Error exit | FAILURE or CRASHED |\n| `< 0` (e.g. `-15`) | Signal termination | TIMEOUT (SIGTERM) or CRASHED (SIGSEGV=-11) |\n| `-11` / `139` | Segmentation fault | CRASHED |\n| `-9` | SIGKILL | TIMEOUT (force kill) |\n\nThe subprocess executor captures exit codes via `Process.exitcode` [src/runtime/subprocess_python_executor.py:160]().\n\nSources: [src/runtime/schemas.py:50-54](), [src/runtime/subprocess_python_executor.py:160]()\n\n## Error Handling and Recovery\n\nAll strategies implement consistent error handling:\n\n1. **Exception Capture**: Python exceptions are caught and stored in `ExecutionResult` with full traceback\n2. **Timeout Detection**: Separate monitoring detects execution exceeding configured timeout\n3. **Crash Detection**: Subprocess strategy detects unexpected process termination\n4. **Output Preservation**: All stdout/stderr captured before termination\n\nThe `ExecutionStatus.get_return_llm()` method [src/runtime/schemas.py:18-47]() formats these errors into actionable messages for the LLM to diagnose and fix code issues.\n\nSources: [src/runtime/schemas.py:18-47](), [src/runtime/subprocess_python_executor.py:120-163]()\n\n## Testing and Validation\n\nThe system includes comprehensive test suites:\n\n- **Unit Tests**: [tests/unit/runtime/test_python_executor.py]() - Tests basic executor functionality\n- **Integration Tests**: [tests/playground/subprocess_output.py]() - Validates timeout, crashes, exceptions\n- **Crash Scenarios**: [tests/playground/crashed.py]() - Tests recursion limits, OOM, SegFault detection\n\nExample test output from [tests/playground/subprocess_output.py:296-705]() demonstrates:\n- Timeout detection with partial output\n- Exception capture with line numbers\n- Segmentation fault detection (exit code 139)\n- Recursion error handling\n- OOM detection\n\nSources: [tests/unit/runtime/test_python_executor.py](), [tests/playground/subprocess_output.py:162-292](), [tests/playground/crashed.py]()\n\n---\n\n# Page: Subprocess Execution\n\n# Subprocess Execution\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [docs/crashed.design.md](docs/crashed.design.md)\n- [src/runtime/cwd.py](src/runtime/cwd.py)\n- [src/runtime/schemas.py](src/runtime/schemas.py)\n- [src/runtime/subprocess_python_executor.py](src/runtime/subprocess_python_executor.py)\n- [tests/playground/crashed.py](tests/playground/crashed.py)\n- [tests/playground/subprocess_output.py](tests/playground/subprocess_output.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page covers the **subprocess execution strategy**, which provides the strongest isolation level for Python code execution in the algo_agent system. Subprocess execution uses separate operating system processes with pipe-based inter-process communication (IPC) to execute user code safely.\n\nThe subprocess executor handles:\n- Full process isolation with separate memory spaces\n- Real-time stdout/stderr capture via pipes\n- Timeout enforcement with graceful termination\n- Crash detection for segmentation faults, stack overflows, and memory exhaustion\n- Working directory management for isolated execution environments\n- Exit code analysis for diagnostic information\n\nFor other execution strategies, see [Subthread Execution](#5.2) and [Direct Execution](#5.3). For the execution result format, see [ExecutionResult and Status Handling](#5.4). For workspace state management, see [Workspace State Management](#5.5).\n\n---\n\n## Architecture Overview\n\nThe subprocess execution system consists of three coordinated components: the parent process that manages execution, the child subprocess that runs user code, and a reader thread that captures output in real-time.\n\n```mermaid\ngraph TB\n    Parent[\"Parent Process<br/>run_structured_in_subprocess\"]\n    Child[\"Child Process<br/>_worker_with_pipe\"]\n    Reader[\"Reader Thread<br/>_reader\"]\n    \n    Pipe[\"Multiprocessing Pipe<br/>parent_conn â child_conn\"]\n    \n    Buffers[\"Output Buffers<br/>subprocess_stdout_buffer<br/>subprocess_result_container\"]\n    \n    Result[\"ExecutionResult<br/>exit_status + exit_code<br/>ret_stdout + arg_globals\"]\n    \n    Parent -->|\"multiprocessing.Process(target=_worker_with_pipe)\"| Child\n    Parent -->|\"threading.Thread(target=_reader)\"| Reader\n    \n    Child -->|\"send(STDOUT, msg)\"| Pipe\n    Child -->|\"send(RESULT, ExecutionResult)\"| Pipe\n    \n    Pipe -->|\"recv()\"| Reader\n    Reader -->|\"append\"| Buffers\n    \n    Parent -->|\"p.join(timeout)\"| Child\n    Parent -->|\"t.join()\"| Reader\n    Parent -->|\"assemble final result\"| Result\n    \n    Buffers -->|\"data source\"| Result\n```\n\n**Diagram: Subprocess Execution Architecture**\n\nThe parent process creates a bidirectional pipe, spawns a child process to execute code, and launches a reader thread to capture output. The child sends two types of messages: real-time stdout and a final ExecutionResult. The parent waits for the child to complete or timeout, then assembles the final result.\n\n**Sources:** [src/runtime/subprocess_python_executor.py:76-163]()\n\n---\n\n## Process Isolation Model\n\nSubprocess execution provides complete isolation by running user code in a separate operating system process. This isolates:\n\n| Isolation Aspect | Implementation | Benefit |\n|------------------|----------------|---------|\n| **Memory Space** | Separate process address space | Prevents memory corruption of parent process |\n| **CPU Resources** | OS-level process scheduling | Can be terminated without affecting parent |\n| **File Handles** | Inherited but independent after fork | File operations don't interfere with parent |\n| **Working Directory** | Changed in child only via `cwd.create_cwd()` | Isolates file I/O operations |\n| **Exception Handling** | Child catches and serializes exceptions | Parent remains stable even on child crash |\n| **Signals** | Child can receive termination signals | Enables force-kill for runaway processes |\n\nThe isolation is achieved through the `multiprocessing.Process` class, which uses platform-appropriate process creation mechanisms (fork on Unix, spawn on Windows).\n\n**Sources:** [src/runtime/subprocess_python_executor.py:106-117](), [docs/crashed.design.md:339-400]()\n\n---\n\n## Communication Protocol\n\nThe subprocess communicates with the parent through a pipe using a tagged message protocol. Messages are tagged with `_PipeType` to distinguish between different data types.\n\n```mermaid\ngraph LR\n    subgraph \"Child Process\"\n        Exec[\"exec(command, _globals, _locals)\"]\n        Writer[\"_PipeWriter<br/>sys.stdout = _PipeWriter\"]\n        SendStdout[\"send(STDOUT, msg)\"]\n        SendResult[\"send(RESULT, ExecutionResult)\"]\n    end\n    \n    subgraph \"Pipe\"\n        Channel[\"Multiprocessing Pipe<br/>bidirectional channel\"]\n    end\n    \n    subgraph \"Parent Thread (_reader)\"\n        Recv[\"recv() blocks until message\"]\n        Dispatch[\"if tag == STDOUT:<br/>    buffer.append(data)<br/>elif tag == RESULT:<br/>    container.append(data)\"]\n    end\n    \n    Exec -->|\"print() calls write()\"| Writer\n    Writer -->|\"PipeType.STDOUT\"| SendStdout\n    Exec -->|\"on completion/exception\"| SendResult\n    \n    SendStdout --> Channel\n    SendResult --> Channel\n    Channel --> Recv\n    Recv --> Dispatch\n```\n\n**Diagram: Pipe-Based Communication Protocol**\n\n### Message Types\n\nThe `_PipeType` enum defines two message types:\n\n```python\n@unique\nclass _PipeType(str, Enum):\n    STDOUT = \"stdout\"  # Real-time output from print() statements\n    RESULT = \"result\"  # Final ExecutionResult object\n```\n\n**Sources:** [src/runtime/subprocess_python_executor.py:13-16]()\n\n### PipeWriter Implementation\n\nThe child process redirects `sys.stdout` and `sys.stderr` to a custom `_PipeWriter` class that sends output through the pipe:\n\n```python\nclass _PipeWriter:\n    def __init__(self, child_conn: PipeConnection):\n        self.child_conn = child_conn\n\n    def write(self, msg: str):\n        if msg:\n            # Tag as STDOUT for real-time output\n            self.child_conn.send((_PipeType.STDOUT, msg))\n\n    def flush(self):\n        pass\n```\n\nThis enables real-time streaming of output back to the parent process without buffering delays.\n\n**Sources:** [src/runtime/subprocess_python_executor.py:30-45]()\n\n---\n\n## Worker Process Implementation\n\nThe `_worker_with_pipe` function executes in the child process and handles code execution, exception capture, and result transmission.\n\n### Execution Flow\n\n```mermaid\nsequenceDiagram\n    participant Main as Child Main\n    participant CWD as cwd.create_cwd\n    participant Redir as stdout/stderr redirect\n    participant Exec as exec()\n    participant Result as ExecutionResult\n    participant Pipe as child_conn\n    \n    Main->>CWD: create_cwd('./wsm/2/g7-2')\n    CWD-->>Main: working directory set\n    \n    Main->>Redir: sys.stdout = _PipeWriter(child_conn)\n    Main->>Redir: sys.stderr = sys.stdout\n    \n    Main->>Exec: exec(command, _globals, _locals)\n    \n    alt Execution succeeds\n        Exec-->>Main: returns normally\n        Main->>Result: ExecutionResult(exit_status=SUCCESS)\n    else Exception occurs\n        Exec-->>Main: raises exception\n        Main->>Result: ExecutionResult(exit_status=FAILURE)<br/>+ exception details\n    end\n    \n    Main->>Pipe: send((RESULT, res))\n    Main->>Pipe: close()\n```\n\n**Diagram: Worker Process Execution Sequence**\n\n### Success Case\n\nWhen code executes successfully, the worker constructs a success result:\n\n```python\nres = ExecutionResult(\n    arg_command=command,\n    arg_globals=_globals or {},\n    arg_timeout=timeout,\n    exit_status=ExecutionStatus.SUCCESS,\n)\n```\n\n**Sources:** [src/runtime/subprocess_python_executor.py:49-57]()\n\n### Failure Case\n\nWhen an exception occurs, the worker captures comprehensive error information:\n\n```python\nres = ExecutionResult(\n    arg_command=command,\n    arg_globals=_globals or {},\n    arg_timeout=timeout,\n    exit_status=ExecutionStatus.FAILURE,\n    exception_repr=repr(e),\n    exception_type=type(e).__name__,\n    exception_value=str(e),\n    exception_traceback=traceback.format_exc(),\n)\n```\n\nThis includes the exception representation, type, value, and full traceback for debugging.\n\n**Sources:** [src/runtime/subprocess_python_executor.py:58-69]()\n\n### Result Transmission\n\nThe final result is always sent through the pipe, even on exceptions:\n\n```python\nfinally:\n    # Tag as RESULT for final object\n    child_conn.send((_PipeType.RESULT, res))\n    child_conn.close()\n```\n\nThe pipe close operation signals EOF to the reader thread.\n\n**Sources:** [src/runtime/subprocess_python_executor.py:70-73]()\n\n---\n\n## Parent Process Coordination\n\nThe `run_structured_in_subprocess` function orchestrates subprocess execution from the parent process. It manages process lifecycle, output capture, timeout detection, and result assembly.\n\n### Setup Phase\n\n```python\n_locals = _globals  # Must be consistent for exec() namespace behavior\nparent_conn, child_conn = multiprocessing.Pipe()\nsubprocess_stdout_buffer: list[str] = []\nsubprocess_result_container: list[ExecutionResult] = []\n```\n\nThe parent creates:\n- A bidirectional pipe for communication\n- A buffer to accumulate stdout messages\n- A container to hold the final ExecutionResult\n\n**Sources:** [src/runtime/subprocess_python_executor.py:83-86]()\n\n### Process and Thread Launch\n\n```python\np = multiprocessing.Process(\n    target=_worker_with_pipe, args=(command, _globals, _locals, timeout, child_conn)\n)\np.start()\n\n# Must close parent's copy of child_conn for EOF detection\nchild_conn.close()\n\nt = threading.Thread(\n    target=_reader,\n    args=(parent_conn, subprocess_stdout_buffer, subprocess_result_container),\n)\nt.start()\n```\n\nKey details:\n- The child process receives `child_conn` for sending messages\n- The parent must close its copy of `child_conn` so the reader can detect EOF\n- The reader thread runs in the parent process and blocks on `recv()`\n\n**Sources:** [src/runtime/subprocess_python_executor.py:114-126]()\n\n### Reader Thread Implementation\n\nThe `_reader` function continuously reads messages from the pipe until EOF:\n\n```python\ndef _reader(\n    parent_conn: PipeConnection,\n    subprocess_stdout_buffer: list[str],\n    subprocess_result_container: list[ExecutionResult],\n) -> None:\n    \"\"\"Read messages from subprocess and dispatch by type\"\"\"\n    try:\n        while True:\n            # Blocking read until EOF\n            msg: tuple[_PipeType, str | ExecutionResult] = parent_conn.recv()\n            # Protocol dispatch\n            if isinstance(msg, tuple) and len(msg) == 2:\n                tag, data = msg\n                if tag == _PipeType.STDOUT:\n                    subprocess_stdout_buffer.append(data)\n                elif tag == _PipeType.RESULT:\n                    subprocess_result_container.append(data)\n    except (EOFError, OSError) as e:\n        # EOF indicates child closed pipe, normal termination\n        pass\n```\n\nThe reader accumulates messages in shared lists that the parent can access after joining.\n\n**Sources:** [src/runtime/subprocess_python_executor.py:88-112]()\n\n---\n\n## Timeout Enforcement\n\nTimeout detection uses `Process.join(timeout)` to wait for the child process with a time limit. Three scenarios are handled:\n\n### Timeout Detection Flow\n\n```mermaid\ngraph TB\n    Start[\"p.join(timeout)\"]\n    Check{\"p.is_alive()\"}\n    \n    Start --> Check\n    \n    Check -->|\"True (timeout)\"| Terminate[\"p.terminate()\"]\n    Terminate --> Join[\"p.join() (wait for termination)\"]\n    Join --> CloseConn[\"parent_conn.close()\"]\n    CloseConn --> JoinThread[\"t.join() (wait for reader)\"]\n    JoinThread --> BuildTimeout[\"Build ExecutionResult<br/>exit_status=TIMEOUT\"]\n    \n    Check -->|\"False (completed)\"| JoinThread2[\"t.join()\"]\n    JoinThread2 --> CheckResult{\"subprocess_result_container<br/>has result?\"}\n    \n    CheckResult -->|\"Yes\"| UseResult[\"Use child's result\"]\n    CheckResult -->|\"No\"| BuildCrashed[\"Build ExecutionResult<br/>exit_status=CRASHED\"]\n```\n\n**Diagram: Timeout and Crash Detection Flow**\n\n### Timeout Case\n\n```python\np.join(timeout)  \nif p.is_alive():\n    global_logger.info(\"---------- 1. è¶æ¶æåµï¼ç±ç¶è¿ç¨æå»º ExecutionResult\")\n    # Ensure process truly terminates\n    p.terminate()  \n    # Close connection to stop reader\n    p.join()  \n    parent_conn.close()\n    t.join()\n    \n    final_res = ExecutionResult(\n        arg_command=command,\n        arg_timeout=timeout,\n        arg_globals=_globals or {},\n        exit_status=ExecutionStatus.TIMEOUT,\n    )\n```\n\nThe parent:\n1. Calls `terminate()` to send SIGTERM to the child\n2. Waits for actual termination with `join()`\n3. Closes the pipe to unblock the reader thread\n4. Constructs a TIMEOUT result\n\n**Sources:** [src/runtime/subprocess_python_executor.py:128-144]()\n\n### Normal Completion\n\n```python\nelse:\n    global_logger.info(\"---------- 2. æ­£å¸¸æå¼å¸¸éåºï¼ä»å­è¿ç¨è·å ExecutionResult\")\n    t.join()\n\n    if subprocess_result_container:\n        global_logger.info(\"---------- 2.1 å­è¿ç¨æ­£å¸¸éåºï¼ execæ­£å¸¸ãexecå¼å¸¸\")\n        final_res: ExecutionResult = subprocess_result_container[0]\n    else:\n        global_logger.info(\"---------- 2.2 å­è¿ç¨å´©æºéåºï¼å¦ SegFault\")\n        final_res = ExecutionResult(\n            arg_command=command,\n            arg_timeout=timeout,\n            arg_globals=_globals or {},\n            exit_status=ExecutionStatus.CRASHED,\n        )\n```\n\nIf the process exits within the timeout:\n- **Case 2.1**: The result container has the child's ExecutionResult (SUCCESS or FAILURE)\n- **Case 2.2**: The result container is empty, indicating a crash before result transmission\n\n**Sources:** [src/runtime/subprocess_python_executor.py:145-159]()\n\n---\n\n## Crash Detection\n\nThe subprocess executor can detect and handle various crash scenarios by examining whether the child sent a result before terminating.\n\n### Crash Scenarios\n\n| Crash Type | Trigger | Exit Code | Result Container | Detection |\n|------------|---------|-----------|------------------|-----------|\n| **SegFault** | `os._exit(139)` | 139 (SIGSEGV) | Empty | No result sent |\n| **Stack Overflow** | Infinite recursion | 1 | Empty* | PicklingError prevents result |\n| **Memory Exhaustion** | Large allocations | 1 | Empty* | PicklingError prevents result |\n| **Normal Exception** | `raise ZeroDivisionError` | 0 | Contains FAILURE | Exception caught and sent |\n| **Timeout** | `time.sleep(long_time)` | -15 (SIGTERM) | Empty | Parent enforces timeout |\n\n*Note: Stack overflow and OOM may capture exceptions but fail to pickle/send the result.\n\n### Exit Code Interpretation\n\nThe parent retrieves the exit code after the process terminates:\n\n```python\nfinal_res.exit_code = p.exitcode  # Fill exitcode field\n```\n\nExit code meanings:\n- `exitcode = 0`: Normal termination (SUCCESS or caught exception)\n- `exitcode > 0`: Error exit (e.g., 1 for unhandled exception)\n- `exitcode < 0`: Killed by signal (e.g., -15 for SIGTERM, -11 for SIGSEGV)\n\n**Sources:** [src/runtime/subprocess_python_executor.py:160](), [src/runtime/schemas.py:50-54]()\n\n### Example: SegFault Detection\n\nWhen a segmentation fault occurs:\n\n```python\n# Code that triggers SegFault\nimport os\nos._exit(139)  # Simulates SIGSEGV (exit code 139)\n```\n\nThe subprocess exits immediately without sending a result. The parent detects this:\n\n1. `p.is_alive()` returns `False` (process terminated)\n2. `subprocess_result_container` is empty (no result sent)\n3. `p.exitcode` is 139 (indicates segmentation fault)\n4. Parent constructs `ExecutionStatus.CRASHED` result\n\n**Sources:** [tests/playground/subprocess_output.py:215-234]()\n\n### Example: Recursion Error\n\nInfinite recursion triggers a `RecursionError`:\n\n```python\ndef recursive_crash(depth=0):\n    if depth % 100 == 0:\n        print(f\"Current recursion depth: {depth}\")\n    recursive_crash(depth + 1)\n\nrecursive_crash()\n```\n\nThe child process catches the exception and builds a FAILURE result, but pickling fails because the function `recursive_crash` cannot be serialized. The result container remains empty, and the parent detects a CRASHED status.\n\n**Sources:** [tests/playground/subprocess_output.py:235-259](), [tests/playground/crashed.py:4-47]()\n\n---\n\n## Working Directory Management\n\nThe child process sets its working directory before executing user code using the `cwd` module.\n\n### Directory Setup\n\n```python\ndef _worker_with_pipe(\n    command: str,\n    _globals: dict[str, Any] | None,\n    _locals: Optional[Dict],\n    timeout: Optional[int],\n    child_conn: PipeConnection,\n) -> None:\n    \"\"\"Execute a command in a subprocess with output captured via pipes.\"\"\"\n    cwd.create_cwd('./wsm/2/g7-2')  # Set working directory for this child\n    \n    # ... rest of execution\n```\n\n**Sources:** [src/runtime/subprocess_python_executor.py:19-28]()\n\n### Directory Creation\n\nThe `create_cwd` function ensures the directory exists and changes to it:\n\n```python\ndef create_cwd(cwd=None):\n    \"\"\"\n    Wrapper function for subprocess to change working directory before executing task.\n    \"\"\"\n    cwd = create_folder.get_or_create_subfolder(fix_relate_from_project=cwd)\n    \n    global_logger.info(f\"Child PID: {os.getpid()} changing to directory: {cwd}\")\n    if not cwd:\n        return False\n    try:\n        os.chdir(cwd)\n        global_logger.info(f\"Child PID: {os.getpid()} changed to: {os.getcwd()}\")\n        return True\n    except OSError as e:\n        global_logger.error(f\"Child PID: {os.getpid()} failed to change directory: {e}\")\n        return False\n```\n\nThis isolates file I/O operations to a specific workspace directory, preventing interference between concurrent executions.\n\n**Sources:** [src/runtime/cwd.py:9-28]()\n\n---\n\n## Execution Result Assembly\n\nAfter the child process completes and the reader thread finishes, the parent assembles the final ExecutionResult by combining data from multiple sources.\n\n### Result Construction\n\n```python\nfinal_res.exit_code = p.exitcode  # Fill exitcode field\nfinal_res.ret_stdout = \"\".join(subprocess_stdout_buffer)  # Fill stdout field\nfinal_res.ret_tool2llm = ExecutionStatus.get_return_llm(final_res.exit_status, final_res)\nreturn final_res\n```\n\nThe final result contains:\n\n| Field | Source | Purpose |\n|-------|--------|---------|\n| `arg_command` | Child or parent | Original code executed |\n| `arg_timeout` | Child or parent | Timeout limit used |\n| `arg_globals` | Child or parent | Global variables (filtered) |\n| `exit_status` | Child or parent | SUCCESS/FAILURE/TIMEOUT/CRASHED |\n| `exit_code` | Parent (`p.exitcode`) | OS-level exit code |\n| `ret_stdout` | Reader buffer | Accumulated output |\n| `ret_tool2llm` | Generated | Formatted message for LLM |\n| `exception_*` | Child (if FAILURE) | Exception details |\n\n**Sources:** [src/runtime/subprocess_python_executor.py:160-163]()\n\n### LLM-Formatted Output\n\nThe `get_return_llm` static method formats the result for the LLM based on execution status:\n\n```python\n@classmethod\ndef get_return_llm(cls, status: \"ExecutionStatus\", result: \"ExecutionResult\") -> str:\n    \"\"\"Generate formatted description for LLM based on execution status\"\"\"\n    _desc_map = {\n        cls.SUCCESS: \"## Code execution successful, output complete, task finished\\n\"\n                    f\"### Terminal output:\\n\"\n                    f\"{result.ret_stdout}\",\n        cls.FAILURE: \"## Code execution failed, exception thrown, debug based on error\\n\"\n                    f\"### Terminal output:\\n\"\n                    f\"{result.ret_stdout}\"\n                    f\"### Original code:\\n\"\n                    f\"{source_code.add_line_numbers(result.arg_command)}\\n\"\n                    f\"### Error information:\\n\"\n                    f\"{result.exception_traceback}\",\n        cls.TIMEOUT: \"## Code execution timeout, forced exit, retry with adjusted timeout\\n\"\n                    f\"### Terminal output:\\n\"\n                    f\"{result.ret_stdout}\"\n                    f\"### Timeout limit: {result.arg_timeout} seconds\\n\",\n        cls.CRASHED: \"## Code execution crashed, process abnormal exit, debug based on error\\n\"\n                    f\"### Terminal output:\\n\"\n                    f\"{result.ret_stdout}\"\n                    f\"### Exit code: {result.exit_code}\\n\",\n    }\n    return _desc_map.get(status, f\"Unknown status: {status}\")\n```\n\nThis provides context-specific messages that help the LLM understand what went wrong and how to proceed.\n\n**Sources:** [src/runtime/schemas.py:18-47]()\n\n---\n\n## Usage Example\n\nA complete example of subprocess execution:\n\n```python\nfrom src.runtime.subprocess_python_executor import run_structured_in_subprocess\n\n# Define code to execute\ntest_code = \"\"\"\nimport time\nprint(\"Starting computation...\")\nresult = sum(range(1000000))\nprint(f\"Result: {result}\")\ntime.sleep(1)\nprint(\"Computation complete\")\n\"\"\"\n\n# Execute with 10-second timeout\nmy_globals = {\"initial_var\": 123}\nresult = run_structured_in_subprocess(\n    command=test_code,\n    _globals=my_globals,\n    _locals=None,\n    timeout=10\n)\n\n# Check result\nprint(f\"Status: {result.exit_status}\")\nprint(f\"Exit code: {result.exit_code}\")\nprint(f\"Output:\\n{result.ret_stdout}\")\nprint(f\"Globals: {result.arg_globals.keys()}\")\n```\n\nThe result will have:\n- `exit_status = ExecutionStatus.SUCCESS`\n- `exit_code = 0`\n- `ret_stdout` containing all print output\n- `arg_globals` containing `initial_var` and `result`\n\n**Sources:** [src/runtime/subprocess_python_executor.py:165-204]()\n\n---\n\n## Summary\n\nThe subprocess execution strategy provides robust isolation and comprehensive error handling:\n\n**Key Features:**\n- Full process isolation with separate memory spaces\n- Real-time output capture via pipe-based IPC\n- Reliable timeout enforcement with process termination\n- Automatic crash detection for segfaults, OOM, and stack overflows\n- Working directory isolation for file operations\n- Detailed exit code analysis for debugging\n\n**Trade-offs:**\n- Higher overhead than thread-based execution\n- Inter-process communication adds latency\n- Global variables must be picklable for transmission\n- Process creation time varies by platform (fork vs spawn)\n\n**When to Use:**\n- Code that may crash or consume excessive resources\n- Untrusted code requiring strong isolation\n- Long-running computations that need timeout guarantees\n- Code that manipulates the file system\n\nFor lighter-weight execution with shared memory, see [Subthread Execution](#5.2). For fastest execution without isolation, see [Direct Execution](#5.3).\n\n**Sources:** [src/runtime/subprocess_python_executor.py](), [src/runtime/schemas.py](), [docs/crashed.design.md]()\n\n---\n\n# Page: Subthread Execution\n\n# Subthread Execution\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/crashed.design.md](docs/crashed.design.md)\n- [src/runtime/before_thread/plt_back_chinese.py](src/runtime/before_thread/plt_back_chinese.py)\n- [src/runtime/schemas.py](src/runtime/schemas.py)\n- [src/runtime/subthread_python_executor.py](src/runtime/subthread_python_executor.py)\n- [tests/playground/crashed.py](tests/playground/crashed.py)\n- [tests/playground/gen/g9/beijing_scenic_spot_schema_with_play_hours.json](tests/playground/gen/g9/beijing_scenic_spot_schema_with_play_hours.json)\n- [tests/playground/gen/g9/beijing_scenic_spot_validated_data_with_play_hours.json](tests/playground/gen/g9/beijing_scenic_spot_validated_data_with_play_hours.json)\n- [tests/playground/gen/g9/g9.py](tests/playground/gen/g9/g9.py)\n- [tests/playground/subprocess_output.py](tests/playground/subprocess_output.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page documents the **subthread execution strategy**, a thread-based approach for running Python code snippets with controlled isolation and output capture. This executor runs code in a separate Python thread within the same process, providing a balance between execution speed and basic isolation.\n\nFor complete process isolation, see [Subprocess Execution](#5.1). For in-process execution without any isolation, see [Direct Execution](#5.3). For the data structures returned by all executors, see [ExecutionResult and Status Handling](#5.4).\n\n---\n\n## Architecture Overview\n\nThe subthread executor uses Python's `threading` module to execute code in a worker thread. Unlike subprocess execution, threads share the same memory space with the parent process, which improves performance but reduces isolation. Output is captured through a buffer-based writer that redirects `sys.stdout` and `sys.stderr`.\n\n### Key Characteristics\n\n| Feature | Implementation |\n|---------|---------------|\n| **Isolation Level** | Thread-based (shared memory space) |\n| **Output Capture** | Buffer-based via custom writer class |\n| **Timeout Support** | Yes, via `Thread.join(timeout)` |\n| **Force Termination** | No (threads cannot be forcefully killed) |\n| **Exit Code** | Not available (set to `None`) |\n| **Performance** | Faster than subprocess, slower than direct |\n\n---\n\n## Core Components\n\n### Entry Point Function\n\nThe main entry point is `run_structured_in_thread`, which orchestrates the entire execution flow:\n\n**Function Signature:**\n```python\ndef run_structured_in_thread(\n    command: str,\n    _globals: dict[str, Any] | None = None,\n    _locals: Optional[Dict] = None,\n    timeout: Optional[int] = None,\n) -> ExecutionResult\n```\n\n**Key Responsibilities:**\n1. Initialize stdout buffer and result container\n2. Create and start worker thread\n3. Wait for completion or timeout\n4. Construct final `ExecutionResult`\n5. Fill in `ret_stdout` and `ret_tool2llm` fields\n\nSources: [src/runtime/subthread_python_executor.py:86-128]()\n\n### Worker Thread Function\n\n```mermaid\ngraph TB\n    subgraph \"_worker_with_buffer\"\n        SETUP[\"Set up _BufferWriter<br/>Redirect stdout/stderr\"]\n        CWD[\"Enter working directory<br/>cwd.ChangeDirectory\"]\n        EXEC[\"Execute code<br/>exec(command, _globals, _locals)\"]\n        SUCCESS[\"Build SUCCESS<br/>ExecutionResult\"]\n        ERROR[\"Catch Exception<br/>Filter traceback\"]\n        FAILURE[\"Build FAILURE<br/>ExecutionResult\"]\n        APPEND[\"Append result to<br/>result_container\"]\n    end\n    \n    SETUP --> CWD\n    CWD --> EXEC\n    EXEC -->|No Exception| SUCCESS\n    EXEC -->|Exception Raised| ERROR\n    ERROR --> FAILURE\n    SUCCESS --> APPEND\n    FAILURE --> APPEND\n```\n\nThe `_worker_with_buffer` function runs in the spawned thread and handles:\n- Output redirection to buffer\n- Working directory changes\n- Code execution via `exec()`\n- Exception capture and traceback filtering\n- Result construction\n\nSources: [src/runtime/subthread_python_executor.py:13-84]()\n\n---\n\n## Output Capture Mechanism\n\n### BufferWriter Class\n\nThe `_BufferWriter` class implements a file-like interface to capture output:\n\n```mermaid\nclassDiagram\n    class _BufferWriter {\n        +list~str~ buffer\n        +__init__(buffer: list)\n        +write(msg: str)\n        +flush()\n    }\n    \n    class SysModule {\n        stdout\n        stderr\n    }\n    \n    _BufferWriter --> SysModule : \"replaces\"\n```\n\n**Implementation Details:**\n- Stores output in a `list[str]` passed by reference\n- Both `sys.stdout` and `sys.stderr` redirected to same instance\n- Thread-safe due to GIL protection\n- No need for explicit flushing\n\n**Why Buffer-Based Instead of Pipes:**\n\n| Aspect | Buffer-Based (Subthread) | Pipe-Based (Subprocess) |\n|--------|--------------------------|-------------------------|\n| Complexity | Simple list append | Requires reader thread, protocol handling |\n| Performance | Fast (in-memory) | Slower (IPC overhead) |\n| Reliability | No broken pipe issues | Must handle EOF, broken pipes |\n| Synchronization | Automatic (GIL) | Manual reader thread coordination |\n\nSources: [src/runtime/subthread_python_executor.py:23-32]()\n\n---\n\n## Execution Flow\n\n### Complete Sequence Diagram\n\n```mermaid\nsequenceDiagram\n    participant Caller\n    participant Main as \"run_structured_in_thread\"\n    participant Thread as \"Worker Thread\"\n    participant Buffer as \"stdout_buffer (list)\"\n    participant Container as \"result_container (list)\"\n    \n    Caller->>Main: command, _globals, timeout\n    Main->>Main: Create stdout_buffer = []\n    Main->>Main: Create result_container = []\n    Main->>Thread: Start thread with _worker_with_buffer\n    \n    Thread->>Thread: Initialize _BufferWriter(stdout_buffer)\n    Thread->>Thread: Redirect sys.stdout/stderr\n    Thread->>Thread: Enter cwd.ChangeDirectory\n    Thread->>Thread: Enter cwd.Change_STDOUT_STDERR\n    \n    alt Code Executes Successfully\n        Thread->>Thread: exec(command, _globals, _locals)\n        Thread->>Buffer: write(...) during execution\n        Thread->>Thread: Build ExecutionResult(SUCCESS)\n        Thread->>Container: Append result\n    else Exception Raised\n        Thread->>Thread: Catch exception\n        Thread->>Buffer: write(...) error output\n        Thread->>Thread: filter_exec_traceback()\n        Thread->>Thread: Build ExecutionResult(FAILURE)\n        Thread->>Container: Append result\n    end\n    \n    Main->>Main: t.join(timeout)\n    \n    alt Thread Completes in Time\n        Main->>Container: Get result_container[0]\n        Main->>Main: final_res = container[0]\n    else Thread Timeout\n        Main->>Main: Build ExecutionResult(TIMEOUT)\n        Main->>Main: Note: Thread still running\n    end\n    \n    Main->>Main: final_res.exit_code = None\n    Main->>Buffer: final_res.ret_stdout = \"\".join(buffer)\n    Main->>Main: final_res.ret_tool2llm = format_for_llm()\n    Main->>Caller: Return ExecutionResult\n```\n\nSources: [src/runtime/subthread_python_executor.py:86-128]()\n\n---\n\n## Timeout Handling\n\n### Mechanism\n\n```python\nt.join(timeout)\nif t.is_alive():\n    # Thread still running - timeout occurred\n    final_res = ExecutionResult(\n        arg_command=command,\n        arg_timeout=timeout,\n        arg_globals=_globals or {},\n        exit_status=ExecutionStatus.TIMEOUT,\n    )\n```\n\n### Critical Limitation\n\n**Threads Cannot Be Force-Killed in Python:**\n\nUnlike subprocesses which can be terminated with `process.terminate()` or killed with signals, Python threads cannot be forcefully stopped. When a timeout occurs:\n\n1. The main thread returns a `TIMEOUT` status\n2. The worker thread **continues running** in the background\n3. Any subsequent operations may be affected by the still-running thread\n\n**Implications:**\n- Infinite loops in user code will leave zombie threads\n- Resource leaks possible if timeout occurs\n- GIL contention from background threads\n- Shared memory may be corrupted by timed-out threads\n\n**Comparison with Subprocess:**\n\n| Executor | Timeout Action | Post-Timeout State |\n|----------|----------------|-------------------|\n| Subthread | Mark as timeout, return | Thread continues running |\n| Subprocess | `terminate()` then `kill()` | Process fully terminated |\n\nSources: [src/runtime/subthread_python_executor.py:102-111]()\n\n---\n\n## Exception Handling and Traceback Filtering\n\n### Why Traceback Filtering Is Needed\n\nWhen code executed via `exec()` raises an exception, the traceback includes:\n1. **External frames** - executor framework code (`_worker_with_buffer`, `exec()` call)\n2. **Internal frames** - user code executed within `<string>`\n\nUsers only need to see the frames from their own code, not the executor internals.\n\n### Filter Implementation\n\n```mermaid\ngraph TB\n    START[\"Get traceback.format_exc()\"]\n    SPLIT[\"Split into lines\"]\n    INIT[\"Initialize exec_lines = []<br/>in_exec_block = False\"]\n    \n    LOOP{\"For each line\"}\n    CHECK1{\"Contains 'Traceback'?\"}\n    CHECK2{\"Contains '<string>'?\"}\n    CHECK3{\"in_exec_block?\"}\n    CHECK4{\"Starts with 'File'<br/>and no '<string>'?\"}\n    \n    ADD[\"Add to exec_lines\"]\n    SETFLAG[\"in_exec_block = True<br/>Add to exec_lines\"]\n    RESETFLAG[\"in_exec_block = False\"]\n    JOIN[\"Join lines with newline\"]\n    \n    START --> SPLIT --> INIT --> LOOP\n    LOOP -->|Yes| CHECK1\n    CHECK1 -->|Yes| ADD\n    CHECK1 -->|No| CHECK2\n    CHECK2 -->|Yes| SETFLAG\n    CHECK2 -->|No| CHECK3\n    CHECK3 -->|Yes| CHECK4\n    CHECK3 -->|No| LOOP\n    CHECK4 -->|Yes| RESETFLAG\n    CHECK4 -->|No| ADD\n    RESETFLAG --> LOOP\n    ADD --> LOOP\n    SETFLAG --> LOOP\n    LOOP -->|Done| JOIN\n```\n\nThe `filter_exec_traceback` function extracts only the relevant stack frames:\n\n**Key Features:**\n- Preserves the \"Traceback\" header line\n- Includes all lines containing `<string>` (exec'd code marker)\n- Includes subsequent lines within the exec block\n- Stops when encountering new stack frames from executor code\n- Returns cleaned traceback string\n\n**Example Transformation:**\n\nBefore filtering:\n```\nTraceback (most recent call last):\n  File \"d:\\...\\subthread_python_executor.py\", line 37, in _worker_with_buffer\n    exec(command, _globals, _locals)\n  File \"<string>\", line 4, in <module>\nZeroDivisionError: division by zero\n```\n\nAfter filtering:\n```\nTraceback (most recent call last):\n  File \"<string>\", line 4, in <module>\nZeroDivisionError: division by zero\n```\n\nSources: [src/runtime/subthread_python_executor.py:47-72]()\n\n---\n\n## Working Directory and Environment Setup\n\n### Directory Management\n\n```mermaid\ngraph LR\n    A[\"Original CWD<br/>(project root)\"]\n    B[\"Enter Context<br/>cwd.ChangeDirectory\"]\n    C[\"Changed CWD<br/>./wsm/4/g9-1\"]\n    D[\"Execute Code<br/>in new directory\"]\n    E[\"Exit Context<br/>Restore CWD\"]\n    F[\"Back to Original\"]\n    \n    A --> B\n    B --> C\n    C --> D\n    D --> E\n    E --> F\n```\n\nThe executor changes the working directory to a workspace-specific path:\n```python\nwith cwd.ChangeDirectory('./wsm/4/g9-1'):\n    with cwd.Change_STDOUT_STDERR(_BufferWriter(stdout_buffer)):\n        exec(command, _globals, _locals)\n```\n\n**Purpose:**\n- Isolate file I/O operations between different executions\n- Provide consistent working directory for relative paths\n- Enable workspace-based file organization\n\n**Context Managers Used:**\n1. `cwd.ChangeDirectory` - Changes `os.getcwd()` temporarily\n2. `cwd.Change_STDOUT_STDERR` - Redirects output streams\n\nSources: [src/runtime/subthread_python_executor.py:35-37]()\n\n---\n\n## Pre-Execution Setup for Matplotlib\n\n### The Threading Issue\n\nMatplotlib has two critical issues when used in non-main threads:\n\n1. **GUI Backend Warning**: Default backends (TkAgg, Qt5Agg) cannot create GUI windows from threads\n2. **Chinese Font Rendering**: Default fonts don't support Chinese characters, causing glyph warnings\n\n### Solution: plt_back_chinese Module\n\n```mermaid\ngraph TB\n    IMPORT[\"Import plt_back_chinese module\"]\n    SETBACK[\"matplotlib.use('Agg')<br/>Set non-GUI backend\"]\n    SETFONT[\"plt.rcParams['font.sans-serif'] = ['SimHei']<br/>Set Chinese font\"]\n    SETMINUS[\"plt.rcParams['axes.unicode_minus'] = False<br/>Fix minus sign\"]\n    \n    IMPORT --> SETBACK\n    SETBACK --> SETFONT\n    SETFONT --> SETMINUS\n    \n    style SETBACK fill:#f9f9f9\n    style SETFONT fill:#f9f9f9\n```\n\n**Module Purpose:**\nThis module must be imported **before any thread executes matplotlib code**. It:\n- Sets backend to `\"Agg\"` (non-interactive, file-only)\n- Configures Chinese font support (`SimHei`)\n- Fixes Unicode minus sign rendering\n\n**Import Location:**\n```python\nfrom src.runtime.before_thread import plt_back_chinese\n```\n\nThis import happens at the top of `subthread_python_executor.py`, ensuring configuration is applied before any threads spawn.\n\nSources: [src/runtime/before_thread/plt_back_chinese.py:1-13](), [src/runtime/subthread_python_executor.py:9]()\n\n---\n\n## ExecutionResult Construction\n\n### Status Determination Logic\n\n```mermaid\ngraph TB\n    START[\"Worker thread executes\"]\n    \n    TIMEOUT[\"Main thread: t.join(timeout)\"]\n    ALIVE{\"t.is_alive()?\"}\n    \n    CONTAINER{\"result_container<br/>has result?\"}\n    \n    SUCCESS[\"ExecutionStatus.SUCCESS<br/>(exec completed)\"]\n    FAILURE[\"ExecutionStatus.FAILURE<br/>(exception caught)\"]\n    TIMEOUT_STATUS[\"ExecutionStatus.TIMEOUT<br/>(parent constructs)\"]\n    CRASHED[\"ExecutionStatus.CRASHED<br/>(no result, but finished)\"]\n    \n    START --> TIMEOUT\n    TIMEOUT --> ALIVE\n    ALIVE -->|True| TIMEOUT_STATUS\n    ALIVE -->|False| CONTAINER\n    CONTAINER -->|Yes, SUCCESS type| SUCCESS\n    CONTAINER -->|Yes, FAILURE type| FAILURE\n    CONTAINER -->|Empty| CRASHED\n```\n\n### Field Population\n\n| Field | Populated By | When |\n|-------|--------------|------|\n| `arg_command` | Worker or Main | Always |\n| `arg_timeout` | Worker or Main | Always |\n| `arg_globals` | Worker or Main | Always (filtered) |\n| `exit_status` | Worker or Main | Always |\n| `exit_code` | Main thread | Always (`None` for threads) |\n| `exception_*` | Worker thread | Only on FAILURE |\n| `ret_stdout` | Main thread | After join (from buffer) |\n| `ret_tool2llm` | Main thread | After status determined |\n\n### Success Case\n\n```python\nres = ExecutionResult(\n    arg_command=command,\n    arg_globals=_globals or {},\n    arg_timeout=timeout,\n    exit_status=ExecutionStatus.SUCCESS,\n)\nresult_container.append(res)\n```\n\nSources: [src/runtime/subthread_python_executor.py:39-44]()\n\n### Failure Case\n\n```python\nres = ExecutionResult(\n    arg_command=command,\n    arg_globals=_globals or {},\n    arg_timeout=timeout,\n    exit_status=ExecutionStatus.FAILURE,\n    exception_repr=repr(e),\n    exception_type=type(e).__name__,\n    exception_value=str(e),\n    exception_traceback=filter_exec_traceback(),\n)\nresult_container.append(res)\n```\n\nSources: [src/runtime/subthread_python_executor.py:73-82]()\n\n### Timeout Case\n\n```python\nfinal_res = ExecutionResult(\n    arg_command=command,\n    arg_timeout=timeout,\n    arg_globals=_globals or {},\n    exit_status=ExecutionStatus.TIMEOUT,\n)\n```\n\nSources: [src/runtime/subthread_python_executor.py:106-111]()\n\n### Crashed Case\n\nWhen the thread finishes but `result_container` is empty (theoretically possible if the thread crashes before appending):\n\n```python\nfinal_res = ExecutionResult(\n    arg_command=command,\n    arg_timeout=timeout,\n    arg_globals=_globals or {},\n    exit_status=ExecutionStatus.CRASHED,\n)\n```\n\nSources: [src/runtime/subthread_python_executor.py:119-124]()\n\n---\n\n## Comparison with Other Executors\n\n### Architecture Comparison\n\n```mermaid\ngraph TB\n    subgraph \"Subprocess Execution\"\n        SP_MAIN[\"Main Process\"]\n        SP_CHILD[\"Child Process<br/>(separate memory)\"]\n        SP_PIPE[\"Pipe/IPC\"]\n        \n        SP_MAIN -->|spawn| SP_CHILD\n        SP_CHILD -->|send result| SP_PIPE\n        SP_PIPE -->|read| SP_MAIN\n    end\n    \n    subgraph \"Subthread Execution\"\n        ST_MAIN[\"Main Thread\"]\n        ST_WORKER[\"Worker Thread<br/>(shared memory)\"]\n        ST_BUFFER[\"Buffer List<br/>(shared reference)\"]\n        \n        ST_MAIN -->|start| ST_WORKER\n        ST_WORKER -->|append| ST_BUFFER\n        ST_MAIN -->|read| ST_BUFFER\n    end\n    \n    subgraph \"Direct Execution\"\n        D_MAIN[\"Main Thread<br/>(no isolation)\"]\n        D_QUEUE[\"Queue<br/>(optional)\"]\n        \n        D_MAIN -->|exec directly| D_QUEUE\n    end\n```\n\n### Feature Matrix\n\n| Feature | Subprocess | Subthread | Direct |\n|---------|-----------|-----------|--------|\n| **Process Isolation** | Full | None | None |\n| **Memory Space** | Separate | Shared | Shared |\n| **Force Termination** | Yes (SIGTERM/SIGKILL) | No | No |\n| **Output Capture** | Pipe-based | Buffer-based | Queue/Direct |\n| **Exit Code** | Yes (from OS) | No | No |\n| **Crash Detection** | Yes (exitcode != 0) | Limited | Limited |\n| **Startup Overhead** | High | Low | None |\n| **GIL Impact** | None (separate process) | Yes (shares GIL) | Yes (shares GIL) |\n| **Variable Pickling** | Required | Not required | Not required |\n| **Working Directory** | Can change safely | Can change safely | Affects main thread |\n\n### When to Use Each Executor\n\n**Use Subprocess When:**\n- Maximum isolation is required\n- Code may crash or corrupt memory\n- Need ability to force-kill on timeout\n- Running untrusted or experimental code\n\n**Use Subthread When:**\n- Need better performance than subprocess\n- Want some isolation from main thread\n- Timeout is likely but not critical\n- Code is relatively safe\n\n**Use Direct When:**\n- Maximum performance required\n- Full trust in code\n- No timeout needed\n- Running simple, fast operations\n\nSources: High-level diagrams, [src/runtime/subthread_python_executor.py:1-183]()\n\n---\n\n## Shared Memory Considerations\n\n### Global Variable Sharing\n\nUnlike subprocess execution, threads share the same global namespace:\n\n```mermaid\ngraph TB\n    subgraph \"Subprocess Execution\"\n        P1[\"Parent Process<br/>globals = {a: 1}\"]\n        P2[\"Child Process<br/>globals = {a: 1} (copy)\"]\n        P1 -->|fork/spawn| P2\n        P2 -.->|no sharing| P1\n    end\n    \n    subgraph \"Subthread Execution\"  \n        T1[\"Main Thread<br/>_globals dict\"]\n        T2[\"Worker Thread<br/>same _globals dict\"]\n        T1 -->|shared reference| T2\n        T2 -->|modifies same object| T1\n    end\n```\n\n**Implications:**\n\n1. **Variable Mutations Are Visible:**\n   ```python\n   _globals = {\"data\": [1, 2, 3]}\n   # Worker thread executes: data.append(4)\n   # Main thread sees: {\"data\": [1, 2, 3, 4]}\n   ```\n\n2. **Race Conditions Possible:**\n   - Multiple threads could modify shared objects\n   - GIL provides some protection but not complete safety\n   - Use locks for thread-safe operations\n\n3. **Module Import Side Effects:**\n   - Imports affect global module cache\n   - Changes to `sys.modules` visible to all threads\n\n### Workspace Globals Management\n\nThe `arg_globals` field in `ExecutionResult` undergoes filtering:\n\n```python\n@field_validator('arg_globals')\n@classmethod        \ndef field_validate_globals(cls, value: Dict[str, Any]) -> Dict[str, Any]:\n    if value is None:\n        return {}\n    return workspace.filter_and_deepcopy_globals(value)\n```\n\n**Filtering Process:**\n1. Exclude built-in objects (`__builtins__`)\n2. Exclude module references\n3. Deep copy remaining values to prevent mutation\n4. Check picklability (for consistency with subprocess)\n\nThis ensures the returned `arg_globals` can be safely persisted in the workspace state.\n\nSources: [src/runtime/schemas.py:77-83](), [src/runtime/subthread_python_executor.py:41]()\n\n---\n\n## Use Cases in the Codebase\n\n### Example: Beijing Scenic Spot Data Processing\n\nThe test files show subthread execution being used for data processing tasks:\n\n```python\nresult = run_structured_in_thread(test_code, {}, timeout=10)\n```\n\n**Typical Use Case Pattern:**\n\n1. **Data Schema Definition:**\n   - Pydantic models for validation\n   - CSV parsing and transformation\n   - JSON schema generation\n\n2. **Geographic Data Processing:**\n   - Coordinate parsing\n   - Statistical calculations\n   - Data validation\n\n3. **Visualization:**\n   - Matplotlib figure generation\n   - Chart saving to files\n   - Non-interactive plotting\n\n**Why Subthread Is Suitable:**\n- Fast enough for data processing\n- Matplotlib works with proper backend setup\n- Shared memory allows efficient data passing\n- No need for maximum isolation\n\nSources: [tests/playground/gen/g9/g9.py:1-153](), [src/runtime/subthread_python_executor.py:130-183]()\n\n---\n\n## Limitations and Risks\n\n### Cannot Force-Terminate Threads\n\n**The Core Problem:**\n```python\nt.join(timeout)\nif t.is_alive():\n    # Thread is STILL RUNNING - we can only mark it as timed out\n    # No way to kill it like subprocess.terminate()\n```\n\n**Consequences:**\n- Infinite loops create zombie threads\n- Resource leaks from uncompleted operations\n- Memory usage grows if many timeouts occur\n- GIL contention affects overall performance\n\n### Shared Memory Corruption Risk\n\n**Scenario:**\n1. Thread starts executing user code\n2. Timeout occurs, main thread returns\n3. Worker thread continues running in background\n4. Worker thread modifies shared globals\n5. Next execution uses corrupted globals\n\n**Mitigation:**\n- Deep copy globals before passing to executor\n- Validate globals after execution\n- Use subprocess executor for untrusted code\n\n### GIL Limitations\n\nThe Global Interpreter Lock (GIL) means:\n- Only one thread executes Python bytecode at a time\n- CPU-bound operations don't parallelize\n- I/O-bound operations still benefit (GIL released during I/O)\n\n**Performance Implications:**\n\n| Operation Type | Benefit from Threading |\n|---------------|----------------------|\n| Pure computation | No (GIL bottleneck) |\n| File I/O | Yes (GIL released) |\n| Network I/O | Yes (GIL released) |\n| Sleep/waiting | Yes (GIL released) |\n\n### No Exit Code Available\n\nUnlike processes, threads don't have exit codes:\n```python\nfinal_res.exit_code = None  # Threads don't have exit codes\n```\n\nThis means:\n- Can't distinguish different crash types via exit code\n- Must rely on exception information\n- No signal information (SIGSEGV, SIGABRT, etc.)\n\nSources: [src/runtime/subthread_python_executor.py:102-125](), [docs/crashed.design.md:236-260]()\n\n---\n\n## Summary\n\nThe subthread execution strategy provides a **middle ground** between full process isolation and direct in-process execution:\n\n**Advantages:**\n- Faster startup than subprocess\n- Buffer-based output capture (simpler than pipes)\n- Shared memory space (no pickling required)\n- Working directory isolation\n- Timeout support\n\n**Disadvantages:**\n- Cannot force-kill timed-out threads\n- Shared memory risks (corruption, race conditions)\n- GIL contention for CPU-bound tasks\n- No exit code information\n- Limited crash detection\n\n**Best Used For:**\n- Data processing tasks with moderate trust level\n- Operations needing matplotlib with proper backend setup\n- Scenarios where subprocess overhead is too high\n- Code that rarely times out\n\nFor maximum safety and isolation, prefer [Subprocess Execution](#5.1). For maximum performance with trusted code, consider [Direct Execution](#5.3).\n\nSources: [src/runtime/subthread_python_executor.py:1-183](), [src/runtime/schemas.py:1-111]()\n\n---\n\n# Page: Direct Execution\n\n# Direct Execution\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/crashed.design.md](docs/crashed.design.md)\n- [docs/python.design.md](docs/python.design.md)\n- [src/runtime/python_executor.py](src/runtime/python_executor.py)\n- [src/runtime/schemas.py](src/runtime/schemas.py)\n- [tests/playground/crashed.py](tests/playground/crashed.py)\n- [tests/playground/subprocess_output.py](tests/playground/subprocess_output.py)\n- [tests/unit/runtime/test_exec_runner.py](tests/unit/runtime/test_exec_runner.py)\n- [tests/unit/runtime/test_python_executor.py](tests/unit/runtime/test_python_executor.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page documents the **direct execution strategy** for running Python code within the algo_agent system. Direct execution runs code in-process (within the same Python interpreter) for maximum performance, eliminating the overhead of process creation and inter-process communication. This strategy is implemented in [src/runtime/python_executor.py]() and provides functions for executing Python code snippets with output capture and state management.\n\nFor isolated execution with stronger safety guarantees, see [Subprocess Execution](#5.1) and [Subthread Execution](#5.2). For information about execution results and status codes, see [ExecutionResult and Status Handling](#5.4).\n\n---\n\n## Overview\n\nDirect execution offers the **fastest execution mode** by running code in the same process as the calling code, avoiding the overhead of:\n- Process creation and teardown\n- Memory serialization/deserialization\n- Inter-process communication (pipes, shared memory)\n\nHowever, this speed comes with trade-offs in isolation and safety. The system provides both simple string-based execution (`run()`) and structured result execution (`run_structured()`) with timeout support via multiprocessing.\n\n### Key Characteristics\n\n| Characteristic | Direct Execution |\n|----------------|------------------|\n| **Performance** | Fastest (no process overhead) |\n| **Isolation** | None (shares memory space) |\n| **Safety** | Lowest (crashes affect parent process) |\n| **Timeout Support** | Via multiprocessing.Process |\n| **Global State** | Can be captured and returned |\n| **Use Case** | Trusted code, maximum speed required |\n\nSources: [src/runtime/python_executor.py:1-164]()\n\n---\n\n## Execution Flow Architecture\n\nThe following diagram shows how direct execution integrates with the multiprocessing system for timeout support:\n\n```mermaid\ngraph TB\n    CALLER[\"Caller<br/>(ExecutePythonCodeTool)\"]\n    \n    subgraph \"Direct Execution Entry Points\"\n        RUN[\"run()<br/>Returns: str\"]\n        RUN_STRUCT[\"run_structured()<br/>Returns: ExecutionResult\"]\n    end\n    \n    subgraph \"Execution Path Decision\"\n        TIMEOUT_CHECK{\"timeout<br/>is None?\"}\n    end\n    \n    subgraph \"Direct Mode (No Timeout)\"\n        DIRECT_WORKER[\"worker_with_globals_capture()<br/>Called directly in-process\"]\n        DIRECT_EXEC[\"exec(command, _globals, _locals)\"]\n        DIRECT_CAPTURE[\"StringIO captures stdout\"]\n    end\n    \n    subgraph \"Multiprocessing Mode (With Timeout)\"\n        QUEUE[\"multiprocessing.Queue\"]\n        PROCESS[\"multiprocessing.Process<br/>target=worker_with_globals_capture\"]\n        WORKER[\"worker_with_globals_capture()<br/>Runs in separate process\"]\n        WORKER_EXEC[\"exec(command, _globals, _locals)\"]\n        WORKER_CAPTURE[\"StringIO captures stdout\"]\n        RESULT_PUT[\"queue.put(ExecutionResult)\"]\n    end\n    \n    RESULT[\"ExecutionResult<br/>or str output\"]\n    \n    CALLER --> RUN\n    CALLER --> RUN_STRUCT\n    RUN --> TIMEOUT_CHECK\n    RUN_STRUCT --> TIMEOUT_CHECK\n    \n    TIMEOUT_CHECK -->|\"Yes<br/>(Direct)\"| DIRECT_WORKER\n    TIMEOUT_CHECK -->|\"No<br/>(Process)\"| QUEUE\n    \n    DIRECT_WORKER --> DIRECT_EXEC\n    DIRECT_EXEC --> DIRECT_CAPTURE\n    DIRECT_CAPTURE --> RESULT\n    \n    QUEUE --> PROCESS\n    PROCESS --> WORKER\n    WORKER --> WORKER_EXEC\n    WORKER_EXEC --> WORKER_CAPTURE\n    WORKER_CAPTURE --> RESULT_PUT\n    RESULT_PUT --> QUEUE\n    QUEUE -->|\"queue.get()\"| RESULT\n    \n    RESULT --> CALLER\n```\n\nSources: [src/runtime/python_executor.py:92-163]()\n\n---\n\n## Worker Functions\n\nThe direct execution system provides two worker functions that handle the actual code execution with different output formats.\n\n### worker_with_globals_capture()\n\nThis function is the **primary worker for structured execution**, capturing both output and global variable state. It returns an `ExecutionResult` object with comprehensive execution information.\n\n**Function Signature:**\n```python\ndef worker_with_globals_capture(\n    command: str,\n    _globals: Optional[Dict],\n    _locals: Optional[Dict],\n    queue: multiprocessing.Queue,\n    timeout: Optional[int],\n) -> None\n```\n\n**Execution Flow:**\n\n```mermaid\ngraph TD\n    START[\"worker_with_globals_capture() called\"]\n    REDIRECT[\"Redirect sys.stdout to StringIO\"]\n    \n    TRY_EXEC[\"Try: exec(command, _globals, _locals)\"]\n    SUCCESS_RESULT[\"Build ExecutionResult<br/>exit_status=SUCCESS<br/>stdout=mystdout.getvalue()\"]\n    \n    EXCEPT[\"Except: Catch exception\"]\n    FAILURE_RESULT[\"Build ExecutionResult<br/>exit_status=FAILURE<br/>exception details captured\"]\n    \n    FINALLY[\"Finally: Restore sys.stdout\"]\n    QUEUE_PUT[\"queue.put(exec_result)\"]\n    END[\"Return None\"]\n    \n    START --> REDIRECT\n    REDIRECT --> TRY_EXEC\n    TRY_EXEC -->|\"Success\"| SUCCESS_RESULT\n    TRY_EXEC -->|\"Exception\"| EXCEPT\n    EXCEPT --> FAILURE_RESULT\n    SUCCESS_RESULT --> FINALLY\n    FAILURE_RESULT --> FINALLY\n    FINALLY --> QUEUE_PUT\n    QUEUE_PUT --> END\n```\n\n**Key Implementation Details:**\n\n1. **Output Redirection**: [src/runtime/python_executor.py:42-43]()\n   ```python\n   old_stdout = sys.stdout\n   sys.stdout = mystdout = StringIO()\n   ```\n\n2. **Success Path**: [src/runtime/python_executor.py:45-54]()\n   - Executes code with `exec(command, exec_globals, exec_locals)`\n   - Builds `ExecutionResult` with `exit_status=ExecutionStatus.SUCCESS`\n   - Captures stdout content\n\n3. **Failure Path**: [src/runtime/python_executor.py:55-67]()\n   - Catches any exception during execution\n   - Captures exception type, value, and traceback\n   - Builds `ExecutionResult` with `exit_status=ExecutionStatus.FAILURE`\n\n4. **Result Communication**: [src/runtime/python_executor.py:69-70]()\n   - Uses multiprocessing.Queue to send result back to parent\n   - Works in both direct and process-isolated modes\n\nSources: [src/runtime/python_executor.py:28-70]()\n\n### worker()\n\nThis function is a **simpler worker for string-based execution**, returning only the captured output or error message as a string.\n\n**Function Signature:**\n```python\ndef worker(\n    command: str,\n    _globals: Optional[Dict],\n    _locals: Optional[Dict],\n    queue: multiprocessing.Queue,\n) -> None\n```\n\n**Differences from `worker_with_globals_capture()`:**\n\n| Feature | worker_with_globals_capture() | worker() |\n|---------|-------------------------------|----------|\n| Return Type | ExecutionResult (via queue) | str (via queue) |\n| Input Sanitization | None | Uses `sanitize_input()` |\n| Exception Handling | Structured with traceback | Returns formatted string |\n| Global State Capture | Yes | No |\n| Use Case | Structured execution pipeline | Legacy/simple execution |\n\n**Implementation**: [src/runtime/python_executor.py:72-89]()\n\nSources: [src/runtime/python_executor.py:72-89]()\n\n---\n\n## Execution Entry Points\n\n### run_structured()\n\nThe **primary entry point** for direct execution with structured results. Decorated with `@traceable` for observability.\n\n**Function Signature:**\n```python\n@traceable\ndef run_structured(\n    command: str, \n    _globals: dict[str, Any] | None = None, \n    _locals: Optional[Dict] = None, \n    timeout: Optional[int] = None\n) -> ExecutionResult\n```\n\n**Execution Logic:**\n\n```mermaid\ngraph TD\n    START[\"run_structured() called\"]\n    CREATE_QUEUE[\"Create multiprocessing.Queue()\"]\n    TIMEOUT_CHECK{\"timeout<br/>is not None?\"}\n    \n    subgraph \"With Timeout (Process Mode)\"\n        CREATE_PROC[\"Create Process<br/>target=worker_with_globals_capture\"]\n        START_PROC[\"p.start()\"]\n        JOIN_PROC[\"p.join(timeout)\"]\n        ALIVE_CHECK{\"p.is_alive()?\"}\n        TERMINATE[\"p.terminate()<br/>Build TIMEOUT ExecutionResult\"]\n        GET_RESULT[\"Get result from queue\"]\n    end\n    \n    subgraph \"Without Timeout (Direct Mode)\"\n        DIRECT_CALL[\"Call worker_with_globals_capture<br/>directly (no process)\"]\n        DIRECT_RESULT[\"Get result from queue\"]\n    end\n    \n    RETURN[\"Return ExecutionResult\"]\n    \n    START --> CREATE_QUEUE\n    CREATE_QUEUE --> TIMEOUT_CHECK\n    \n    TIMEOUT_CHECK -->|\"Yes\"| CREATE_PROC\n    CREATE_PROC --> START_PROC\n    START_PROC --> JOIN_PROC\n    JOIN_PROC --> ALIVE_CHECK\n    ALIVE_CHECK -->|\"Yes<br/>(Timeout)\"| TERMINATE\n    ALIVE_CHECK -->|\"No<br/>(Finished)\"| GET_RESULT\n    TERMINATE --> RETURN\n    GET_RESULT --> RETURN\n    \n    TIMEOUT_CHECK -->|\"No\"| DIRECT_CALL\n    DIRECT_CALL --> DIRECT_RESULT\n    DIRECT_RESULT --> RETURN\n```\n\n**Timeout Handling**: [src/runtime/python_executor.py:114-124]()\n\nWhen a timeout occurs:\n1. Process is still alive after `p.join(timeout)`\n2. `p.terminate()` is called to forcibly stop execution\n3. An `ExecutionResult` with `exit_status=ExecutionStatus.TIMEOUT` is constructed by the parent process\n4. No globals are captured (unsafe to retrieve from terminated process)\n\n**Direct Mode**: [src/runtime/python_executor.py:126-130]()\n\nWhen `timeout is None`:\n- `worker_with_globals_capture()` is called **directly in the current process**\n- No multiprocessing overhead\n- Maximum performance\n- Result still uses queue for API consistency\n\nSources: [src/runtime/python_executor.py:92-130]()\n\n### run()\n\nA **legacy/simpler entry point** that returns execution output as a plain string rather than a structured result.\n\n**Function Signature:**\n```python\ndef run(\n    command: str, \n    _globals: dict[str, Any] | None = None, \n    _locals: Optional[Dict] = None, \n    timeout: Optional[int] = None\n) -> str\n```\n\n**Key Differences from `run_structured()`:**\n\n1. Uses the simpler `worker()` function instead of `worker_with_globals_capture()`\n2. Returns string output or error message instead of `ExecutionResult`\n3. Timeout handling returns `\"Execution timed out\"` string\n4. Does not capture or return global variable state\n\n**Implementation**: [src/runtime/python_executor.py:132-163]()\n\nSources: [src/runtime/python_executor.py:132-163]()\n\n---\n\n## Queue-Based Communication\n\nThe direct execution system uses `multiprocessing.Queue` for communication between the caller and the worker, even in direct mode. This provides a **consistent API** regardless of execution mode.\n\n### Queue Communication Pattern\n\n```mermaid\nsequenceDiagram\n    participant Caller as Caller\n    participant Queue as multiprocessing.Queue\n    participant Worker as worker_with_globals_capture\n    \n    Caller->>Queue: Create queue = Queue()\n    \n    alt With Timeout (Process Mode)\n        Caller->>Worker: Start Process(target=worker, args=(queue,))\n        Worker->>Worker: Execute code\n        Worker->>Worker: Build ExecutionResult\n        Worker->>Queue: queue.put(result)\n        Caller->>Queue: result = queue.get()\n    else Without Timeout (Direct Mode)\n        Caller->>Worker: Call worker() directly with queue\n        Worker->>Worker: Execute code in-process\n        Worker->>Worker: Build ExecutionResult\n        Worker->>Queue: queue.put(result)\n        Caller->>Queue: result = queue.get()\n    end\n    \n    Queue-->>Caller: Return ExecutionResult\n```\n\n**Why Use Queue in Direct Mode?**\n\nEven when executing code directly (no timeout), the system uses a queue because:\n1. **API Consistency**: Both modes use the same calling convention\n2. **Clean Result Passing**: Queue provides a standard result container\n3. **Future Extensibility**: Easy to switch modes without API changes\n4. **Exception Isolation**: Queue naturally handles serializable results\n\nSources: [src/runtime/python_executor.py:104-130]()\n\n---\n\n## Integration with ExecutionResult Schema\n\nDirect execution integrates with the structured result system through the `ExecutionResult` schema.\n\n### Result Construction\n\n**Success Case**: [src/runtime/python_executor.py:47-54]()\n```python\nres = ExecutionResult(\n    arg_command=command,\n    arg_globals=_globals or {},\n    arg_timeout=timeout,\n    exit_status=ExecutionStatus.SUCCESS,\n    # ret_stdout filled later from StringIO\n)\n```\n\n**Failure Case**: [src/runtime/python_executor.py:56-67]()\n```python\nres = ExecutionResult(\n    arg_command=command,\n    arg_globals=_globals or {},\n    arg_timeout=timeout,\n    exit_status=ExecutionStatus.FAILURE,\n    exception_repr=repr(e),\n    exception_type=type(e).__name__,\n    exception_value=str(e),\n    exception_traceback=source_code.get_exception_traceback()\n)\n```\n\n**Timeout Case** (constructed by caller): [src/runtime/python_executor.py:117-124]()\n```python\nreturn ExecutionResult(\n    arg_command=command,\n    arg_timeout=timeout,\n    arg_globals=_globals or {},\n    exit_status=ExecutionStatus.TIMEOUT,\n    ret_stdout=\"\",\n)\n```\n\n### Status Handling Summary\n\n| Status | Created By | Globals Captured? | Exception Info? |\n|--------|-----------|-------------------|-----------------|\n| SUCCESS | Worker | Yes (filtered & deepcopied) | No |\n| FAILURE | Worker | Yes (pre-exception state) | Yes (full traceback) |\n| TIMEOUT | Caller | No (unsafe) | No |\n| CRASHED | Not applicable | Not applicable | Not applicable* |\n\n*CRASHED status is only possible in subprocess execution (covered in [5.1](#5.1))\n\nSources: [src/runtime/python_executor.py:47-67](), [src/runtime/python_executor.py:117-124](), [src/runtime/schemas.py:11-16]()\n\n---\n\n## Use Cases and Trade-offs\n\n### When to Use Direct Execution\n\n**Appropriate Use Cases:**\n1. **Trusted Code**: Executing code from trusted sources or pre-validated snippets\n2. **Performance Critical**: When execution speed is paramount and overhead must be minimized\n3. **State Sharing**: When you need direct access to the caller's namespace\n4. **Simple Operations**: Quick calculations or transformations without external dependencies\n5. **Testing**: Unit tests where isolation isn't necessary\n\n**Example from Tests**: [tests/unit/runtime/test_python_executor.py:78-86]()\n```python\nres = run_structured(\"\"\"import time\nc = 10\ntime.sleep(5)\"\"\", my_globals, my_locals, timeout=1)\n```\n\n### When NOT to Use Direct Execution\n\n**Avoid Direct Execution For:**\n1. **Untrusted Code**: User-provided code that hasn't been sandboxed\n2. **Long-Running Tasks**: Operations that might hang indefinitely\n3. **Memory-Intensive**: Code that could exhaust system memory\n4. **Crash-Prone Code**: Operations that could cause segfaults or crashes\n5. **Isolation Required**: When you need guaranteed separation from parent process\n\n**Use Subprocess Instead**: For these scenarios, use [Subprocess Execution](#5.1)\n\nSources: [src/runtime/python_executor.py:92-163](), [tests/unit/runtime/test_python_executor.py:1-104]()\n\n### Performance Comparison\n\n```mermaid\ngraph LR\n    subgraph \"Execution Overhead (Relative)\"\n        DIRECT[\"Direct Execution<br/>Baseline: 1x\"]\n        SUBTHREAD[\"Subthread Execution<br/>~2-3x overhead\"]\n        SUBPROCESS[\"Subprocess Execution<br/>~10-50x overhead\"]\n    end\n    \n    DIRECT -->|\"Thread creation,<br/>buffer setup\"| SUBTHREAD\n    SUBTHREAD -->|\"Process creation,<br/>pipe setup,<br/>serialization\"| SUBPROCESS\n```\n\n| Strategy | Startup Time | Memory Overhead | Isolation | Safety |\n|----------|--------------|-----------------|-----------|--------|\n| **Direct** | ~0ms | None | None | Lowest |\n| **Subthread** | ~1-5ms | Minimal | Partial | Medium |\n| **Subprocess** | ~10-100ms | High (full copy) | Complete | Highest |\n\nSources: [src/runtime/python_executor.py:1-164]()\n\n---\n\n## Limitations and Risks\n\n### Critical Limitations\n\n1. **No Crash Protection**\n   - If executed code crashes (segfault, stack overflow), the **entire parent process crashes**\n   - No mechanism to recover from catastrophic failures\n   - See crash examples in [tests/playground/crashed.py:1-124]()\n\n2. **No Isolation**\n   - Executed code shares the same memory space as the caller\n   - Can access and modify parent process variables\n   - Can import and interfere with system modules\n\n3. **Timeout Implementation**\n   - Timeout support requires multiprocessing.Process\n   - When timeout is used, it's **no longer truly \"direct\"** - it becomes subprocess execution\n   - True direct mode (`timeout=None`) has **no timeout protection**\n\n4. **Global State Pollution**\n   - Executed code can modify `sys.modules`, global variables, etc.\n   - Changes persist after execution completes\n   - No cleanup mechanism for side effects\n\n### Security Considerations\n\n**Never use direct execution for untrusted code:**\n\n```python\n# DANGEROUS - Direct execution of user input\nuser_code = request.get(\"code\")  # From HTTP request\nresult = run_structured(user_code, timeout=None)  # NO ISOLATION!\n```\n\n**Safe alternative - Use subprocess:**\n```python\n# SAFE - Subprocess isolation for untrusted code\nuser_code = request.get(\"code\")\nresult = run_structured_in_subprocess(user_code, timeout=10)  # Isolated\n```\n\n### Namespace Behavior\n\nThe interaction between `_globals` and `_locals` requires careful handling. From [docs/python.design.md:427-534](), Python's `exec()` has specific rules:\n\n- When both `_globals` and `_locals` are provided separately, module-level assignments go to `_locals`\n- To ensure functions are accessible in `_globals`, use: `_locals = _globals`\n- Direct execution in this codebase uses: [src/runtime/python_executor.py:39-40]()\n  ```python\n  exec_globals = _globals\n  exec_locals  = _locals\n  ```\n\nSources: [src/runtime/python_executor.py:28-70](), [tests/playground/crashed.py:1-124](), [docs/python.design.md:427-534](), [docs/crashed.design.md:1-404]()\n\n---\n\n## Comparison with Other Execution Strategies\n\n### Direct vs Subthread vs Subprocess\n\n```mermaid\ngraph TB\n    subgraph \"Isolation Levels\"\n        DIRECT[\"Direct Execution<br/>âââââââââââââ<br/>â¢ Same process<br/>â¢ Same memory<br/>â¢ No protection\"]\n        \n        SUBTHREAD[\"Subthread Execution<br/>âââââââââââââ<br/>â¢ Same process<br/>â¢ Shared memory<br/>â¢ Timeout via threading\"]\n        \n        SUBPROCESS[\"Subprocess Execution<br/>âââââââââââââ<br/>â¢ Separate process<br/>â¢ Isolated memory<br/>â¢ Full protection\"]\n    end\n    \n    RISK[\"Risk Level\"]\n    SPEED[\"Speed\"]\n    \n    DIRECT -->|\"Increasing\"| RISK\n    SUBTHREAD -->|\"Increasing\"| RISK\n    SUBPROCESS -->|\"Increasing\"| RISK\n    \n    SUBPROCESS -->|\"Increasing\"| SPEED\n    SUBTHREAD -->|\"Increasing\"| SPEED\n    DIRECT -->|\"Increasing\"| SPEED\n```\n\n| Feature | Direct | Subthread | Subprocess |\n|---------|--------|-----------|------------|\n| **Implementation** | `exec()` in-process | `Thread` + `exec()` | `Process` + pipe IPC |\n| **Crash Protection** | None | None | Full |\n| **Memory Isolation** | None | None | Full |\n| **Timeout Method** | Process (when needed) | Thread + flag | Process termination |\n| **Globals Capture** | Direct reference | Shared mutable state | Serialized via pipe |\n| **Startup Overhead** | None (~0ms) | Minimal (~1ms) | Significant (~50ms) |\n| **Best For** | Trusted, fast operations | I/O-bound tasks | Untrusted code |\n\nSources: [src/runtime/python_executor.py:1-164](), [tests/playground/subprocess_output.py:1-293]()\n\n---\n\n## Code Examples\n\n### Basic Direct Execution\n\nFrom [tests/unit/runtime/test_python_executor.py:10-21]():\n\n```python\nmy_globals = {\"a\": 123, \"b\": [1, 2, 3]}\nmy_locals = {\"a\": 123, \"b\": [1, 2, 3]}\n\nres = run(\"a+=100000\", my_globals, my_locals)  # Direct execution\nprint(res)  # Empty string (assignment has no output)\n\nres = run(\"print(a)\", my_globals, my_locals)\nprint(res)  # \"100123\" (modified value)\n```\n\n### Structured Execution with Globals Capture\n\nFrom [tests/unit/runtime/test_python_executor.py:89-97]():\n\n```python\nmy_globals = {\"a\": 123, \"b\": [1, 2, 3]}\nres = run_structured(\"\"\"import time\nc = 10\nimport scipy\n\"\"\", my_globals, None, timeout=20000)\n\nprint(res.exit_status)  # ExecutionStatus.SUCCESS\nprint(res.arg_globals)   # {'a': 123, 'b': [1, 2, 3], 'c': 10}\n```\n\n### Exception Handling\n\nFrom [tests/unit/runtime/test_python_executor.py:37-49]():\n\n```python\nmy_globals = {\"a\": 123, \"b\": [1, 2, 3]}\nres = run(\"print(c)\", my_globals, my_locals)  # NameError\nprint(res)  # Returns formatted error message\n```\n\nSources: [tests/unit/runtime/test_python_executor.py:10-104]()\n\n---\n\n## Summary\n\nDirect execution provides the **fastest code execution** strategy in the algo_agent system by running code in-process without isolation overhead. It is most appropriate for:\n\nâ Trusted code execution  \nâ Performance-critical operations  \nâ Simple calculations and transformations  \nâ Testing and development scenarios\n\nIt should **not be used** for:\n\nâ Untrusted user input  \nâ Long-running or hanging operations  \nâ Memory-intensive tasks  \nâ Code that might crash or cause instability\n\nFor production use with untrusted code, prefer [Subprocess Execution](#5.1) which provides complete isolation and crash protection.\n\nSources: [src/runtime/python_executor.py:1-164]()\n\n---\n\n# Page: ExecutionResult and Status Handling\n\n# ExecutionResult and Status Handling\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [docs/crashed.design.md](docs/crashed.design.md)\n- [src/runtime/cwd.py](src/runtime/cwd.py)\n- [src/runtime/schemas.py](src/runtime/schemas.py)\n- [src/runtime/subprocess_python_executor.py](src/runtime/subprocess_python_executor.py)\n- [tests/playground/crashed.py](tests/playground/crashed.py)\n- [tests/playground/subprocess_output.py](tests/playground/subprocess_output.py)\n\n</details>\n\n\n\nThis document covers the structured result format used to capture Python code execution outcomes and the status classification system. The `ExecutionResult` schema and `ExecutionStatus` enum provide a comprehensive way to represent successful executions, failures, timeouts, and process crashes, along with all relevant diagnostic information.\n\nFor information about how these results are generated by different execution strategies, see [Subprocess Execution](#5.1), [Subthread Execution](#5.2), and [Direct Execution](#5.3). For details on how workspace state is managed within execution results, see [Workspace State Management](#5.5).\n\n---\n\n## ExecutionStatus Enum\n\nThe `ExecutionStatus` enum defines four distinct states that categorize all possible execution outcomes. This enum is used throughout the execution runtime to classify results and determine appropriate error handling.\n\n| Status | Value | Meaning | When Assigned |\n|--------|-------|---------|---------------|\n| `SUCCESS` | `\"success\"` | Code executed without errors | Process exits normally (exit code 0) and exec() completes |\n| `FAILURE` | `\"failure\"` | Code raised an exception | Process exits normally but exec() raises a catchable exception |\n| `TIMEOUT` | `\"timeout\"` | Execution exceeded time limit | Parent process terminates subprocess after timeout period |\n| `CRASHED` | `\"crashed\"` | Process terminated abnormally | Process exits without sending result (SegFault, OOM, etc.) |\n\n**Sources:** [src/runtime/schemas.py:10-16]()\n\n```mermaid\ngraph TD\n    START[\"Code Execution Begins\"]\n    SUBPROCESS[\"Subprocess Starts\"]\n    EXEC[\"exec() Call\"]\n    \n    SUCCESS[\"ExecutionStatus.SUCCESS\"]\n    FAILURE[\"ExecutionStatus.FAILURE\"]\n    TIMEOUT[\"ExecutionStatus.TIMEOUT\"]\n    CRASHED[\"ExecutionStatus.CRASHED\"]\n    \n    START --> SUBPROCESS\n    SUBPROCESS --> TIMEOUT_CHECK{\"Timeout<br/>Exceeded?\"}\n    TIMEOUT_CHECK -->|Yes| TIMEOUT\n    TIMEOUT_CHECK -->|No| EXEC\n    \n    EXEC --> EXCEPTION{\"Exception<br/>Raised?\"}\n    EXCEPTION -->|No| RESULT_SENT{\"Result<br/>Sent?\"}\n    EXCEPTION -->|Yes| FAILURE\n    \n    RESULT_SENT -->|Yes| SUCCESS\n    RESULT_SENT -->|No| CRASHED\n    \n    SUCCESS --> END[\"Return Result\"]\n    FAILURE --> END\n    TIMEOUT --> END\n    CRASHED --> END\n```\n\n**Diagram:** ExecutionStatus State Machine\n\n**Sources:** [src/runtime/subprocess_python_executor.py:76-163](), [tests/playground/subprocess_output.py:68-156]()\n\n---\n\n## ExecutionResult Schema\n\nThe `ExecutionResult` is a Pydantic model that captures all information about a code execution attempt. It contains three categories of fields: input parameters, execution results, and parent-populated fields.\n\n### Field Categories\n\n**Input Parameters** (captured at execution start):\n- `arg_command`: The Python code string that was executed\n- `arg_timeout`: The timeout limit in seconds\n- `arg_globals`: The filtered and deep-copied global variables dictionary\n\n**Execution Results** (populated by subprocess or parent):\n- `exit_status`: The `ExecutionStatus` enum value\n- `exit_code`: Process exit code (0=normal, >0=error, <0=signal terminated)\n- `exception_repr`: String representation of the exception (`repr(e)`)\n- `exception_type`: Exception class name (e.g., `\"ZeroDivisionError\"`)\n- `exception_value`: Exception message (`str(e)`)\n- `exception_traceback`: Full traceback with stack frames\n\n**Parent-Populated Fields** (filled after subprocess completes):\n- `ret_stdout`: Combined stdout and stderr output\n- `ret_tool2llm`: Formatted message for LLM consumption\n\n**Sources:** [src/runtime/schemas.py:56-71]()\n\n### Field Population Table\n\n| Field | SUCCESS | FAILURE | TIMEOUT | CRASHED |\n|-------|---------|---------|---------|---------|\n| `arg_command` | â Set | â Set | â Set | â Set |\n| `arg_timeout` | â Set | â Set | â Set | â Set |\n| `arg_globals` | â Filtered | â Filtered | â Original | â Original |\n| `exit_status` | SUCCESS | FAILURE | TIMEOUT | CRASHED |\n| `exit_code` | 0 | 0 | -15 (SIGTERM) | Variable |\n| `exception_repr` | None | â Set | None | None |\n| `exception_type` | None | â Set | None | None |\n| `exception_value` | None | â Set | None | None |\n| `exception_traceback` | None | â Set | None | None |\n| `ret_stdout` | â Output | â Output | â Partial | â Partial |\n| `ret_tool2llm` | â Generated | â Generated | â Generated | â Generated |\n\n---\n\n## Status Determination Logic\n\nThe execution status is determined through a decision tree that evaluates process lifecycle events and outcomes. The logic differs slightly between subprocess execution (most comprehensive) and other execution modes.\n\n### Subprocess Execution Flow\n\n```mermaid\nsequenceDiagram\n    participant Parent as \"Parent Process\"\n    participant Subprocess as \"Child Process\"\n    participant Reader as \"Reader Thread\"\n    participant Buffer as \"stdout_buffer\"\n    participant Container as \"result_container\"\n    \n    Parent->>Subprocess: Start with timeout\n    Parent->>Reader: Start reader thread\n    Subprocess->>Subprocess: Redirect stdout/stderr\n    \n    alt Code executes successfully\n        Subprocess->>Buffer: Send stdout messages\n        Subprocess->>Container: Send ExecutionResult (SUCCESS)\n        Subprocess->>Subprocess: Exit with code 0\n        Parent->>Parent: join() returns (not alive)\n        Parent->>Parent: exit_status = SUCCESS\n    else Code raises exception\n        Subprocess->>Buffer: Send stdout messages\n        Subprocess->>Subprocess: Catch exception in try-except\n        Subprocess->>Container: Send ExecutionResult (FAILURE)\n        Subprocess->>Subprocess: Exit with code 0\n        Parent->>Parent: join() returns (not alive)\n        Parent->>Parent: exit_status = FAILURE\n    else Timeout exceeded\n        Parent->>Parent: join(timeout) - still alive\n        Parent->>Subprocess: terminate()\n        Parent->>Parent: Build ExecutionResult (TIMEOUT)\n        Parent->>Parent: exit_code = -15\n    else Process crashes\n        Subprocess->>Buffer: Send partial stdout\n        Subprocess->>Subprocess: SegFault/OOM - no result sent\n        Parent->>Parent: join() returns (not alive)\n        Parent->>Parent: result_container is empty\n        Parent->>Parent: Build ExecutionResult (CRASHED)\n        Parent->>Parent: exit_code = 139 or 1\n    end\n    \n    Parent->>Parent: Populate ret_stdout from buffer\n    Parent->>Parent: Generate ret_tool2llm\n    Parent->>Parent: Return ExecutionResult\n```\n\n**Diagram:** Status Determination in Subprocess Execution\n\n**Sources:** [src/runtime/subprocess_python_executor.py:128-163]()\n\n### Status Assignment Rules\n\n**SUCCESS Status** [src/runtime/subprocess_python_executor.py:49-57]():\n- Subprocess completes normally (process.is_alive() == False)\n- Result container has exactly one ExecutionResult\n- exec() completed without raising exceptions\n- Exit code is 0\n\n**FAILURE Status** [src/runtime/subprocess_python_executor.py:58-69]():\n- Subprocess completes normally (process.is_alive() == False)\n- Result container has exactly one ExecutionResult\n- exec() raised a Python exception that was caught\n- Exception details captured in try-except block\n- Exit code is 0 (process didn't crash, just code failed)\n\n**TIMEOUT Status** [src/runtime/subprocess_python_executor.py:130-144]():\n- Parent process determines subprocess is still alive after timeout\n- Parent calls process.terminate() to force termination\n- Parent constructs ExecutionResult with TIMEOUT status\n- Exit code is -15 (SIGTERM signal)\n- arg_globals contains original input (not filtered output)\n\n**CRASHED Status** [src/runtime/subprocess_python_executor.py:145-159]():\n- Subprocess terminates (process.is_alive() == False)\n- Result container is empty (no ExecutionResult received)\n- Indicates process-level failure (SegFault, signal termination, etc.)\n- Exit code varies: 139 (SIGSEGV), 1 (unhandled error), etc.\n- arg_globals contains original input (not filtered output)\n\n**Sources:** [src/runtime/subprocess_python_executor.py:76-163](), [tests/playground/subprocess_output.py:68-156]()\n\n---\n\n## Result Population Flow\n\nThe ExecutionResult is populated in stages depending on where execution succeeds or fails.\n\n```mermaid\ngraph TB\n    subgraph \"Subprocess Context\"\n        INIT[\"Initialize<br/>arg_command, arg_timeout, arg_globals\"]\n        EXEC[\"Execute with exec()\"]\n        SUCCESS_PATH[\"Success Path\"]\n        FAILURE_PATH[\"Exception Path\"]\n        \n        INIT --> EXEC\n        EXEC --> SUCCESS_PATH\n        EXEC --> FAILURE_PATH\n        \n        SUCCESS_PATH --> SUB_SUCCESS[\"Create ExecutionResult<br/>exit_status=SUCCESS<br/>arg_globals=filtered\"]\n        FAILURE_PATH --> SUB_FAILURE[\"Create ExecutionResult<br/>exit_status=FAILURE<br/>exception_*=populated\"]\n        \n        SUB_SUCCESS --> SEND1[\"Send via pipe\"]\n        SUB_FAILURE --> SEND2[\"Send via pipe\"]\n    end\n    \n    subgraph \"Parent Context\"\n        WAIT[\"Wait for subprocess\"]\n        CHECK{\"Process<br/>State?\"}\n        \n        WAIT --> CHECK\n        \n        CHECK -->|Timeout| PARENT_TIMEOUT[\"Create ExecutionResult<br/>exit_status=TIMEOUT\"]\n        CHECK -->|Completed| RECV_CHECK{\"Result<br/>Received?\"}\n        \n        RECV_CHECK -->|Yes| RECV_RESULT[\"Use subprocess result\"]\n        RECV_CHECK -->|No| PARENT_CRASHED[\"Create ExecutionResult<br/>exit_status=CRASHED\"]\n        \n        RECV_RESULT --> POPULATE\n        PARENT_TIMEOUT --> POPULATE\n        PARENT_CRASHED --> POPULATE\n        \n        POPULATE[\"Populate parent fields:<br/>- exit_code from process<br/>- ret_stdout from buffer<br/>- ret_tool2llm via get_return_llm\"]\n    end\n    \n    SEND1 --> WAIT\n    SEND2 --> WAIT\n```\n\n**Diagram:** ExecutionResult Population Stages\n\n**Sources:** [src/runtime/subprocess_python_executor.py:76-163]()\n\n### Global Variables Filtering\n\nThe `arg_globals` field undergoes automatic filtering and deep copying through a Pydantic field validator:\n\n```python\n@field_validator('arg_globals')\n@classmethod        \ndef field_validate_globals(cls, value: Dict[str, Any]) -> Dict[str, Any]:\n    if value is None:\n        return {}\n    return workspace.filter_and_deepcopy_globals(value)\n```\n\n**Sources:** [src/runtime/schemas.py:77-83]()\n\nThis filtering:\n- Removes `__builtins__` and module objects\n- Excludes non-picklable objects\n- Creates deep copies to prevent shared state issues\n- Only applies to SUCCESS and FAILURE statuses (when exec() completes)\n- For TIMEOUT and CRASHED, original globals are retained (execution didn't complete)\n\nFor complete details on workspace filtering, see [Workspace State Management](#5.5).\n\n---\n\n## LLM-Formatted Output Generation\n\nThe `get_return_llm` class method generates human-readable messages formatted specifically for LLM consumption. Each status has a distinct template optimized for debugging and iteration.\n\n### Template Structure\n\n```mermaid\ngraph LR\n    STATUS[\"ExecutionStatus\"]\n    METHOD[\"get_return_llm(status, result)\"]\n    \n    STATUS --> METHOD\n    \n    METHOD --> SUCCESS_MSG[\"SUCCESS:<br/>## Code executed successfully<br/>### Terminal output\"]\n    METHOD --> FAILURE_MSG[\"FAILURE:<br/>## Code failed<br/>### Terminal output<br/>### Original code with line numbers<br/>### Error traceback\"]\n    METHOD --> TIMEOUT_MSG[\"TIMEOUT:<br/>## Execution timeout<br/>### Terminal output<br/>### Timeout limit\"]\n    METHOD --> CRASHED_MSG[\"CRASHED:<br/>## Process crashed<br/>### Terminal output<br/>### Exit status code\"]\n```\n\n**Diagram:** LLM Message Generation by Status\n\n**Sources:** [src/runtime/schemas.py:18-47]()\n\n### Message Templates\n\n**SUCCESS Template** [src/runtime/schemas.py:24-27]():\n```\n## ä»£ç æ§è¡æåï¼è¾åºç»æå®æ´ï¼ä»»å¡å®æ\n### ç»ç«¯è¾åºï¼\n{result.ret_stdout}\n```\n\n**FAILURE Template** [src/runtime/schemas.py:28-35]():\n```\n## ä»£ç æ§è¡å¤±è´¥ï¼ä»£ç æåºå¼å¸¸ï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\n{result.ret_stdout}\n### åå§ä»£ç ï¼\n{source_code.add_line_numbers(result.arg_command)}\n### æ¥éä¿¡æ¯ï¼\n{result.exception_traceback}\n```\n\n**TIMEOUT Template** [src/runtime/schemas.py:36-40]():\n```\n## ä»£ç æ§è¡è¶æ¶ï¼å¼ºå¶éåºæ§è¡ï¼è°æ´è¶æ¶æ¶é´åéè¯\n### ç»ç«¯è¾åºï¼\n{result.ret_stdout}\n### è¶åºéå¶çæ¶é´ï¼{result.arg_timeout} ç§\n```\n\n**CRASHED Template** [src/runtime/schemas.py:41-45]():\n```\n## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\n{result.ret_stdout}\n### éåºç¶æç ï¼{result.exit_code}\n```\n\nThe FAILURE template uniquely includes line-numbered source code to help the LLM identify the exact location of errors.\n\n**Sources:** [src/runtime/schemas.py:18-47](), [src/runtime/source_code.py]()\n\n---\n\n## Exit Code Interpretation\n\nProcess exit codes provide additional diagnostic information, particularly for CRASHED status:\n\n| Exit Code Range | Meaning | Example Scenarios |\n|----------------|---------|-------------------|\n| 0 | Normal exit | Code executed successfully, or caught exception properly |\n| > 0 | Error exit | Code logic errors, command execution failures |\n| -1 to -255 | Signal termination | Exit code = -(signal number) |\n| -15 | SIGTERM | Timeout termination by parent process |\n| -9 | SIGKILL | Force kill (cannot be caught) |\n| -11 or 139 | SIGSEGV | Segmentation fault (139 = 128 + 11) |\n\n**Sources:** [src/runtime/schemas.py:50-54](), [tests/playground/subprocess_output.py:215-234]()\n\n---\n\n## Status Examples\n\n### SUCCESS Example\n\n**Code:**\n```python\nimport scipy\nc = 10\nprint(\"scipy imported\")\n```\n\n**Result:**\n```python\nExecutionResult(\n    arg_command='import time\\nc = 10\\nimport scipy\\nprint(\"scipy imported\")\\n',\n    arg_globals={'a': 123, 'b': [1, 2, 3], 'c': 10},\n    arg_timeout=20000,\n    exit_status=ExecutionStatus.SUCCESS,\n    exit_code=0,\n    exception_repr=None,\n    exception_type=None,\n    exception_value=None,\n    exception_traceback=None,\n    ret_stdout='scipy imported\\n',\n    ret_tool2llm='## ä»£ç æ§è¡æåï¼è¾åºç»æå®æ´ï¼ä»»å¡å®æ\\n### ç»ç«¯è¾åºï¼\\nscipy imported\\n'\n)\n```\n\n**Sources:** [tests/playground/subprocess_output.py:183-198](), [tests/playground/subprocess_output.py:362-372]()\n\n### FAILURE Example\n\n**Code:**\n```python\na = 123\nb = 0\nc = a/b\n```\n\n**Result:**\n```python\nExecutionResult(\n    arg_command='\\na = 123\\nb = 0\\nc = a/b\\n',\n    arg_globals={'a': 123, 'b': 0},\n    arg_timeout=20000,\n    exit_status=ExecutionStatus.FAILURE,\n    exit_code=0,\n    exception_repr=\"ZeroDivisionError('division by zero')\",\n    exception_type='ZeroDivisionError',\n    exception_value='division by zero',\n    exception_traceback='Traceback (most recent call last):\\n'\n                        '  File \"...subprocess_output.py\", line 42, in _worker_with_pipe\\n'\n                        '    exec(command, _globals, _locals)\\n'\n                        '  File \"<string>\", line 4, in <module>\\n'\n                        'ZeroDivisionError: division by zero\\n',\n    ret_stdout='',\n    ret_tool2llm='## ä»£ç æ§è¡å¤±è´¥ï¼ä»£ç æåºå¼å¸¸ï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\\n...'\n)\n```\n\n**Sources:** [tests/playground/subprocess_output.py:199-213](), [tests/playground/subprocess_output.py:396-427]()\n\n### TIMEOUT Example\n\n**Code:**\n```python\nimport time\nprint(\"Start sleeping...\", flush=True)\ntime.sleep(10)\nprint(\"Finished sleeping.\", flush=True)\n```\n\n**Result (timeout=3):**\n```python\nExecutionResult(\n    arg_command='\\nimport time\\nprint(\"Start sleeping...\", flush=True)\\ntime.sleep(10)\\n...',\n    arg_globals={'a': 123, 'b': [1, 2, 3]},  # Original, not filtered\n    arg_timeout=3,\n    exit_status=ExecutionStatus.TIMEOUT,\n    exit_code=-15,  # SIGTERM\n    exception_repr=None,\n    exception_type=None,\n    exception_value=None,\n    exception_traceback=None,\n    ret_stdout='Start sleeping...\\n',  # Partial output before termination\n    ret_tool2llm='## ä»£ç æ§è¡è¶æ¶ï¼å¼ºå¶éåºæ§è¡ï¼è°æ´è¶æ¶æ¶é´åéè¯\\n...'\n)\n```\n\n**Sources:** [tests/playground/subprocess_output.py:166-177](), [tests/playground/subprocess_output.py:320-337]()\n\n### CRASHED Example (SegFault)\n\n**Code:**\n```python\nimport os\nos._exit(139)  # Simulate SIGSEGV\n```\n\n**Result:**\n```python\nExecutionResult(\n    arg_command='\\nimport os\\ntry:\\n    os._exit(139)\\nexcept Exception as e:\\n...',\n    arg_globals={'a': 123, 'b': [1, 2, 3]},  # Original, not filtered\n    arg_timeout=3,\n    exit_status=ExecutionStatus.CRASHED,\n    exit_code=139,  # SIGSEGV = 128 + 11\n    exception_repr=None,  # No exception caught - process died\n    exception_type=None,\n    exception_value=None,\n    exception_traceback=None,\n    ret_stdout='',\n    ret_tool2llm='## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\\n...'\n)\n```\n\n**Sources:** [tests/playground/subprocess_output.py:215-234](), [tests/playground/subprocess_output.py:451-468]()\n\n### CRASHED Example (RecursionError with PickleError)\n\nWhen a RecursionError occurs and the resulting ExecutionResult cannot be pickled for IPC, the subprocess crashes during the send operation:\n\n**Code:**\n```python\ndef recursive_crash(depth=0):\n    if depth%100==0:\n        print(f\"å½åéå½æ·±åº¦ï¼{depth}\")\n    recursive_crash(depth + 1)\n\ntry:\n    recursive_crash()\nexcept RecursionError as e:\n    print(f\"\\nå´©æºåå ï¼{e}\")\n    raise e\n```\n\n**Result:**\n```python\nExecutionResult(\n    arg_command='...',\n    arg_globals={'a': 123, 'b': [1, 2, 3]},\n    arg_timeout=3,\n    exit_status=ExecutionStatus.CRASHED,\n    exit_code=1,  # Generic error exit\n    exception_repr=None,\n    exception_type=None,\n    exception_value=None,\n    exception_traceback=None,\n    ret_stdout='å½åéå½æ·±åº¦ï¼0\\n...\\n'\n                'å´©æºåå ï¼maximum recursion depth exceeded\\n'\n                'Process Process-5:\\n'\n                'Traceback (most recent call last):\\n'\n                '  ...\\n'\n                \"_pickle.PicklingError: Can't pickle <function recursive_crash at 0x...>\\n\",\n    ret_tool2llm='## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\\n...'\n)\n```\n\nThis scenario demonstrates a unique case where:\n1. The subprocess catches the RecursionError in the except block\n2. Creates an ExecutionResult with FAILURE status\n3. Attempts to send the result via pipe\n4. Pickling fails because `recursive_crash` function is in arg_globals\n5. Subprocess crashes with exit code 1 during the send operation\n6. Parent receives no result and classifies as CRASHED\n7. The pickle error appears in ret_stdout (subprocess stderr)\n\n**Sources:** [tests/playground/subprocess_output.py:235-259](), [tests/playground/subprocess_output.py:492-593]()\n\n---\n\n## Integration with Execution Strategies\n\nAll three execution strategies (subprocess, subthread, direct) return ExecutionResult objects, though their status determination capabilities differ:\n\n| Capability | Subprocess | Subthread | Direct |\n|------------|-----------|-----------|--------|\n| SUCCESS detection | â Full | â Full | â Full |\n| FAILURE detection | â Full | â Full | â Full |\n| TIMEOUT detection | â Full | â Limited | â Limited |\n| CRASHED detection | â Full | â None | â None |\n| Process isolation | â Complete | Partial | None |\n| Exit code capture | â Yes | â No | â No |\n\nOnly subprocess execution provides true CRASHED status detection because it:\n- Runs in a separate process with independent memory\n- Can be forcibly terminated without affecting parent\n- Provides exit codes for signal-based termination\n- Detects when no result is sent (process died before completing)\n\n**Sources:** [src/runtime/subprocess_python_executor.py:76-163](), [Diagram 3 from high-level architecture]()\n\n---\n\n# Page: Workspace State Management\n\n# Workspace State Management\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [docs/error correction.design.md](docs/error correction.design.md)\n- [docs/log.md](docs/log.md)\n- [src/agent/tool/python_tool.py](src/agent/tool/python_tool.py)\n- [src/agent/tool/todo_tool.py](src/agent/tool/todo_tool.py)\n- [src/runtime/workspace.py](src/runtime/workspace.py)\n- [src/utils/__pycache__/__init__.cpython-312.pyc](src/utils/__pycache__/__init__.cpython-312.pyc)\n\n</details>\n\n\n\nThis page documents the workspace state management system responsible for persisting Python global variables across multiple code executions. This enables stateful code execution similar to Jupyter notebooks, where variables defined in one execution remain available in subsequent executions.\n\nFor information about how code is actually executed in different isolation modes, see [Subprocess Execution](#5.1), [Subthread Execution](#5.2), and [Direct Execution](#5.3). For details on execution results and status codes, see [ExecutionResult and Status Handling](#5.4).\n\n---\n\n## Purpose and Scope\n\nThe workspace state management system solves a critical challenge: maintaining Python variable state across multiple isolated code executions. When the agent executes Python code snippets through the `ExecutePythonCodeTool`, each execution needs access to variables and objects created in previous executions, while ensuring only serializable, safe objects are persisted.\n\nThe system handles:\n- **State persistence**: Variables defined in execution N are available in execution N+1\n- **Serialization filtering**: Exclusion of non-picklable objects (modules, builtins, file handles, etc.)\n- **Deep copying**: Isolation of state to prevent unintended mutations\n- **Workspace initialization**: Setting up clean execution environments\n\nSources: [src/runtime/workspace.py:1-108]()\n\n---\n\n## Architecture Overview\n\nThe workspace system uses a **two-list architecture** where state flows from input â execution â output in a linear chain:\n\n```mermaid\ngraph TB\n    subgraph \"State Storage\"\n        ARG[\"arg_globals_list<br/>(list[dict])\"]\n        OUT[\"out_globals_list<br/>(list[dict])\"]\n    end\n    \n    subgraph \"State Processing\"\n        INIT[\"initialize_workspace()\"]\n        FILTER[\"filter_and_deepcopy_globals()\"]\n        GET[\"get_arg_globals()\"]\n        APPEND[\"append_out_globals()\"]\n    end\n    \n    subgraph \"Execution Flow\"\n        EXEC1[\"Execution 1\"]\n        EXEC2[\"Execution 2\"]\n        EXEC3[\"Execution 3\"]\n    end\n    \n    INIT -->|\"Creates initial<br/>workspace dict\"| FILTER\n    FILTER -->|\"Filters & copies\"| GET\n    GET -->|\"Provides input<br/>globals\"| EXEC1\n    \n    EXEC1 -->|\"Returns modified<br/>globals\"| APPEND\n    APPEND -->|\"Stores in<br/>out_globals_list\"| OUT\n    \n    OUT -->|\"Becomes input<br/>for next execution\"| GET\n    GET --> EXEC2\n    EXEC2 --> APPEND\n    \n    OUT --> GET\n    GET --> EXEC3\n    \n    ARG -.->|\"Tracks input<br/>state history\"| GET\n    OUT -.->|\"Tracks output<br/>state history\"| APPEND\n```\n\n**Key insight**: `arg_globals_list` stores the input state for each execution, while `out_globals_list` stores the output state. The output of execution N becomes the input of execution N+1 after filtering and deep copying.\n\nSources: [src/runtime/workspace.py:10-11](), [src/runtime/workspace.py:81-98]()\n\n---\n\n## Global State Lists\n\nThe workspace module maintains two module-level lists that form the backbone of state persistence:\n\n| Variable | Type | Purpose |\n|----------|------|---------|\n| `arg_globals_list` | `list[dict]` | Stores the **input** globals dictionary for each execution |\n| `out_globals_list` | `list[dict]` | Stores the **output** globals dictionary after each execution |\n\n**State Flow Pattern**:\n```\nExecution 1:  arg_globals_list[0] â execute â out_globals_list[0]\nExecution 2:  arg_globals_list[1] (= filtered out_globals_list[0]) â execute â out_globals_list[1]\nExecution 3:  arg_globals_list[2] (= filtered out_globals_list[1]) â execute â out_globals_list[2]\n```\n\nThis architecture provides:\n- **Version history**: Both lists grow with each execution, creating an audit trail\n- **Rollback capability**: Previous states remain accessible in the lists\n- **State isolation**: Deep copying prevents accidental mutations between executions\n\nSources: [src/runtime/workspace.py:10-11]()\n\n---\n\n## Workspace Initialization\n\nA fresh workspace dictionary is created using the `initialize_workspace()` function:\n\n```mermaid\ngraph LR\n    CREATE[\"__create_workspace()\"] --> EXEC[\"exec('', workspace)\"]\n    EXEC --> WORKSPACE[\"workspace dict<br/>(with builtins)\"]\n    WORKSPACE --> UPDATE[\"workspace.update()<br/>{'__name__': '__main__'}\"]\n    UPDATE --> RETURN[\"return workspace\"]\n```\n\n**Implementation Details**:\n- [src/runtime/workspace.py:14-17]() - `__create_workspace()` executes an empty string in a new dictionary, which populates it with Python builtins\n- [src/runtime/workspace.py:20-23]() - `initialize_workspace()` adds `__name__ = '__main__'` to make the workspace behave like a main module\n- The workspace dictionary mirrors the `globals()` dictionary you would find in a normal Python script\n\n**Why `exec(\"\")` is used**: Executing an empty string in a dictionary automatically populates it with the correct builtins, providing a properly initialized execution environment.\n\nSources: [src/runtime/workspace.py:14-23]()\n\n---\n\n## State Filtering and Serialization\n\nThe `filter_and_deepcopy_globals()` function is the most critical component, ensuring only safe, serializable objects persist between executions:\n\n```mermaid\ngraph TB\n    INPUT[\"Input: original_globals<br/>(dict)\"]\n    \n    INPUT --> LOOP[\"For each key, value<br/>in globals\"]\n    \n    LOOP --> CHECK1{\"key ==<br/>'__builtins__'?\"}\n    CHECK1 -->|Yes| SKIP1[\"Continue<br/>(skip this item)\"]\n    CHECK1 -->|No| CHECK2\n    \n    CHECK2{\"value is<br/>module type?\"}\n    CHECK2 -->|Yes| SKIP2[\"Continue<br/>(skip this item)\"]\n    CHECK2 -->|No| CHECK3\n    \n    CHECK3{\"_is_serializable()<br/>returns True?\"}\n    CHECK3 -->|No| SKIP3[\"Continue<br/>(skip this item)\"]\n    CHECK3 -->|Yes| DEEPCOPY\n    \n    DEEPCOPY[\"copy.deepcopy(value)\"] --> ADD[\"Add to<br/>filtered_dict\"]\n    \n    ADD --> LOOP\n    SKIP1 --> LOOP\n    SKIP2 --> LOOP\n    SKIP3 --> LOOP\n    \n    LOOP -->|Done| RETURN[\"Return filtered_dict\"]\n```\n\n### Serialization Checking\n\nThe internal `_is_serializable()` helper function [src/runtime/workspace.py:45-64]() tests if an object can be pickled:\n\n```python\ndef _is_serializable(value) -> bool:\n    import pickle\n    try:\n        pickle.dumps(value, protocol=pickle.HIGHEST_PROTOCOL)\n        return True\n    except (pickle.PicklingError, TypeError, AttributeError, \n            RecursionError, MemoryError):\n        return False\n```\n\n**Objects excluded from persistence**:\n- Built-in modules (`__builtins__`)\n- Imported modules (e.g., `import numpy` â `numpy` module object)\n- Non-picklable objects: file handles, thread locks, socket connections, lambda functions with closures\n- Objects causing `RecursionError` or `MemoryError` during serialization\n\n**Why deep copy?** Prevents mutations in one execution from affecting the stored state. Each execution gets an isolated copy of the previous state.\n\nSources: [src/runtime/workspace.py:38-78]()\n\n---\n\n## State Retrieval Process\n\nThe `get_arg_globals()` function [src/runtime/workspace.py:81-91]() provides the input globals for each execution:\n\n### First Execution\n```mermaid\ngraph LR\n    CHECK[\"arg_globals_list<br/>empty?\"]\n    CHECK -->|Yes| INIT[\"initialize_workspace()\"]\n    INIT --> FILTER[\"filter_and_deepcopy_globals()\"]\n    FILTER --> APPEND[\"arg_globals_list.append()\"]\n    APPEND --> RETURN[\"return filter_arg_globals\"]\n```\n\n### Subsequent Executions\n```mermaid\ngraph LR\n    CHECK[\"arg_globals_list<br/>not empty?\"]\n    CHECK -->|Yes| GET[\"arg_globals =<br/>out_globals_list[-1]\"]\n    GET --> FILTER[\"filter_and_deepcopy_globals()\"]\n    FILTER --> APPEND[\"arg_globals_list.append()\"]\n    APPEND --> RETURN[\"return filter_arg_globals\"]\n```\n\n**Key Logic**:\n- **Line 83-86**: If `arg_globals_list` is empty (first execution), initialize a fresh workspace\n- **Line 88-90**: Otherwise, take the **last output state** from `out_globals_list[-1]` as the new input\n- Both paths apply filtering and deep copying before appending to `arg_globals_list`\n\nThis creates a **state chain**: each execution's output becomes the next execution's input.\n\nSources: [src/runtime/workspace.py:81-91]()\n\n---\n\n## State Persistence Process\n\nAfter each code execution completes, `append_out_globals()` [src/runtime/workspace.py:94-97]() stores the output state:\n\n```mermaid\ngraph LR\n    EXEC[\"Code execution<br/>completes\"]\n    EXEC --> RESULT[\"ExecutionResult<br/>with arg_globals\"]\n    RESULT --> FILTER[\"filter_and_deepcopy_globals()\"]\n    FILTER --> APPEND[\"out_globals_list.append()\"]\n```\n\n**Implementation**:\n```python\ndef append_out_globals(out_globals: dict):\n    global out_globals_list\n    filter_out_globals = filter_and_deepcopy_globals(out_globals)\n    out_globals_list.append(filter_out_globals)\n```\n\n**Critical behavior**: The output globals are **filtered and deep copied** before storage, ensuring:\n- Non-serializable objects from the execution are excluded\n- The stored state is isolated from future modifications\n- State consistency is maintained\n\nSources: [src/runtime/workspace.py:94-97]()\n\n---\n\n## Serialization Rules and Exclusions\n\nThe following table summarizes what persists and what doesn't:\n\n| Object Type | Persists? | Reason |\n|-------------|-----------|--------|\n| Primitive values (`int`, `str`, `float`, `bool`) | â Yes | Picklable |\n| Lists, tuples, sets | â Yes | Picklable if contents are |\n| Dictionaries | â Yes | Picklable if contents are |\n| Custom class instances | â Yes | If class is picklable |\n| Functions (simple `def`) | â Yes | Picklable |\n| Pydantic models | â Yes | Picklable |\n| NumPy arrays | â Yes | Picklable |\n| Pandas DataFrames | â Yes | Picklable |\n| `__builtins__` | â No | Explicitly excluded |\n| Imported modules | â No | Not picklable |\n| Lambda functions | â Maybe | Depends on closure |\n| File handles | â No | Not picklable |\n| Thread objects | â No | Not picklable |\n| Socket connections | â No | Not picklable |\n\n**Common Pattern**: **Always re-import modules** at the start of each code snippet, as they won't persist. Example from the tool description [src/agent/tool/python_tool.py:25]():\n> \"è½ç¶åéä¼æç»­å­å¨ï¼ä½ç±äºåºååéå¶ï¼å¯¼å¥çæ¨¡åï¼å¦ `math`ã`json`ï¼å¯è½ä¸ä¼å¨æ¯æ¬¡è°ç¨ä¸­æç»­å­å¨ã**å¨æ¯ä¸ªä»£ç çæ®µä¸­å§ç»éæ°å¯¼å¥å¿è¦çæ¨¡å**ã\"\n\nSources: [src/runtime/workspace.py:38-78](), [src/agent/tool/python_tool.py:25-26]()\n\n---\n\n## Integration with Execution System\n\nThe workspace system integrates with the Python execution tool through two key calls:\n\n```mermaid\nsequenceDiagram\n    participant Tool as \"ExecutePythonCodeTool\"\n    participant WS as \"workspace module\"\n    participant Executor as \"subthread_python_executor\"\n    participant Lists as \"arg_globals_list<br/>out_globals_list\"\n    \n    Tool->>WS: \"get_arg_globals()\"\n    WS->>Lists: \"Check arg_globals_list\"\n    alt \"First execution\"\n        WS->>WS: \"initialize_workspace()\"\n    else \"Subsequent execution\"\n        Lists->>WS: \"out_globals_list[-1]\"\n    end\n    WS->>WS: \"filter_and_deepcopy_globals()\"\n    WS->>Lists: \"append to arg_globals_list\"\n    WS-->>Tool: \"return execution_context\"\n    \n    Tool->>Executor: \"run_structured_in_thread(<br/>code, _globals=execution_context)\"\n    Executor-->>Tool: \"ExecutionResult<br/>with arg_globals\"\n    \n    Tool->>WS: \"append_out_globals(<br/>exec_result.arg_globals)\"\n    WS->>WS: \"filter_and_deepcopy_globals()\"\n    WS->>Lists: \"append to out_globals_list\"\n```\n\n**Code Integration Points**:\n\n1. **Before execution** [src/agent/tool/python_tool.py:42]():\n   ```python\n   execution_context: Optional[Dict[str, Any]] = workspace.get_arg_globals()\n   ```\n   Retrieves the globals dictionary to pass to the executor.\n\n2. **After execution** [src/agent/tool/python_tool.py:49]():\n   ```python\n   workspace.append_out_globals(exec_result.arg_globals)\n   ```\n   Stores the modified globals dictionary for the next execution.\n\n3. **Executor integration** [src/agent/tool/python_tool.py:44-48]():\n   The execution context is passed to `run_structured_in_thread()` as the `_globals` parameter, which becomes the globals dictionary for the `exec()` call.\n\nSources: [src/agent/tool/python_tool.py:41-50]()\n\n---\n\n## Workspace Utility Functions\n\nAdditional helper functions provide introspection into workspace state:\n\n### `get_workspace_globals_dict()`\n[src/runtime/workspace.py:26-29]() - Extracts key-value pairs from a workspace dictionary:\n```python\ndef get_workspace_globals_dict(workspace: dict, include_special_vars: bool = False):\n    if include_special_vars:\n        return {k: v for k, v in workspace.items()}\n    return {k: v for k, v in workspace.items() if not k.startswith('__')}\n```\n\n### `get_workspace_globals_keys()`\n[src/runtime/workspace.py:32-35]() - Extracts only the keys from a workspace dictionary:\n```python\ndef get_workspace_globals_keys(workspace: dict, include_special_vars: bool = False):\n    if include_special_vars:\n        return [k for k in workspace.keys()]\n    return [k for k in workspace.keys() if not k.startswith('__')]\n```\n\n**Use case**: Debugging and inspecting what variables are currently in the workspace.\n\nSources: [src/runtime/workspace.py:26-35]()\n\n---\n\n## Complete State Flow Example\n\nHere's a concrete example of how state flows through three executions:\n\n```mermaid\ngraph TB\n    subgraph \"Execution 1: Define variable\"\n        E1_INPUT[\"Input globals: {}<br/>(empty, first execution)\"]\n        E1_CODE[\"Code: x = 10\"]\n        E1_OUTPUT[\"Output globals:<br/>{x: 10}\"]\n        E1_INPUT --> E1_CODE --> E1_OUTPUT\n    end\n    \n    subgraph \"Execution 2: Use and modify\"\n        E2_INPUT[\"Input globals:<br/>{x: 10}<br/>(from E1 output)\"]\n        E2_CODE[\"Code: y = x * 2\"]\n        E2_OUTPUT[\"Output globals:<br/>{x: 10, y: 20}\"]\n        E2_INPUT --> E2_CODE --> E2_OUTPUT\n    end\n    \n    subgraph \"Execution 3: Import filtered out\"\n        E3_INPUT[\"Input globals:<br/>{x: 10, y: 20}<br/>(from E2 output)\"]\n        E3_CODE[\"Code: import math<br/>z = math.sqrt(y)\"]\n        E3_OUTPUT[\"Output globals:<br/>{x: 10, y: 20, z: 4.47...}<br/>(math module excluded)\"]\n        E3_INPUT --> E3_CODE --> E3_OUTPUT\n    end\n    \n    E1_OUTPUT -.->|\"filter & deepcopy\"| E2_INPUT\n    E2_OUTPUT -.->|\"filter & deepcopy\"| E3_INPUT\n```\n\n**State Lists After Execution 3**:\n```\narg_globals_list = [\n    {},                          # E1 input\n    {x: 10},                     # E2 input\n    {x: 10, y: 20}               # E3 input\n]\n\nout_globals_list = [\n    {x: 10},                     # E1 output\n    {x: 10, y: 20},              # E2 output\n    {x: 10, y: 20, z: 4.47...}   # E3 output (math filtered out)\n]\n```\n\n**Note**: The `math` module imported in Execution 3 is **not** persisted to `out_globals_list` because it's filtered out by `filter_and_deepcopy_globals()`. If Execution 4 needs `math`, it must re-import it.\n\nSources: [src/runtime/workspace.py:38-97]()\n\n---\n\n## State Management Best Practices\n\nBased on the system design, follow these guidelines when using the workspace:\n\n1. **Always re-import modules**: Modules are filtered out and won't persist between executions.\n   \n2. **Use simple, picklable data structures**: Complex objects with non-picklable components (file handles, connections) will be silently excluded.\n\n3. **Be aware of state accumulation**: Variables persist indefinitely. Consider explicitly deleting variables when no longer needed:\n   ```python\n   del large_dataframe  # Free memory\n   ```\n\n4. **Check variable existence**: Since state persists, check if variables already exist before redefining:\n   ```python\n   if 'config' not in globals():\n       config = load_config()\n   ```\n\n5. **Debugging serialization issues**: If a variable isn't persisting, it likely failed the `_is_serializable()` check. Look for:\n   - Embedded file handles\n   - Thread locks or queues\n   - Lambda functions with complex closures\n   - Objects with `__getstate__` methods that fail\n\nSources: [src/agent/tool/python_tool.py:25-26](), [src/runtime/workspace.py:45-64]()\n\n---\n\n# Page: Working Directory and Environment\n\n# Working Directory and Environment\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [src/runtime/before_thread/plt_back_chinese.py](src/runtime/before_thread/plt_back_chinese.py)\n- [src/runtime/cwd.py](src/runtime/cwd.py)\n- [src/runtime/subprocess_python_executor.py](src/runtime/subprocess_python_executor.py)\n- [src/runtime/subthread_python_executor.py](src/runtime/subthread_python_executor.py)\n- [tests/playground/gen/g9/beijing_scenic_spot_schema_with_play_hours.json](tests/playground/gen/g9/beijing_scenic_spot_schema_with_play_hours.json)\n- [tests/playground/gen/g9/beijing_scenic_spot_validated_data_with_play_hours.json](tests/playground/gen/g9/beijing_scenic_spot_validated_data_with_play_hours.json)\n- [tests/playground/gen/g9/g9.py](tests/playground/gen/g9/g9.py)\n\n</details>\n\n\n\nThis page covers how the execution runtime manages working directories, redirects standard output streams, and configures the execution environment for isolated code execution. These mechanisms ensure that Python code snippets executed by the agent run in controlled, isolated environments with proper output capture.\n\nFor information about the overall execution strategies (subprocess, subthread, direct), see [Execution Runtime](#5). For details on workspace state management and global variable persistence, see [Workspace State Management](#5.5).\n\n---\n\n## Purpose and Scope\n\nThe working directory and environment management system provides:\n\n- **Working directory isolation**: Each execution can run in a dedicated directory to prevent file system conflicts\n- **Output stream capture**: stdout and stderr are redirected to capture execution output\n- **Environment configuration**: Execution-specific settings (e.g., matplotlib backend) are applied before code execution\n- **Cross-platform compatibility**: Utilities work consistently across subprocess and subthread execution modes\n\n---\n\n## Working Directory Management\n\n### Workspace Directory Structure\n\nThe system uses a workspace directory structure under `wsm/` (workspace management) to isolate execution environments. The typical structure is:\n\n```\nwsm/\nâââ 1/\nâ   âââ g4-1/      # Workspace for specific execution context\nâââ 2/\nâ   âââ g7-2/      # Another isolated workspace\nâââ 3/\nâ   âââ g8-1/\nâââ 4/\n    âââ g9-1/\n```\n\nThis structure is ignored by git via [.gitignore:213]() which excludes `/ws*/` patterns.\n\n**Sources:** [.gitignore:213](), [src/runtime/subprocess_python_executor.py:28]()\n\n### The create_cwd Function\n\nThe `create_cwd` function initializes a working directory for subprocess execution. It creates the directory if it doesn't exist and changes the process's current working directory.\n\n```mermaid\ngraph LR\n    A[\"Subprocess<br/>Launch\"] --> B[\"create_cwd\"]\n    B --> C[\"get_or_create_subfolder\"]\n    C --> D{\"Directory<br/>Exists?\"}\n    D -->|No| E[\"Create Directory\"]\n    D -->|Yes| F[\"os.chdir\"]\n    E --> F\n    F --> G[\"Log New CWD\"]\n    G --> H[\"Execute Code\"]\n```\n\n**Function Signature:**\n\n```python\ndef create_cwd(cwd=None) -> bool\n```\n\n**Implementation Details:**\n\n- **Location**: [src/runtime/cwd.py:9-28]()\n- **Parameters**: `cwd` - relative path from project root (e.g., `'./wsm/2/g7-2'`)\n- **Returns**: `True` if successful, `False` on error\n- **Logging**: Records PID, target directory, and success/failure status\n\nThe function is called in subprocess execution at [src/runtime/subprocess_python_executor.py:28]():\n\n```python\ncwd.create_cwd('./wsm/2/g7-2')\n```\n\n**Error Handling:**\n\n| Error Type | Handling |\n|------------|----------|\n| `OSError` | Logged with PID and error message, returns `False` |\n| `Exception` | Generic exception logged, returns `False` |\n\n**Sources:** [src/runtime/cwd.py:9-28](), [src/runtime/subprocess_python_executor.py:27-28]()\n\n### The ChangeDirectory Context Manager\n\nThe `ChangeDirectory` context manager provides automatic directory restoration after execution, ensuring the working directory is reset even if exceptions occur.\n\n```mermaid\ngraph TB\n    subgraph \"ChangeDirectory Context Manager\"\n        A[\"__enter__\"] --> B[\"Save Current Directory<br/>self.original_dir\"]\n        B --> C[\"Create Target Directory<br/>get_or_create_subfolder\"]\n        C --> D[\"os.chdir(target_dir)\"]\n        D --> E[\"Log Switch\"]\n        E --> F[\"Execute User Code\"]\n        F --> G[\"__exit__\"]\n        G --> H[\"os.chdir(original_dir)\"]\n        H --> I[\"Log Restoration\"]\n    end\n    \n    J[\"Exception?\"] -->|Yes| G\n    J -->|No| G\n    G --> K[\"Propagate Exception\"]\n```\n\n**Class Definition:**\n\n```python\nclass ChangeDirectory:\n    def __init__(self, target_dir)\n    def __enter__(self)\n    def __exit__(self, exc_type, exc_val, exc_tb)\n```\n\n**Implementation**: [src/runtime/cwd.py:31-54]()\n\n**Usage in Subthread Executor:**\n\nThe context manager is used in subthread execution at [src/runtime/subthread_python_executor.py:35-37]():\n\n```python\nwith cwd.ChangeDirectory('./wsm/4/g9-1'):\n    with cwd.Change_STDOUT_STDERR(_BufferWriter(stdout_buffer)):\n        exec(command, _globals, _locals)\n```\n\n**Usage in Test Files:**\n\nExample from [tests/playground/gen/g9/g9.py:1-3]():\n\n```python\nfrom src.runtime import cwd\ncwd.create_cwd('./tests/playground/gen/g9')\n```\n\n**Sources:** [src/runtime/cwd.py:31-54](), [src/runtime/subthread_python_executor.py:35-37](), [tests/playground/gen/g9/g9.py:1-3]()\n\n---\n\n## Output Stream Redirection\n\n### The Change_STDOUT_STDERR Context Manager\n\nThe `Change_STDOUT_STDERR` context manager redirects `sys.stdout` and `sys.stderr` to custom writers, enabling output capture during code execution.\n\n```mermaid\ngraph LR\n    A[\"Original<br/>sys.stdout\"] --> B[\"Change_STDOUT_STDERR<br/>__enter__\"]\n    B --> C[\"sys.stdout = custom_writer\"]\n    B --> D[\"sys.stderr = custom_writer\"]\n    C --> E[\"exec(command)\"]\n    D --> E\n    E --> F[\"Output Captured<br/>in Buffer/Pipe\"]\n    F --> G[\"__exit__\"]\n    G --> H[\"Restore Original<br/>sys.stdout\"]\n    G --> I[\"Restore Original<br/>sys.stderr\"]\n```\n\n**Class Definition:**\n\n```python\nclass Change_STDOUT_STDERR:\n    def __init__(self, new_stdout, new_stderr=None)\n    def __enter__(self)\n    def __exit__(self, exc_type, exc_val, exc_tb)\n```\n\n**Implementation**: [src/runtime/cwd.py:56-77]()\n\n**Key Features:**\n\n- Saves original stdout/stderr in `__enter__`\n- Redirects both streams to custom writer (default: stderr uses same as stdout if not specified)\n- Restores original streams in `__exit__` regardless of exceptions\n- Returns `False` from `__exit__` to propagate exceptions\n\n**Sources:** [src/runtime/cwd.py:56-77]()\n\n### Subprocess Output Capture\n\nIn subprocess execution, a custom `_PipeWriter` class redirects output to the parent process via multiprocessing pipes.\n\n```mermaid\ngraph TB\n    subgraph \"Subprocess\"\n        A[\"sys.stdout = _PipeWriter\"] --> B[\"write(msg)\"]\n        B --> C[\"child_conn.send<br/>(_PipeType.STDOUT, msg)\"]\n    end\n    \n    subgraph \"Parent Process\"\n        D[\"_reader Thread\"] --> E[\"parent_conn.recv()\"]\n        E --> F{\"Message Type\"}\n        F -->|STDOUT| G[\"Append to<br/>subprocess_stdout_buffer\"]\n        F -->|RESULT| H[\"Append to<br/>subprocess_result_container\"]\n    end\n    \n    C --> E\n```\n\n**_PipeWriter Implementation**: [src/runtime/subprocess_python_executor.py:30-45]()\n\n```python\nclass _PipeWriter:\n    def __init__(self, child_conn: PipeConnection):\n        self.child_conn = child_conn\n    \n    def write(self, msg: str):\n        if msg:\n            self.child_conn.send((_PipeType.STDOUT, msg))\n    \n    def flush(self):\n        pass\n```\n\nThe writer is activated at [src/runtime/subprocess_python_executor.py:47-48]():\n\n```python\nsys.stdout = _PipeWriter(child_conn)\nsys.stderr = sys.stdout\n```\n\n**Output Collection:**\n\nThe parent process reads messages in a dedicated thread [src/runtime/subprocess_python_executor.py:88-112]() and stores them in `subprocess_stdout_buffer`. After execution completes, the buffer is joined into the final result [src/runtime/subprocess_python_executor.py:161]():\n\n```python\nfinal_res.ret_stdout = \"\".join(subprocess_stdout_buffer)\n```\n\n**Sources:** [src/runtime/subprocess_python_executor.py:30-48](), [src/runtime/subprocess_python_executor.py:88-112](), [src/runtime/subprocess_python_executor.py:161]()\n\n### Subthread Output Capture\n\nIn subthread execution, a `_BufferWriter` class appends output to a shared list.\n\n```mermaid\ngraph LR\n    A[\"sys.stdout = _BufferWriter\"] --> B[\"write(msg)\"]\n    B --> C[\"stdout_buffer.append(msg)\"]\n    C --> D[\"Shared Memory<br/>List\"]\n    D --> E[\"Main Thread<br/>Reads Buffer\"]\n    E --> F[\"Join to String\"]\n```\n\n**_BufferWriter Implementation**: [src/runtime/subthread_python_executor.py:23-32]()\n\n```python\nclass _BufferWriter:\n    def __init__(self, buffer: list[str]):\n        self.buffer = buffer\n    \n    def write(self, msg: str):\n        if msg:\n            self.buffer.append(msg)\n    \n    def flush(self):\n        pass\n```\n\nThe writer is used within nested context managers [src/runtime/subthread_python_executor.py:35-37]():\n\n```python\nwith cwd.ChangeDirectory('./wsm/4/g9-1'):\n    with cwd.Change_STDOUT_STDERR(_BufferWriter(stdout_buffer)):\n        exec(command, _globals, _locals)\n```\n\nAfter execution, the buffer is joined [src/runtime/subthread_python_executor.py:126]():\n\n```python\nfinal_res.ret_stdout = \"\".join(stdout_buffer)\n```\n\n**Sources:** [src/runtime/subthread_python_executor.py:23-32](), [src/runtime/subthread_python_executor.py:35-37](), [src/runtime/subthread_python_executor.py:126]()\n\n---\n\n## Environment Setup\n\n### Matplotlib Backend Configuration\n\nWhen executing code in subthreads, matplotlib requires specific configuration to avoid GUI-related errors. The `plt_back_chinese` module performs this initialization.\n\n```mermaid\ngraph TB\n    A[\"Subthread Execution\"] --> B[\"Import before_thread.plt_back_chinese\"]\n    B --> C[\"matplotlib.use('Agg')<br/>Non-interactive Backend\"]\n    C --> D[\"Configure Chinese Fonts<br/>SimHei\"]\n    D --> E[\"Disable Unicode Minus\"]\n    E --> F[\"Execute User Code<br/>with Matplotlib\"]\n```\n\n**Module Location**: [src/runtime/before_thread/plt_back_chinese.py]()\n\n**Implementation**: [src/runtime/before_thread/plt_back_chinese.py:4-12]()\n\n```python\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n# Set non-GUI backend (critical)\nmatplotlib.use(\"Agg\")  \n\n# Configure Chinese font support\nplt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\nplt.rcParams[\"axes.unicode_minus\"] = False\n```\n\n**Purpose:**\n\n| Configuration | Reason |\n|---------------|--------|\n| `matplotlib.use(\"Agg\")` | Prevents GUI initialization errors in subthreads |\n| `font.sans-serif = [\"SimHei\"]` | Enables Chinese character rendering |\n| `axes.unicode_minus = False` | Prevents Unicode minus sign display issues |\n\nThe module is imported at [src/runtime/subthread_python_executor.py:9]() before any user code executes matplotlib commands.\n\n**Logging:**\n\nThe module logs initialization steps at [src/runtime/before_thread/plt_back_chinese.py:3]() and [src/runtime/before_thread/plt_back_chinese.py:13]() to track configuration completion.\n\n**Sources:** [src/runtime/before_thread/plt_back_chinese.py:1-13](), [src/runtime/subthread_python_executor.py:9]()\n\n### Execution Mode-Specific Setup\n\nDifferent execution modes require different environment setups:\n\n```mermaid\ngraph TB\n    A[\"Execution Request\"] --> B{\"Execution Mode\"}\n    \n    B -->|Subprocess| C[\"create_cwd<br/>New Process Space\"]\n    C --> D[\"_PipeWriter<br/>IPC Output\"]\n    D --> E[\"Isolated Environment\"]\n    \n    B -->|Subthread| F[\"ChangeDirectory<br/>Context Manager\"]\n    F --> G[\"Change_STDOUT_STDERR<br/>Buffer Output\"]\n    G --> H[\"plt_back_chinese<br/>Matplotlib Setup\"]\n    H --> I[\"Shared Memory Space\"]\n    \n    B -->|Direct| J[\"No Directory Change\"]\n    J --> K[\"Queue-based Output\"]\n    K --> L[\"Same Process Space\"]\n```\n\n**Subprocess Setup Sequence:**\n\n1. Launch subprocess with `multiprocessing.Process` [src/runtime/subprocess_python_executor.py:114-116]()\n2. Call `create_cwd` to set working directory [src/runtime/subprocess_python_executor.py:28]()\n3. Redirect stdout/stderr to `_PipeWriter` [src/runtime/subprocess_python_executor.py:47-48]()\n4. Execute code with `exec(command, _globals, _locals)` [src/runtime/subprocess_python_executor.py:50]()\n\n**Subthread Setup Sequence:**\n\n1. Launch thread with `threading.Thread` [src/runtime/subthread_python_executor.py:97-100]()\n2. Import matplotlib configuration [src/runtime/subthread_python_executor.py:9]()\n3. Enter `ChangeDirectory` context [src/runtime/subthread_python_executor.py:35]()\n4. Enter `Change_STDOUT_STDERR` context [src/runtime/subthread_python_executor.py:36]()\n5. Execute code with `exec(command, _globals, _locals)` [src/runtime/subthread_python_executor.py:37]()\n\n**Sources:** [src/runtime/subprocess_python_executor.py:27-50](), [src/runtime/subprocess_python_executor.py:114-116](), [src/runtime/subthread_python_executor.py:9](), [src/runtime/subthread_python_executor.py:35-37](), [src/runtime/subthread_python_executor.py:97-100]()\n\n---\n\n## Integration with Executors\n\n### Directory and Environment Flow\n\nThe following diagram shows how directory and environment management integrates with the execution flow:\n\n```mermaid\nsequenceDiagram\n    participant Tool as ExecutePythonCodeTool\n    participant Exec as run_structured_in_*\n    participant Env as Environment Setup\n    participant FS as File System\n    participant Code as exec(command)\n    \n    Tool->>Exec: Execute code snippet\n    \n    alt Subprocess Mode\n        Exec->>Env: create_cwd('./wsm/2/g7-2')\n        Env->>FS: Create/verify directory\n        Env->>FS: os.chdir(directory)\n        Exec->>Env: Create _PipeWriter\n        Env->>Code: sys.stdout = _PipeWriter\n    else Subthread Mode\n        Exec->>Env: Import plt_back_chinese\n        Env->>Env: matplotlib.use('Agg')\n        Exec->>Env: ChangeDirectory.__enter__()\n        Env->>FS: os.chdir(target_dir)\n        Exec->>Env: Change_STDOUT_STDERR.__enter__()\n        Env->>Code: sys.stdout = _BufferWriter\n    end\n    \n    Exec->>Code: exec(command, globals, locals)\n    Code-->>Env: Output generated\n    Env-->>Exec: Captured output\n    \n    alt Subprocess Mode\n        Exec->>Env: Close pipes\n    else Subthread Mode\n        Exec->>Env: Change_STDOUT_STDERR.__exit__()\n        Env->>Env: Restore sys.stdout/stderr\n        Exec->>Env: ChangeDirectory.__exit__()\n        Env->>FS: os.chdir(original_dir)\n    end\n    \n    Exec-->>Tool: ExecutionResult with output\n```\n\n**Sources:** [src/runtime/subprocess_python_executor.py:76-163](), [src/runtime/subthread_python_executor.py:87-128]()\n\n### Output Capture Comparison\n\n| Aspect | Subprocess | Subthread |\n|--------|-----------|-----------|\n| **Directory Setup** | `create_cwd()` function | `ChangeDirectory` context manager |\n| **Stdout Redirect** | `_PipeWriter` class | `_BufferWriter` class |\n| **Communication** | Pipe-based IPC | Shared memory list |\n| **Restoration** | Not needed (isolated process) | Automatic via `__exit__` |\n| **Matplotlib Setup** | Not needed (separate process) | `plt_back_chinese` module required |\n\n**Sources:** [src/runtime/subprocess_python_executor.py:30-48](), [src/runtime/subthread_python_executor.py:23-37]()\n\n---\n\n## Example Usage Patterns\n\n### Pattern 1: Subprocess Execution with Directory Isolation\n\nFrom [src/runtime/subprocess_python_executor.py:165-204]():\n\n```python\ntest_code = \"\"\"\nimport os\nprint(os.getcwd())\n\"\"\"\nresult = run_structured_in_subprocess(test_code, {}, timeout=600)\n```\n\nThe working directory will be `./wsm/2/g7-2` as set in [src/runtime/subprocess_python_executor.py:28]().\n\n### Pattern 2: Subthread Execution with Context Managers\n\nFrom [src/runtime/subthread_python_executor.py:171-183]():\n\n```python\nplt_code = \"\"\"\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(4, 3))\nplt.title(\"æµè¯ä¸­ææ¾ç¤º\")\nplt.plot([1, 2, 3], [1, 4, 9])\nplt.savefig(\"test_output.png\")\nplt.close()\nprint(\"å­çº¿ç¨ç»å¾å®æå¹¶æåä¿å­ test_output.png\")\n\"\"\"\nresult = run_structured_in_thread(plt_code, {}, timeout=10)\n```\n\nThis uses matplotlib with proper backend setup from `plt_back_chinese` and directory isolation via `ChangeDirectory`.\n\n### Pattern 3: Direct Directory Setup in Application Code\n\nFrom [tests/playground/gen/g9/g9.py:1-3]():\n\n```python\nfrom src.runtime import cwd\nimport matplotlib.pyplot as plt\ncwd.create_cwd('./tests/playground/gen/g9')\n```\n\nThis pattern is used in test scripts that need consistent working directories for file I/O operations.\n\n**Sources:** [src/runtime/subprocess_python_executor.py:165-204](), [src/runtime/subthread_python_executor.py:171-183](), [tests/playground/gen/g9/g9.py:1-3]()\n\n---\n\n## Summary\n\nThe working directory and environment management system provides:\n\n1. **Directory Isolation**: Via `create_cwd()` for subprocesses and `ChangeDirectory` for subthreads\n2. **Output Capture**: Via `_PipeWriter` for subprocesses and `_BufferWriter` for subthreads\n3. **Stream Management**: Via `Change_STDOUT_STDERR` context manager for automatic restoration\n4. **Environment Configuration**: Via `plt_back_chinese` for matplotlib in subthreads\n5. **Error Recovery**: Context managers ensure cleanup even on exceptions\n\nThese utilities work together to create isolated, reproducible execution environments that capture all output while maintaining system stability.\n\n**Sources:** [src/runtime/cwd.py](), [src/runtime/subprocess_python_executor.py](), [src/runtime/subthread_python_executor.py](), [src/runtime/before_thread/plt_back_chinese.py]()\n\n---\n\n# Page: Observability and Logging\n\n# Observability and Logging\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/utils/__pycache__/log_decorator.cpython-312.pyc](src/utils/__pycache__/log_decorator.cpython-312.pyc)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document describes the comprehensive logging and observability infrastructure in the algo_agent system. The logging system provides automatic instrumentation of function calls, execution timing, parameter tracking, exception handling, and multi-level logging capabilities. This infrastructure enables debugging, performance analysis, error detection, and system behavior reconstruction.\n\nFor information about the agent execution flow that produces these logs, see [Agent Orchestration](#3). For details about execution runtime error handling, see [Execution Runtime](#5).\n\n---\n\n## System Overview\n\nThe logging system is built around a decorator-based architecture that automatically instruments functions throughout the codebase. The system captures function calls, parameters, return values, execution times, and exceptions with minimal code intrusion. Multiple log files segregate information by purpose, and all logs use UTF-8 encoding with automatic directory management.\n\n**Key Capabilities:**\n- Automatic function call instrumentation via decorators\n- Multi-level logging hierarchy (all â trace â print)\n- Execution timing and performance tracking\n- Full stack trace capture on exceptions\n- Parameter and return value formatting\n- Rotating file handlers with automatic backup management\n\nSources: [src/utils/log_decorator.py:1-322]()\n\n---\n\n## Logging Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Application Layer\"\n        AGENT[\"user_query()<br/>deep_research.py\"]\n        TOOLS[\"Tool Functions<br/>python_tool.py<br/>todo_tool.py\"]\n        RUNTIME[\"Execution Functions<br/>subprocess_python_executor.py\"]\n        LLM[\"LLM Functions<br/>llm.py\"]\n    end\n    \n    subgraph \"Decorator Layer\"\n        TRACEABLE[\"@traceable<br/>Detailed function tracing\"]\n        LOGGER[\"global_logger<br/>User-facing logs\"]\n    end\n    \n    subgraph \"Logger Configuration\"\n        SETUP[\"setup_logger()<br/>- Creates logger instances<br/>- Configures handlers<br/>- Sets formatters\"]\n        LOGFUNC[\"log_function()<br/>Decorator factory<br/>- Captures args/kwargs<br/>- Times execution<br/>- Handles exceptions\"]\n    end\n    \n    subgraph \"Handler Layer\"\n        CONSOLE[\"ConsoleHandler<br/>stdout output\"]\n        FILE[\"RotatingFileHandler<br/>10MB max, 5 backups\"]\n    end\n    \n    subgraph \"Log Files\"\n        ALL[\"logs/all.log<br/>All system logs\"]\n        TRACE[\"logs/trace.log<br/>Detailed traces\"]\n        PRINT[\"logs/print.log<br/>User operations\"]\n    end\n    \n    AGENT --> TRACEABLE\n    AGENT --> LOGGER\n    TOOLS --> TRACEABLE\n    RUNTIME --> TRACEABLE\n    LLM --> TRACEABLE\n    \n    TRACEABLE --> LOGFUNC\n    LOGGER --> SETUP\n    LOGFUNC --> SETUP\n    \n    SETUP --> CONSOLE\n    SETUP --> FILE\n    \n    FILE --> ALL\n    FILE --> TRACE\n    FILE --> PRINT\n    \n    CONSOLE -.-> Terminal\n    \n    style LOGFUNC fill:#f9f9f9\n    style SETUP fill:#f9f9f9\n    style ALL fill:#e8f4f8\n    style TRACE fill:#e8f4f8\n    style PRINT fill:#e8f4f8\n```\n\n**Architecture Components:**\n\n| Component | File | Line Range | Purpose |\n|-----------|------|------------|---------|\n| `setup_logger()` | log_decorator.py | 19-63 | Creates configured logger instances |\n| `log_function()` | log_decorator.py | 153-261 | Decorator factory for function instrumentation |\n| `format_value()` | log_decorator.py | 68-86 | Formats complex objects for logging |\n| `all_logger` | log_decorator.py | 292-294 | Root logger instance |\n| `traceable` | log_decorator.py | 296-302 | Decorator for detailed function tracing |\n| `global_logger` | log_decorator.py | 304-306 | Logger for user-facing operations |\n\nSources: [src/utils/log_decorator.py:1-322]()\n\n---\n\n## Logger Configuration and Instances\n\n### Logger Setup Function\n\nThe `setup_logger()` function creates and configures logger instances with both console and file handlers:\n\n```mermaid\ngraph LR\n    INPUT[\"setup_logger()<br/>parameters\"]\n    \n    subgraph \"Configuration\"\n        CHECK[\"Check if logger exists\"]\n        CREATE[\"Create Logger instance\"]\n        LEVEL[\"Set log level\"]\n        FMT[\"Create Formatter\"]\n    end\n    \n    subgraph \"Handlers\"\n        CONSOLE[\"StreamHandler<br/>Console output\"]\n        FILE[\"RotatingFileHandler<br/>10MB, 5 backups\"]\n    end\n    \n    subgraph \"Setup\"\n        MKDIR[\"Create log directory<br/>if not exists\"]\n        ATTACH[\"Attach handlers\"]\n    end\n    \n    OUTPUT[\"Configured Logger\"]\n    \n    INPUT --> CHECK\n    CHECK -->|New| CREATE\n    CHECK -->|Exists| OUTPUT\n    CREATE --> LEVEL\n    LEVEL --> FMT\n    FMT --> CONSOLE\n    FMT --> FILE\n    FILE --> MKDIR\n    CONSOLE --> ATTACH\n    MKDIR --> ATTACH\n    ATTACH --> OUTPUT\n```\n\n**Key Features:**\n\n| Feature | Implementation | Location |\n|---------|---------------|----------|\n| Automatic directory creation | `os.makedirs(log_dir, exist_ok=True)` | [log_decorator.py:24]() |\n| Rotating file handler | 10MB max, 5 backup files | [log_decorator.py:54-59]() |\n| UTF-8 encoding | All file handlers use UTF-8 | [log_decorator.py:58]() |\n| Logger reuse | Checks `logging.root.manager.loggerDict` | [log_decorator.py:29-30]() |\n| Format | `[%(asctime)s]  %(message)s` | [log_decorator.py:42]() |\n\nSources: [src/utils/log_decorator.py:19-63]()\n\n### Pre-configured Logger Instances\n\nThree logger instances are pre-configured for different purposes:\n\n```mermaid\ngraph TB\n    subgraph \"Logger Hierarchy\"\n        ROOT[\"all_logger<br/>logger_name: 'root.all'<br/>file: logs/all.log\"]\n        TRACE[\"traceable_logger<br/>logger_name: 'root.all.trace'<br/>file: logs/trace.log\"]\n        GLOBAL[\"global_logger<br/>logger_name: 'root.all.print'<br/>file: logs/print.log\"]\n    end\n    \n    subgraph \"Usage Patterns\"\n        DECORATORS[\"@traceable<br/>Function instrumentation\"]\n        MANUAL[\"global_logger.info()<br/>Manual logging calls\"]\n    end\n    \n    subgraph \"Configuration\"\n        ALLLEVEL[\"level: DEBUG\"]\n        TRACELEVEL[\"level: DEBUG\"]\n        GLOBALLEVEL[\"level: DEBUG\"]\n    end\n    \n    ROOT --> TRACE\n    ROOT --> GLOBAL\n    \n    TRACE --> DECORATORS\n    GLOBAL --> MANUAL\n    \n    DECORATORS --> TRACELEVEL\n    MANUAL --> GLOBALLEVEL\n```\n\n**Logger Configurations:**\n\n| Logger | Variable | Logger Name | Log File | Purpose |\n|--------|----------|-------------|----------|---------|\n| All Logger | `all_logger` | `root.all` | `logs/all.log` | Comprehensive system-wide logs |\n| Traceable | `traceable` | `root.all.trace` | `logs/trace.log` | Detailed function execution traces |\n| Global | `global_logger` | `root.all.print` | `logs/print.log` | User-facing operation logs |\n\n**Usage Examples:**\n\n```python\n# Using the traceable decorator\n@traceable\ndef my_function(param1, param2):\n    return result\n\n# Using global_logger for manual logging\nglobal_logger.info(f\"User input: {user_query}\")\n```\n\nSources: [src/utils/log_decorator.py:288-306](), [src/agent/deep_research.py:4]()\n\n---\n\n## The log_function Decorator\n\nThe `log_function` decorator provides automatic instrumentation of functions with comprehensive logging capabilities:\n\n```mermaid\nsequenceDiagram\n    participant Caller\n    participant Decorator as @log_function\n    participant Logger\n    participant Function\n    \n    Caller->>Decorator: Call decorated function\n    Decorator->>Decorator: Extract function metadata<br/>(name, module, class, line)\n    Decorator->>Decorator: Format args/kwargs\n    Decorator->>Logger: Log [è°ç¨å¼å§]<br/>params, timestamp\n    \n    alt record_stack=True\n        Decorator->>Logger: Log [è°ç¨æ ]<br/>filtered stack trace\n    end\n    \n    Decorator->>Function: Execute function(*args, **kwargs)\n    \n    alt Success\n        Function-->>Decorator: Return result\n        Decorator->>Decorator: Calculate elapsed time\n        Decorator->>Decorator: Format result\n        Decorator->>Logger: Log [è°ç¨æå]<br/>time, result\n        Decorator-->>Caller: Return result\n    else Exception\n        Function-->>Decorator: Raise exception\n        Decorator->>Decorator: Capture traceback\n        Decorator->>Logger: Log [è°ç¨å¤±è´¥]<br/>exception details, stack\n        Decorator->>Decorator: Re-raise exception\n        Decorator-->>Caller: Exception propagates\n    end\n```\n\n**Decorator Parameters:**\n\n| Parameter | Type | Default | Purpose |\n|-----------|------|---------|---------|\n| `logger_name` | str | Required | Name for logger instance |\n| `log_file` | Optional[str] | None | Path to log file |\n| `level` | int | `logging.DEBUG` | Logging level |\n| `exclude_args` | Optional[list] | None | Argument names to exclude (e.g., passwords) |\n| `record_stack` | bool | True | Whether to record call stack |\n| `default_return_value` | Any | None | Fallback return value on exception |\n\n**Logged Information:**\n\n1. **Call Start** (`[è°ç¨å¼å§]`):\n   - Stack path: `module.class.function`\n   - Start timestamp\n   - Positional arguments (formatted)\n   - Keyword arguments (formatted)\n\n2. **Call Stack** (`[è°ç¨æ ]`):\n   - Filtered to project files only\n   - Format: `filename:lineno function_name`\n   - Excludes decorator wrapper frames\n\n3. **Call Success** (`[è°ç¨æå]`):\n   - Stack path\n   - Execution time in milliseconds\n   - Formatted return value\n\n4. **Call Failure** (`[è°ç¨å¤±è´¥]`):\n   - Stack path\n   - Execution time\n   - Exception type and message\n   - Full traceback\n   - Exception location\n\nSources: [src/utils/log_decorator.py:153-261]()\n\n### Value Formatting\n\nThe `format_value()` function handles complex object serialization:\n\n```mermaid\ngraph TB\n    INPUT[\"Value to format\"]\n    \n    PFORMAT[\"Try pprint.pformat()\"]\n    \n    HASDICT{\"Has __dict__<br/>attribute?\"}\n    COLLECTION{\"Is set<br/>or tuple?\"}\n    FALLBACK[\"Convert to string<br/>Truncate if > 500 chars\"]\n    \n    OUTPUT[\"Formatted string\"]\n    \n    INPUT --> PFORMAT\n    PFORMAT -->|Success| OUTPUT\n    PFORMAT -->|TypeError/ValueError| HASDICT\n    \n    HASDICT -->|Yes| OBJDICT[\"Format as:<br/>[ClassName] {...}\"]\n    HASDICT -->|No| COLLECTION\n    \n    COLLECTION -->|Yes| COLLSTR[\"Format as:<br/>type([items...])\"]\n    COLLECTION -->|No| FALLBACK\n    \n    OBJDICT --> OUTPUT\n    COLLSTR --> OUTPUT\n    FALLBACK --> OUTPUT\n```\n\n**Formatting Rules:**\n\n| Value Type | Format Strategy | Truncation |\n|------------|----------------|------------|\n| Standard types | `pprint.pformat()` | None |\n| Objects with `__dict__` | `[ClassName] {first 10 items}...` | 10 items |\n| Sets/Tuples | `type([first 20 items])...` | 20 items |\n| Other types | `str(value)` | 500 chars |\n\nSources: [src/utils/log_decorator.py:68-86]()\n\n---\n\n## Log Files and Content Structure\n\n### Log File Organization\n\n```mermaid\ngraph TB\n    subgraph \"logs/ Directory\"\n        subgraph \"Timestamp Subdirectory\"\n            ALL[\"all.log<br/>All logs from all loggers\"]\n            TRACE[\"trace.log<br/>@traceable decorated functions\"]\n            PRINT[\"print.log<br/>global_logger manual logs\"]\n        end\n        \n        BACKUP1[\"all.log.1<br/>Backup (oldest)\"]\n        BACKUP2[\"all.log.2\"]\n        BACKUP3[\"all.log.3\"]\n        BACKUP4[\"all.log.4\"]\n        BACKUP5[\"all.log.5<br/>Backup (newest)\"]\n    end\n    \n    subgraph \"Content Flow\"\n        DECORATED[\"Decorated functions<br/>@traceable\"]\n        MANUAL[\"Manual logging<br/>global_logger.info()\"]\n    end\n    \n    DECORATED --> TRACE\n    DECORATED --> ALL\n    MANUAL --> PRINT\n    MANUAL --> ALL\n    \n    ALL -.->|Rotation<br/>10MB threshold| BACKUP1\n```\n\n**Log File Characteristics:**\n\n| Log File | Purpose | Typical Contents | Size Limit |\n|----------|---------|------------------|------------|\n| `all.log` | Comprehensive logs | All logged events system-wide | 10MB |\n| `trace.log` | Function traces | Decorated function calls with timing | 10MB |\n| `print.log` | User operations | User queries, tool outputs, final answers | 10MB |\n| `*.log.N` | Backups | Rotated log files (N=1 is oldest) | 10MB each |\n\nSources: [src/utils/log_decorator.py:288-306](), [logs/utils.log:1-296](), [logs/global.log:1-907]()\n\n### Log Entry Format\n\nEach log entry follows a structured format:\n\n```\n[Timestamp] [Event Type] Details\n```\n\n**Example Log Entries:**\n\n```\n[2025-11-25 17:51:32,252]  ãè°ç¨å¼å§ã æ è·¯å¾ï¼ __main__.None.user_query | \n    å¼å§æ¶é´ï¼ 2025-11-25 17:51:32.252333 | \n    ä½ç½®åæ°ï¼ ('user input text...',) | \n    å³é®å­åæ°ï¼ {}\n\n[2025-11-25 17:51:32,383]  ãè°ç¨æ ã \n          D:\\path\\to\\file.py:202 wrapper\n          d:\\path\\to\\other.py:110 <module>\n\n[2025-11-25 17:52:14,089]  ãè°ç¨æåã æ è·¯å¾ï¼ llm.None.generate_assistant_output_append | \n    èæ¶ï¼ 41680.934ms | \n    è¿åå¼ï¼ ChatCompletionMessage(...)\n\n[2025-11-26 03:39:11,797]  å·¥å·è¾åºä¿¡æ¯ï¼ ## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\nProcess Process-1:\nTraceback (most recent call last):\n  File \"subprocess_python_executor.py\", line 44, in _worker_with_pipe\n    exec(command, _globals, _locals)\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265\n```\n\nSources: [logs/global.log:1-907](), [logs/utils.log:1-296]()\n\n---\n\n## Usage Patterns in the Codebase\n\n### Agent Layer Usage\n\nThe agent uses both `@traceable` decorators and manual `global_logger` calls:\n\n```mermaid\ngraph TB\n    subgraph \"deep_research.py\"\n        USER[\"user_query()<br/>Decorated with @traceable\"]\n        \n        USERINPUT[\"global_logger.info()<br/>'ç¨æ·è¾å¥ï¼'\"]\n        TOOLOUT[\"global_logger.info()<br/>'å·¥å·è¾åºä¿¡æ¯ï¼'\"]\n        FINALANS[\"global_logger.info()<br/>'æç»ç­æ¡ï¼'\"]\n        \n        USER --> USERINPUT\n        USER --> TOOLOUT\n        USER --> FINALANS\n    end\n    \n    subgraph \"Log Output\"\n        TRACE_START[\"[è°ç¨å¼å§] user_query<br/>with parameters\"]\n        TRACE_STACK[\"[è°ç¨æ ] filtered stack\"]\n        USER_MSG[\"User input message\"]\n        TOOL_MSG[\"Tool execution results\"]\n        FINAL_MSG[\"Final answer\"]\n        TRACE_END[\"[è°ç¨æå] user_query<br/>with timing\"]\n    end\n    \n    USERINPUT --> USER_MSG\n    TOOLOUT --> TOOL_MSG\n    FINALANS --> FINAL_MSG\n    \n    USER -.->|@traceable| TRACE_START\n    TRACE_START --> TRACE_STACK\n    TRACE_STACK --> TRACE_END\n```\n\n**Key Logging Points:**\n\n| Location | Line | Log Type | Message |\n|----------|------|----------|---------|\n| User input | [deep_research.py:17]() | `global_logger.info()` | `ç¨æ·è¾å¥ï¼ {user_input}` |\n| Tool output | [deep_research.py:44,62]() | `global_logger.info()` | `å·¥å· [function/tool] call è¾åºä¿¡æ¯ï¼` |\n| Final answer | [deep_research.py:73]() | `global_logger.info()` | `æç»ç­æ¡ï¼ {answer}` |\n| LLM output | [deep_research.py:68-72]() | `global_logger.info()` | `ç¬¬{N}è½®å¤§æ¨¡åè¾åºä¿¡æ¯ï¼` |\n\nSources: [src/agent/deep_research.py:1-129](), [logs/utils.log:1-296]()\n\n### Function Instrumentation Pattern\n\nFunctions decorated with `@traceable` automatically generate detailed logs:\n\n```mermaid\nsequenceDiagram\n    participant Code as Application Code\n    participant Dec as @traceable Decorator\n    participant Log as trace.log\n    participant Time as Timer\n    \n    Code->>Dec: Call function(args)\n    Dec->>Time: Start timer\n    Dec->>Log: [è°ç¨å¼å§] with args\n    Dec->>Log: [è°ç¨æ ] if enabled\n    \n    alt Successful execution\n        Dec->>Code: Execute function\n        Code-->>Dec: Return value\n        Dec->>Time: Stop timer\n        Dec->>Log: [è°ç¨æå]<br/>elapsed time, return value\n        Dec-->>Code: Return value\n    else Exception occurs\n        Dec->>Code: Execute function\n        Code-->>Dec: Exception raised\n        Dec->>Time: Stop timer\n        Dec->>Log: [è°ç¨å¤±è´¥]<br/>exception type, message, traceback\n        Dec-->>Code: Re-raise exception\n    end\n```\n\n**Example Instrumented Functions:**\n\n| Function | File | Decorator | Purpose |\n|----------|------|-----------|---------|\n| `user_query()` | deep_research.py | `@traceable` | Agent query processing |\n| `init_messages_with_system_prompt()` | memory.py | `@traceable` | Message initialization |\n| `get_tools_schema()` | tool/schema.py | `@traceable` | Tool schema generation |\n| `generate_assistant_output_append()` | llm.py | `@traceable` | LLM interaction |\n| `call_tools_safely()` | action.py | `@traceable` | Tool execution |\n\nSources: [logs/global.log:1-907]()\n\n---\n\n## Debugging and Analysis Capabilities\n\n### Error Detection and Diagnosis\n\nThe logging system captures comprehensive error information for debugging:\n\n```mermaid\ngraph TB\n    subgraph \"Error Types Captured\"\n        UNICODE[\"UnicodeDecodeError<br/>File encoding issues\"]\n        PICKLE[\"PickleError<br/>Serialization failures\"]\n        TIMEOUT[\"Timeout errors<br/>Long-running code\"]\n        CRASH[\"Process crashes<br/>SegFault, etc.\"]\n        GENERIC[\"General exceptions<br/>With full traceback\"]\n    end\n    \n    subgraph \"Log Information\"\n        LOC[\"Exception location<br/>file:line\"]\n        TYPE[\"Exception type\"]\n        MSG[\"Exception message\"]\n        TRACE[\"Full traceback\"]\n        CONTEXT[\"Function parameters\"]\n        TIMING[\"Execution time before failure\"]\n    end\n    \n    subgraph \"Analysis\"\n        REPRODUCE[\"Failure reproduction<br/>Parameters logged\"]\n        PATTERN[\"Pattern detection<br/>Similar errors\"]\n        HOTSPOT[\"Hotspot identification<br/>Frequent failures\"]\n    end\n    \n    UNICODE --> LOC\n    PICKLE --> TYPE\n    TIMEOUT --> MSG\n    CRASH --> TRACE\n    GENERIC --> CONTEXT\n    \n    LOC --> REPRODUCE\n    TYPE --> PATTERN\n    MSG --> PATTERN\n    TRACE --> REPRODUCE\n    CONTEXT --> REPRODUCE\n    TIMING --> HOTSPOT\n```\n\n**Example Error Log Entry:**\n\n```\n[2025-11-26 03:39:11,797]  å·¥å·è¾åºä¿¡æ¯ï¼ ## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåº\n### ç»ç«¯è¾åºï¼\nProcess Process-1:\nTraceback (most recent call last):\n  File \"subprocess_python_executor.py\", line 44, in _worker_with_pipe\n    exec(command, _globals, _locals)\n  File \"<string>\", line 4, in <module>\n  File \"json\\__init__.py\", line 293, in load\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265: illegal multibyte sequence\n```\n\nSources: [logs/utils.log:132-180]()\n\n### Performance Analysis\n\nThe logging system provides execution timing for performance analysis:\n\n```mermaid\ngraph LR\n    subgraph \"Logged Metrics\"\n        START[\"Start timestamp<br/>ç²¾ç¡®å°å¾®ç§\"]\n        END[\"End timestamp\"]\n        ELAPSED[\"Elapsed time<br/>æ¯«ç§ç²¾åº¦\"]\n    end\n    \n    subgraph \"Analysis Types\"\n        BOTTLENECK[\"Bottleneck identification<br/>Slowest functions\"]\n        TREND[\"Performance trends<br/>Over time\"]\n        COMPARE[\"Cross-function comparison<br/>Relative performance\"]\n    end\n    \n    START --> ELAPSED\n    END --> ELAPSED\n    \n    ELAPSED --> BOTTLENECK\n    ELAPSED --> TREND\n    ELAPSED --> COMPARE\n```\n\n**Performance Log Examples:**\n\n| Function | Execution Time | Location |\n|----------|---------------|----------|\n| `init_messages_with_system_prompt()` | 2.031ms | [global.log:115]() |\n| `get_tools_schema()` | 8.008ms | [global.log:189]() |\n| `generate_chat_completion()` | 41635.670ms | [global.log:891]() |\n| `generate_assistant_output_append()` | 41680.934ms | [global.log:893]() |\n\nSources: [logs/global.log:1-907]()\n\n### Call Stack Analysis\n\nThe decorator captures filtered call stacks for debugging execution flow:\n\n```mermaid\ngraph TB\n    subgraph \"Stack Filtering\"\n        RAW[\"inspect.stack()\"]\n        FILTER[\"Filter to project files\"]\n        FORMAT[\"Format as file:line function\"]\n        EXCLUDE[\"Exclude wrapper frames\"]\n    end\n    \n    subgraph \"Stack Information\"\n        FILE[\"File path\"]\n        LINE[\"Line number\"]\n        FUNC[\"Function name\"]\n    end\n    \n    subgraph \"Use Cases\"\n        FLOW[\"Execution flow tracing\"]\n        CALLER[\"Caller identification\"]\n        DEBUG[\"Step-by-step debugging\"]\n    end\n    \n    RAW --> FILTER\n    FILTER --> EXCLUDE\n    EXCLUDE --> FORMAT\n    \n    FORMAT --> FILE\n    FORMAT --> LINE\n    FORMAT --> FUNC\n    \n    FILE --> FLOW\n    LINE --> CALLER\n    FUNC --> DEBUG\n```\n\n**Example Stack Trace:**\n\n```\n[è°ç¨æ ]\n    D:\\zyt\\git_ln\\algo_agent\\src\\utils\\log_decorator.py:202 wrapper\n    d:\\zyt\\git_ln\\algo_agent\\src\\agent\\deep_research.py:26 user_query\n    D:\\zyt\\git_ln\\algo_agent\\src\\utils\\log_decorator.py:217 wrapper\n    d:\\zyt\\git_ln\\algo_agent\\src\\agent\\deep_research.py:110 <module>\n```\n\n**Stack Filtering Logic:**\n- Only includes files within the project directory\n- Uses `os.path.normcase()` for cross-platform compatibility\n- Excludes the decorator's `wrapper` function from the trace\n- Formats as: `{filename}:{lineno} {function}`\n\nSources: [src/utils/log_decorator.py:205-217](), [logs/global.log:54-57,110-114]()\n\n---\n\n## Configuration and Customization\n\n### Log Directory Management\n\nThe system automatically manages log directories with timestamp-based organization:\n\n```mermaid\ngraph TB\n    INIT[\"create_folder.get_or_create_subfolder()\"]\n    \n    CHECK{\"Directory<br/>exists?\"}\n    CREATE[\"os.makedirs()<br/>exist_ok=True\"]\n    \n    TIMESTAMP[\"Timestamp-based<br/>subdirectory\"]\n    \n    LOGDIR[\"sub_folder_for_logs\"]\n    \n    FILES[\"Log files created<br/>in subdirectory\"]\n    \n    INIT --> TIMESTAMP\n    TIMESTAMP --> CHECK\n    CHECK -->|No| CREATE\n    CHECK -->|Yes| LOGDIR\n    CREATE --> LOGDIR\n    LOGDIR --> FILES\n```\n\n**Directory Configuration:**\n\n| Variable | Value | Line |\n|----------|-------|------|\n| `sub_folder_for_logs` | `create_folder.get_or_create_subfolder(gen_time_path_from_project=\"logs\")` | [log_decorator.py:288]() |\n| `all_logger_file_name` | `os.path.join(sub_folder_for_logs, \"all.log\")` | [log_decorator.py:292]() |\n| `traceable_logger_file_name` | `os.path.join(sub_folder_for_logs, \"trace.log\")` | [log_decorator.py:296]() |\n| `global_logger_file_name` | `os.path.join(sub_folder_for_logs, \"print.log\")` | [log_decorator.py:304]() |\n\nSources: [src/utils/log_decorator.py:288-306]()\n\n### Creating Custom Loggers\n\nTo create a custom logger with specific configuration:\n\n**Example Custom Logger Creation:**\n\n```python\n# Create a logger for a specific module\ncustom_logger = setup_logger(\n    logger_name=\"module.custom\",\n    log_file=\"logs/custom.log\",\n    level=logging.INFO\n)\n\n# Use in code\ncustom_logger.info(\"Custom log message\")\n\n# Or create a decorator for automatic instrumentation\ncustom_decorator = lambda func: log_function(\n    logger_name=\"module.custom_trace\",\n    log_file=\"logs/custom_trace.log\",\n    level=logging.DEBUG,\n    exclude_args=[\"password\", \"secret\"],\n    record_stack=True\n)(func)\n\n@custom_decorator\ndef my_function(param1, password):\n    return result\n```\n\n**Decorator Parameters:**\n\n| Parameter | Purpose | Example Value |\n|-----------|---------|---------------|\n| `logger_name` | Unique logger identifier | `\"module.subsystem\"` |\n| `log_file` | Path to log file | `\"logs/subsystem.log\"` |\n| `level` | Minimum log level | `logging.DEBUG` |\n| `exclude_args` | Args to exclude from logs | `[\"password\", \"token\"]` |\n| `record_stack` | Enable stack tracing | `True` or `False` |\n| `default_return_value` | Fallback on exception | `[]`, `None`, etc. |\n\nSources: [src/utils/log_decorator.py:153-261,296-302]()\n\n---\n\n## Advanced Features\n\n### Exception Handling with Fallback Values\n\nThe decorator can provide fallback return values when exceptions occur (currently disabled in the code):\n\n```mermaid\ngraph TB\n    FUNC[\"Execute function\"]\n    \n    EXCEPTION{\"Exception<br/>raised?\"}\n    \n    LOG[\"Log exception details:<br/>- Type<br/>- Message<br/>- Traceback<br/>- Execution time\"]\n    \n    FALLBACK[\"Get default return value:<br/>1. Manual: default_return_value<br/>2. Auto: get_default_return_value()\"]\n    \n    RAISE[\"Re-raise exception<br/>(current behavior)\"]\n    \n    RETURN[\"Return fallback value<br/>(commented out)\"]\n    \n    FUNC --> EXCEPTION\n    EXCEPTION -->|Yes| LOG\n    EXCEPTION -->|No| SUCCESS[\"Return actual result\"]\n    \n    LOG --> RAISE\n    LOG -.->|If enabled| FALLBACK\n    FALLBACK -.-> RETURN\n```\n\n**Fallback Value Inference:**\n\n| Return Type Annotation | Default Value |\n|-----------------------|---------------|\n| `int` | `0` |\n| `float` | `0.0` |\n| `str` | `\"\"` |\n| `bool` | `False` |\n| `list` or `List[T]` | `[]` |\n| `dict` or `Dict[K,V]` | `{}` |\n| `tuple` or `Tuple[...]` | `()` |\n| `set` or `Set[T]` | `set()` |\n| Custom class | `ClassName()` (if possible) |\n| No annotation | `None` |\n\nThe `get_default_return_value()` function uses type hints to infer appropriate fallback values, but the current implementation re-raises exceptions instead of returning fallbacks.\n\nSources: [src/utils/log_decorator.py:91-148,233-257]()\n\n### Parameter Formatting and Truncation\n\nThe system intelligently formats different parameter types:\n\n```mermaid\ngraph TB\n    VALUE[\"Input value\"]\n    \n    PPRINT{\"pprint.pformat()<br/>succeeds?\"}\n    \n    HASDICT{\"Has __dict__<br/>attribute?\"}\n    \n    COLLECTION{\"Is set/tuple?\"}\n    \n    SUCCESS[\"Use pformat output\"]\n    DICT[\"Format as [ClassName]<br/>{first 10 dict items}...\"]\n    COLL[\"Format as type()<br/>[first 20 items]...\"]\n    STR[\"str(value)[:500]...\"]\n    \n    VALUE --> PPRINT\n    PPRINT -->|Yes| SUCCESS\n    PPRINT -->|No| HASDICT\n    HASDICT -->|Yes| DICT\n    HASDICT -->|No| COLLECTION\n    COLLECTION -->|Yes| COLL\n    COLLECTION -->|No| STR\n```\n\n**Truncation Rules:**\n\n| Object Type | Display Strategy | Truncation Limit |\n|-------------|-----------------|------------------|\n| Standard types | Pretty-printed | None |\n| Objects | `[ClassName] {first_10_items}...` | 10 items |\n| Large sets/tuples | `type([first_20_items])...` | 20 items |\n| Long strings | `str_value[:500]...` | 500 characters |\n\nSources: [src/utils/log_decorator.py:68-86]()\n\n---\n\n## Integration with System Components\n\n### Agent Integration\n\n```mermaid\ngraph LR\n    subgraph \"Agent Functions\"\n        UQ[\"user_query()\"]\n        IM[\"init_messages<br/>_with_system_prompt()\"]\n        GTS[\"get_tools_schema()\"]\n        GAO[\"generate_assistant<br/>_output_append()\"]\n        CT[\"call_tools_safely()\"]\n    end\n    \n    subgraph \"Logging\"\n        TR[\"@traceable<br/>decorator\"]\n        GL[\"global_logger\"]\n    end\n    \n    subgraph \"Log Files\"\n        TL[\"trace.log\"]\n        PL[\"print.log\"]\n        AL[\"all.log\"]\n    end\n    \n    UQ --> TR\n    IM --> TR\n    GTS --> TR\n    GAO --> TR\n    CT --> TR\n    \n    UQ --> GL\n    \n    TR --> TL\n    TR --> AL\n    GL --> PL\n    GL --> AL\n```\n\n**Logged Agent Operations:**\n\n| Operation | Log Type | Information Captured |\n|-----------|----------|---------------------|\n| User query received | `global_logger.info()` | Full user input text |\n| Message initialization | `@traceable` | System prompt, user message |\n| Tool schema generation | `@traceable` | Tool list, generated schemas |\n| LLM interaction | `@traceable` | Messages, tools, response timing |\n| Tool execution | `@traceable` + `global_logger` | Tool name, arguments, results |\n| Final answer | `global_logger.info()` | Model's final response |\n\nSources: [src/agent/deep_research.py:15-73](), [logs/utils.log:1-296]()\n\n### Runtime Integration\n\nExecution runtime components use logging for debugging code execution:\n\n```mermaid\ngraph TB\n    subgraph \"Subprocess Execution\"\n        WORKER[\"_worker_with_pipe()\"]\n        EXEC[\"exec(command)\"]\n        ERROR[\"Exception handling\"]\n    end\n    \n    subgraph \"Logged Events\"\n        PID[\"Process PID\"]\n        WORKDIR[\"Working directory change\"]\n        EXCEPTION[\"Subprocess exceptions\"]\n        TIMEOUT[\"Timeout detection\"]\n        CRASH[\"Process crash\"]\n    end\n    \n    subgraph \"Log Output\"\n        UTILS[\"utils.log\"]\n        GLOBAL[\"global.log\"]\n    end\n    \n    WORKER --> PID\n    WORKER --> WORKDIR\n    EXEC --> EXCEPTION\n    EXEC --> TIMEOUT\n    EXEC --> CRASH\n    \n    PID --> UTILS\n    WORKDIR --> UTILS\n    EXCEPTION --> UTILS\n    TIMEOUT --> UTILS\n    CRASH --> UTILS\n    \n    UTILS --> GLOBAL\n```\n\n**Runtime Log Entries:**\n\n| Event | Format | Example |\n|-------|--------|---------|\n| Process creation | `å­è¿ç¨ PID: {pid}` | [utils.log:130]() |\n| Directory change | `å­è¿ç¨ PID: {pid} å·²å°å·¥ä½ç®å½æ´æ¹ä¸º: {path}` | [utils.log:131]() |\n| Exception exit | `---------- 2.1.2 å­è¿ç¨å¼å¸¸ç»æ` | [utils.log:132]() |\n| Pipe error | `Pipe reader error: type={type}, value={value}` | [utils.log:133]() |\n| Crash exit | `---------- 2.2 å­è¿ç¨å´©æºéåº` | [utils.log:136]() |\n| Tool output | `å·¥å·è¾åºä¿¡æ¯ï¼ ## ä»£ç æ§è¡å´©æº` | [utils.log:136-180]() |\n\nSources: [logs/utils.log:130-288]()\n\n---\n\n## Best Practices and Recommendations\n\n### When to Use Each Logger\n\n| Scenario | Logger | Rationale |\n|----------|--------|-----------|\n| Function entry/exit/timing | `@traceable` | Automatic instrumentation, detailed traces |\n| User-facing operations | `global_logger.info()` | Clean, high-level operation logs |\n| Debugging complex flows | `@traceable` with `record_stack=True` | Full call stack for diagnosis |\n| Sensitive data | Custom logger with `exclude_args` | Exclude passwords, tokens, secrets |\n| Performance monitoring | `@traceable` | Automatic timing capture |\n\n### Log Analysis Workflow\n\n```mermaid\ngraph TB\n    START[\"Issue or question\"]\n    \n    IDENTIFY[\"Identify relevant log file:<br/>- trace.log for function flows<br/>- print.log for user operations<br/>- all.log for comprehensive view\"]\n    \n    SEARCH[\"Search for key terms:<br/>- Function names<br/>- Error messages<br/>- Timestamps\"]\n    \n    TRACE[\"Follow call stack:<br/>ãè°ç¨æ ãentries\"]\n    \n    TIMING[\"Analyze timing:<br/>ãè°ç¨æåãentries\"]\n    \n    ERROR[\"Examine errors:<br/>ãè°ç¨å¤±è´¥ãentries\"]\n    \n    REPRODUCE[\"Reproduce issue:<br/>Use logged parameters\"]\n    \n    START --> IDENTIFY\n    IDENTIFY --> SEARCH\n    SEARCH --> TRACE\n    SEARCH --> TIMING\n    SEARCH --> ERROR\n    ERROR --> REPRODUCE\n    TIMING --> IDENTIFY\n```\n\n### Performance Considerations\n\n| Aspect | Impact | Mitigation |\n|--------|--------|-----------|\n| File I/O overhead | Minimal due to buffering | Use rotating handlers to limit file size |\n| Formatting overhead | Low for simple types | Complex objects are truncated automatically |\n| Stack trace collection | Moderate overhead | Disable with `record_stack=False` if not needed |\n| Log file growth | Can become large over time | Automatic rotation (10MB max, 5 backups) |\n| Parameter serialization | Can slow down for large objects | Truncation limits (500 chars, 10 items, etc.) |\n\nSources: [src/utils/log_decorator.py:19-322]()\n\n---\n\n# Page: Logging System Architecture\n\n# Logging System Architecture\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/utils/__pycache__/log_decorator.cpython-312.pyc](src/utils/__pycache__/log_decorator.cpython-312.pyc)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document describes the logging system architecture in the algo_agent codebase, focusing on the core logging infrastructure, decorator-based instrumentation, and logger hierarchy. The logging system provides comprehensive observability across all components through automated function tracing, parameter capture, execution timing, and error tracking.\n\nFor information about the actual log files and how to use them for debugging, see [Log Files and Analysis](#6.2). For performance monitoring and bottleneck identification, see [Tracing and Performance Monitoring](#6.3).\n\n**Sources:** [src/utils/log_decorator.py:1-322]()\n\n---\n\n## Architecture Overview\n\nThe logging system is built around three core components: a flexible logger setup function, a powerful decorator for automatic instrumentation, and a hierarchy of specialized logger instances.\n\n### System Components Diagram\n\n```mermaid\ngraph TB\n    subgraph \"Logger Setup Layer\"\n        setup_logger[\"setup_logger()<br/>Configuration Function\"]\n        formatter[\"LogFormatter<br/>[asctime] message\"]\n        console[\"ConsoleHandler\"]\n        file[\"RotatingFileHandler<br/>10MB max, 5 backups\"]\n    end\n    \n    subgraph \"Decorator Layer\"\n        log_function[\"log_function()<br/>Core Decorator Factory\"]\n        traceable_dec[\"traceable<br/>Lambda Decorator\"]\n        wrapper[\"wrapper()<br/>Instrumented Function\"]\n    end\n    \n    subgraph \"Logger Instance Hierarchy\"\n        all_logger[\"all_logger<br/>root.all<br/>logs/all.log\"]\n        traceable_logger[\"traceable<br/>root.all.trace<br/>logs/trace.log\"]\n        global_logger[\"global_logger<br/>root.all.print<br/>logs/print.log\"]\n    end\n    \n    subgraph \"Application Code\"\n        agent[\"deep_research.py<br/>Uses global_logger\"]\n        tools[\"Tool Functions<br/>Uses traceable decorator\"]\n    end\n    \n    setup_logger --> formatter\n    setup_logger --> console\n    setup_logger --> file\n    \n    log_function --> wrapper\n    traceable_dec --> log_function\n    \n    setup_logger --> all_logger\n    setup_logger --> traceable_logger\n    setup_logger --> global_logger\n    \n    traceable_dec --> traceable_logger\n    \n    agent --> global_logger\n    tools --> traceable_dec\n    \n    all_logger -.parent.-> traceable_logger\n    all_logger -.parent.-> global_logger\n```\n\n**Sources:** [src/utils/log_decorator.py:19-63](), [src/utils/log_decorator.py:288-306]()\n\n---\n\n## Logger Setup and Configuration\n\n### The `setup_logger` Function\n\nThe `setup_logger` function creates and configures logger instances with both console and file handlers. It implements automatic log directory creation and rotating file handlers to prevent disk space exhaustion.\n\n**Function Signature:**\n\n```python\ndef setup_logger(\n    logger_name: str,\n    log_file: Optional[str] = None,\n    level: int = logging.DEBUG\n) -> logging.Logger\n```\n\n**Key Features:**\n\n| Feature | Implementation | Purpose |\n|---------|---------------|---------|\n| **Automatic Directory Creation** | `os.makedirs(log_dir, exist_ok=True)` | Creates log directories if missing |\n| **Singleton Pattern** | Checks `logging.root.manager.loggerDict` | Reuses existing loggers |\n| **Rotating File Handler** | `maxBytes=10*1024*1024`, `backupCount=5` | Prevents disk exhaustion |\n| **UTF-8 Encoding** | `encoding=\"utf-8\"` | Handles international characters |\n| **Log Propagation** | `logger.propagate = True` | Enables hierarchical logging |\n\n**Sources:** [src/utils/log_decorator.py:19-63]()\n\n### Logger Format Configuration\n\nThe system uses a simplified format for readability:\n\n```\n[%(asctime)s]  %(message)s\n```\n\nThis format was chosen after iterating through more verbose formats. The simplified version focuses on timestamp and message, with contextual information added by the decorator rather than the formatter.\n\n**Sources:** [src/utils/log_decorator.py:36-46]()\n\n### Log Directory Management\n\nLog files are stored in timestamped subdirectories to organize execution sessions:\n\n```python\nsub_folder_for_logs = create_folder.get_or_create_subfolder(\n    gen_time_path_from_project=\"logs\"\n)\n```\n\nThis creates a structure like `logs/2024-01-15_14-30-45/` for each run, preventing log file collisions across multiple executions.\n\n**Sources:** [src/utils/log_decorator.py:288-289]()\n\n---\n\n## The `log_function` Decorator\n\nThe `log_function` decorator is the core instrumentation mechanism that automatically logs function calls, parameters, execution time, results, and errors.\n\n### Decorator Architecture Diagram\n\n```mermaid\nsequenceDiagram\n    participant App as \"Application Code\"\n    participant Wrapper as \"wrapper()<br/>Decorator Logic\"\n    participant Logger as \"Logger Instance\"\n    participant OrigFunc as \"Original Function\"\n    participant Utils as \"Utility Functions\"\n    \n    App->>Wrapper: Call decorated function(args, kwargs)\n    \n    Wrapper->>Wrapper: Extract function metadata<br/>(name, module, class, line)\n    Wrapper->>Utils: format_value(args)\n    Wrapper->>Utils: format_value(kwargs)\n    Utils-->>Wrapper: Formatted parameters\n    \n    Wrapper->>Logger: Log \"ãè°ç¨å¼å§ã\"<br/>(stack path, time, params)\n    \n    alt record_stack=True\n        Wrapper->>Wrapper: inspect.stack()\n        Wrapper->>Logger: Log \"ãè°ç¨æ ã\"<br/>(filtered stack trace)\n    end\n    \n    Wrapper->>OrigFunc: Execute func(args, kwargs)\n    \n    alt Success\n        OrigFunc-->>Wrapper: result\n        Wrapper->>Utils: format_value(result)\n        Utils-->>Wrapper: Formatted result\n        Wrapper->>Logger: Log \"ãè°ç¨æåã\"<br/>(elapsed time, result)\n        Wrapper-->>App: Return result\n    else Exception\n        OrigFunc-->>Wrapper: Exception\n        Wrapper->>Wrapper: Extract traceback\n        Wrapper->>Logger: Log \"ãè°ç¨å¤±è´¥ã\"<br/>(elapsed time, exception, traceback)\n        Wrapper-->>App: Re-raise exception\n    end\n```\n\n**Sources:** [src/utils/log_decorator.py:153-261]()\n\n### Decorator Parameters\n\n| Parameter | Type | Default | Purpose |\n|-----------|------|---------|---------|\n| `logger_name` | `str` | Required | Logger instance name |\n| `log_file` | `Optional[str]` | `None` | File path for logs |\n| `level` | `int` | `logging.DEBUG` | Minimum log level |\n| `exclude_args` | `Optional[list]` | `None` | Parameter names to exclude from logs |\n| `record_stack` | `bool` | `True` | Whether to record call stack |\n| `default_return_value` | `Any` | `None` | Fallback value on exception (currently disabled) |\n\n**Sources:** [src/utils/log_decorator.py:153-161]()\n\n### Function Metadata Extraction\n\nThe decorator extracts comprehensive metadata about each function call:\n\n```python\nfunc_name = func.__name__\nmodule_name = inspect.getmodule(func).__name__\nlineno = inspect.getsourcelines(func)[1]\nfile_path = inspect.getfile(func)\n```\n\nFor methods, it also extracts the class name:\n\n```python\nif args:\n    first_arg = args[0]\n    if hasattr(first_arg, func_name) and not inspect.isfunction(first_arg):\n        class_name = first_arg.__class__.__name__\n    elif inspect.isclass(first_arg) and func_name in dir(first_arg):\n        class_name = first_arg.__name__\n```\n\nThis creates a full stack path like `module_name.class_name.func_name` for precise tracking.\n\n**Sources:** [src/utils/log_decorator.py:167-182]()\n\n### Parameter Formatting\n\nThe `format_value` function handles complex Python objects gracefully:\n\n**Formatting Strategy:**\n\n```mermaid\ngraph TD\n    input[\"Input Value\"]\n    \n    pformat[\"Try pprint.pformat()\"]\n    has_dict[\"Has __dict__?\"]\n    is_collection[\"Is set/tuple?\"]\n    fallback[\"str(value)\"]\n    \n    input --> pformat\n    pformat -->|Success| output1[\"Pretty-printed string\"]\n    pformat -->|TypeError/ValueError| has_dict\n    \n    has_dict -->|Yes| dict_format[\"Format first 10 attributes<br/>as JSON\"]\n    has_dict -->|No| is_collection\n    \n    is_collection -->|Yes| coll_format[\"Format first 20 items\"]\n    is_collection -->|No| fallback\n    \n    fallback --> truncate[\"Truncate to 500 chars<br/>if longer\"]\n    \n    dict_format --> output2[\"Limited object repr\"]\n    coll_format --> output3[\"Limited collection\"]\n    truncate --> output4[\"Truncated string\"]\n```\n\n**Sources:** [src/utils/log_decorator.py:68-86]()\n\n### Call Stack Recording\n\nWhen `record_stack=True`, the decorator captures and filters the call stack:\n\n```python\ndef is_in_project(file_path):\n    project_root = os.path.abspath(os.getcwd())\n    stack_abs_path = os.path.abspath(file_path)\n    return os.path.normcase(stack_abs_path).startswith(\n        os.path.normcase(project_root)\n    )\n\nstack_str = \"\\n\".join([\n    f\"          {frame.filename}:{frame.lineno} {frame.function}\" \n    for frame in stack_info \n    if is_in_project(frame.filename) and not frame.function == \"wrapper\"\n])\n```\n\nThis filters out standard library frames and focuses on project-specific code, making stack traces more relevant.\n\n**Sources:** [src/utils/log_decorator.py:205-217]()\n\n### Error Handling and Tracing\n\nOn exception, the decorator captures comprehensive error information:\n\n```python\nexception_type = type(e).__name__\nexception_msg = str(e)\nexc_lineno = inspect.trace()[-1][2] if inspect.trace() else lineno\ntraceback_str = traceback.format_exc()\n\nlogger.error(\n    f\"ãè°ç¨å¤±è´¥ã æ è·¯å¾ï¼ {stack_full_path} | èæ¶ï¼ {elapsed_time:.3f}ms \"\n    f\"| å¼å¸¸ä½ç½®ï¼ {module_name}.{class_name}.{func_name}:{exc_lineno} \"\n    f\"| å¼å¸¸ç±»åï¼ {exception_type} | å¼å¸¸ä¿¡æ¯ï¼ {exception_msg} \"\n    f\"| å æ ä¿¡æ¯ï¼ {traceback_str}\",\n    exc_info=True\n)\n\nraise  # Re-raise to maintain normal exception flow\n```\n\nThe decorator re-raises exceptions after logging, ensuring the application behaves normally while capturing diagnostic information.\n\n**Sources:** [src/utils/log_decorator.py:233-257]()\n\n---\n\n## Logger Instance Hierarchy\n\nThe system defines three specialized logger instances with a hierarchical naming scheme that enables log propagation.\n\n### Logger Hierarchy Diagram\n\n```mermaid\ngraph TB\n    subgraph \"Logger Hierarchy\"\n        root[\"root logger<br/>(Python built-in)\"]\n        \n        all[\"all_logger<br/>Logger Name: root.all<br/>File: logs/{timestamp}/all.log<br/>Level: DEBUG\"]\n        \n        trace[\"traceable decorator<br/>Logger Name: root.all.trace<br/>File: logs/{timestamp}/trace.log<br/>Level: DEBUG<br/>Excludes: password, token, secret\"]\n        \n        print[\"global_logger<br/>Logger Name: root.all.print<br/>File: logs/{timestamp}/print.log<br/>Level: DEBUG\"]\n    end\n    \n    subgraph \"Usage Patterns\"\n        manual[\"Manual Logging<br/>global_logger.info()\"]\n        decorated[\"Decorated Functions<br/>@traceable\"]\n    end\n    \n    root -.propagation.-> all\n    all -.propagation.-> trace\n    all -.propagation.-> print\n    \n    manual --> print\n    decorated --> trace\n    \n    trace -.also writes to.-> all\n    print -.also writes to.-> all\n```\n\n**Sources:** [src/utils/log_decorator.py:292-306]()\n\n### `all_logger` - Comprehensive Logging\n\nThe root logger for the entire system:\n\n```python\nall_logger_file_name = os.path.join(sub_folder_for_logs, \"all.log\")\nall_logger = setup_logger(\n    logger_name=\"root.all\",\n    log_file=all_logger_file_name,\n    level=logging.DEBUG\n)\n```\n\n**Purpose:** Receives all log messages from child loggers through propagation, providing a complete system log.\n\n**Sources:** [src/utils/log_decorator.py:292-294]()\n\n### `traceable` - Detailed Function Tracing\n\nA lambda-wrapped decorator for comprehensive function instrumentation:\n\n```python\ntraceable_logger_file_name = os.path.join(sub_folder_for_logs, \"trace.log\")\ntraceable = lambda func: log_function(\n    logger_name=\"root.all.trace\",\n    log_file=traceable_logger_file_name,\n    exclude_args=[\"password\", \"token\", \"secret\"],\n    level=logging.DEBUG\n)(func)\n```\n\n**Key Features:**\n- Records full call stacks\n- Excludes sensitive parameters (`password`, `token`, `secret`)\n- Captures timing and exceptions\n- Used for tools and executors\n\n**Sources:** [src/utils/log_decorator.py:296-302]()\n\n### `global_logger` - User Operation Logging\n\nThe logger used for manual application-level logging:\n\n```python\nglobal_logger_file_name = os.path.join(sub_folder_for_logs, \"print.log\")\nglobal_logger = setup_logger(\n    logger_name=\"root.all.print\",\n    log_file=global_logger_file_name,\n    level=logging.DEBUG\n)\n```\n\n**Purpose:** Used in the agent for logging user queries, tool outputs, and final answers. Does not use the decorator; requires manual `logger.info()` calls.\n\n**Sources:** [src/utils/log_decorator.py:304-306]()\n\n---\n\n## Integration with Application Code\n\n### Agent Integration\n\nThe deep research agent uses `global_logger` for milestone logging:\n\n```python\nfrom src.utils import global_logger, traceable\n\ndef user_query(user_input):\n    user_hint = \"ç¨æ·è¾å¥ï¼\"\n    global_logger.info(f\"{user_hint} ï¼ {user_input}\\n\\n\")\n    \n    # ... LLM interaction ...\n    \n    global_logger.info(f\"å·¥å· tool call è¾åºä¿¡æ¯ï¼ {tool_output}\\n\")\n    global_logger.info(\"-\" * 60)\n    \n    global_logger.info(f\"æç»ç­æ¡ï¼ {assistant_output.content}\")\n```\n\n**Pattern:** The agent uses `global_logger` for human-readable milestone events (user input, tool outputs, final answers) rather than fine-grained function tracing.\n\n**Sources:** [src/agent/deep_research.py:4](), [src/agent/deep_research.py:17](), [src/agent/deep_research.py:28-29](), [src/agent/deep_research.py:44-45](), [src/agent/deep_research.py:62-63](), [src/agent/deep_research.py:68-73]()\n\n### Tool and Executor Decoration\n\nWhile not shown in the provided files, tools and executors use the `@traceable` decorator for automatic instrumentation:\n\n```python\n@traceable\ndef tool_function(param1, param2):\n    # Tool implementation\n    return result\n```\n\nThis pattern automatically logs:\n- Function entry with parameters\n- Execution time\n- Return values or exceptions\n- Full call stack on errors\n\n**Sources:** [src/utils/log_decorator.py:297-302]()\n\n---\n\n## Advanced Features\n\n### Automatic Default Return Values\n\nThe system includes (currently disabled) functionality to generate safe default return values on exceptions:\n\n```python\ndef get_default_return_value(func: Callable) -> Any:\n    \"\"\"\n    æ ¹æ®å½æ°è¿åå¼æ³¨è§£ï¼çæå¯¹åºçé»è®¤å¼ï¼ååºç¨ï¼\n    æ¯æï¼ åºç¡ç±»åãæ³åï¼List/Dict/Tupleï¼ãèªå®ä¹ç±»ãNone\n    \"\"\"\n    type_hints = get_type_hints(func)\n    return_type = type_hints.get(\"return\", None)\n    \n    # Handle basic types\n    if return_type == int: return 0\n    elif return_type == str: return \"\"\n    elif return_type == list: return []\n    # ... etc\n```\n\nThis feature analyzes function type hints to return appropriate default values instead of raising exceptions, enabling graceful degradation. However, it's currently disabled in favor of raising exceptions to maintain normal error flow.\n\n**Sources:** [src/utils/log_decorator.py:91-148](), [src/utils/log_decorator.py:249-257]()\n\n### Parameter Exclusion\n\nThe decorator supports excluding sensitive parameters from logs:\n\n```python\nexclude_args=[\"password\", \"token\", \"secret\"]\n```\n\nThis prevents credential leakage in log files while maintaining comprehensive logging for other parameters.\n\n**Sources:** [src/utils/log_decorator.py:157](), [src/utils/log_decorator.py:300]()\n\n### Performance Timing\n\nEvery logged function call includes microsecond-precision timing:\n\n```python\nstart_time = time.perf_counter()\n# ... function execution ...\nelapsed_time = (time.perf_counter() - start_time) * 1000  # Convert to ms\n```\n\nThis enables performance analysis across the entire system. See [Tracing and Performance Monitoring](#6.3) for analysis techniques.\n\n**Sources:** [src/utils/log_decorator.py:183](), [src/utils/log_decorator.py:224]()\n\n---\n\n## Usage Examples\n\n### Example: Manual Logging\n\n```python\nfrom src.utils import global_logger\n\ndef process_query(query):\n    global_logger.info(f\"Processing query: {query}\")\n    # ... processing ...\n    global_logger.info(f\"Query complete\")\n```\n\n### Example: Automatic Function Tracing\n\n```python\nfrom src.utils import traceable\n\n@traceable\ndef test_function(a: int, b: str, c: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Test function with automatic tracing\"\"\"\n    return {\"result\": a + int(b) + c[\"key\"]}\n```\n\n**Log Output:**\n```\n[2024-01-15 14:30:45.123]  ãè°ç¨å¼å§ã æ è·¯å¾ï¼ __main__.None.test_function | å¼å§æ¶é´ï¼ 2024-01-15 14:30:45.123456 | ä½ç½®åæ°ï¼ (1, '2', {'key': 3}) | å³é®å­åæ°ï¼ {}\n[2024-01-15 14:30:45.125]  ãè°ç¨æ ã \n          test.py:10 <module>\n[2024-01-15 14:30:45.126]  ãè°ç¨æåã æ è·¯å¾ï¼ __main__.None.test_function | èæ¶ï¼ 3.456ms | è¿åå¼ï¼ {'result': 6}\n```\n\n**Sources:** [src/utils/log_decorator.py:310-319]()\n\n---\n\n## Logger Configuration Summary\n\n| Logger | Name | File | Used For | Decorated |\n|--------|------|------|----------|-----------|\n| `all_logger` | `root.all` | `all.log` | Receives all logs via propagation | N/A |\n| `traceable` | `root.all.trace` | `trace.log` | Function-level tracing with stacks | Decorator |\n| `global_logger` | `root.all.print` | `print.log` | Agent milestone events | Manual calls |\n\nAll loggers write to timestamped subdirectories under `logs/` and use rotating file handlers (10MB max, 5 backups) with UTF-8 encoding.\n\n**Sources:** [src/utils/log_decorator.py:288-306]()\n\n---\n\n# Page: Log Files and Analysis\n\n# Log Files and Analysis\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/utils/__pycache__/log_decorator.cpython-312.pyc](src/utils/__pycache__/log_decorator.cpython-312.pyc)\n\n</details>\n\n\n\nThis document describes the logging infrastructure in the algo_agent system, including the structure and content of various log files, how to interpret them, and techniques for analyzing logs for debugging and performance monitoring.\n\nFor information about the logging system architecture and decorators, see [Logging System Architecture](#6.1). For performance profiling with the `@traceable` decorator, see [Tracing and Performance Monitoring](#6.3).\n\n---\n\n## Overview\n\nThe algo_agent system maintains multiple specialized log files that capture different aspects of system execution. These logs provide comprehensive visibility into user operations, system-level function calls, code execution, and errors. All log files are stored in the `logs/` directory with rotating file handlers to manage disk space.\n\n**Key Log Files:**\n\n| Log File | Purpose | Content Type | Primary Use Case |\n|----------|---------|--------------|------------------|\n| `utils.log` | User operations | User queries, tool outputs, LLM responses | User interaction tracking |\n| `global.log` | System traces | Function calls, parameters, timing, stack traces | Debugging and performance analysis |\n| `trace.log` | Detailed execution | Decorated function traces with full context | Deep debugging and tracing |\n| `all.log` | Comprehensive logs | All log entries from all sources | Complete system audit |\n\nSources: [logs/utils.log:1-296](), [logs/global.log:1-1500]()\n\n---\n\n## Log File Structure and Format\n\n### Common Format Elements\n\nAll log files share a consistent timestamp format and structure:\n\n```\n[YYYY-MM-DD HH:MM:SS,mmm] <message content>\n```\n\n**Example:**\n```\n[2025-11-25 17:51:32,379]  ç¨æ·è¾å¥ï¼ ï¼ \nä½ æå¨çå·¥ä½è·¯å¾ä¸é¢ï¼å¯ä»¥è¯»åä¸ä¸æä»¶...\n```\n\nThe timestamp includes millisecond precision (`,379`) for accurate timing correlation across multiple log entries.\n\nSources: [logs/utils.log:1-5](), [logs/global.log:1-10]()\n\n---\n\n## utils.log - User Operations Log\n\n### Purpose and Content\n\n`utils.log` captures high-level user interactions and tool execution results. It provides a chronological view of:\n\n- User input queries\n- Tool execution outputs (Python code execution, task planning)\n- LLM responses (both intermediate and final)\n- Process-level errors and crashes\n\n### Log Entry Types\n\n#### User Input Entries\n\n```\n[2025-11-25 17:51:32,379]  ç¨æ·è¾å¥ï¼ ï¼ \n<user query content>\n```\n\nThese entries mark the start of a new user interaction cycle.\n\n#### Tool Output Entries\n\n```\n[2025-11-25 17:52:14,152]  å·¥å·è¾åºä¿¡æ¯ï¼ \n\n[2025-11-25 17:52:14,153]  ------------------------------------------------------------\n```\n\nTool outputs are followed by separator lines and include execution results, errors, or status messages.\n\n#### LLM Response Entries\n\n```\n[2025-11-26 03:39:09,868]  \nç¬¬2è½®å¤§æ¨¡åè¾åºä¿¡æ¯ï¼ChatCompletionMessage(content='', refusal=None, ...)\n```\n\nLLM responses show which iteration of the decision loop is running and include structured message objects with tool calls.\n\n#### Process Execution Logs\n\n```\n[2025-11-26 03:39:11,530]  å­è¿ç¨ PID: 11336 è¦å°å·¥ä½ç®å½æ´æ¹ä¸º: D:\\zyt\\git_ln\\algo_agent\\wsm\\1\\g4-1\n[2025-11-26 03:39:11,531]  å­è¿ç¨ PID: 11336 å·²å°å·¥ä½ç®å½æ´æ¹ä¸º: D:\\zyt\\git_ln\\algo_agent\\wsm\\1\\g4-1\n```\n\nThese trace subprocess creation and working directory changes for isolated code execution.\n\n#### Error Messages\n\n```\n[2025-11-26 03:39:11,797]  å·¥å·è¾åºä¿¡æ¯ï¼ ## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\nProcess Process-1:\nTraceback (most recent call last):\n  File \"D:\\zyt\\git_ln\\algo_agent\\src\\runtime\\subprocess_python_executor.py\", line 44, in _worker_with_pipe\n    exec(command, _globals, _locals)\n  File \"<string>\", line 4, in <module>\n  File \"C:\\Users\\...\\json\\__init__.py\", line 293, in load\n    return loads(fp.read(),\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265: illegal multibyte sequence\n```\n\nError entries include full stack traces, exception types, and contextual information for debugging.\n\nSources: [logs/utils.log:1-296]()\n\n---\n\n## global.log - System-Level Traces\n\n### Purpose and Content\n\n`global.log` provides detailed function-level tracing with automatic parameter capture, timing, and return value logging. This file is generated by the `@log_function` decorator and captures the complete execution flow.\n\n### Function Call Structure\n\nEach function call is logged with three markers:\n\n#### 1. Call Start Marker\n\n```\n[2025-11-25 17:51:32,252]  ãè°ç¨å¼å§ã æ è·¯å¾ï¼ __main__.None.user_query | å¼å§æ¶é´ï¼ 2025-11-25 17:51:32.252333 | ä½ç½®åæ°ï¼ (...) | å³é®å­åæ°ï¼ {}\n```\n\n**Components:**\n- **æ è·¯å¾ (Stack Path):** Module.Class.Function format (e.g., `__main__.None.user_query`)\n- **å¼å§æ¶é´ (Start Time):** Precise timestamp with microseconds\n- **ä½ç½®åæ° (Positional Arguments):** Full dump of all positional arguments\n- **å³é®å­åæ° (Keyword Arguments):** Dictionary of keyword arguments\n\n#### 2. Call Stack Trace\n\n```\n[2025-11-25 17:51:32,379]  ãè°ç¨æ ã \n          D:\\zyt\\git_ln\\algo_agent\\src\\utils\\log_decorator.py:202 wrapper\n          d:\\zyt\\git_ln\\algo_agent\\src\\agent\\deep_research.py:110 <module>\n```\n\nShows the complete call stack with file paths and line numbers, enabling precise location tracking.\n\n#### 3. Call Completion Marker\n\n**Success:**\n```\n[2025-11-25 17:51:32,385]  ãè°ç¨æåã æ è·¯å¾ï¼ memory.None.init_messages_with_system_prompt | èæ¶ï¼ 2.031ms | è¿åå¼ï¼ [...]\n```\n\n**Failure:**\n```\n[2025-11-26 03:39:14,380]  ãè°ç¨å¤±è´¥ã æ è·¯å¾ï¼ ... | èæ¶ï¼ 12.345ms | å¼å¸¸ä½ç½®ï¼ ... | å¼å¸¸ç±»åï¼ ... | å¼å¸¸ä¿¡æ¯ï¼ ... | å æ ä¿¡æ¯ï¼ ...\n```\n\n**Timing Information:**\n- **èæ¶ (Elapsed Time):** Execution duration in milliseconds with microsecond precision\n- **è¿åå¼ (Return Value):** Full dump of return value (for success)\n- **å¼å¸¸ä¿¡æ¯ (Exception Info):** Complete exception details (for failure)\n\nSources: [logs/global.log:1-1000]()\n\n---\n\n## Log Analysis Techniques\n\n### Tracing User Request Flow\n\nTo trace a complete user request from input to output:\n\n```mermaid\ngraph TD\n    Start[\"User Input in utils.log\"]\n    AgentStart[\"Agent function call in global.log<br/>ãè°ç¨å¼å§ã __main__.None.user_query\"]\n    MemoryInit[\"Memory initialization<br/>ãè°ç¨å¼å§ã memory.None.init_messages_with_system_prompt\"]\n    LLMCall[\"LLM call<br/>ãè°ç¨å¼å§ã llm.None.generate_assistant_output_append\"]\n    ToolCall[\"Tool execution<br/>ãè°ç¨å¼å§ã action.None.call_tools_safely\"]\n    CodeExec[\"Subprocess execution in utils.log<br/>å­è¿ç¨ PID: XXXXX\"]\n    Result[\"Tool output in utils.log\"]\n    AgentEnd[\"Agent completion<br/>ãè°ç¨æåã __main__.None.user_query\"]\n    \n    Start --> AgentStart\n    AgentStart --> MemoryInit\n    MemoryInit --> LLMCall\n    LLMCall --> ToolCall\n    ToolCall --> CodeExec\n    CodeExec --> Result\n    Result --> AgentEnd\n    \n    style Start fill:#f9f9f9\n    style AgentEnd fill:#f9f9f9\n```\n\n**Search Pattern:**\n1. Find user input timestamp in `utils.log`: `[YYYY-MM-DD HH:MM:SS,mmm]  ç¨æ·è¾å¥`\n2. Search `global.log` for same timestamp range and `user_query` function\n3. Follow the call stack through child function calls\n4. Track tool execution results back in `utils.log`\n\nSources: [logs/utils.log:1-100](), [logs/global.log:1-500]()\n\n### Identifying Performance Bottlenecks\n\nThe `èæ¶` (elapsed time) field in `global.log` enables performance analysis:\n\n```\n[2025-11-25 17:52:14,072]  ãè°ç¨æåã æ è·¯å¾ï¼ llm.None.generate_chat_completion | èæ¶ï¼ 41635.670ms | è¿åå¼ï¼ ChatCompletion(...)\n```\n\n**Interpretation:**\n- Times above 10,000ms (10 seconds) indicate LLM API calls\n- Times above 1,000ms (1 second) may indicate I/O operations or subprocess execution\n- Times below 100ms indicate in-process operations\n\n**Analysis Workflow:**\n\n```mermaid\ngraph LR\n    Extract[\"Extract timing data<br/>grep 'èæ¶'\"]\n    Parse[\"Parse milliseconds<br/>awk/sed extraction\"]\n    Sort[\"Sort by duration<br/>sort -n\"]\n    Identify[\"Identify outliers<br/>> 5000ms\"]\n    Correlate[\"Correlate with<br/>function names\"]\n    \n    Extract --> Parse\n    Parse --> Sort\n    Sort --> Identify\n    Identify --> Correlate\n```\n\nSources: [logs/global.log:891-892]()\n\n### Debugging Code Execution Errors\n\nWhen Python code execution fails, logs contain detailed error information:\n\n#### Error Pattern in utils.log\n\n```\n[2025-11-26 03:39:11,797]  å·¥å·è¾åºä¿¡æ¯ï¼ ## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\nProcess Process-1:\nTraceback (most recent call last):\n  File \"D:\\zyt\\git_ln\\algo_agent\\src\\runtime\\subprocess_python_executor.py\", line 44, in _worker_with_pipe\n    exec(command, _globals, _locals)\n  File \"<string>\", line 4, in <module>\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265: illegal multibyte sequence\n### éåºç¶æç ï¼1\n```\n\n**Debugging Steps:**\n\n1. **Locate the Error:**\n   - Search for \"ä»£ç æ§è¡å´©æº\" or \"è¿ç¨å¼å¸¸éåº\" in `utils.log`\n   - Note the timestamp and PID\n\n2. **Identify Root Cause:**\n   - Read the exception type (e.g., `UnicodeDecodeError`)\n   - Examine the stack trace to find the failing line\n   - Check file and line number references\n\n3. **Check Related Context:**\n   - Search `global.log` for the same timestamp\n   - Find the tool call that triggered execution\n   - Review input parameters in the \"ä½ç½®åæ°\" section\n\n4. **Common Error Types:**\n\n| Error Type | Common Cause | Log Indicator |\n|------------|--------------|---------------|\n| `UnicodeDecodeError` | File encoding mismatch (UTF-8 vs GBK) | `'gbk' codec can't decode` |\n| `PickleError` | Non-serializable objects in globals | `cannot pickle 'TextIOWrapper'` |\n| `TimeoutError` | Code execution exceeded timeout | Status indicator in execution result |\n| `UnboundLocalError` | Variable access before assignment | `cannot access local variable` |\n\nSources: [logs/utils.log:136-287]()\n\n### Tracking Task Planning Evolution\n\nThe `RecursivePlanTreeTodoTool` creates hierarchical task plans logged in detail:\n\n```mermaid\ngraph TD\n    T1[\"T1: Read schema.json<br/>Status: pending\"]\n    T2[\"T2: Read data files<br/>Depends: T1\"]\n    T3[\"T3: Build data models<br/>Depends: T1\"]\n    T4[\"T4: Implement loader<br/>Depends: T2, T3\"]\n    T5[\"T5: Analyze distribution<br/>Depends: T4\"]\n    \n    T1 --> T2\n    T1 --> T3\n    T2 --> T4\n    T3 --> T4\n    T4 --> T5\n    \n    style T1 fill:#f9f9f9\n```\n\n**Log Location:**\n- Task creation: Search for `\"recursive_plan_tree_todo\"` in `global.log`\n- Task structure: Look for `\"tree_nodes\"` field with full JSON structure\n- Task dependencies: Examine `\"dependencies\"` arrays\n\nSources: [logs/global.log:909-1574]()\n\n---\n\n## Error Patterns and Resolution\n\n### Common Error Scenarios\n\n#### 1. Encoding Errors (UnicodeDecodeError)\n\n**Log Pattern:**\n```\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265: illegal multibyte sequence\n```\n\n**Root Cause:** Windows default encoding (GBK) conflicts with UTF-8 encoded JSON files.\n\n**Resolution:** Add `encoding='utf-8'` parameter to all file operations:\n```python\nwith open('file.json', 'r', encoding='utf-8') as f:\n    data = json.load(f)\n```\n\n**Log Location:** [logs/utils.log:132-147]()\n\n#### 2. Serialization Errors (PickleError)\n\n**Log Pattern:**\n```\nTypeError: cannot pickle 'TextIOWrapper' instances\n```\n\n**Root Cause:** File handles or other non-serializable objects in workspace globals.\n\n**Resolution:** Close file handles before function returns, or use context managers.\n\n**Log Location:** [logs/utils.log:148-179]()\n\n#### 3. Subprocess Communication Errors\n\n**Log Pattern:**\n```\n[2025-11-26 03:39:11,624]  Pipe reader error: type=EOFError, value=, kv=EOFError()\n[2025-11-26 03:39:11,770]  ---------- 2. æ­£å¸¸æå¼å¸¸éåºï¼ä»å­è¿ç¨è·å ExecutionResult\n[2025-11-26 03:39:11,777]  ---------- 2.2 å­è¿ç¨å´©æºéåºï¼å¦ SegFault\n```\n\n**Root Cause:** Subprocess crashed before sending execution result.\n\n**Debugging:** Check the stack trace above the EOFError for the actual crash reason.\n\n**Log Location:** [logs/utils.log:132-180]()\n\nSources: [logs/utils.log:130-290]()\n\n---\n\n## Log File Rotation and Management\n\n### File Size Management\n\nThe system uses `RotatingFileHandler` for automatic log rotation:\n\n- **Max Size:** Each log file can grow to a configured maximum size (default: typically 10MB)\n- **Backup Count:** Multiple backup files are maintained (e.g., `utils.log.1`, `utils.log.2`)\n- **Rotation Trigger:** When max size is reached, current log is renamed to `.1`, previous `.1` becomes `.2`, etc.\n\n### Log File Locations\n\n```\nlogs/\nâââ utils.log           # Current user operations log\nâââ global.log          # Current system traces\nâââ trace.log           # Current detailed execution traces\nâââ all.log             # Current comprehensive log\nâââ utils.log.1         # Previous rotation\nâââ utils.log.2         # Older rotation\nâââ ...\n```\n\n**Configuration:** Defined in the `setup_logger` function, which sets up handlers with rotation parameters.\n\nSources: Based on Diagram 4 from high-level architecture\n\n---\n\n## Advanced Analysis Scenarios\n\n### Correlating Logs Across Files\n\nTo fully understand a failure, correlate entries across multiple log files:\n\n```mermaid\ngraph LR\n    UtilsLog[\"utils.log<br/>Timestamp T1<br/>User query starts\"]\n    GlobalLog1[\"global.log<br/>Timestamp T1<br/>Function calls begin\"]\n    GlobalLog2[\"global.log<br/>Timestamp T2<br/>Tool execution\"]\n    UtilsLog2[\"utils.log<br/>Timestamp T2<br/>Subprocess error\"]\n    GlobalLog3[\"global.log<br/>Timestamp T3<br/>Error handling\"]\n    \n    UtilsLog -->|\"Search timestamp T1\"| GlobalLog1\n    GlobalLog1 -->|\"Follow call chain\"| GlobalLog2\n    GlobalLog2 -->|\"Check tool output\"| UtilsLog2\n    UtilsLog2 -->|\"Return to error handler\"| GlobalLog3\n```\n\n**Process:**\n1. Identify the problematic timestamp in `utils.log`\n2. Search `global.log` for entries within Â±1 second\n3. Build a timeline of function calls and events\n4. Cross-reference stack traces with error messages\n5. Identify the first point of failure in the call chain\n\n### Performance Profiling\n\nExtract timing data for statistical analysis:\n\n**Command-line example:**\n```bash\n# Extract all timing data\ngrep \"èæ¶\" logs/global.log | awk -F'èæ¶ï¼' '{print $2}' | awk -F'ms' '{print $1}' > timings.txt\n\n# Calculate statistics\ncat timings.txt | sort -n | awk '\n    BEGIN { sum=0; count=0; }\n    { values[count++]=$1; sum+=$1; }\n    END {\n        print \"Count:\", count\n        print \"Mean:\", sum/count, \"ms\"\n        print \"Median:\", values[int(count/2)], \"ms\"\n        print \"95th percentile:\", values[int(count*0.95)], \"ms\"\n        print \"Max:\", values[count-1], \"ms\"\n    }\n'\n```\n\nThis produces performance distribution metrics for identifying outliers and bottlenecks.\n\n### Call Graph Reconstruction\n\nReconstruct the call graph from stack traces:\n\n```mermaid\ngraph TD\n    MainUserQuery[\"__main__.user_query<br/>Line: deep_research.py:110\"]\n    MemoryInit[\"memory.init_messages_with_system_prompt<br/>Line: deep_research.py:19\"]\n    ToolSchema[\"tool.schema.get_tools_schema<br/>Line: deep_research.py:20\"]\n    LLMGenerate[\"llm.generate_assistant_output_append<br/>Line: deep_research.py:26\"]\n    LLMExtract[\"llm.extract_assistant_output_from_chat<br/>Line: llm.py:33\"]\n    LLMChat[\"llm.generate_chat_completion<br/>Line: llm.py:25\"]\n    ActionCall[\"action.call_tools_safely<br/>Line: deep_research.py:43\"]\n    ActionTools[\"action.call_tools<br/>Line: action.py:42\"]\n    \n    MainUserQuery --> MemoryInit\n    MainUserQuery --> ToolSchema\n    MainUserQuery --> LLMGenerate\n    LLMGenerate --> LLMExtract\n    LLMExtract --> LLMChat\n    MainUserQuery --> ActionCall\n    ActionCall --> ActionTools\n    \n    style MainUserQuery fill:#f9f9f9\n```\n\n**Data Source:** Stack traces in the \"ãè°ç¨æ ã\" sections of `global.log`.\n\nSources: [logs/global.log:54-115](), [logs/global.log:496-694]()\n\n---\n\n## Best Practices for Log Analysis\n\n### 1. Use Timestamps for Correlation\n\nAlways correlate log entries using precise timestamps. The millisecond precision allows exact matching across files.\n\n### 2. Follow the Call Chain\n\nStart from high-level user operations in `utils.log`, then drill down into `global.log` for detailed function traces.\n\n### 3. Understand Log Hierarchy\n\n- `utils.log`: User-facing events\n- `global.log`: System-level function traces\n- `trace.log`: Detailed instrumented traces\n- `all.log`: Complete audit trail\n\n### 4. Look for Pattern Markers\n\nKey markers to search for:\n- `ç¨æ·è¾å¥` - User query start\n- `ãè°ç¨å¼å§ã` - Function entry\n- `ãè°ç¨æåã` - Successful completion\n- `ãè°ç¨å¤±è´¥ã` - Function failure\n- `ä»£ç æ§è¡å´©æº` - Code execution crash\n- `Pipe reader error` - Subprocess communication failure\n\n### 5. Check Both Success and Failure Paths\n\nEven when debugging failures, review successful executions to understand the expected flow.\n\n### 6. Monitor Execution Times\n\nTrack `èæ¶` values to identify:\n- LLM API calls (typically 10-60 seconds)\n- Code execution (varies, typically < 5 seconds)\n- In-process operations (typically < 100ms)\n\nUnusual timing values often indicate problems even when errors aren't explicitly logged.\n\nSources: [logs/utils.log:1-296](), [logs/global.log:1-2000]()\n\n---\n\n# Page: Tracing and Performance Monitoring\n\n# Tracing and Performance Monitoring\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/utils/__pycache__/log_decorator.cpython-312.pyc](src/utils/__pycache__/log_decorator.cpython-312.pyc)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page documents the tracing and performance monitoring capabilities in the algo_agent system. It covers how to use the `@traceable` decorator to instrument functions, how to read and analyze execution traces in log files, and techniques for identifying performance bottlenecks.\n\nFor information about the underlying logging infrastructure and decorator implementation, see [Logging System Architecture](#6.1). For details on the structure and content of specific log files, see [Log Files and Analysis](#6.2).\n\n---\n\n## Overview\n\nThe algo_agent system provides comprehensive tracing through the `@log_function` decorator, which automatically captures:\n\n- **Function entry and exit** with timestamps\n- **Execution time** in milliseconds with microsecond precision\n- **Call stack paths** showing the complete invocation hierarchy\n- **Arguments and return values** with intelligent serialization\n- **Exception traces** with full stack information\n\nAll traced functions write to [logs/global.log]() and [logs/trace.log](), enabling post-execution performance analysis and debugging.\n\n---\n\n## Trace Log Format\n\n### Standard Trace Entry Structure\n\nEach function invocation generates three log entries:\n\n```\n[timestamp] ãè°ç¨å¼å§ã æ è·¯å¾ï¼ <stack_path> | å¼å§æ¶é´ï¼ <datetime> | ä½ç½®åæ°ï¼ <args> | å³é®å­åæ°ï¼ <kwargs>\n[timestamp] ãè°ç¨æ ã \n          <file_path:line> <function>\n          <file_path:line> <caller_function>\n          ...\n[timestamp] ãè°ç¨æåã æ è·¯å¾ï¼ <stack_path> | èæ¶ï¼ <time>ms | è¿åå¼ï¼ <result>\n```\n\n**Example from logs/global.log:**\n\n```\n[2025-11-25 17:51:32,252] ãè°ç¨å¼å§ã æ è·¯å¾ï¼ __main__.None.user_query | å¼å§æ¶é´ï¼ 2025-11-25 17:51:32.252333 | ä½ç½®åæ°ï¼ (...)\n[2025-11-25 17:51:32,379] ãè°ç¨æ ã \n          D:\\zyt\\git_ln\\algo_agent\\src\\utils\\log_decorator.py:202 wrapper\n          d:\\zyt\\git_ln\\algo_agent\\src\\agent\\deep_research.py:110 <module>\n[2025-11-25 17:52:14,089] ãè°ç¨æåã æ è·¯å¾ï¼ llm.None.generate_assistant_output_append | èæ¶ï¼ 41680.934ms | è¿åå¼ï¼ <ChatCompletionMessage>\n```\n\n**Sources:** [logs/global.log:1-900]()\n\n### Stack Path Format\n\nThe stack path follows the pattern: `<module>.<class>.<function>`\n\n- `__main__.None.user_query` - top-level function in main module\n- `llm.None.generate_chat_completion` - module-level function in llm\n- `action.None.call_tools_safely` - function in action module\n\n**Sources:** [logs/global.log:1-900]()\n\n---\n\n## Timing Measurement Architecture\n\n### How Execution Time is Captured\n\n```mermaid\ngraph TB\n    FunctionEntry[\"Function Entry<br/>@log_function decorated\"]\n    StartTimer[\"Record start_time<br/>time.perf_counter()\"]\n    StartLog[\"Log ãè°ç¨å¼å§ã<br/>with timestamp and args\"]\n    \n    Execute[\"Execute Function Body\"]\n    \n    Success[\"Success Path\"]\n    Exception[\"Exception Path\"]\n    \n    EndTimer[\"Calculate elapsed_time<br/>end - start (ms)\"]\n    SuccessLog[\"Log ãè°ç¨æåã<br/>with elapsed_time and result\"]\n    \n    ExceptionLog[\"Log ãè°ç¨å¤±è´¥ã<br/>with elapsed_time and traceback\"]\n    \n    Return[\"Return Result\"]\n    Raise[\"Re-raise Exception\"]\n    \n    FunctionEntry --> StartTimer\n    StartTimer --> StartLog\n    StartLog --> Execute\n    \n    Execute --> Success\n    Execute --> Exception\n    \n    Success --> EndTimer\n    Exception --> EndTimer\n    \n    EndTimer --> SuccessLog\n    EndTimer --> ExceptionLog\n    \n    SuccessLog --> Return\n    ExceptionLog --> Raise\n```\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n### Precision and Accuracy\n\nThe system uses `time.perf_counter()` for timing measurements, providing:\n- **Microsecond precision** (displayed as fractional milliseconds)\n- **Monotonic clock** (unaffected by system clock adjustments)\n- **Sub-millisecond accuracy** in timing differences\n\n**Example timing outputs:**\n- `2.031ms` - Fast operation\n- `41680.934ms` - Long-running LLM call\n- `4.624ms` - Quick utility function\n\n**Sources:** [logs/global.log:115-900]()\n\n---\n\n## Analyzing Performance from Logs\n\n### Identifying Slow Operations\n\n#### Workflow for Performance Analysis\n\n```mermaid\ngraph LR\n    ReadLog[\"Read logs/global.log\"]\n    ExtractTiming[\"Extract ãè°ç¨æåã entries<br/>with èæ¶ field\"]\n    SortByTime[\"Sort by execution time\"]\n    IdentifyBottleneck[\"Identify functions > threshold\"]\n    \n    AnalyzeStack[\"Analyze ãè°ç¨æ ã<br/>for context\"]\n    CheckFrequency[\"Check call frequency<br/>in log\"]\n    \n    Optimize[\"Target optimization\"]\n    \n    ReadLog --> ExtractTiming\n    ExtractTiming --> SortByTime\n    SortByTime --> IdentifyBottleneck\n    \n    IdentifyBottleneck --> AnalyzeStack\n    IdentifyBottleneck --> CheckFrequency\n    \n    AnalyzeStack --> Optimize\n    CheckFrequency --> Optimize\n```\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n#### Example Analysis from Logs\n\n**Slowest Operations (from logs/global.log):**\n\n| Function | Execution Time | Call Count | Total Time | Bottleneck Level |\n|----------|----------------|------------|------------|------------------|\n| `generate_chat_completion` | 41635.670ms | 1 | 41635ms | **Critical** |\n| `generate_assistant_output_append` | 41680.934ms | 1 | 41680ms | **Critical** |\n| `user_query` | ~42000ms | 1 | 42000ms | **Critical** |\n| `call_tools_safely` | 34.693ms | 1 | 34ms | Moderate |\n| `get_tools_schema` | 8.008ms | 1 | 8ms | Fast |\n| `init_messages_with_system_prompt` | 2.031ms | 1 | 2ms | Fast |\n\n**Key Insight:** The LLM generation operations dominate execution time (~99% of total), with `generate_chat_completion` being the primary bottleneck.\n\n**Sources:** [logs/global.log:891-900]()\n\n---\n\n## Call Stack Analysis\n\n### Understanding Call Hierarchies\n\nThe `ãè°ç¨æ ã` entries show the complete invocation path from entry point to current function:\n\n```\nãè°ç¨æ ã \n          D:\\zyt\\git_ln\\algo_agent\\src\\utils\\log_decorator.py:202 wrapper\n          d:\\zyt\\git_ln\\algo_agent\\src\\agent\\llm.py:33 generate_assistant_output_append\n          D:\\zyt\\git_ln\\algo_agent\\src\\utils\\log_decorator.py:217 wrapper\n          d:\\zyt\\git_ln\\algo_agent\\src\\agent\\deep_research.py:26 user_query\n          D:\\zyt\\git_ln\\algo_agent\\src\\utils\\log_decorator.py:217 wrapper\n          d:\\zyt\\git_ln\\algo_agent\\src\\agent\\deep_research.py:110 <module>\n```\n\n**Reading the Stack (bottom-to-top):**\n1. Entry point: [src/agent/deep_research.py:110]() `<module>`\n2. Calls through wrapper: [src/utils/log_decorator.py:217]()\n3. Into: [src/agent/deep_research.py:26]() `user_query`\n4. Calls through wrapper again\n5. Into: [src/agent/llm.py:33]() `generate_assistant_output_append`\n\n**Sources:** [logs/global.log:496-500](), [logs/global.log:688-694]()\n\n### Call Graph Reconstruction\n\n```mermaid\ngraph TB\n    MainModule[\"src/agent/deep_research.py<br/>main entry point\"]\n    UserQuery[\"user_query()<br/>41680ms total\"]\n    \n    InitMessages[\"init_messages_with_system_prompt()<br/>2.031ms\"]\n    GetTools[\"get_tools_schema()<br/>8.008ms\"]\n    GenAssist[\"generate_assistant_output_append()<br/>41680.934ms\"]\n    \n    ExtractOutput[\"extract_assistant_output_from_chat()<br/>41658.357ms\"]\n    GenChat[\"generate_chat_completion()<br/>41635.670ms\"]\n    \n    HasTool[\"has_tool_call()<br/>4.624ms\"]\n    CallTools[\"call_tools_safely()<br/>34.693ms\"]\n    CallToolsInner[\"call_tools()<br/>9.949ms\"]\n    \n    MainModule --> UserQuery\n    UserQuery --> InitMessages\n    UserQuery --> GetTools\n    UserQuery --> GenAssist\n    \n    GenAssist --> ExtractOutput\n    ExtractOutput --> GenChat\n    \n    UserQuery --> HasTool\n    UserQuery --> CallTools\n    CallTools --> CallToolsInner\n```\n\n**Sources:** [logs/global.log:1-1574]()\n\n---\n\n## Argument and Return Value Tracing\n\n### Serialization Strategy\n\nThe `log_function` decorator uses intelligent serialization for arguments and return values:\n\n**Supported Types:**\n- Primitive types: `str`, `int`, `float`, `bool`\n- Collections: `list`, `dict`, `tuple`, `set`\n- Pydantic models: Serialized via `.dict()` or JSON\n- Custom objects: Attempts `__dict__` inspection or pretty-print\n\n**Truncation Rules:**\n- Large strings: Show first 1000 characters + `\"...\"`\n- Long lists: Show first 10 items + `\"...\"`\n- Deep structures: Limited recursion depth\n\n**Example - Complex Return Value:**\n```\nè¿åå¼ï¼ ChatCompletionMessage(content='', refusal=None, role='assistant', ...)\n```\n\n**Sources:** [logs/global.log:891-892]()\n\n### Filtering Sensitive Data\n\nThe decorator automatically filters sensitive fields from logs:\n- `password`\n- `token`\n- `secret`\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n---\n\n## Exception Tracing\n\n### Exception Capture Format\n\nWhen a traced function raises an exception:\n\n```\nãè°ç¨å¤±è´¥ã æ è·¯å¾ï¼ <path> | èæ¶ï¼ <time>ms | å¼å¸¸ä½ç½®ï¼ <location> | å¼å¸¸ç±»åï¼ <type> | å¼å¸¸ä¿¡æ¯ï¼ <message> | å æ ä¿¡æ¯ï¼ <traceback>\n```\n\n**Example from logs/utils.log:**\n```\n## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\nProcess Process-1:\nTraceback (most recent call last):\n  File \"D:\\zyt\\git_ln\\algo_agent\\src\\runtime\\subprocess_python_executor.py\", line 44, in _worker_with_pipe\n    exec(command, _globals, _locals)\n  File \"<string>\", line 4, in <module>\n  File \"C:\\Users\\zooos\\AppData\\Roaming\\uv\\python\\cpython-3.12.11-windows-x86_64-none\\Lib\\json\\__init__.py\", line 293, in load\n    return loads(fp.read(),\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265: illegal multibyte sequence\n```\n\n**Sources:** [logs/utils.log:132-146]()\n\n### Exception Analysis Workflow\n\n```mermaid\ngraph TB\n    ExceptionRaised[\"Exception Raised<br/>in Traced Function\"]\n    CaptureTime[\"Calculate elapsed_time<br/>up to exception point\"]\n    CaptureType[\"Capture exception type<br/>and message\"]\n    \n    GetTraceback[\"Extract full traceback<br/>traceback.format_exc()\"]\n    GetLocation[\"Identify exception line<br/>file:line\"]\n    \n    LogFailure[\"Log ãè°ç¨å¤±è´¥ã entry<br/>with all details\"]\n    Reraise[\"Re-raise exception<br/>(for upstream handling)\"]\n    \n    ExceptionRaised --> CaptureTime\n    ExceptionRaised --> CaptureType\n    ExceptionRaised --> GetTraceback\n    ExceptionRaised --> GetLocation\n    \n    CaptureTime --> LogFailure\n    CaptureType --> LogFailure\n    GetTraceback --> LogFailure\n    GetLocation --> LogFailure\n    \n    LogFailure --> Reraise\n```\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n---\n\n## Performance Metrics Extraction\n\n### Key Metrics Available\n\nFrom trace logs, you can extract:\n\n1. **Per-Function Metrics**\n   - Average execution time\n   - Min/max execution time\n   - Call frequency\n   - Failure rate\n\n2. **System-Wide Metrics**\n   - Total request processing time\n   - Time distribution by component\n   - Bottleneck identification\n   - Concurrency level (via process/thread IDs)\n\n### Example Metrics Calculation\n\n**From logs/global.log analysis:**\n\n```python\n# Pseudocode for metrics extraction\nmetrics = {\n    \"user_query\": {\n        \"total_time\": 42000,  # ms\n        \"calls\": 1,\n        \"avg_time\": 42000\n    },\n    \"generate_chat_completion\": {\n        \"total_time\": 41635.670,\n        \"calls\": 1,\n        \"percentage_of_total\": 99.1  # % of user_query time\n    },\n    \"get_tools_schema\": {\n        \"total_time\": 8.008,\n        \"calls\": 1,\n        \"percentage_of_total\": 0.02\n    }\n}\n```\n\n**Sources:** [logs/global.log:1-1574]()\n\n---\n\n## Practical Usage Patterns\n\n### Pattern 1: Identifying LLM Call Overhead\n\n**Goal:** Measure time spent in LLM API calls vs. local processing\n\n**Steps:**\n1. Search logs for `generate_chat_completion` entries\n2. Extract execution times for all LLM calls\n3. Compare to parent function (`generate_assistant_output_append`) time\n4. Identify overhead from serialization/network\n\n**Finding:** In the example trace, LLM call takes 41635ms of 41680ms total (99.9% efficiency).\n\n**Sources:** [logs/global.log:891-892]()\n\n### Pattern 2: Finding Redundant Tool Calls\n\n**Goal:** Detect repeated tool invocations with identical parameters\n\n**Steps:**\n1. Extract all `call_tools_safely` entries\n2. Compare `tool_call_arguments` fields\n3. Identify duplicate calls within same query\n4. Calculate wasted time from duplicates\n\n**Sources:** [logs/global.log:908-1574]()\n\n### Pattern 3: Detecting Memory Serialization Bottlenecks\n\n**Goal:** Find slow serialization in workspace globals\n\n**Steps:**\n1. Look for `PickleError` or `UnicodeDecodeError` in logs\n2. Check elapsed time for functions that filter/serialize globals\n3. Identify problematic object types\n\n**Example Issue Found:**\n```\nTypeError: cannot pickle 'TextIOWrapper' instances\n```\n**Root Cause:** File handles left in global scope during code execution\n\n**Sources:** [logs/utils.log:132-231]()\n\n---\n\n## Tracing Configuration\n\n### Decorator Application\n\nTo enable tracing on a function, apply the `@log_function` decorator:\n\n```python\nfrom src.utils.log_decorator import log_function\n\n@log_function(\n    exclude_args=['password', 'secret_key'],  # Filter sensitive args\n    record_stack=True,  # Include call stack\n    default_return_value=None  # Fallback if exception during logging\n)\ndef my_function(arg1, arg2):\n    # Function implementation\n    pass\n```\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n### Logger Selection\n\nThe system uses multiple loggers for different purposes:\n\n| Logger | Log File | Use Case |\n|--------|----------|----------|\n| `global_logger` | logs/global.log | System-level function traces |\n| `traceable_logger` | logs/trace.log | Detailed execution traces with stack info |\n| `all_logger` | logs/all.log | Comprehensive logs (all sources) |\n\nFunctions decorated with `@log_function` write to `global_logger` by default.\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n---\n\n## Bottleneck Identification Guide\n\n### Common Bottleneck Patterns\n\n#### 1. External API Calls (Most Common)\n\n**Signature in Logs:**\n- Function names containing `generate`, `call`, `completion`\n- Execution times in seconds (1000+ ms)\n- Low call count but high individual time\n\n**Example:**\n```\nãè°ç¨æåã æ è·¯å¾ï¼ llm.None.generate_chat_completion | èæ¶ï¼ 41635.670ms\n```\n\n**Action:** Consider caching, async execution, or response streaming.\n\n**Sources:** [logs/global.log:891]()\n\n#### 2. Serialization Overhead\n\n**Signature in Logs:**\n- Functions in `workspace.py` or `schemas.py`\n- `PickleError` or deep copy exceptions\n- Multiple consecutive calls with similar times\n\n**Action:** Optimize data structures, avoid unnecessary deep copies.\n\n**Sources:** [logs/utils.log:150-178]()\n\n#### 3. Repeated Computation\n\n**Signature in Logs:**\n- Same function called multiple times with identical args\n- Each call has similar execution time\n- Total time = sum of individual calls\n\n**Action:** Implement memoization or caching layer.\n\n---\n\n## Integration with Other Monitoring Tools\n\n### Log Aggregation\n\nThe trace logs can be ingested into standard log aggregation tools:\n\n**Compatible Formats:**\n- **ELK Stack:** Parse structured fields from log lines\n- **Splunk:** Index on timestamp and stack path\n- **Prometheus:** Extract timing metrics via log scraping\n- **Grafana Loki:** Query by function name and execution time\n\n**Key Fields for Extraction:**\n- Timestamp: `[2025-11-25 17:51:32,252]`\n- Stack path: `llm.None.generate_chat_completion`\n- Execution time: `41635.670ms`\n- Status: `ãè°ç¨æåã` or `ãè°ç¨å¤±è´¥ã`\n\n**Sources:** [logs/global.log:1-1574]()\n\n### Trace Visualization\n\n```mermaid\ngraph LR\n    LogFile[\"logs/global.log<br/>Raw trace data\"]\n    Parser[\"Log Parser<br/>Extract timing data\"]\n    \n    TimeSeriesDB[\"Time Series DB<br/>Store metrics\"]\n    Dashboard[\"Grafana Dashboard<br/>Visualize performance\"]\n    \n    FlameGraph[\"Flame Graph<br/>Call hierarchy\"]\n    \n    LogFile --> Parser\n    Parser --> TimeSeriesDB\n    Parser --> FlameGraph\n    \n    TimeSeriesDB --> Dashboard\n```\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n---\n\n## Best Practices\n\n### DO:\n- â Use `@log_function` on all public API functions\n- â Use `@log_function` on functions that perform I/O or external calls\n- â Set `exclude_args` for sensitive parameters\n- â Monitor log file sizes and implement rotation\n- â Aggregate timing metrics for trend analysis\n- â Use execution times to guide optimization efforts\n\n### DON'T:\n- â Trace hot path functions called thousands of times (logging overhead)\n- â Log large data structures without truncation (disk space)\n- â Use tracing in production without log rotation (disk full)\n- â Rely on tracing for real-time monitoring (use metrics instead)\n- â Forget to filter sensitive data from traces (security risk)\n\n### Performance Impact\n\n**Logging Overhead:**\n- Function entry/exit: ~0.1-0.5ms per call\n- Stack trace capture: ~1-2ms per call\n- Argument serialization: Variable (0.1-10ms depending on size)\n\n**Recommendation:** For functions called > 100 times per second, consider sampling (trace 1 in N calls).\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n---\n\n## Troubleshooting Guide\n\n### Issue: Missing Trace Entries\n\n**Symptom:** Expected function not appearing in logs\n\n**Possible Causes:**\n1. Function not decorated with `@log_function`\n2. Logger not properly initialized\n3. Log level too high (set to WARNING or ERROR)\n4. Exception during logging itself\n\n**Solution:** Verify decorator application and check logger configuration.\n\n---\n\n### Issue: Inaccurate Timing\n\n**Symptom:** Execution time doesn't match wall clock time\n\n**Possible Causes:**\n1. System clock adjustment during execution\n2. Nested decorator timing (measure includes decorator overhead)\n3. Concurrent execution (parallel threads/processes)\n\n**Solution:** Use `perf_counter()` for relative timing, avoid comparing across processes.\n\n---\n\n### Issue: Log File Too Large\n\n**Symptom:** logs/global.log growing to GB size\n\n**Possible Causes:**\n1. No log rotation configured\n2. Logging large return values\n3. High-frequency function tracing\n\n**Solution:** Enable `RotatingFileHandler` with size limits, increase truncation limits.\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n---\n\n# Page: Use Cases and Examples\n\n# Use Cases and Examples\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n\n</details>\n\n\n\nThis page demonstrates real-world usage patterns of the `algo_agent` system through concrete examples extracted from actual execution logs and code samples. It illustrates how the agent orchestrates tools, handles complex multi-step tasks, and processes data-intensive workloads.\n\nFor implementation details on tool execution, see [Python Code Execution Tool](#4.2). For task planning structures, see [Recursive Task Planning Tool](#4.3). For troubleshooting common issues encountered in these examples, see [Common Execution Errors](#10.1).\n\n---\n\n## Overview of Use Case Categories\n\nThe system has been deployed across several application domains:\n\n| Domain | Primary Tools Used | Typical Workflow |\n|--------|-------------------|------------------|\n| Emergency Response Planning | `RecursivePlanTreeTodoTool`, `ExecutePythonCodeTool` | Schema analysis â Data loading â Algorithm design â Performance testing |\n| Geographic Data Processing | `ExecutePythonCodeTool` | Data reading â Filtering â Graph construction â Visualization |\n| Multi-Day Travel Planning | `ExecutePythonCodeTool` | Route optimization â Time constraints â Metro/subway integration |\n| Pydantic Data Modeling | `ExecutePythonCodeTool` | Schema validation â Type checking â Data transformation |\n\n---\n\n## End-to-End Example: Emergency Response Planning\n\n### Use Case Description\n\nThe agent is tasked with analyzing emergency response data to design and test multi-agent scheduling algorithms. This involves reading JSON schemas, loading multiple data files, building Python data models, and implementing various scheduling strategies.\n\n**Input Data:**\n- `schema.json` - Complete data structure schema\n- `emergency_response_data_01.json` through `emergency_response_data_05.json` - Five scenario datasets\n\n**User Query (from logs):**\n\n```\nä½ çç®æ æ¯åºäºä»¥ä¸æ°æ®ç¹ç¹åä½¿ç¨å»ºè®®ï¼å¶å®ä¸ä¸ªè¯¦ç»çç ç©¶è®¡åï¼å¸®å©ä½ æ·±å¥äºè§£æ°æ®ç»æï¼\nå¹¶åæå¦ä½å©ç¨è¿äºæ°æ®è§£å³åºæ¥ç©èµè¿è¾è°åº¦ä¸­çé®é¢ã\nè¯·ååºå·ä½çä»»å¡æ­¥éª¤ï¼åæ¬ä½ä¸éäºä»¥ä¸æ¹é¢ï¼\n1. æ°æ®ç»æåæï¼å¦ä½çè§£åè§£æ schema.json ä¸­å®ä¹çåä¸ªå®ä½åå¶å³ç³»ã\n2. æ°æ®çæé»è¾ï¼å¦ä½çè§£éæºæ°æ®çæçç­ç¥åæ¹æ³ã\n3. åºæ¯æ¨¡æï¼å¦ä½å©ç¨çæçæ°æ®æ¨¡æä¸åçåºæ¥ææ´åºæ¯ã\n4. ç®æ³æµè¯ï¼å¦ä½è®¾è®¡å¤ç§ä¸åçç®æ³å®éªæ¥æµè¯å¤æºè½ä½ååè°åº¦ç®æ³çæ§è½åææã\n```\n\n### Agent Execution Flow\n\n```mermaid\ngraph TB\n    UserQuery[\"User Query:<br/>Emergency Response Planning\"]\n    \n    InitPlan[\"LLM generates initial plan<br/>RecursivePlanTreeTodoTool\"]\n    \n    Tasks[\"Task Decomposition<br/>T1-T12 created\"]\n    \n    T1[\"T1: Read schema.json<br/>ExecutePythonCodeTool\"]\n    T2[\"T2: Read data files 01-05<br/>ExecutePythonCodeTool\"]\n    T3[\"T3: Build data model classes<br/>(Pydantic/dataclass)\"]\n    \n    Error[\"UnicodeDecodeError<br/>gbk vs utf-8\"]\n    Retry[\"LLM analyzes error<br/>Suggests encoding fix\"]\n    \n    T4[\"T4: Implement data loader<br/>Batch loading + validation\"]\n    T5[\"T5: Analyze data distribution<br/>Statistics + visualization\"]\n    \n    T6T7[\"T6: Simulate single scenario<br/>T7: Design scheduler framework\"]\n    T8T9[\"T8: Greedy algorithm<br/>T9: Weighted scoring model\"]\n    T10T11[\"T10: Path replanning<br/>T11: Evaluate metrics\"]\n    T12[\"T12: Run comparison experiments<br/>Generate report\"]\n    \n    UserQuery --> InitPlan\n    InitPlan --> Tasks\n    Tasks --> T1\n    Tasks --> T2\n    Tasks --> T3\n    T1 --> Error\n    T2 --> Error\n    Error --> Retry\n    Retry --> T1\n    T2 --> T4\n    T3 --> T4\n    T4 --> T5\n    T4 --> T6T7\n    T6T7 --> T8T9\n    T8T9 --> T10T11\n    T10T11 --> T12\n    \n    style Error fill:#ffcccc\n    style Retry fill:#ffffcc\n```\n\n**Sources:** [logs/utils.log:1-296](), [logs/global.log:1-1792](), [src/agent/deep_research.py:15-129]()\n\n### Task Planning Structure\n\nThe agent decomposes the problem into 12 hierarchical tasks:\n\n```mermaid\ngraph TD\n    Core[\"Core Goal:<br/>Design multi-agent scheduling algorithm\"]\n    \n    T1[\"T1: Read schema.json\"]\n    T2[\"T2: Read data files 01-05<br/>depends: T1\"]\n    T3[\"T3: Build data models<br/>depends: T1\"]\n    T4[\"T4: Data loader<br/>depends: T2, T3\"]\n    T5[\"T5: Analyze distribution<br/>depends: T4\"]\n    T6[\"T6: Simulate scenario<br/>depends: T4\"]\n    T7[\"T7: Scheduler framework<br/>depends: T3\"]\n    T8[\"T8: Greedy algorithm<br/>depends: T7\"]\n    T9[\"T9: Weighted scoring<br/>depends: T8\"]\n    T10[\"T10: Path replanning<br/>depends: T9\"]\n    T11[\"T11: Evaluate metrics<br/>depends: T8, T9\"]\n    T12[\"T12: Comparison experiments<br/>depends: T11\"]\n    \n    Core --> T1\n    Core --> T2\n    Core --> T3\n    T1 --> T2\n    T1 --> T3\n    T2 --> T4\n    T3 --> T4\n    T4 --> T5\n    T4 --> T6\n    T3 --> T7\n    T7 --> T8\n    T8 --> T9\n    T9 --> T10\n    T8 --> T11\n    T9 --> T11\n    T11 --> T12\n```\n\n**Sources:** [logs/utils.log:56-62](), [logs/global.log:908-1791]()\n\n### Key Implementation Details\n\n#### Task T1: Schema Reading with Error Handling\n\n**Initial attempt (failed):**\n\n```python\nimport json\nwith open('schema.json', 'r') as file:\n    schema = json.load(file)\n    print(json.dumps(schema, indent=2))\n```\n\n**Error encountered:**\n```\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265: illegal multibyte sequence\n```\n\n**LLM analysis and correction:**\n```python\nimport json\ntry:\n    with open('schema.json', 'r', encoding='utf-8') as f:\n        schema = json.load(f)\n    print(json.dumps(schema, indent=2, ensure_ascii=False))\nexcept Exception as e:\n    print(f\"Error reading schema.json: {e}\")\n```\n\nThe agent identifies the root cause: Windows default `gbk` encoding vs UTF-8 file encoding, and automatically suggests the fix.\n\n**Sources:** [logs/utils.log:130-290](), [logs/global.log:132-179]()\n\n#### Task T3: Data Model Construction\n\n**Recommended approach (from task description):**\n\n- Use `dataclass` or `pydantic` for strong typing\n- Core entities: `Task`, `Carrier`, `Resource`, `Location`, `Path`, `RiskPoint`\n- Relationships: task-to-carrier assignment, path-to-risk-point association\n\n#### Task T8-T9: Algorithm Implementation\n\n**T8 - Greedy Strategy:**\n- Select nearest available carrier for each task\n- Use Haversine formula for distance calculation\n- Track carrier status (idle/busy)\n\n**T9 - Weighted Scoring:**\n- Multi-factor evaluation: distance, priority, ETA, risk coefficient\n- Normalize dimensions\n- Dynamic weight adjustment\n\n**Sources:** [logs/global.log:1041-1070]()\n\n---\n\n## Multi-Day Tourism Route Planning Example\n\n### Problem Statement\n\nFrom [src/agent/deep_research.py:93-118](), the system solves a complex optimization problem:\n\n**Core Constraints:**\n- Maximize scenic spot quality (5A > 4A > 3A) and quantity\n- Minimize commute time\n- Daily constraint: â¤8 hours (daytime only, playtime + commute)\n- Each spot visit time â¥ recommended duration\n- Night transit allowed (doesn't count toward 8-hour limit)\n\n**Transportation Rules:**\n- Metro priority: 80 km/h\n- Walking fallback: 3.6 km/h\n- Night metro doesn't count toward daily limit\n\n**Input Data:**\n- `metro-draw-schema.json`, `metro-draw-data-80%.json` - Metro network\n- `beijing_scenic_spot_schema_with_play_hours.json`, `beijing_scenic_spot_validated_data_with_play_hours.json` - Scenic spots\n- Starting point: `[116.39088849999999, 39.92767]`\n\n### Expected Output\n\n```mermaid\ngraph LR\n    Input[\"Input Data<br/>Metro + Scenic Spots\"]\n    Parse[\"Parse JSON schemas<br/>Validate data\"]\n    \n    Algo1[\"Enumerate 1-10 day plans\"]\n    Algo2[\"Optimize for each day:<br/>- Maximize 5A/4A spots<br/>- Respect time limits\"]\n    \n    Path[\"Generate detailed paths:<br/>- Day/night transit<br/>- Distance/time tracking\"]\n    \n    Output[\"Output Files (UTF-8):<br/>- Route details<br/>- Time breakdown\"]\n    \n    Viz[\"Visualization:<br/>- Numbered charts<br/>- Day/night paths<br/>- Time annotations\"]\n    \n    Input --> Parse\n    Parse --> Algo1\n    Algo1 --> Algo2\n    Algo2 --> Path\n    Path --> Output\n    Path --> Viz\n```\n\n**Sources:** [src/agent/deep_research.py:76-129]()\n\n### Code Execution Pattern\n\nThe user query explicitly requires:\n\n```python\n# From the prompt structure:\nuser_query_prompt = \"\"\"\néæ±æ¯åºäºç»å®çJSONæ°æ®æä»¶...éè¿Pythonç¼ç¨å®ç°å¤æ¥ææ¸¸è·¯çº¿è§å\n\"\"\"\n\ntool_use_prompt = \"\"\"\n1. å¿é¡»ä½¿ç¨pythonå·¥å·è¿è¡ç®æ³ç¼ç è¾åºå¾å°è®¡ç®ç­æ¡ï¼ä¸è½ç´æ¥ç»åºç­æ¡ã\n2. å¨æ¯ä¸æ¬¡è¯»åæèè®¡ç®ç»æï¼å°æ°æ®ç»å¾å­ä¸æ¥èµ·ä¸ä¸ªå¸¦ç¼å·çææä¹çåå­ï¼\n3. å¹¶å°å½åå·¥ä½è·¯å¾åå¾çåå­æ¼æ¥èµ·æ¥è¾åºå¾ççç»å¯¹è·¯å¾è¾åºåºæ¥ã\n4. å¦æå­å¨æä»¶ä¹è¦ä½¿ç¨utf-8ç¼ç å­å¨ã\n\"\"\"\n```\n\n**Key requirements:**\n- Must use `ExecutePythonCodeTool` for implementation\n- Save visualizations with numbered, meaningful filenames\n- Output absolute paths for generated images\n- Use UTF-8 encoding for all file operations\n\n**Sources:** [src/agent/deep_research.py:120-128]()\n\n---\n\n## Code Entity Mapping: From User Intent to System Execution\n\n### Agent Orchestration Layer\n\n```mermaid\ngraph TB\n    subgraph \"User Layer\"\n        UQ[\"user_query(user_input)\"]\n    end\n    \n    subgraph \"Agent Core: deep_research.py\"\n        Init[\"memory.init_messages_with_system_prompt()\"]\n        Schema[\"tool.schema.get_tools_schema()\"]\n        LLMCall[\"llm.generate_assistant_output_append()\"]\n        CheckTool[\"llm.has_tool_call()\"]\n        Dispatch[\"action.call_tools_safely()\"]\n    end\n    \n    subgraph \"Tool Layer\"\n        PyTool[\"ExecutePythonCodeTool.run()\"]\n        TodoTool[\"RecursivePlanTreeTodoTool.run()\"]\n    end\n    \n    subgraph \"Execution Layer\"\n        SubProc[\"subprocess_python_executor.run_structured_in_subprocess()\"]\n        WorkDir[\"WORKING_DIR = wsm/1/g4-1\"]\n        ExecResult[\"ExecutionResult(status, stdout, stderr, globals)\"]\n    end\n    \n    UQ --> Init\n    UQ --> Schema\n    Init --> LLMCall\n    Schema --> LLMCall\n    LLMCall --> CheckTool\n    CheckTool -->|\"has tool_calls\"| Dispatch\n    Dispatch --> PyTool\n    Dispatch --> TodoTool\n    PyTool --> SubProc\n    SubProc --> WorkDir\n    SubProc --> ExecResult\n    ExecResult --> Dispatch\n    Dispatch --> LLMCall\n```\n\n**Sources:** [src/agent/deep_research.py:15-74](), [src/agent/action.py:1-60](), [src/tool/python_tool.py:1-100]()\n\n### Tool Execution Flow with Error Recovery\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Agent as deep_research.user_query\n    participant LLM as llm.generate_assistant_output_append\n    participant Action as action.call_tools_safely\n    participant PyTool as ExecutePythonCodeTool\n    participant SubProc as subprocess_python_executor\n    participant Logger as global_logger\n    \n    User->>Agent: Emergency response planning query\n    Agent->>LLM: Generate with tool schemas\n    LLM-->>Agent: tool_calls=[execute_python_code]\n    \n    Agent->>Action: call_tools_safely(tool_info)\n    Action->>PyTool: run(code_snippet)\n    PyTool->>SubProc: run_structured_in_subprocess()\n    \n    SubProc-->>PyTool: ExecutionResult(CRASHED)<br/>UnicodeDecodeError\n    PyTool-->>Action: Error message\n    Action->>Logger: Log execution failure\n    Action-->>Agent: Return error to messages\n    \n    Agent->>LLM: Generate with error context\n    LLM-->>Agent: Analysis + corrected code<br/>with encoding='utf-8'\n    \n    Agent->>Action: Retry with fixed code\n    Action->>PyTool: run(fixed_code)\n    PyTool->>SubProc: run_structured_in_subprocess()\n    SubProc-->>PyTool: ExecutionResult(SUCCESS)\n    PyTool-->>Action: Success + output\n    Action-->>Agent: Continue workflow\n```\n\n**Sources:** [src/agent/deep_research.py:32-65](), [src/agent/action.py:15-60](), [logs/global.log:132-290]()\n\n---\n\n## Common Patterns Across Use Cases\n\n### Pattern 1: Iterative Schema Understanding\n\n```mermaid\ngraph LR\n    Read[\"Read schema.json\"]\n    Parse[\"Parse JSON structure\"]\n    Extract[\"Extract entity definitions\"]\n    Model[\"Generate Pydantic models\"]\n    Validate[\"Validate against data files\"]\n    \n    Read --> Parse\n    Parse --> Extract\n    Extract --> Model\n    Model --> Validate\n    Validate -->|\"Schema mismatch\"| Extract\n```\n\n**Code pattern:**\n```python\n# Step 1: Read schema\nimport json\nwith open('schema.json', 'r', encoding='utf-8') as f:\n    schema = json.load(f)\n\n# Step 2: Extract entities\nentities = schema['definitions']  # or similar structure\n\n# Step 3: Build models (implied in task descriptions)\nfrom pydantic import BaseModel\nclass Task(BaseModel):\n    task_id: str\n    priority: int\n    # ... other fields from schema\n```\n\n**Sources:** [logs/global.log:1131-1423]()\n\n### Pattern 2: Batch Data Processing with Error Handling\n\n```python\nimport glob\nimport json\n\ndata_files = glob.glob('emergency_response_data_*.json')\ndatasets = []\n\nfor file_path in data_files:\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n            datasets.append(data)\n    except Exception as e:\n        print(f\"Error loading {file_path}: {e}\")\n\n# Aggregate statistics\nprint(f\"Loaded {len(datasets)} datasets\")\n```\n\n**Sources:** [logs/global.log:1160-1180]()\n\n### Pattern 3: Visualization with Path Output\n\n```python\nimport matplotlib.pyplot as plt\nimport os\n\n# Generate visualization\nplt.figure(figsize=(12, 8))\n# ... plotting code ...\n\n# Save with absolute path\ncurrent_dir = os.getcwd()\nfilename = \"01_data_distribution.png\"\noutput_path = os.path.join(current_dir, filename)\nplt.savefig(output_path, dpi=300, bbox_inches='tight')\n\nprint(f\"Visualization saved: {output_path}\")\n```\n\n**Sources:** [src/agent/deep_research.py:121-123]()\n\n---\n\n## Tool Call Patterns in Practice\n\n### RecursivePlanTreeTodoTool Usage\n\n**When the agent needs to:**\n- Establish initial research plan\n- Decompose complex problems\n- Track task dependencies\n- Maintain status across multiple steps\n\n**JSON structure from logs:**\n\n```json\n{\n  \"tool_call_purpose\": \"å»ºç«åå§åæè®¡åæ ï¼æç¡®ç ç©¶ç®æ åä»»å¡åè§£æ¹å\",\n  \"recursive_plan_tree\": {\n    \"core_goal\": \"åºäºæä¾çåºæ¥ææ´æ°æ®...è®¾è®¡å¯æµè¯çå¤æºè½ä½ååè°åº¦ç®æ³\",\n    \"tree_nodes\": [\n      {\n        \"task_id\": \"T1\",\n        \"task_name\": \"è¯»åå¹¶è§£æ schema.json æä»¶\",\n        \"status\": \"pending\",\n        \"dependencies\": null,\n        \"references\": [\"./schema.json\"]\n      },\n      {\n        \"task_id\": \"T2\",\n        \"task_name\": \"è¯»åå¹¶æ£æ¥åºæ¥ååºæ°æ®æä»¶\",\n        \"status\": \"pending\",\n        \"dependencies\": [\"è¯»åå¹¶è§£æ schema.json æä»¶\"],\n        \"references\": [\"./emergency_response_data_01.json\", \"...\"]\n      }\n    ]\n  }\n}\n```\n\n**Sources:** [logs/utils.log:56-62](), [logs/global.log:908-1125]()\n\n### ExecutePythonCodeTool Usage\n\n**When the agent needs to:**\n- Load and parse data files\n- Perform calculations\n- Generate visualizations\n- Test algorithms\n- Validate data structures\n\n**Example from error logs:**\n\n```json\n{\n  \"tool_call_purpose\": \"è¯»åå¹¶è§£æ schema.json æä»¶ï¼äºè§£æ°æ®ç»æå®ä¹\",\n  \"python_code_snippet\": \"import json\\n\\nwith open('schema.json', 'r') as file:\\n    schema = json.load(file)\\n    print(json.dumps(schema, indent=2))\"\n}\n```\n\n**Sources:** [logs/utils.log:127-129]()\n\n---\n\n## Performance Characteristics\n\n### Execution Times (from logs)\n\n| Operation | Time (ms) | Source |\n|-----------|-----------|--------|\n| LLM call (planning) | 41,636 | [logs/global.log:892]() |\n| Schema generation | 8 | [logs/global.log:189]() |\n| Tool dispatch | 35 | [logs/global.log:1574]() |\n| Memory initialization | 2 | [logs/global.log:115]() |\n\n### Working Directory Structure\n\n```\nwsm/\nâââ 1/\n    âââ g4-1/           # WORKING_DIR for subprocess execution\n        âââ schema.json\n        âââ emergency_response_data_01.json\n        âââ ...\n        âââ output/     # Generated visualizations and results\n```\n\n**Sources:** [logs/utils.log:130-131](), [src/runtime/subprocess_python_executor.py:44]()\n\n---\n\n## Summary: Key Takeaways\n\n1. **Multi-step reasoning**: The agent breaks down complex tasks into 10+ subtasks with dependencies\n2. **Error recovery**: Automatically analyzes failures (e.g., UnicodeDecodeError) and suggests fixes\n3. **Tool orchestration**: Seamlessly switches between planning (TodoTool) and execution (PythonTool)\n4. **State persistence**: Maintains global variables across multiple code executions\n5. **Comprehensive logging**: All operations tracked in [logs/global.log]() and [logs/utils.log]()\n\n**Sources:** [src/agent/deep_research.py:15-129](), [logs/utils.log:1-296](), [logs/global.log:1-1792]()\n\n---\n\n# Page: Emergency Response Planning Example\n\n# Emergency Response Planning Example\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/agent/tool/python_tool.py](src/agent/tool/python_tool.py)\n- [src/agent/tool/todo_tool.py](src/agent/tool/todo_tool.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page demonstrates a real-world use case from the system logs showing how the algo_agent processes a complex emergency response planning problem. The example illustrates the complete agent workflow: from receiving a user query about emergency data files, to building a hierarchical task plan, to executing Python code for data analysis and algorithm testing.\n\nThis example specifically showcases:\n- Multi-step research planning using the **RecursivePlanTreeTodoTool**\n- Handling of file I/O and data loading with **ExecutePythonCodeTool**\n- Error recovery from Unicode encoding and serialization issues\n- Task decomposition for multi-agent scheduling algorithm development\n\nFor details on the tools used, see [Python Code Execution Tool](#4.2) and [Recursive Task Planning Tool](#4.3). For troubleshooting the specific errors encountered, see [Common Execution Errors](#10.1).\n\n---\n\n## Problem Domain Overview\n\nThe user provided a set of emergency response scenario data files designed to test multi-agent coordination algorithms for disaster relief material transportation:\n\n### Data Files Structure\n\n| File | Description |\n|------|-------------|\n| `schema.json` | Complete data structure schema defining entities and relationships |\n| `emergency_response_data_01.json` to `emergency_response_data_05.json` | Five randomly generated emergency rescue scenarios |\n\n### Data Characteristics\n\nThe emergency response dataset exhibits the following properties:\n\n**Realism**\n- Coordinates generated within real geographic boundaries\n- Multiple disaster scenario types (earthquakes, floods, fires, etc.)\n- Realistic material and transport carrier attributes\n\n**Diversity**\n- Multiple task types with varying priority levels\n- Different transportation carrier types (trucks, drones, helicopters)\n- Randomly generated risk points and route obstacles\n\n**Completeness**\n- All core entities and their relationships included\n- Various state combinations covered (pending, in-progress, completed, failed)\n- Task feedback and rescue performance metrics\n\n**Extensibility**\n- Clear code structure for adding new fields or entity types\n- Adjustable generation strategies for specific scenarios\n\n### Intended Use Cases\n\n1. **Algorithm Testing**: Task assignment, path planning, ETA prediction algorithms\n2. **System Integration**: Emergency material transport scheduling system validation\n3. **Scenario Simulation**: Disaster rescue exercises and decision support system testing\n\nSources: [logs/utils.log:1-124](), [logs/global.log:1-180]()\n\n---\n\n## Agent Decision Process Flow\n\nThe following diagram shows how the agent processed the emergency response planning query through multiple decision points:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Agent as \"user_query function\"\n    participant Memory as \"init_messages_with_system_prompt\"\n    participant LLM as \"qwen-plus LLM\"\n    participant TodoTool as \"RecursivePlanTreeTodoTool\"\n    participant PythonTool as \"ExecutePythonCodeTool\"\n    \n    User->>Agent: \"å¶å®ç ç©¶è®¡ååæåºæ¥ææ´æ°æ®\"\n    Agent->>Memory: Initialize with system prompt + user query\n    Memory-->>Agent: messages list\n    \n    Agent->>LLM: \"generate_assistant_output_append()\"\n    Note over LLM: Analyze query requirements:<br/>- Data structure analysis<br/>- Algorithm testing<br/>- Scenario simulation\n    \n    LLM->>TodoTool: \"recursive_plan_tree_todo\"\n    Note over TodoTool: Create hierarchical task tree:<br/>T1-T12 tasks with dependencies\n    TodoTool-->>Agent: Task tree created\n    \n    Agent->>LLM: \"generate_assistant_output_append()\"\n    LLM->>PythonTool: \"execute_python_code\" x3\n    Note over PythonTool: Attempt to read:<br/>- schema.json<br/>- emergency_response_data_01.json<br/>- emergency_response_data_02.json\n    \n    PythonTool-->>Agent: UnicodeDecodeError (gbk codec)\n    Note over Agent: Error detected in all 3 executions\n    \n    Agent->>LLM: \"generate_assistant_output_append()\"\n    Note over LLM: Analyze errors:<br/>1. UTF-8 encoding issue<br/>2. File handle serialization issue\n    \n    LLM->>PythonTool: \"execute_python_code\" (corrected)\n    Note over PythonTool: Add encoding='utf-8'<br/>Close file properly\n    PythonTool-->>Agent: Success\n```\n\n**Key Decision Points:**\n\n1. **Initial Analysis** (Lines 309-495 in global.log): LLM receives user query and available tool schemas\n2. **Task Planning** (Line 57 in utils.log): Agent creates 12-task hierarchical plan via `RecursivePlanTreeTodoTool`\n3. **Execution Attempts** (Lines 127-289 in utils.log): Three parallel Python code executions attempted\n4. **Error Detection** (Line 289 in utils.log): All three executions crash with encoding errors\n5. **Error Analysis** (Line 289 in utils.log): LLM analyzes root causes and proposes fixes\n\nSources: [logs/utils.log:55-289](), [logs/global.log:309-495]()\n\n---\n\n## Hierarchical Task Planning\n\nThe agent decomposed the emergency response analysis into a 12-task recursive plan tree. Below is the structure and task dependencies:\n\n```mermaid\ngraph TB\n    T1[\"T1: è¯»åå¹¶è§£æ schema.json æä»¶\"]\n    T2[\"T2: è¯»åå¹¶æ£æ¥åºæ¥ååºæ°æ®æä»¶\"]\n    T3[\"T3: æå»ºæ°æ®æ¨¡åç±»\"]\n    T4[\"T4: å®ç°æ°æ®å è½½å¨\"]\n    T5[\"T5: åææ°æ®åå¸ç¹å¾\"]\n    T6[\"T6: æ¨¡æåä¸ææ´åºæ¯\"]\n    T7[\"T7: è®¾è®¡å¨æè°åº¦ç®æ³æ¡æ¶\"]\n    T8[\"T8: å®ç°åºç¡ææ´¾ç®æ³ï¼è´ªå¿ç­ç¥ï¼\"]\n    T9[\"T9: å®ç°é«çº§ææ´¾ç®æ³ï¼å æè¯åæ¨¡åï¼\"]\n    T10[\"T10: è®¾è®¡è·¯å¾éè§åæºå¶\"]\n    T11[\"T11: è¯ä¼°ç®æ³æ§è½ææ \"]\n    T12[\"T12: è¿è¡å¤ç»å¯¹æ¯å®éª\"]\n    \n    T1 --> T2\n    T1 --> T3\n    T2 --> T4\n    T3 --> T4\n    T4 --> T5\n    T4 --> T6\n    T3 --> T7\n    T7 --> T8\n    T8 --> T9\n    T9 --> T10\n    T8 --> T11\n    T9 --> T11\n    T11 --> T12\n    \n    style T1 fill:#f9f9f9\n    style T2 fill:#f9f9f9\n    style T3 fill:#f9f9f9\n    style T7 fill:#f9f9f9\n```\n\n### Task Breakdown Details\n\n**Phase 1: Data Understanding (T1-T4)**\n\n| Task ID | Task Name | Description | Dependencies | Status |\n|---------|-----------|-------------|--------------|--------|\n| T1 | è¯»åå¹¶è§£æ schema.json æä»¶ | Load schema.json, extract core entities (Task, Carrier, Resource, Location, Path, RiskPoint) | None | pending |\n| T2 | è¯»åå¹¶æ£æ¥åºæ¥ååºæ°æ®æä»¶ | Batch read data_01.json through data_05.json, validate against schema | T1 | pending |\n| T3 | æå»ºæ°æ®æ¨¡åç±» | Define Python classes using dataclass/pydantic based on schema | T1 | pending |\n| T4 | å®ç°æ°æ®å è½½å¨ | Write functions to load all JSON files into memory objects | T2, T3 | pending |\n\n**Phase 2: Data Analysis (T5-T6)**\n\n| Task ID | Task Name | Description | Research Directions |\n|---------|-----------|-------------|---------------------|\n| T5 | åææ°æ®åå¸ç¹å¾ | Statistical analysis of task priorities, disaster types, carrier distributions | Generate charts, calculate descriptive statistics |\n| T6 | æ¨¡æåä¸ææ´åºæ¯ | Select one data file, construct static rescue scenario, manually assign tasks | Define task-carrier matching rules, record unassigned tasks |\n\n**Phase 3: Algorithm Development (T7-T10)**\n\n| Task ID | Task Name | Key Approach |\n|---------|-----------|--------------|\n| T7 | è®¾è®¡å¨æè°åº¦ç®æ³æ¡æ¶ | Event-driven vs time-step simulation, state machine for carrier behavior |\n| T8 | å®ç°åºç¡ææ´¾ç®æ³ï¼è´ªå¿ç­ç¥ï¼ | Greedy assignment: select nearest available carrier, use Haversine distance formula |\n| T9 | å®ç°é«çº§ææ´¾ç®æ³ï¼å æè¯åæ¨¡åï¼ | Multi-factor scoring: distance, priority, ETA, risk coefficient with normalized weights |\n| T10 | è®¾è®¡è·¯å¾éè§åæºå¶ | Integrate A* or Dijkstra algorithm, dynamic graph weight updates for risk points |\n\n**Phase 4: Evaluation (T11-T12)**\n\n| Task ID | Task Name | Metrics |\n|---------|-----------|---------|\n| T11 | è¯ä¼°ç®æ³æ§è½ææ  | Task completion rate, average response delay, carrier utilization, high-priority task success rate |\n| T12 | è¿è¡å¤ç»å¯¹æ¯å®éª | Controlled experiments, cross-algorithm comparison with baseline, result visualization |\n\nThe `RecursivePlanTreeTodoTool` created this structure by instantiating a `RecursivePlanTree` object with `tree_nodes` containing nested `RecursivePlanTreeNode` instances. Each node includes:\n- `task_id`: Unique identifier (T1-T12)\n- `task_name`: Globally unique name referenced by dependencies\n- `status`: TaskStatus enum (pending/processing/completed/failed/retry/skipped)\n- `dependencies`: List of task_name values that must complete first\n- `research_directions`: Optional exploration directions for complex tasks\n\nSources: [logs/utils.log:57-58](), [src/agent/tool/todo_tool.py:1-133](), [src/memory/tree_todo/schemas.py]()\n\n---\n\n## Code Execution Flow and Error Handling\n\nThe agent attempted to execute Python code to read the data files, encountering multiple errors before succeeding. This demonstrates the system's error recovery capabilities.\n\n### Execution Attempt Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Tool Call from LLM\"\n        TC1[\"tool_calls[0]: execute_python_code<br/>purpose: è¯»å schema.json\"]\n        TC2[\"tool_calls[1]: execute_python_code<br/>purpose: è¯»å data_01.json\"]\n        TC3[\"tool_calls[2]: execute_python_code<br/>purpose: è¯»å data_02.json\"]\n    end\n    \n    subgraph \"ExecutePythonCodeTool\"\n        EPT[\"python_tool.py:run()\"]\n        WS[\"workspace.get_arg_globals()\"]\n    end\n    \n    subgraph \"Subprocess Execution\"\n        SE1[\"subprocess_python_executor<br/>run_structured_in_thread<br/>PID: 11336\"]\n        SE2[\"subprocess_python_executor<br/>run_structured_in_thread<br/>PID: 9308\"]\n        SE3[\"subprocess_python_executor<br/>run_structured_in_thread<br/>PID: 10528\"]\n    end\n    \n    subgraph \"Error Results\"\n        ER1[\"ExecutionResult<br/>status: CRASHED<br/>UnicodeDecodeError<br/>PickleError\"]\n        ER2[\"ExecutionResult<br/>status: CRASHED<br/>UnicodeDecodeError<br/>PickleError\"]\n        ER3[\"ExecutionResult<br/>status: CRASHED<br/>UnicodeDecodeError<br/>PickleError\"]\n    end\n    \n    TC1 --> EPT\n    TC2 --> EPT\n    TC3 --> EPT\n    \n    EPT --> WS\n    WS --> SE1\n    WS --> SE2\n    WS --> SE3\n    \n    SE1 --> ER1\n    SE2 --> ER2\n    SE3 --> ER3\n```\n\n### Error Cascade Analysis\n\nThe execution failures occurred in three stages, each revealing a different aspect of the error handling system:\n\n**Stage 1: Initial UnicodeDecodeError**\n\n```python\n# Original code (lines 127-146 in utils.log)\nimport json\n\nwith open('schema.json', 'r') as file:\n    schema = json.load(file)\n    print(json.dumps(schema, indent=2))\n```\n\n**Error Stack Trace:**\n```\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265: illegal multibyte sequence\n  File \"subprocess_python_executor.py\", line 44, in _worker_with_pipe\n    exec(command, _globals, _locals)\n  File \"<string>\", line 4, in <module>\n  File \"json/__init__.py\", line 293, in load\n    return loads(fp.read(), ...)\n```\n\n**Root Cause:** Windows default encoding is `gbk`, but JSON files were UTF-8 encoded with non-ASCII characters (Chinese text describing disaster scenarios).\n\n**Stage 2: Serialization Failure**\n\nAfter the initial error, the subprocess executor attempted to create an `ExecutionResult` object to report the failure, but encountered a second error:\n\n```\nTypeError: cannot pickle 'TextIOWrapper' instances\n  File \"subprocess_python_executor.py\", line 54, in _worker_with_pipe\n    res = ExecutionResult(...)\n  File \"schemas.py\", line 83, in field_validate_globals\n    return workspace.filter_and_deepcopy_globals(value)\n  File \"workspace.py\", line 55, in filter_and_deepcopy_globals\n    filtered_dict[key] = copy.deepcopy(value)\n```\n\n**Root Cause:** The file handle object (`file`) remained in the local namespace. When `ExecutionResult` tried to serialize `_globals` to return it to the parent process, the `copy.deepcopy()` operation in `workspace.filter_and_deepcopy_globals()` failed because file handles cannot be pickled.\n\n**Stage 3: Unbound Variable Error**\n\nThe exception handler in the subprocess worker attempted to send an error result but encountered:\n\n```\nUnboundLocalError: cannot access local variable 'res' where it is not associated with a value\n  File \"subprocess_python_executor.py\", line 66, in _worker_with_pipe\n    child_conn.send((_PipeType.RESULT, res))\n```\n\n**Root Cause:** The exception handler assumed `res` was defined, but the previous exception occurred during `res` construction, leaving the variable unbound.\n\nSources: [logs/utils.log:130-288](), [src/runtime/subprocess_python_executor.py:44-66](), [src/runtime/workspace.py:55]()\n\n### Agent Error Analysis Response\n\nAfter receiving three crashed execution results, the LLM analyzed the errors and proposed a corrected solution:\n\n```mermaid\ngraph LR\n    subgraph \"Error Detection\"\n        E1[\"3x ExecutionResult<br/>status: CRASHED\"]\n        E2[\"Extract error patterns:<br/>- UnicodeDecodeError<br/>- PickleError<br/>- UnboundLocalError\"]\n    end\n    \n    subgraph \"Root Cause Analysis\"\n        RC1[\"Issue 1:<br/>Missing encoding parameter<br/>Windows default: gbk<br/>Files encoded: utf-8\"]\n        RC2[\"Issue 2:<br/>File handle in globals<br/>TextIOWrapper not picklable<br/>Blocks serialization\"]\n    end\n    \n    subgraph \"Solution Strategy\"\n        S1[\"Add encoding='utf-8'<br/>to all open() calls\"]\n        S2[\"Ensure with block closes file<br/>Don't expose file handle<br/>Return only serializable data\"]\n    end\n    \n    E1 --> E2\n    E2 --> RC1\n    E2 --> RC2\n    RC1 --> S1\n    RC2 --> S2\n```\n\n**Corrected Code:**\n```python\nimport json\n\ntry:\n    with open('schema.json', 'r', encoding='utf-8') as f:\n        schema = json.load(f)\n    print(json.dumps(schema, indent=2, ensure_ascii=False))\nexcept Exception as e:\n    print(f\"Error reading schema.json: {e}\")\n```\n\n**Key Improvements:**\n1. Explicit `encoding='utf-8'` parameter\n2. File variable renamed to `f` (shorter scope)\n3. File closed before `print()` statement\n4. `ensure_ascii=False` for proper Unicode output\n5. Exception handling for graceful error reporting\n\nSources: [logs/utils.log:289-290](), [src/agent/tool/python_tool.py:41-50]()\n\n---\n\n## Code-Level Implementation Mapping\n\nThis section maps the high-level emergency response planning workflow to concrete code entities in the system.\n\n### Tool Invocation Pipeline\n\n```mermaid\ngraph TB\n    subgraph \"Agent Layer\"\n        UQ[\"user_query()<br/>deep_research.py:user_query\"]\n        GAOA[\"generate_assistant_output_append()<br/>llm.py:generate_assistant_output_append\"]\n        CTS[\"call_tools_safely()<br/>tool_coordinator.py:call_tools_safely\"]\n    end\n    \n    subgraph \"Tool Layer\"\n        RPT[\"RecursivePlanTreeTodoTool<br/>todo_tool.py:RecursivePlanTreeTodoTool\"]\n        EPT[\"ExecutePythonCodeTool<br/>python_tool.py:ExecutePythonCodeTool\"]\n        RPTRUN[\"run()<br/>todo_tool.py:28-36\"]\n        EPTRUN[\"run()<br/>python_tool.py:41-50\"]\n    end\n    \n    subgraph \"Execution Layer\"\n        RSTIT[\"run_structured_in_thread()<br/>subthread_python_executor.py\"]\n        GAG[\"get_arg_globals()<br/>workspace.py\"]\n        AOG[\"append_out_globals()<br/>workspace.py\"]\n    end\n    \n    subgraph \"Data Models\"\n        RPT_MODEL[\"RecursivePlanTree<br/>tree_todo/schemas.py\"]\n        RPTN_MODEL[\"RecursivePlanTreeNode<br/>tree_todo/schemas.py\"]\n        ER_MODEL[\"ExecutionResult<br/>runtime/schemas.py\"]\n    end\n    \n    UQ --> GAOA\n    GAOA --> CTS\n    CTS --> RPT\n    CTS --> EPT\n    \n    RPT --> RPTRUN\n    EPT --> EPTRUN\n    \n    RPTRUN --> RPT_MODEL\n    RPTRUN --> RPTN_MODEL\n    \n    EPTRUN --> GAG\n    EPTRUN --> RSTIT\n    RSTIT --> ER_MODEL\n    ER_MODEL --> AOG\n    AOG --> EPTRUN\n```\n\n### Key Function Execution Sequence\n\n| Step | Function | File | Purpose |\n|------|----------|------|---------|\n| 1 | `user_query()` | [src/agent/deep_research.py]() | Entry point for user query processing |\n| 2 | `init_messages_with_system_prompt()` | [src/agent/memory.py]() | Initialize conversation with system prompt |\n| 3 | `get_tools_schema()` | [src/agent/tool/schema.py]() | Generate JSON schemas for available tools |\n| 4 | `generate_assistant_output_append()` | [src/agent/llm.py]() | Request LLM response with tool calling |\n| 5 | `call_tools_safely()` | [src/agent/tool_coordinator.py]() | Dispatch tool calls from LLM |\n| 6 | `RecursivePlanTreeTodoTool.run()` | [src/agent/tool/todo_tool.py:28-36]() | Execute task tree management |\n| 7 | `ExecutePythonCodeTool.run()` | [src/agent/tool/python_tool.py:41-50]() | Execute Python code snippet |\n| 8 | `workspace.get_arg_globals()` | [src/runtime/workspace.py]() | Retrieve persistent global variables |\n| 9 | `run_structured_in_thread()` | [src/runtime/subthread_python_executor.py]() | Execute code in isolated thread |\n| 10 | `workspace.append_out_globals()` | [src/runtime/workspace.py]() | Persist updated global variables |\n\nSources: [src/agent/deep_research.py](), [src/agent/tool/python_tool.py:41-50](), [src/agent/tool/todo_tool.py:28-36](), [logs/global.log:1-695]()\n\n---\n\n## Data Schema Entities\n\nThe emergency response domain involves several interconnected entities. While the actual `schema.json` file was not successfully loaded in the logged session, the user description and task planning reveal the expected structure:\n\n### Core Entity Types\n\n```mermaid\nerDiagram\n    TASK ||--o{ CARRIER : \"assigned_to\"\n    TASK ||--|| RESOURCE : \"requires\"\n    TASK ||--o{ LOCATION : \"from/to\"\n    CARRIER ||--o{ PATH : \"follows\"\n    PATH ||--o{ RISK_POINT : \"contains\"\n    TASK ||--o{ TASK_FEEDBACK : \"has\"\n    \n    TASK {\n        string task_id PK\n        string task_type\n        string priority\n        string status\n        datetime created_at\n        string disaster_type\n    }\n    \n    CARRIER {\n        string carrier_id PK\n        string carrier_type\n        float capacity\n        string status\n        coordinate location\n    }\n    \n    RESOURCE {\n        string resource_id PK\n        string resource_type\n        float quantity\n        string unit\n    }\n    \n    LOCATION {\n        string location_id PK\n        coordinate lat_lng\n        string address\n        string location_type\n    }\n    \n    PATH {\n        string path_id PK\n        array waypoints\n        float distance\n        float estimated_time\n    }\n    \n    RISK_POINT {\n        string risk_id PK\n        coordinate location\n        string risk_type\n        float risk_level\n    }\n    \n    TASK_FEEDBACK {\n        string feedback_id PK\n        datetime timestamp\n        string status_update\n        float progress_percent\n    }\n```\n\n### RecursivePlanTreeNode Schema Mapping\n\nThe `RecursivePlanTreeNode` schema used by the agent mirrors the hierarchical structure needed for algorithm planning:\n\n```python\n# From src/memory/tree_todo/schemas.py\nclass RecursivePlanTreeNode(BaseModel):\n    task_id: str                              # \"T1\", \"T2\", etc.\n    task_name: str                            # Globally unique name\n    description: str = \"\"                     # Detailed execution requirements\n    status: TaskStatus = TaskStatus.PENDING   # pending/processing/completed/failed/retry/skipped\n    dependencies: Optional[List[str]] = None  # References to task_name values\n    children: Optional[List['RecursivePlanTreeNode']] = None  # Nested sub-tasks\n    research_directions: Optional[List[str]] = None\n    output: str = \"\"                          # Execution result\n```\n\nThis schema enables:\n- **Hierarchical decomposition**: Tasks can contain nested sub-tasks via `children`\n- **Dependency tracking**: `dependencies` list prevents premature execution\n- **Status management**: `TaskStatus` enum tracks progress through the workflow\n- **Persistent state**: `output` field stores execution results for subsequent tasks\n\nSources: [src/memory/tree_todo/schemas.py](), [logs/utils.log:57-58]()\n\n---\n\n## Lessons and Patterns\n\nThis emergency response planning example demonstrates several key patterns in the algo_agent system:\n\n### 1. Hierarchical Task Decomposition Pattern\n\n**Pattern:** Break complex problems into a tree of dependent sub-tasks\n\n**Implementation:**\n```python\n# From RecursivePlanTreeTodoTool\nRecursivePlanTree(\n    core_goal=\"åºäºæä¾çåºæ¥ææ´æ°æ®ï¼æ·±å¥çè§£æ°æ®ç»æå¹¶è®¾è®¡å¯æµè¯çå¤æºè½ä½ååè°åº¦ç®æ³\",\n    tree_nodes=[\n        RecursivePlanTreeNode(\n            task_name=\"è¯»åå¹¶è§£æ schema.json æä»¶\",\n            dependencies=None  # Root task\n        ),\n        RecursivePlanTreeNode(\n            task_name=\"æå»ºæ°æ®æ¨¡åç±»\",\n            dependencies=[\"è¯»åå¹¶è§£æ schema.json æä»¶\"]  # Depends on T1\n        )\n    ]\n)\n```\n\n**Benefits:**\n- Maintains clear execution order through dependency chains\n- Enables parallel execution of independent tasks (T2 and T3 both depend only on T1)\n- Provides checkpoints for error recovery\n\n### 2. Stateful Code Execution Pattern\n\n**Pattern:** Persist variables across multiple code executions using workspace globals\n\n**Implementation:**\n```python\n# From python_tool.py:41-50\ndef run(self) -> str:\n    execution_context: Optional[Dict[str, Any]] = workspace.get_arg_globals()\n    exec_result: ExecutionResult = subthread_python_executor.run_structured_in_thread(\n        command=self.python_code_snippet,\n        _globals=execution_context,  # Pass previous state\n        timeout=self.timeout\n    )\n    workspace.append_out_globals(exec_result.arg_globals)  # Persist new state\n    return exec_result.ret_tool2llm\n```\n\n**Benefits:**\n- Sequential code snippets can build on previous definitions\n- Enables Jupyter-notebook-like interactive development\n- Avoids redundant imports and setup in each execution\n\n### 3. Multi-Level Error Recovery Pattern\n\n**Pattern:** Cascade error information through multiple abstraction layers for LLM analysis\n\n**Error Flow:**\n1. **Execution Layer** (`subprocess_python_executor.py`): Captures raw exceptions\n2. **Result Schema** (`ExecutionResult`): Structures error information\n3. **Tool Layer** (`ExecutePythonCodeTool.run()`): Returns formatted error message\n4. **Agent Layer** (`user_query()`): Passes error to LLM for analysis\n5. **LLM Response**: Diagnoses root cause and proposes fix\n\n**Implementation in ExecutionResult:**\n```python\n# From runtime/schemas.py\nclass ExecutionResult(BaseModel):\n    status: ExecutionStatus  # SUCCESS/FAILURE/TIMEOUT/CRASHED\n    stdout: str              # Captured output\n    stderr: str              # Error messages\n    exception: str           # Formatted traceback\n```\n\n### 4. Encoding Safety Pattern\n\n**Anti-Pattern (Fails):**\n```python\nwith open('file.json', 'r') as f:  # Uses platform default encoding\n    data = json.load(f)\n```\n\n**Correct Pattern:**\n```python\nwith open('file.json', 'r', encoding='utf-8') as f:  # Explicit encoding\n    data = json.load(f)\n```\n\n**Why It Matters:**\n- Windows default encoding is `cp1252` or `gbk` depending on locale\n- JSON files typically use UTF-8 encoding\n- Non-ASCII characters (Chinese, emoji, etc.) cause `UnicodeDecodeError`\n\n### 5. Serialization Safety Pattern\n\n**Anti-Pattern (Fails):**\n```python\nfile = open('data.json', 'r', encoding='utf-8')\ndata = json.load(file)\n# 'file' object remains in namespace, blocks serialization\n```\n\n**Correct Pattern:**\n```python\nwith open('data.json', 'r', encoding='utf-8') as f:\n    data = json.load(f)\n# 'f' goes out of scope, only 'data' remains\n```\n\n**Why It Matters:**\n- `ExecutionResult` serializes `_globals` to transfer state between processes\n- File handles (`TextIOWrapper`) cannot be pickled\n- Context managers ensure resources are released before serialization\n\n### 6. Parallel Tool Invocation Pattern\n\nThe LLM can request multiple tool calls in a single response:\n\n```python\n# From logs showing tool_calls array\ntool_calls=[\n    ChatCompletionMessageFunctionToolCall(id='call_3085f1f75d534390a7c2b7', \n                                          function=Function(name='execute_python_code', ...)),\n    ChatCompletionMessageFunctionToolCall(id='call_ab025bb2bb3e430aaefd92',\n                                          function=Function(name='execute_python_code', ...)),\n    ChatCompletionMessageFunctionToolCall(id='call_92fd80e050c34c78a18ea9',\n                                          function=Function(name='execute_python_code', ...))\n]\n```\n\n**Benefits:**\n- Read multiple files concurrently\n- Reduces round trips to LLM\n- All results available for next reasoning step\n\n**Risks:**\n- If one execution fails, all may fail with same error\n- Harder to debug than sequential execution\n\nSources: [src/agent/tool/python_tool.py:41-50](), [src/runtime/workspace.py](), [logs/utils.log:127-290]()\n\n---\n\n## Performance Considerations\n\n### Subprocess Creation Overhead\n\nEach `ExecutePythonCodeTool` invocation spawns a new subprocess:\n\n```python\n# From subprocess_python_executor.py\np = Process(target=_worker_with_pipe, args=(parent_conn, child_conn, ...))\np.start()\n```\n\n**Metrics from logs:**\n- Process ID 11336: Started at 03:39:11.529, crashed at 03:39:11.770 (241ms)\n- Process ID 9308: Started at 03:39:12.906, crashed at 03:39:13.080 (174ms)\n- Process ID 10528: Started at 03:39:14.170, crashed at 03:39:14.370 (200ms)\n\n**Implications:**\n- ~200ms overhead per execution (including crash handling)\n- Three parallel executions took ~3 seconds total (serial bottleneck)\n- Successful execution would have been faster (no exception handling overhead)\n\n### LLM Latency\n\nFrom the logs, LLM response generation times:\n- First response (task tree creation): 42 seconds (17:51:32 â 17:52:14)\n- Second response (error analysis): 17 seconds (03:39:14 â 03:39:31)\n\n**Optimization Opportunities:**\n- Cache common task tree patterns for similar queries\n- Use streaming responses for faster user feedback\n- Implement speculative execution for predictable next steps\n\nSources: [logs/utils.log:127-289](), [logs/global.log:309-501]()\n\n---\n\n## Summary\n\nThe emergency response planning example demonstrates the algo_agent's capability to:\n\n1. **Decompose Complex Problems**: Created a 12-task hierarchical plan with clear dependencies\n2. **Execute Code Iteratively**: Attempted multiple Python executions with state persistence\n3. **Recover from Errors**: Analyzed encoding and serialization failures, proposed corrections\n4. **Bridge Language and Code**: Translated natural language requirements into concrete Python implementations\n5. **Maintain Context**: Tracked conversation history, tool results, and error states across multiple LLM calls\n\nThis use case exemplifies the core value proposition: an autonomous agent that can reason about data analysis workflows, write and debug code, and iteratively approach solutions to complex multi-step problems.\n\nFor similar examples with different problem domains, see [Data Processing and Visualization](#7.2) and [Geographic Data Processing](#7.3).\n\nSources: [logs/utils.log:1-296](), [logs/global.log:1-695](), [src/agent/tool/python_tool.py](), [src/agent/tool/todo_tool.py]()\n\n---\n\n# Page: Data Processing and Visualization\n\n# Data Processing and Visualization\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [src/runtime/before_thread/plt_back_chinese.py](src/runtime/before_thread/plt_back_chinese.py)\n- [src/runtime/subthread_python_executor.py](src/runtime/subthread_python_executor.py)\n- [tests/playground/gen/g7/01_valid7.py](tests/playground/gen/g7/01_valid7.py)\n- [tests/playground/gen/g7/02_plt.py](tests/playground/gen/g7/02_plt.py)\n- [tests/playground/gen/g7/03_valid.py](tests/playground/gen/g7/03_valid.py)\n- [tests/playground/gen/g7/04_plt.py](tests/playground/gen/g7/04_plt.py)\n- [tests/playground/gen/g7/05.dump_.py](tests/playground/gen/g7/05.dump_.py)\n- [tests/playground/gen/g7/06.dump_all.py](tests/playground/gen/g7/06.dump_all.py)\n- [tests/playground/gen/g7/metro-draw-data.json](tests/playground/gen/g7/metro-draw-data.json)\n- [tests/playground/gen/g7/metro-draw-schema.json](tests/playground/gen/g7/metro-draw-schema.json)\n- [tests/playground/gen/g7/subway.md](tests/playground/gen/g7/subway.md)\n- [tests/playground/gen/g9/beijing_scenic_spot_schema_with_play_hours.json](tests/playground/gen/g9/beijing_scenic_spot_schema_with_play_hours.json)\n- [tests/playground/gen/g9/beijing_scenic_spot_validated_data_with_play_hours.json](tests/playground/gen/g9/beijing_scenic_spot_validated_data_with_play_hours.json)\n- [tests/playground/gen/g9/g9.py](tests/playground/gen/g9/g9.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page demonstrates how the algo_agent system handles data processing and visualization tasks through concrete examples. It covers Pydantic-based data modeling, validation workflows, schema generation, data transformation pipelines, and matplotlib visualization with multi-language support. These examples showcase the system's capability to process structured data from various sources (CSV, JSON) and generate visual outputs.\n\nFor emergency response planning use cases, see [Emergency Response Planning Example](#7.1). For geographic data processing with street networks, see [Geographic Data Processing](#7.3).\n\n---\n\n## Pydantic Data Modeling\n\n### Scenic Spot Data Model\n\nThe system uses Pydantic for robust data validation and schema generation. The Beijing scenic spots example demonstrates enum validation, field validators, and CSV parsing.\n\n```mermaid\ngraph TB\n    CSV[\"åäº¬æ¯ç¹ç­çº§åå»ºè®®æ¸¸ç©æ¶é¿.csv<br/>Raw CSV Data\"]\n    READER[\"read_csv_to_pydantic()<br/>CSV Parser\"]\n    MODEL[\"ScenicSpot Model<br/>Pydantic BaseModel\"]\n    ENUMS[\"StarRatingEnum<br/>OpenStatusEnum<br/>Field Validators\"]\n    SCHEMA[\"beijing_scenic_spot_schema_with_play_hours.json\"]\n    DATA[\"beijing_scenic_spot_validated_data_with_play_hours.json\"]\n    \n    CSV --> READER\n    READER --> MODEL\n    MODEL --> ENUMS\n    ENUMS --> |\"Validates\"| MODEL\n    MODEL --> |\"model_json_schema()\"| SCHEMA\n    MODEL --> |\"model_dump()\"| DATA\n    \n    style MODEL fill:#f9f9f9\n    style ENUMS fill:#f9f9f9\n```\n\n**Diagram: Scenic Spot Data Processing Pipeline**\n\nThe `ScenicSpot` model includes multiple validation layers:\n\n| Field | Type | Validation | Purpose |\n|-------|------|------------|---------|\n| `name` | `str` | Required | Scenic spot name |\n| `star_rating` | `StarRatingEnum` | Enum validation | 3A/4A/5A rating |\n| `open_status` | `OpenStatusEnum` | Enum validation | open/close status |\n| `longitude` | `float` | Auto-conversion | Longitude coordinate |\n| `latitude` | `float` | Auto-conversion | Latitude coordinate |\n| `suggested_play_hours` | `float` | Custom validator | Must be > 0 |\n\nSources: [tests/playground/gen/g9/g9.py:19-73]()\n\n### Enum-Based Validation\n\nThe system defines strict enumerations for categorical data:\n\n```python\nclass StarRatingEnum(str, Enum):\n    THREE_A = \"3A\"\n    FOUR_A = \"4A\"\n    FIVE_A = \"5A\"\n\nclass OpenStatusEnum(str, Enum):\n    OPEN = \"open\"\n    CLOSE = \"close\"\n```\n\nField validators provide enhanced error messages when validation fails:\n\nSources: [tests/playground/gen/g9/g9.py:20-31](), [tests/playground/gen/g9/g9.py:42-72]()\n\n### CSV to Pydantic Conversion\n\nThe `read_csv_to_pydantic()` function demonstrates robust CSV parsing with error handling:\n\n```mermaid\ngraph LR\n    READ[\"Open CSV File\"]\n    PARSE[\"csv.DictReader\"]\n    LOOP[\"Iterate Rows\"]\n    EXTRACT[\"Extract Fields\"]\n    SPLIT[\"Split Coordinates\"]\n    VALIDATE[\"Pydantic Validation\"]\n    APPEND[\"Append to List\"]\n    ERROR[\"Log Error & Continue\"]\n    \n    READ --> PARSE\n    PARSE --> LOOP\n    LOOP --> EXTRACT\n    EXTRACT --> SPLIT\n    SPLIT --> VALIDATE\n    VALIDATE --> |\"Success\"| APPEND\n    VALIDATE --> |\"Exception\"| ERROR\n    ERROR --> LOOP\n    APPEND --> LOOP\n```\n\n**Diagram: CSV Parsing Workflow with Error Recovery**\n\nThe function handles malformed data gracefully by logging errors and continuing to the next row:\n\nSources: [tests/playground/gen/g9/g9.py:75-112]()\n\n---\n\n## Complex Schema Processing\n\n### Beijing Subway Data Model\n\nThe Beijing subway data demonstrates handling deeply nested JSON structures with multiple model layers.\n\n```mermaid\ngraph TB\n    subgraph \"Data Layers\"\n        DRW[\"DrwSubwayData<br/>Root Model\"]\n        LINE[\"DrwLine<br/>Line Model\"]\n        STATION[\"DrwStation<br/>Station Model\"]\n    end\n    \n    subgraph \"Station Fields\"\n        COORDS[\"sl: longitude,latitude\"]\n        TYPE[\"t: 1=normal, 2=transfer\"]\n        ROUTES[\"r: route_id|route_id\"]\n        MULTI[\"udpx, udsi, udli<br/>Semicolon-separated\"]\n    end\n    \n    DRW --> |\"l: List[DrwLine]\"| LINE\n    LINE --> |\"st: List[DrwStation]\"| STATION\n    STATION --> COORDS\n    STATION --> TYPE\n    STATION --> ROUTES\n    STATION --> MULTI\n```\n\n**Diagram: Subway Data Schema Hierarchy**\n\nThe schema includes multi-valued fields using delimiter-separated strings:\n\n| Field | Delimiter | Example | Description |\n|-------|-----------|---------|-------------|\n| `r` | `\\|` | `\"900000069871\\|110100023339\"` | Multiple route IDs |\n| `udsi` | `;` | `\"900000069872017;900000069871009\"` | Up/down station IDs |\n| `udli` | `;` | `\"900000069872;900000069871\"` | Up/down line IDs |\n\nSources: [tests/playground/gen/g7/subway.md:19-89](), [tests/playground/gen/g7/01_valid7.py:6-36]()\n\n### Field Validation with Coordinate Parsing\n\nThe `DrwStation` model includes a custom validator for coordinate fields:\n\n```python\n@field_validator('sl')\ndef parse_coordinate(cls, v: str) -> str:\n    if not v:\n        raise ValueError(\"ç»çº¬åº¦ä¸è½ä¸ºç©º\")\n    lon, lat = v.split(',')\n    try:\n        float(lon), float(lat)\n    except ValueError:\n        raise ValueError(f\"æ æçç»çº¬åº¦æ ¼å¼ï¼{v}\")\n    return v\n```\n\nThis ensures all coordinates are valid before the model is instantiated.\n\nSources: [tests/playground/gen/g7/01_valid7.py:6-24](), [tests/playground/gen/g7/03_valid.py:37-48]()\n\n---\n\n## Data Transformation Pipelines\n\n### Schema Generation and Simplification\n\nThe system supports transforming raw complex schemas into simplified models optimized for specific use cases.\n\n```mermaid\ngraph LR\n    RAW[\"BeijingMetroRaw<br/>Complex Source Data\"]\n    PARSE[\"read_raw_metro_json()\"]\n    TRANSFORM[\"convert_raw_to_draw_model()\"]\n    \n    subgraph \"Transformation Steps\"\n        EXTRACT[\"Extract all lines<br/>get_all_lines()\"]\n        DEDUPE[\"Deduplicate stations<br/>get_all_stations()\"]\n        COLOR[\"Generate colors<br/>generate_rainbow_colors()\"]\n        BUILD[\"Build DrawLine objects\"]\n    end\n    \n    DRAW[\"MetroDrawSchema<br/>Simplified Draw Model\"]\n    SCHEMA_OUT[\"metro-draw-schema.json\"]\n    DATA_OUT[\"metro-draw-data.json\"]\n    \n    RAW --> PARSE\n    PARSE --> TRANSFORM\n    TRANSFORM --> EXTRACT\n    EXTRACT --> DEDUPE\n    DEDUPE --> COLOR\n    COLOR --> BUILD\n    BUILD --> DRAW\n    DRAW --> |\"model_json_schema()\"| SCHEMA_OUT\n    DRAW --> |\"model_dump()\"| DATA_OUT\n```\n\n**Diagram: Data Transformation Pipeline from Raw to Draw-Optimized Model**\n\nThe transformation process:\n\n1. **Extraction**: Groups stations by line using `get_all_lines()`\n2. **Deduplication**: Identifies transfer stations appearing in multiple lines\n3. **Color Assignment**: Generates rainbow gradient colors for visual distinction\n4. **Model Building**: Creates `DrawStation` and `DrawLine` objects\n\nSources: [tests/playground/gen/g7/05.dump_.py:94-163](), [tests/playground/gen/g7/06.dump_all.py:94-163]()\n\n### Transfer Station Detection\n\nThe system automatically identifies transfer stations using two criteria:\n\n```python\nis_transfer = station_raw.is_transfer() or len(line_set) > 1\n```\n\nA station is marked as a transfer station if:\n- The raw data explicitly marks it (`t == '1'`)\n- It appears in multiple lines during deduplication\n\nSources: [tests/playground/gen/g7/05.dump_.py:137](), [tests/playground/gen/g7/06.dump_all.py:137]()\n\n---\n\n## Matplotlib Visualization\n\n### Thread-Safe Visualization Setup\n\nWhen executing visualization code in subthreads, the system requires proper matplotlib backend configuration.\n\n```mermaid\ngraph TB\n    BEFORE[\"before_thread/plt_back_chinese.py<br/>Pre-execution Setup\"]\n    \n    subgraph \"Initialization Steps\"\n        BACKEND[\"matplotlib.use('Agg')<br/>Non-GUI Backend\"]\n        FONT[\"Set Chinese Fonts<br/>SimHei/PingFang SC\"]\n        MINUS[\"Fix Unicode Minus<br/>unicode_minus=False\"]\n    end\n    \n    THREAD[\"subthread_python_executor.py<br/>Thread Execution\"]\n    IMPORT[\"Import matplotlib in thread\"]\n    SAFE[\"Thread-safe execution\"]\n    \n    BEFORE --> BACKEND\n    BACKEND --> FONT\n    FONT --> MINUS\n    MINUS --> THREAD\n    THREAD --> IMPORT\n    IMPORT --> SAFE\n```\n\n**Diagram: Matplotlib Thread-Safe Initialization Flow**\n\nThe setup must occur before any matplotlib operations in the subthread:\n\nSources: [src/runtime/before_thread/plt_back_chinese.py:1-13](), [src/runtime/subthread_python_executor.py:9]()\n\n### Chinese Font Configuration\n\nPlatform-specific font configuration ensures Chinese characters render correctly:\n\n| Platform | Font Family | Configuration |\n|----------|-------------|---------------|\n| Windows | `SimHei` | `plt.rcParams['font.sans-serif'] = ['SimHei']` |\n| macOS | `PingFang SC` | `plt.rcParams['font.sans-serif'] = ['PingFang SC']` |\n| Linux | `WenQuanYi Micro Hei` | `plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei']` |\n\nThe system also sets `axes.unicode_minus = False` to prevent minus sign rendering issues.\n\nSources: [tests/playground/gen/g7/04_plt.py:194-202]()\n\n### Subway Network Visualization\n\nThe `plot_subway_network()` function demonstrates complex multi-layer plotting:\n\n```mermaid\ngraph TB\n    INIT[\"Initialize Canvas<br/>plt.subplots()\"]\n    \n    subgraph \"Layer 1: Lines\"\n        MAP[\"Build route mapping<br/>get_route_station_mapping()\"]\n        COLORS[\"Assign colors<br/>plt.cm.tab10()\"]\n        PLOT_LINE[\"Plot line segments<br/>ax.plot()\"]\n    end\n    \n    subgraph \"Layer 2: Stations\"\n        NORMAL[\"Plot normal stations<br/>ax.scatter() white\"]\n        TRANSFER[\"Plot transfer stations<br/>ax.scatter() red\"]\n    end\n    \n    subgraph \"Layer 3: Annotations\"\n        LABEL[\"Annotate key stations<br/>ax.annotate()\"]\n    end\n    \n    BEAUTY[\"Add legends, grid, title\"]\n    SAVE[\"Save figure<br/>plt.savefig()\"]\n    \n    INIT --> MAP\n    MAP --> COLORS\n    COLORS --> PLOT_LINE\n    PLOT_LINE --> NORMAL\n    NORMAL --> TRANSFER\n    TRANSFER --> LABEL\n    LABEL --> BEAUTY\n    BEAUTY --> SAVE\n```\n\n**Diagram: Multi-Layer Subway Visualization Process**\n\nThe plotting strategy uses layering to ensure visual clarity:\n- Lines plotted first at lower z-order\n- Normal stations as white circles with gray borders\n- Transfer stations as larger red circles on top\n- Selective annotation of transfer stations to avoid clutter\n\nSources: [tests/playground/gen/g7/02_plt.py:115-223]()\n\n### Route-Based Line Plotting\n\nThe system groups stations by route ID for proper line connectivity:\n\n```python\ndef get_route_station_mapping(drw_data: DrwSubwayData) -> dict[str, list[DrwStation]]:\n    route_mapping = defaultdict(list)\n    for line in drw_data.l:\n        for station in line.st:\n            route_ids = station.r.split(\"|\")\n            for rid in route_ids:\n                if rid and station.sl:\n                    route_mapping[rid].append(station)\n    return route_mapping\n```\n\nThis ensures stations are connected in the correct order along each route segment.\n\nSources: [tests/playground/gen/g7/02_plt.py:99-113]()\n\n---\n\n## Complete Example Workflows\n\n### Scenic Spot Processing Workflow\n\n```mermaid\ngraph TD\n    START[\"CSV File Input\"]\n    READ[\"read_csv_to_pydantic()<br/>Parse & Validate\"]\n    \n    subgraph \"Validation Layer\"\n        ENUM[\"Enum Validation<br/>StarRatingEnum, OpenStatusEnum\"]\n        COORD[\"Coordinate Parsing<br/>longitude, latitude\"]\n        HOURS[\"Hours Validation<br/>suggested_play_hours > 0\"]\n    end\n    \n    MODEL[\"List[ScenicSpot]<br/>Validated Models\"]\n    \n    subgraph \"Output Generation\"\n        SCHEMA_GEN[\"Generate Schema<br/>model_json_schema()\"]\n        DATA_GEN[\"Generate Data<br/>model_dump()\"]\n        STATS[\"Calculate Statistics<br/>Count by star rating<br/>Average play hours\"]\n    end\n    \n    OUT_SCHEMA[\"beijing_scenic_spot_schema_with_play_hours.json\"]\n    OUT_DATA[\"beijing_scenic_spot_validated_data_with_play_hours.json\"]\n    CONSOLE[\"Console Statistics Output\"]\n    \n    START --> READ\n    READ --> ENUM\n    READ --> COORD\n    READ --> HOURS\n    ENUM --> MODEL\n    COORD --> MODEL\n    HOURS --> MODEL\n    MODEL --> SCHEMA_GEN\n    MODEL --> DATA_GEN\n    MODEL --> STATS\n    SCHEMA_GEN --> OUT_SCHEMA\n    DATA_GEN --> OUT_DATA\n    STATS --> CONSOLE\n```\n\n**Diagram: End-to-End Scenic Spot Data Processing Workflow**\n\nThe workflow generates both schema documentation and validated data files, along with descriptive statistics:\n\n```\nâ æåå¤ç 15 æ¡æ¯åºæ°æ®\nð æçº§åå¸:\n   - 3A: 9 æ¡\n   - 4A: 5 æ¡\n   - 5A: 1 æ¡\nð æ¸¸ç©æ¶é¿ç»è®¡:\n   - æ»å»ºè®®æ¸¸ç©æ¶é¿: 26.5 å°æ¶\n   - å¹³åå»ºè®®æ¸¸ç©æ¶é¿: 1.8 å°æ¶\n```\n\nSources: [tests/playground/gen/g9/g9.py:115-153]()\n\n### Subway Visualization Workflow\n\n```mermaid\ngraph TD\n    JSON[\"1100_drw_beijing.json<br/>Raw Subway Data\"]\n    VALIDATE[\"read_drw_subway_data()<br/>Pydantic Validation\"]\n    MODEL[\"DrwSubwayData<br/>Validated Model\"]\n    \n    subgraph \"Visualization Preparation\"\n        EXTRACT[\"Extract coordinates<br/>parse_lon_lat()\"]\n        ROUTE_MAP[\"Build route mapping<br/>get_route_station_mapping()\"]\n        CLASSIFY[\"Classify stations<br/>Transfer vs Normal\"]\n    end\n    \n    subgraph \"Matplotlib Rendering\"\n        CANVAS[\"Create canvas<br/>plt.subplots()\"]\n        LINES[\"Plot line segments<br/>ax.plot() with colors\"]\n        STATIONS[\"Plot station nodes<br/>ax.scatter()\"]\n        ANNOTATE[\"Add station labels<br/>ax.annotate()\"]\n        LEGEND[\"Add legend & grid<br/>ax.legend()\"]\n    end\n    \n    OUTPUT[\"subway_network.png<br/>High-DPI Image\"]\n    \n    JSON --> VALIDATE\n    VALIDATE --> MODEL\n    MODEL --> EXTRACT\n    MODEL --> ROUTE_MAP\n    MODEL --> CLASSIFY\n    EXTRACT --> CANVAS\n    ROUTE_MAP --> LINES\n    CLASSIFY --> STATIONS\n    STATIONS --> ANNOTATE\n    ANNOTATE --> LEGEND\n    LEGEND --> OUTPUT\n```\n\n**Diagram: Subway Network Visualization Workflow**\n\nThe visualization process separates data validation from rendering logic, ensuring the plotting code works with clean, validated data structures.\n\nSources: [tests/playground/gen/g7/02_plt.py:64-223]()\n\n---\n\n## Key Design Patterns\n\n### Model Composition Pattern\n\nThe system uses hierarchical Pydantic models to represent nested structures:\n\n```\nDrwSubwayData (root)\nâââ l: List[DrwLine]\n    âââ st: List[DrwStation]\n        âââ sl: str (coordinate)\n        âââ t: str (type)\n        âââ r: str (routes)\n```\n\nThis composition allows type-safe access at each level and automatic validation of the entire structure.\n\nSources: [tests/playground/gen/g7/01_valid7.py:27-36]()\n\n### Validation Chain Pattern\n\nField validators can reference other validators or call custom validation functions:\n\n1. **Type coercion**: Pydantic converts string to float automatically\n2. **Format validation**: Custom validator checks coordinate format\n3. **Range validation**: Ensures values are within acceptable bounds\n4. **Cross-field validation**: Validates relationships between fields\n\nSources: [tests/playground/gen/g9/g9.py:42-72]()\n\n### Separation of Concerns Pattern\n\nThe codebase separates:\n- **Data models**: Pydantic classes define structure and validation\n- **Transformation logic**: Functions convert between model types\n- **Visualization logic**: Matplotlib code operates on validated models\n- **I/O operations**: File reading/writing isolated from business logic\n\nThis separation enables testing each component independently and reusing models across different operations.\n\nSources: [tests/playground/gen/g7/06.dump_all.py:94-212]()\n\n---\n\n## Execution in Agent Context\n\nWhen the agent executes data processing code, it uses one of the execution strategies described in [Execution Runtime](#5). The matplotlib setup is particularly important for subthread execution:\n\n1. The `before_thread/plt_back_chinese.py` module is imported before any matplotlib code\n2. This sets the non-GUI backend (`Agg`) to prevent threading issues\n3. Chinese fonts are configured for proper character rendering\n4. Code executes safely in the isolated subthread environment\n\nSources: [src/runtime/subthread_python_executor.py:9](), [src/runtime/before_thread/plt_back_chinese.py:1-13]()\n\nThe workspace state management (see [Workspace State Management](#5.5)) ensures that data loaded in one execution persists for subsequent operations, allowing multi-step data processing workflows.\n\n---\n\n# Page: Geographic Data Processing\n\n# Geographic Data Processing\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [tests/playground/gen/g6/13_fil.py](tests/playground/gen/g6/13_fil.py)\n- [tests/playground/gen/g7/01_valid7.py](tests/playground/gen/g7/01_valid7.py)\n- [tests/playground/gen/g7/02_plt.py](tests/playground/gen/g7/02_plt.py)\n- [tests/playground/gen/g7/1100_drw_beijing.json](tests/playground/gen/g7/1100_drw_beijing.json)\n- [tests/playground/gen/g7/1100_info_beijing.json](tests/playground/gen/g7/1100_info_beijing.json)\n- [tests/playground/gen/g7/subway.md](tests/playground/gen/g7/subway.md)\n\n</details>\n\n\n\nThis page documents the geographic data processing capabilities in the algo_agent system, specifically focusing on Beijing subway/metro data and street network analysis. These examples demonstrate the agent's ability to work with structured geographic data, perform spatial filtering, construct network graphs, and generate visualizations.\n\nFor Python code execution mechanics, see [Python Code Execution Tool](#4.2). For data visualization examples, see [Data Processing and Visualization](#7.2).\n\n---\n\n## Overview\n\nThe geographic data processing examples showcase two primary use cases:\n\n1. **Beijing Subway Data Processing** - Parsing hierarchical JSON data representing subway lines and stations with geographic coordinates, validating with Pydantic models, and generating network visualizations\n2. **Beijing Street Network Filtering** - Filtering large-scale address/street datasets based on geographic proximity and connectivity using graph analysis\n\nThese examples demonstrate the agent's capability to handle real-world geographic datasets with multiple file formats (JSON, TSV), perform coordinate-based spatial operations, and apply graph algorithms for connectivity analysis.\n\n---\n\n## Beijing Subway Data Processing\n\n### Data Schema Overview\n\nThe Beijing subway data consists of two complementary JSON files with abbreviated field names:\n\n| File | Purpose | Root Fields |\n|------|---------|-------------|\n| `1100_drw_beijing.json` | Geographic/drawing data | `s` (subject), `i` (region ID), `l` (lines array) |\n| `1100_info_beijing.json` | Schedule/operational data | `i` (region ID), `l` (lines array) |\n\n**Key Field Mappings**:\n\n| Abbreviated | Full Name | Description |\n|-------------|-----------|-------------|\n| `st` | stations | Array of station objects |\n| `sl` | station location | Coordinates as \"longitude,latitude\" |\n| `n` | name | Station name |\n| `sid` | station ID | Unique station identifier |\n| `r` | route | Route IDs (pipe-separated) |\n| `t` | type | Station type (1=normal, 2=transfer) |\n| `ls` | line-station | Line-station association ID |\n| `lt` | leave time | Departure time |\n| `ft` | first/arrive time | Arrival time |\n\n**Sources**: [tests/playground/gen/g7/subway.md:19-48]()\n\n### Pydantic Data Models\n\nThe system uses Pydantic models to validate and parse the JSON data with type safety:\n\n```mermaid\nclassDiagram\n    class DrwSubwayData {\n        +str s\n        +str i\n        +List~DrwLine~ l\n    }\n    \n    class DrwLine {\n        +List~DrwStation~ st\n    }\n    \n    class DrwStation {\n        +Optional~str~ rs\n        +Optional~str~ sl\n        +str n\n        +str sid\n        +Optional~str~ r\n        +Optional~str~ t\n        +str si\n    }\n    \n    class InfoSubwayData {\n        +str i\n        +List~InfoLine~ l\n    }\n    \n    class InfoLine {\n        +str a\n        +List~InfoStation~ st\n    }\n    \n    class InfoStation {\n        +str ac\n        +List~InfoDirection~ d\n        +str si\n    }\n    \n    class InfoDirection {\n        +str ls\n        +Optional~str~ lt\n        +str n\n        +Optional~str~ ft\n    }\n    \n    DrwSubwayData --> DrwLine\n    DrwLine --> DrwStation\n    InfoSubwayData --> InfoLine\n    InfoLine --> InfoStation\n    InfoStation --> InfoDirection\n```\n\n**Model Definitions**:\n\n- **`DrwSubwayData`** [tests/playground/gen/g7/01_valid7.py:32-36]() - Root model for geographic drawing data\n- **`DrwLine`** [tests/playground/gen/g7/01_valid7.py:27-29]() - Represents a subway line with stations\n- **`DrwStation`** [tests/playground/gen/g7/01_valid7.py:6-24]() - Station with coordinates, routes, and metadata\n- **`InfoSubwayData`** [tests/playground/gen/g7/01_valid7.py:61-64]() - Root model for schedule/operational data\n- **`InfoLine`** [tests/playground/gen/g7/01_valid7.py:55-58]() - Line with operational station info\n- **`InfoStation`** [tests/playground/gen/g7/01_valid7.py:48-52]() - Station with directional schedule data\n- **`InfoDirection`** [tests/playground/gen/g7/01_valid7.py:40-45]() - Direction-specific timing information\n\n**Sources**: [tests/playground/gen/g7/01_valid7.py:1-135]()\n\n### Data Loading and Parsing\n\n**File Reading Functions**:\n\n```mermaid\ngraph LR\n    JSON[\"1100_drw_beijing.json<br/>1100_info_beijing.json\"] --> read_drw[\"read_drw_subway_data()\"]\n    JSON --> read_info[\"read_info_subway_data()\"]\n    \n    read_drw --> DrwSubwayData[\"DrwSubwayData<br/>Pydantic Model\"]\n    read_info --> InfoSubwayData[\"InfoSubwayData<br/>Pydantic Model\"]\n    \n    DrwSubwayData --> validation[\"Type Validation<br/>Field Checking\"]\n    InfoSubwayData --> validation\n    \n    validation --> error[\"ValidationError<br/>(if invalid)\"]\n    validation --> success[\"Parsed Model<br/>Instance\"]\n```\n\n**Key Functions**:\n\n- **`read_drw_subway_data(file_path)`** [tests/playground/gen/g7/01_valid7.py:72-89]() - Loads and validates geographic data\n  - Checks file existence with `Path.exists()`\n  - Opens file with UTF-8 encoding\n  - Instantiates `DrwSubwayData` model with automatic validation\n  - Raises `FileNotFoundError` if file missing\n\n- **`read_info_subway_data(file_path)`** [tests/playground/gen/g7/01_valid7.py:92-109]() - Loads and validates schedule data\n  - Same pattern as `read_drw_subway_data()`\n  - Returns `InfoSubwayData` instance\n\n**Sources**: [tests/playground/gen/g7/01_valid7.py:67-109]()\n\n### Geographic Coordinate Processing\n\n**Coordinate Parsing Flow**:\n\n```mermaid\ngraph TD\n    raw[\"Raw Coordinate String<br/>sl: '116.178945,39.925686'\"] --> parse[\"parse_lon_lat()\"]\n    \n    parse --> split[\"str.split(',')\"]\n    split --> convert[\"float(lon), float(lat)\"]\n    convert --> tuple[\"tuple[float, float]\"]\n    convert --> error[\"ValueError\"]\n    \n    tuple --> success[\"(116.178945, 39.925686)\"]\n    error --> none[\"None\"]\n```\n\n**`parse_lon_lat(sl_str)`** [tests/playground/gen/g7/02_plt.py:83-96]():\n- Extracts longitude and latitude from comma-separated string\n- Returns `tuple[float, float]` or `None` if parsing fails\n- Handles `None` input and malformed strings with exception handling\n\n**Route-Station Mapping**:\n\n**`get_route_station_mapping(drw_data)`** [tests/playground/gen/g7/02_plt.py:99-113]():\n- Constructs `dict[str, list[DrwStation]]` mapping route IDs to stations\n- Splits pipe-separated route IDs from `station.r` field\n- Filters stations without coordinates (`station.sl`)\n- Used for drawing connected line segments in visualization\n\n**Sources**: [tests/playground/gen/g7/02_plt.py:83-113]()\n\n### Subway Network Visualization\n\n**Visualization Architecture**:\n\n```mermaid\ngraph TB\n    data[\"DrwSubwayData\"] --> mapping[\"get_route_station_mapping()\"]\n    \n    mapping --> routes[\"route_mapping<br/>dict[route_id, stations]\"]\n    \n    routes --> plot_lines[\"Draw Line Segments<br/>ax.plot()\"]\n    data --> plot_stations[\"Draw Station Nodes<br/>ax.scatter()\"]\n    \n    plot_lines --> colors[\"Route Colors<br/>plt.cm.tab10\"]\n    plot_stations --> types[\"Station Types<br/>normal vs transfer\"]\n    \n    colors --> canvas[\"Matplotlib Figure\"]\n    types --> canvas\n    \n    canvas --> annotate[\"Annotate Transfer Stations<br/>ax.annotate()\"]\n    annotate --> save[\"plt.savefig()\"]\n    annotate --> display[\"plt.show()\"]\n```\n\n**`plot_subway_network(drw_data, figsize)`** [tests/playground/gen/g7/02_plt.py:115-223]():\n\n**Configuration**:\n- Sets Chinese font support: `plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]`\n- Initializes figure with specified `figsize` (default 12x10)\n\n**Rendering Steps**:\n\n1. **Line Segments** [tests/playground/gen/g7/02_plt.py:133-154]()\n   - Iterates through route mappings\n   - Filters stations with valid coordinates\n   - Assigns colors from `plt.cm.tab10` color map\n   - Plots connected line segments with `ax.plot(lons, lats, ...)`\n   - Uses `linewidth=2, alpha=0.7`\n\n2. **Station Nodes** [tests/playground/gen/g7/02_plt.py:157-191]()\n   - Separates normal stations (`t==\"1\"`) and transfer stations (`t==\"2\"`)\n   - **Normal stations**: white fill, gray border, size=50\n   - **Transfer stations**: red fill, dark red border, size=100 (more prominent)\n   - Renders with `ax.scatter()`\n\n3. **Annotations** [tests/playground/gen/g7/02_plt.py:194-203]()\n   - Labels first 10 transfer stations to avoid clutter\n   - Uses `ax.annotate()` with offset `(5, 5)` points\n   - Yellow background with rounded box style\n\n4. **Output** [tests/playground/gen/g7/02_plt.py:221-223]()\n   - Saves to `subway_network.png` at 300 DPI\n   - Displays interactively with `plt.show()`\n\n**Sources**: [tests/playground/gen/g7/02_plt.py:115-242]()\n\n---\n\n## Beijing Street Network Processing\n\n### Dataset Structure\n\nThe street network processing works with three TSV files containing OpenStreetMap-derived data:\n\n| File | Key Fields | Purpose |\n|------|-----------|---------|\n| `CN-addresses-beijing.tsv` | `postal_code_`, `city_`, `street_`, `x_min`, `y_min`, `x_max`, `y_max` | Address ranges with bounding boxes |\n| `CN-houses-beijing.tsv` | `postal_code`, `city`, `street`, `house_number`, `x`, `y` | Individual house locations |\n| `CN-streets-beijing.tsv` | `city`, `state`, `province`, `street_name`, `postal_code` | Street metadata |\n\n**Sources**: [tests/playground/gen/g6/13_fil.py:8-16]()\n\n### Data Filtering Pipeline\n\n**Overall Processing Flow**:\n\n```mermaid\ngraph TD\n    raw_files[\"Raw TSV Files\"] --> read[\"pd.read_csv()\"]\n    \n    read --> normalize[\"Column Name Normalization<br/>str.strip().replace()\"]\n    \n    normalize --> filter_streets[\"Filter Streets<br/>state=='åäº¬å¸'\"]\n    \n    filter_streets --> get_names[\"Extract Street Names<br/>df['street_name'].unique()\"]\n    \n    get_names --> filter_addr[\"Filter Addresses<br/>street_ in valid_streets\"]\n    \n    filter_addr --> graph[\"build_street_graph()<br/>NetworkX Graph\"]\n    \n    graph --> components[\"nx.connected_components()\"]\n    \n    components --> largest[\"max(components, key=len)\"]\n    \n    largest --> final_filter[\"Final Filter:<br/>- No missing fields<br/>- In largest component\"]\n    \n    final_filter --> output[\"Output Filtered TSVs\"]\n```\n\n**Key Processing Steps**:\n\n1. **Data Loading** [tests/playground/gen/g6/13_fil.py:13-21]()\n   - Reads TSV files with `pd.read_csv(sep=\"\\t\", encoding=\"utf-8-sig\")`\n   - Normalizes column names: strip whitespace, replace spaces with underscores\n\n2. **Geographic Filtering** [tests/playground/gen/g6/13_fil.py:24-38]()\n   - Filters streets for Beijing: `df_street[\"state\"] == \"åäº¬å¸\"`\n   - Creates valid street name set from both `df_street_bj` and `df_house`\n   - Filters addresses where `street_` appears in valid set\n\n3. **Graph Construction** [tests/playground/gen/g6/13_fil.py:41-97]()\n   - Builds connectivity graph (see below)\n\n4. **Component Analysis** [tests/playground/gen/g6/13_fil.py:100-106]()\n   - Finds largest connected component: `max(nx.connected_components(street_graph), key=len)`\n\n5. **Final Filtering** [tests/playground/gen/g6/13_fil.py:109-136]()\n   - Validates all required fields are non-null\n   - Filters to streets in largest connected component\n   - Ensures data consistency across all three files\n\n**Sources**: [tests/playground/gen/g6/13_fil.py:1-150]()\n\n### Street Connectivity Graph\n\n**Graph Construction Algorithm**:\n\n```mermaid\ngraph TD\n    init[\"Initialize NetworkX Graph<br/>G = nx.Graph()\"] --> nodes[\"Add All Street Nodes<br/>G.add_nodes_from()\"]\n    \n    nodes --> coord_extract[\"Extract Coordinates<br/>addresses: x_min, y_min<br/>houses: x, y\"]\n    \n    coord_extract --> loop_addr[\"Loop: addresses Ã addresses\"]\n    coord_extract --> loop_house[\"Loop: houses Ã houses\"]\n    coord_extract --> loop_cross[\"Loop: addresses Ã houses\"]\n    \n    loop_addr --> calc_dist[\"Calculate Distance<br/>sqrt((x1-x2)Â² + (y1-y2)Â²)\"]\n    loop_house --> calc_dist\n    loop_cross --> calc_dist\n    \n    calc_dist --> check[\"dist < threshold<br/>(0.001 degrees)\"]\n    \n    check --> edge[\"Add Edge<br/>G.add_edge(street1, street2)\"]\n    \n    edge --> graph_out[\"Connected Street Graph\"]\n```\n\n**`build_street_graph(df_addr, df_house)`** [tests/playground/gen/g6/13_fil.py:41-97]():\n\n**Parameters**:\n- `df_addr`: Addresses DataFrame with `street_`, `x_min`, `y_min` columns\n- `df_house`: Houses DataFrame with `street`, `x`, `y` columns\n\n**Algorithm**:\n\n1. **Initialization** [tests/playground/gen/g6/13_fil.py:47-56]()\n   - Creates empty `nx.Graph()`\n   - Extracts valid coordinate data from both DataFrames\n   - Adds all unique street names as nodes\n\n2. **Distance Threshold** [tests/playground/gen/g6/13_fil.py:60]()\n   - Uses `threshold = 0.001` degrees (~100 meters at Beijing's latitude)\n\n3. **Edge Construction** [tests/playground/gen/g6/13_fil.py:63-96]()\n   - Compares all pairs of street coordinates (O(nÂ²) complexity)\n   - Calculates Euclidean distance: `np.sqrt((x1-x2)**2 + (y1-y2)**2)`\n   - Adds edge if distance < threshold\n   - Handles three combinations:\n     - Address-to-address comparisons [lines 63-73]\n     - House-to-house comparisons [lines 76-86]\n     - Address-to-house comparisons [lines 88-96]\n\n**Return**: NetworkX `Graph` object with streets as nodes and proximity relationships as edges\n\n**Sources**: [tests/playground/gen/g6/13_fil.py:41-97]()\n\n### Field Completeness Validation\n\n**Required Fields by File**:\n\n```mermaid\ngraph LR\n    subgraph Addresses\n        addr_fields[\"postal_code_<br/>city_<br/>street_<br/>x_min, x_max<br/>y_min, y_max<br/>house_min, house_max<br/>house_odd, house_even\"]\n    end\n    \n    subgraph Houses\n        house_fields[\"postal_code<br/>city<br/>street<br/>house_number<br/>x, y<br/>country\"]\n    end\n    \n    subgraph Streets\n        street_fields[\"city<br/>country<br/>state<br/>province<br/>postal_code<br/>street_name\"]\n    end\n    \n    addr_fields --> check_addr[\"df[cols].notna().all(axis=1)\"]\n    house_fields --> check_house[\"df[cols].notna().all(axis=1)\"]\n    street_fields --> check_street[\"df[cols].notna().all(axis=1)\"]\n    \n    check_addr --> valid[\"Valid Records Only\"]\n    check_house --> valid\n    check_street --> valid\n```\n\n**Validation Implementation**:\n\n- **Addresses** [tests/playground/gen/g6/13_fil.py:111-118]()\n  - Required: `[\"postal_code_\", \"city_\", \"street_\", \"x_min\", \"x_max\", \"y_min\", \"y_max\", \"house_min\", \"house_max\", \"house_odd\", \"house_even\"]`\n  - Filter: `df_addr_bj[addr_required_cols].notna().all(axis=1)`\n\n- **Houses** [tests/playground/gen/g6/13_fil.py:122-129]()\n  - Required: `[\"postal_code\", \"city\", \"street\", \"house_number\", \"x\", \"y\", \"country\"]`\n  - Filter: `df_house_bj[house_required_cols].notna().all(axis=1)`\n\n- **Streets** [tests/playground/gen/g6/13_fil.py:132-136]()\n  - Required: `[\"city\", \"country\", \"state\", \"province\", \"postal_code\", \"street_name\"]`\n  - Filter: `df_street_bj[street_required_cols].notna().all(axis=1)`\n\nAll three filters also check membership in the largest connected component via:\n```python\ndf[\"street_name\"].isin(largest_cc)\n```\n\n**Sources**: [tests/playground/gen/g6/13_fil.py:109-136]()\n\n### Output Generation\n\n**File Writing**:\n\n```mermaid\ngraph LR\n    final_dfs[\"Filtered DataFrames\"] --> create_dir[\"output_dir.mkdir(exist_ok=True)\"]\n    \n    create_dir --> write_addr[\"df_addr_final.to_csv()<br/>CN-addresses.tsv\"]\n    create_dir --> write_house[\"df_house_final.to_csv()<br/>CN-houses.tsv\"]\n    create_dir --> write_street[\"df_street_final.to_csv()<br/>CN-streets.tsv\"]\n    \n    write_addr --> stats[\"Print Statistics\"]\n    write_house --> stats\n    write_street --> stats\n```\n\n**Output Path**: [tests/playground/gen/g6/13_fil.py:139-140]()\n```python\noutput_dir = data_dir / \"filtered_bj\"\noutput_dir.mkdir(exist_ok=True)\n```\n\n**Writing Parameters**: [tests/playground/gen/g6/13_fil.py:142-144]()\n- Separator: `sep=\"\\t\"`\n- No index: `index=False`\n- Encoding: `encoding=\"utf-8-sig\"` (BOM for Excel compatibility)\n\n**Statistics Output**: [tests/playground/gen/g6/13_fil.py:146-150]()\n- Filtered address count\n- Filtered house count\n- Filtered street count\n- Size of largest connected component\n\n**Sources**: [tests/playground/gen/g6/13_fil.py:138-150]()\n\n---\n\n## Common Patterns and Utilities\n\n### Data Loading Pattern\n\nBoth examples follow a consistent pattern for loading structured geographic data:\n\n1. **Path Validation**: Check file existence with `Path.exists()`\n2. **Encoding Specification**: Use explicit UTF-8 encoding\n3. **Error Handling**: Raise descriptive exceptions (`FileNotFoundError`)\n4. **Type Validation**: Pydantic models or pandas DataFrame operations\n\n### Coordinate Systems\n\n**Coordinate Formats**:\n\n| System | Format | Example | Use Case |\n|--------|--------|---------|----------|\n| Geographic (WGS84) | `\"longitude,latitude\"` string | `\"116.178945,39.925686\"` | Subway station locations |\n| Coordinate pair | `x`, `y` float columns | `x=116.332, y=39.723` | House/address positions |\n| Bounding box | `x_min`, `x_max`, `y_min`, `y_max` | Address ranges | Spatial filtering |\n\n### Distance Calculations\n\n**Euclidean Distance** [tests/playground/gen/g6/13_fil.py:68-70]():\n```python\ndist = np.sqrt((row1[\"x_min\"] - row2[\"x_min\"])**2 + \n               (row1[\"y_min\"] - row2[\"y_min\"])**2)\n```\n\n**Threshold Selection**:\n- 0.001 degrees â 100 meters at Beijing's latitude (~40Â°N)\n- Suitable for identifying adjacent streets\n- Trade-off between connectivity and false positives\n\n### Graph Analysis Tools\n\n**NetworkX Integration**:\n- **Graph Construction**: `G = nx.Graph()`\n- **Node Addition**: `G.add_nodes_from(street_list)`\n- **Edge Addition**: `G.add_edge(node1, node2)`\n- **Connected Components**: `nx.connected_components(G)` returns iterator of sets\n- **Largest Component**: `max(components, key=len)`\n\n**Sources**: [tests/playground/gen/g6/13_fil.py:47-106]()\n\n---\n\n## Usage Examples\n\n### Subway Data Processing\n\n**Complete workflow** [tests/playground/gen/g7/01_valid7.py:113-135]():\n\n```python\nimport os\ndir = os.path.dirname(__file__)\n\n# Load drawing data\ndrw_data = read_drw_subway_data(os.path.join(dir, \"1100_drw_beijing.json\"))\nprint(f\"ä¸»é¢ï¼{drw_data.s}ï¼åºåIDï¼{drw_data.i}\")\n\n# Access first station\nfirst_station = drw_data.l[0].st[0]\nprint(f\"ç¬¬ä¸ä¸ªç«ç¹åç§°ï¼{first_station.n}ï¼ç»çº¬åº¦ï¼{first_station.sl}\")\n\n# Load schedule data\ninfo_data = read_info_subway_data(os.path.join(dir, \"1100_info_beijing.json\"))\nprint(f\"åºåIDï¼{info_data.i}\")\n\n# Access schedule info\nfirst_direction = info_data.l[0].st[0].d[0]\nprint(f\"çº¿è·¯ç«ç¹IDï¼{first_direction.ls}ï¼åè½¦æ¶é´ï¼{first_direction.lt}\")\n```\n\n### Visualization\n\n**Generate network plot** [tests/playground/gen/g7/02_plt.py:226-242]():\n\n```python\nfrom pathlib import Path\n\n# Read data\npath = Path(__file__).parent\ndrw_file_path = path / \"1100_drw_beijing.json\"\ndrw_data = read_drw_subway_data(drw_file_path)\nprint(f\"æåè¯»åæ°æ®ï¼{drw_data.s}ï¼å±{len(drw_data.l)}æ¡çº¿è·¯\")\n\n# Generate visualization\nplot_subway_network(drw_data)\nprint(\"å°éç½ç»ç»å¶å®æï¼å·²ä¿å­ä¸º subway_network.png\")\n```\n\n### Street Network Filtering\n\n**Complete filtering process** [tests/playground/gen/g6/13_fil.py:6-150]():\n\nConfiguration and execution produces three filtered output files with statistics on:\n- Number of filtered addresses\n- Number of filtered houses  \n- Number of filtered streets\n- Size of largest connected street component\n\n**Sources**: [tests/playground/gen/g7/01_valid7.py:113-135](), [tests/playground/gen/g7/02_plt.py:226-242](), [tests/playground/gen/g6/13_fil.py:1-150]()\n\n---\n\n# Page: Development and Testing\n\n# Development and Testing\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.vscode/settings.json](.vscode/settings.json)\n- [docs/python.design.md](docs/python.design.md)\n- [docs/sql.md](docs/sql.md)\n- [pyproject.toml](pyproject.toml)\n- [src/agent/tool/base_tool.py](src/agent/tool/base_tool.py)\n- [src/memory/tree_todo/schemas.py](src/memory/tree_todo/schemas.py)\n- [src/memory/tree_todo/todo_track.py](src/memory/tree_todo/todo_track.py)\n- [uv.lock](uv.lock)\n\n</details>\n\n\n\nThis page provides an overview of development practices, testing strategies, and debugging approaches for contributors to the algo_agent codebase. It covers setting up the development environment, testing different system components, and leveraging the observability infrastructure for effective debugging.\n\nFor specific topics:\n- Testing execution modes and namespace behavior: see [Testing the Execution System](#8.1)\n- Implementing new tools: see [Creating New Tools](#8.2)\n- Setting up databases for examples: see [Database Setup for Examples](#8.3)\n\n---\n\n## Development Environment Setup\n\n### Project Configuration\n\nThe project uses Python 3.12+ with `uv` for dependency management. Core dependencies are defined in [pyproject.toml:7-20]().\n\n**Key dependencies for development:**\n\n| Dependency | Purpose |\n|------------|---------|\n| `openai>=2.7.2` | LLM integration (DashScope API compatible) |\n| `pydantic` | Data validation and schema generation |\n| `decorator>=5.2.1` | Function instrumentation (`@traceable`, `@log_function`) |\n| `deepdiff>=8.6.1` | Task tree change detection |\n| `matplotlib>=3.10.7` | Visualization in examples |\n| `pandas>=2.3.3` | Data processing |\n\n### VS Code Configuration\n\nThe project includes VS Code settings for Python development at [.vscode/settings.json:1-34](). Key configurations:\n\n- **PYTHONPATH setup**: Workspace folder automatically added to Python path\n- **Formatting**: Black formatter configured but auto-format disabled to prevent unwanted changes during refactoring\n- **Analysis mode**: Set to `openFilesOnly` to reduce resource usage during development\n\n```json\n{\n  \"python.analysis.diagnosticMode\": \"openFilesOnly\",\n  \"python.formatting.autoFormat\": false\n}\n```\n\nSources: [pyproject.toml:1-21](), [.vscode/settings.json:1-34]()\n\n---\n\n## Testing Philosophy\n\nThe codebase emphasizes **isolation testing** and **observability-driven debugging** rather than traditional unit test suites. This approach leverages:\n\n1. **Multiple execution strategies** with different isolation levels\n2. **Comprehensive logging** capturing every function call and state change\n3. **Structured execution results** with explicit status tracking\n4. **Version-controlled task state** for regression detection\n\n```mermaid\ngraph TB\n    DEV[\"Developer\"]\n    TEST[\"Test Code/Query\"]\n    EXEC[\"Execution System\"]\n    LOG[\"Log Analysis\"]\n    \n    DEV -->|\"Write\"| TEST\n    TEST -->|\"Execute in\"| EXEC\n    \n    subgraph \"Execution Modes\"\n        SUBPROCESS[\"run_structured_in_subprocess\"]\n        SUBTHREAD[\"run_structured_in_subthread\"]\n        DIRECT[\"run_structured_direct\"]\n    end\n    \n    EXEC --> SUBPROCESS\n    EXEC --> SUBTHREAD\n    EXEC --> DIRECT\n    \n    SUBPROCESS -->|\"ExecutionResult\"| LOG\n    SUBTHREAD -->|\"ExecutionResult\"| LOG\n    DIRECT -->|\"ExecutionResult\"| LOG\n    \n    LOG -->|\"Inspect traces\"| DEV\n    \n    subgraph \"Log Files\"\n        GLOBAL[\"logs/global.log\"]\n        TRACE[\"logs/trace.log\"]\n        UTILS[\"logs/utils.log\"]\n    end\n    \n    LOG --> GLOBAL\n    LOG --> TRACE\n    LOG --> UTILS\n```\n\n**Testing Strategy: Isolation Levels**\n\nSources: Architecture diagrams (Diagram 3: Multi-Strategy Execution System)\n\n---\n\n## Testing Individual Components\n\n### Tool System Testing\n\nWhen developing or testing tools, use the `BaseTool` interface contracts defined in [src/agent/tool/base_tool.py:7-76]():\n\n```mermaid\ngraph LR\n    TOOL[\"CustomTool extends BaseTool\"]\n    SCHEMA[\"get_tool_schema()\"]\n    RUN[\"run()\"]\n    VALIDATE[\"Pydantic validation\"]\n    \n    TOOL -->|\"Implements\"| SCHEMA\n    TOOL -->|\"Implements\"| RUN\n    TOOL -->|\"Auto-validates\"| VALIDATE\n    \n    TEST[\"Test Code\"]\n    TEST -->|\"1. Instantiate with params\"| TOOL\n    TEST -->|\"2. Call run()\"| TOOL\n    TEST -->|\"3. Assert output\"| RUN\n    \n    SCHEMA -->|\"Returns dict\"| AGENT[\"Agent can discover\"]\n```\n\n**Testing a tool implementation:**\n\n1. **Schema validation**: Ensure `get_tool_schema()` returns valid JSON Schema\n2. **Parameter validation**: Test with invalid parameters (Pydantic will raise `ValidationError`)\n3. **Execution logic**: Call `run()` and verify output matches expected format\n4. **Tool discovery**: Check that `tool_name()` returns expected identifier\n\nExample test pattern:\n```python\n# Test tool schema generation\ntool_schema = MyCustomTool.get_tool_schema()\nassert tool_schema[\"type\"] == \"function\"\nassert \"parameters\" in tool_schema[\"function\"]\n\n# Test execution with valid params\ntool = MyCustomTool(tool_call_purpose=\"test\", param1=\"value\")\nresult = tool.run()\nassert isinstance(result, str)\n\n# Test parameter validation\ntry:\n    invalid_tool = MyCustomTool(tool_call_purpose=\"test\")  # Missing required param\n    assert False, \"Should have raised ValidationError\"\nexcept ValidationError:\n    pass  # Expected\n```\n\nSources: [src/agent/tool/base_tool.py:7-76]()\n\n---\n\n## Testing Code Execution\n\n### Understanding Namespace Behavior\n\nThe execution system's complexity comes from Python's `exec()` namespace semantics. Key behaviors documented in [docs/python.design.md:1-645]():\n\n**Critical namespace rules:**\n\n| Scenario | `globals` | `locals` | Variable Assignment Destination |\n|----------|-----------|----------|-------------------------------|\n| `exec(code)` | Current `globals()` | Current `locals()` | Depends on scope |\n| `exec(code, {})` | Empty dict + `__builtins__` | Same as `globals` | `globals` dict |\n| `exec(code, {}, {})` | Empty dict + `__builtins__` | Empty dict | `locals` dict |\n\n**Testing namespace isolation:**\n\n```python\n# Test 1: Subprocess isolation\narg_globals = {\"__name__\": \"__main__\", \"x\": 10}\nresult = run_structured_in_subprocess(\n    code_snippet=\"y = x + 5\\nprint(y)\",\n    arg_globals=arg_globals,\n    timeout=5\n)\nassert result.status == ExecutionStatus.SUCCESS\nassert \"15\" in result.stdout\n\n# Test 2: Verify workspace persistence\nresult2 = run_structured_in_subprocess(\n    code_snippet=\"z = y * 2\",  # Uses y from previous execution\n    arg_globals=result.out_globals,  # Passed forward\n    timeout=5\n)\nassert result2.status == ExecutionStatus.SUCCESS\n```\n\nSources: [docs/python.design.md:1-98](), [docs/python.design.md:427-645]()\n\n---\n\n## Debugging with Observability\n\n### Log File Structure\n\nThe logging system (importance: 10.93) produces multiple specialized log files:\n\n```mermaid\ngraph TB\n    APP[\"Application Code\"]\n    DECORATOR[\"@log_function / @traceable\"]\n    \n    APP -->|\"Decorated with\"| DECORATOR\n    \n    subgraph \"Logger Instances\"\n        GLOBAL_LOGGER[\"global_logger\"]\n        TRACE_LOGGER[\"traceable_logger\"]\n        ALL_LOGGER[\"all_logger\"]\n    end\n    \n    DECORATOR --> GLOBAL_LOGGER\n    DECORATOR --> TRACE_LOGGER\n    DECORATOR --> ALL_LOGGER\n    \n    GLOBAL_LOGGER -->|\"System traces\"| GLOBAL_LOG[\"logs/global.log\"]\n    TRACE_LOGGER -->|\"Detailed execution\"| TRACE_LOG[\"logs/trace.log\"]\n    ALL_LOGGER -->|\"Combined\"| ALL_LOG[\"logs/all.log\"]\n    \n    subgraph \"Log Content\"\n        FUNC[\"Function calls & args\"]\n        TIME[\"Execution timing\"]\n        STACK[\"Stack traces on error\"]\n        STATE[\"Variable states\"]\n    end\n    \n    GLOBAL_LOG --> FUNC\n    GLOBAL_LOG --> TIME\n    TRACE_LOG --> STACK\n    TRACE_LOG --> STATE\n```\n\n**Debugging workflow:**\n\n1. **Start with `logs/utils.log`**: User-level operations and query processing\n2. **Check `logs/global.log`**: Function call traces with timing data\n3. **Deep dive `logs/trace.log`**: Full stack traces and parameter values\n4. **Correlate `logs/all.log`**: Combined view for complex issues\n\n**Common debugging patterns:**\n\n| Issue | Log File | What to Look For |\n|-------|----------|------------------|\n| Tool execution failure | `global.log` | `call_tools_safely` function call with tool arguments |\n| Timeout in execution | `trace.log` | Subprocess/thread execution duration |\n| Serialization error | `trace.log` | `filter_and_deepcopy_globals` exceptions (PickleError, UnicodeDecodeError) |\n| Task tree changes | `global.log` | `RecursivePlanTreeTodoTool.run()` calls and `_analyze_changes` output |\n| LLM response format | `global.log` | `generate_assistant_output_append` with response structure |\n\nSources: Architecture diagram (Diagram 4: Observability and Logging Architecture)\n\n---\n\n## Task Tree Version Tracking\n\n### Testing Task Management\n\nThe task tracking system maintains version history in `arg_todo_list` and `out_todo_list` at [src/memory/tree_todo/todo_track.py:6-18]():\n\n```mermaid\ngraph LR\n    V1[\"Version 1<br/>RecursivePlanTree\"]\n    V2[\"Version 2<br/>RecursivePlanTree\"]\n    V3[\"Version 3<br/>RecursivePlanTree\"]\n    \n    V1 -->|\"append to\"| ARG[\"arg_todo_list\"]\n    V2 -->|\"append to\"| ARG\n    V3 -->|\"append to\"| ARG\n    \n    DIFF[\"_analyze_changes()\"]\n    V1 -->|\"compare\"| DIFF\n    V2 -->|\"with\"| DIFF\n    DIFF -->|\"Detects\"| CHANGES[\"New tasks<br/>Status changes<br/>Level adjustments\"]\n    \n    TEST[\"Test Assertions\"]\n    TEST -->|\"Check history\"| ARG\n    TEST -->|\"Verify diff\"| CHANGES\n```\n\n**Testing change detection:**\n\n```python\n# Create initial task tree\ninitial_tree = RecursivePlanTree(\n    core_goal=\"Test Goal\",\n    tree_nodes=[\n        RecursivePlanTreeNode(\n            task_id=\"TASK-1\",\n            task_name=\"Setup\",\n            status=TaskStatus.PENDING\n        )\n    ]\n)\n\n# Run tool (saves to arg_todo_list)\nresult1 = run(initial_tree)\n\n# Modify tree\nupdated_tree = RecursivePlanTree(\n    core_goal=\"Test Goal\",\n    tree_nodes=[\n        RecursivePlanTreeNode(\n            task_id=\"TASK-1\",\n            task_name=\"Setup\",\n            status=TaskStatus.COMPLETED  # Changed status\n        ),\n        RecursivePlanTreeNode(\n            task_id=\"TASK-2\",\n            task_name=\"Execute\",  # New task\n            status=TaskStatus.PENDING\n        )\n    ]\n)\n\nresult2 = run(updated_tree)\n\n# Assert change detection\nassert \"ð ç¶æåæ´\" in result2[\"changes_summary\"]\nassert \"ð æ°å¢ä»»å¡\" in result2[\"changes_summary\"]\n```\n\nSources: [src/memory/tree_todo/todo_track.py:1-201](), [src/memory/tree_todo/schemas.py:1-81]()\n\n---\n\n## Common Development Workflows\n\n### Workflow 1: Adding Python Execution Features\n\n```mermaid\ngraph TB\n    START[\"Identify requirement\"]\n    DESIGN[\"Design namespace handling\"]\n    \n    START --> DESIGN\n    \n    IMPL[\"Implement in executor\"]\n    TEST_ISO[\"Test isolation\"]\n    TEST_SERIAL[\"Test serialization\"]\n    \n    DESIGN --> IMPL\n    IMPL --> TEST_ISO\n    TEST_ISO --> TEST_SERIAL\n    \n    subgraph \"Test Cases\"\n        T1[\"Subprocess: Full isolation\"]\n        T2[\"Subthread: Shared memory\"]\n        T3[\"Direct: In-process\"]\n    end\n    \n    TEST_ISO --> T1\n    TEST_ISO --> T2\n    TEST_ISO --> T3\n    \n    VERIFY[\"Verify globals persistence\"]\n    LOG_CHECK[\"Check trace logs\"]\n    \n    T1 --> VERIFY\n    T2 --> VERIFY\n    T3 --> VERIFY\n    VERIFY --> LOG_CHECK\n    \n    LOG_CHECK --> DONE[\"Complete\"]\n```\n\n**Key considerations:**\n- Document namespace behavior in [docs/python.design.md]()\n- Test all three execution strategies\n- Verify `filter_and_deepcopy_globals` handles new data types\n- Check logs for serialization errors (UnicodeDecodeError, PickleError)\n\n### Workflow 2: Extending Tool System\n\n1. **Define tool class** extending `BaseTool`\n2. **Add Pydantic fields** with descriptions\n3. **Implement `run()` method** with business logic\n4. **Test schema generation** via `get_tool_schema()`\n5. **Register in agent** (add to tool registry)\n6. **Test end-to-end** with agent query\n\nSee [Creating New Tools](#8.2) for detailed guide.\n\nSources: [src/agent/tool/base_tool.py:7-76]()\n\n---\n\n## Best Practices\n\n### 1. Execution Mode Selection\n\n| Use Case | Recommended Mode | Reason |\n|----------|-----------------|---------|\n| Production agent | Subprocess | Maximum isolation, crash safety |\n| Development/debugging | Subthread | Faster, shared memory easier to inspect |\n| Unit tests | Direct | Fast execution, deterministic |\n| Untrusted code | Subprocess | Process isolation prevents escape |\n\n### 2. Logging Strategy\n\n- **Always use decorators**: Apply `@traceable` to new functions for automatic logging\n- **Log before serialization**: Critical for debugging pickle errors\n- **Include context**: Log function arguments for reproducibility\n- **Filter sensitive data**: Don't log credentials or API keys\n\n### 3. State Management\n\n- **Explicit workspace passing**: Always pass `arg_globals` between executions\n- **Deep copy caution**: Understand `model_copy(deep=True)` semantics in Pydantic V2\n- **Validate serialization**: Test that custom objects can be pickled\n- **Document exclusions**: Update `filter_and_deepcopy_globals` for non-serializable types\n\n### 4. Error Handling\n\n```python\n# Pattern: Structured execution with explicit status\nresult = run_structured_in_subprocess(code, globals, timeout)\n\nmatch result.status:\n    case ExecutionStatus.SUCCESS:\n        # Process result.stdout and result.out_globals\n        pass\n    case ExecutionStatus.TIMEOUT:\n        # Handle timeout (log.warning, retry, or fail gracefully)\n        pass\n    case ExecutionStatus.CRASHED:\n        # Process died (log.error, investigate core dump)\n        pass\n    case ExecutionStatus.FAILURE:\n        # Exception occurred (check result.exception_type and result.exception_str)\n        pass\n```\n\n### 5. Testing Guidelines\n\n- **Isolate components**: Test tools independently before integration\n- **Mock LLM calls**: Don't hit real APIs in tests\n- **Verify logs**: Assert expected log entries for critical paths\n- **Test failure paths**: Intentionally trigger errors to verify handling\n- **Version task trees**: Save task state snapshots for regression testing\n\nSources: Architecture diagrams, [src/agent/tool/base_tool.py:7-76](), [docs/python.design.md:274-426]()\n\n---\n\n## Next Steps\n\nFor hands-on guidance:\n- [Testing the Execution System](#8.1) - Deep dive into executor testing and namespace debugging\n- [Creating New Tools](#8.2) - Step-by-step tool implementation guide\n- [Database Setup for Examples](#8.3) - Configure PostgreSQL/MySQL for data processing examples\n\nFor system internals:\n- [Execution Runtime](#5) - Detailed executor architecture\n- [Observability and Logging](#6) - Complete logging system reference\n- [Tool System](#4) - Tool architecture and registry\n\n---\n\n# Page: Testing the Execution System\n\n# Testing the Execution System\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/python.design.md](docs/python.design.md)\n- [src/runtime/python_executor.py](src/runtime/python_executor.py)\n- [tests/unit/runtime/test_exec_runner.py](tests/unit/runtime/test_exec_runner.py)\n- [tests/unit/runtime/test_python_executor.py](tests/unit/runtime/test_python_executor.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document provides guidance for developers who need to test and understand the execution system's behavior. It covers how to test the various execution modes, understand Python's `exec()` namespace semantics, and validate subprocess isolation. \n\nFor information about the execution strategies themselves (subprocess, subthread, direct), see [Execution Runtime](#5). For details on implementing new tools that use the execution system, see [Creating New Tools](#8.2).\n\n---\n\n## Understanding exec() Namespace Behavior\n\nBefore testing the execution system, it is critical to understand how Python's `exec()` function handles global and local namespaces, as this directly affects how code executes and how state is managed.\n\n### Key Namespace Principles\n\nWhen `exec()` is called with custom namespaces, Python applies specific rules:\n\n1. **Empty Dictionary Behavior**: When `exec(code, {}, {})` is called with two empty dictionaries, Python automatically injects `__builtins__` into the global namespace, but does **not** inject `__name__` or other typical module attributes\n2. **Namespace Priority**: Variable lookup follows the order: local namespace â global namespace â `__builtins__`\n3. **Assignment Target**: When both `globals` and `locals` are provided, module-level assignments (including function definitions) are written to `locals`, not `globals`\n\n### The __name__ Anomaly\n\n```mermaid\ngraph TB\n    subgraph \"exec() with Default Namespaces\"\n        A[\"exec(code)\"]\n        B[\"Uses current globals()\"]\n        C[\"__name__ = '__main__'\"]\n    end\n    \n    subgraph \"exec() with Empty Dicts\"\n        D[\"exec(code, {}, {})\"]\n        E[\"Python injects __builtins__\"]\n        F[\"__name__ resolves to builtins module\"]\n        G[\"No __name__ variable created\"]\n    end\n    \n    A --> B --> C\n    D --> E --> F\n    E --> G\n```\n\n**Sources**: [docs/python.design.md:1-98]()\n\n### globals() vs locals() Modification Support\n\nThe ability to dynamically modify these namespaces depends on the execution scope:\n\n| Scope | `globals()` Modifiable | `locals()` Modifiable | Notes |\n|-------|------------------------|----------------------|-------|\n| Module level | â Yes | â Yes | `locals()` is same as `globals()` |\n| Class body | â Yes | â Yes | Changes sync to class namespace |\n| Function body | â Yes | â No | `locals()` returns read-only snapshot |\n\n**Sources**: [docs/python.design.md:535-645]()\n\n---\n\n## Test Infrastructure Overview\n\nThe test suite for the execution system is located in `tests/unit/runtime/test_python_executor.py` and validates three key aspects: basic execution, structured results, and timeout handling.\n\n```mermaid\ngraph LR\n    subgraph \"Test Functions\"\n        T1[\"test_python_executor()\"]\n        T2[\"test_exception()\"]\n        T3[\"test_structured_executor()\"]\n    end\n    \n    subgraph \"Tested Functions\"\n        F1[\"run()\"]\n        F2[\"run_structured()\"]\n        F3[\"worker()\"]\n        F4[\"worker_with_globals_capture()\"]\n    end\n    \n    subgraph \"Execution Backend\"\n        E1[\"multiprocessing.Process\"]\n        E2[\"multiprocessing.Queue\"]\n        E3[\"ExecutionResult\"]\n        E4[\"ExecutionStatus\"]\n    end\n    \n    T1 --> F1\n    T2 --> F1\n    T3 --> F2\n    \n    F1 --> F3\n    F2 --> F4\n    \n    F3 --> E1\n    F4 --> E1\n    F3 --> E2\n    F4 --> E2\n    F4 --> E3\n    F4 --> E4\n```\n\n**Sources**: [tests/unit/runtime/test_python_executor.py:1-105](), [src/runtime/python_executor.py:1-164]()\n\n---\n\n## Testing Basic Execution with run()\n\nThe `run()` function executes code and returns string output. It demonstrates critical namespace behaviors that must be tested.\n\n### Test Case: Namespace Priority\n\nThe test at [tests/unit/runtime/test_python_executor.py:10-35]() validates how `exec()` handles conflicting values in `globals` and `locals`:\n\n```python\nmy_globals = {\"a\": 123, \"b\": [1, 2, 3]}\nmy_locals = {\"a\": 123, \"b\": [1, 2, 3]}\n\nrun(\"a+=100000\", my_globals, my_locals)  # Modifies locals[\"a\"]\nrun(\"print(a)\", my_globals, my_locals)   # Outputs: 100123 (from locals)\n```\n\n**Key Testing Insight**: When both `globals` and `locals` are provided, modifications target `locals` first. After execution:\n- `my_globals[\"a\"]` remains `123`\n- `my_locals[\"a\"]` becomes `100123`\n\n### Test Case: Exception Handling\n\nThe test at [tests/unit/runtime/test_python_executor.py:37-50]() validates error reporting:\n\n```python\nrun(\"print(c)\", my_globals, my_locals)  \n# Returns formatted error with traceback\n```\n\nThe `worker()` function at [src/runtime/python_executor.py:72-90]() uses `source_code.get_code_and_traceback()` to capture detailed error information including line numbers and exception context.\n\n**Sources**: [tests/unit/runtime/test_python_executor.py:10-50](), [src/runtime/python_executor.py:72-90]()\n\n---\n\n## Testing Structured Execution with run_structured()\n\nThe `run_structured()` function returns an `ExecutionResult` object instead of a string, enabling programmatic status checking and state capture.\n\n### Execution Result Schema\n\n```mermaid\ngraph TB\n    subgraph \"ExecutionResult Fields\"\n        ER[\"ExecutionResult\"]\n        CMD[\"command: str\"]\n        TO[\"timeout: Optional[int]\"]\n        GLOB[\"globals: Dict[str, Any]\"]\n        STAT[\"exit_status: ExecutionStatus\"]\n        OUT[\"stdout: str\"]\n        EXC_T[\"exception_type: Optional[str]\"]\n        EXC_V[\"exception_value: Optional[str]\"]\n        EXC_TB[\"exception_traceback: Optional[str]\"]\n    end\n    \n    subgraph \"ExecutionStatus Enum\"\n        S1[\"SUCCESS\"]\n        S2[\"FAILURE\"]\n        S3[\"TIMEOUT\"]\n        S4[\"CRASHED\"]\n    end\n    \n    ER --> CMD\n    ER --> TO\n    ER --> GLOB\n    ER --> STAT\n    ER --> OUT\n    ER --> EXC_T\n    ER --> EXC_V\n    ER --> EXC_TB\n    \n    STAT --> S1\n    STAT --> S2\n    STAT --> S3\n    STAT --> S4\n```\n\n**Sources**: [src/runtime/python_executor.py:8](), [src/runtime/schemas.py]()\n\n### Test Case: Success Path\n\n```python\nmy_globals = {\"a\": 123, \"b\": [1, 2, 3]}\nresult = run_structured(\"a += 100000\", my_globals, None)\n\nassert result.exit_status == ExecutionStatus.SUCCESS\nassert result.globals[\"a\"] == 100123\nassert result.exception_type is None\n```\n\nThe `worker_with_globals_capture()` function at [src/runtime/python_executor.py:28-71]() executes code and captures the modified globals dictionary in the `ExecutionResult`.\n\n### Test Case: Timeout Detection\n\nThe timeout test at [tests/unit/runtime/test_python_executor.py:79-86]() validates subprocess termination:\n\n```python\nresult = run_structured(\"\"\"\nimport time\nc = 10\ntime.sleep(5)\n\"\"\", my_globals, my_locals, timeout=1)\n\nassert result.exit_status == ExecutionStatus.TIMEOUT\nassert result.stdout == \"\"\n```\n\nThe timeout mechanism at [src/runtime/python_executor.py:114-124]() uses `Process.join(timeout)` followed by `Process.terminate()` if the process is still alive.\n\n**Sources**: [tests/unit/runtime/test_python_executor.py:52-97](), [src/runtime/python_executor.py:28-71](), [src/runtime/python_executor.py:92-130]()\n\n---\n\n## Testing Subprocess Isolation\n\nA critical aspect of the execution system is process-level isolation. Tests must validate that subprocess execution does **not** share memory with the parent process.\n\n### Process Communication Architecture\n\n```mermaid\nsequenceDiagram\n    participant P as \"Parent Process\"\n    participant Q as \"multiprocessing.Queue\"\n    participant C as \"Child Process\"\n    participant W as \"worker_with_globals_capture()\"\n    \n    P->>Q: Create Queue\n    P->>C: Spawn Process(target=worker_with_globals_capture)\n    P->>C: Start process\n    C->>W: Execute in isolated memory\n    W->>W: exec(code, globals, locals)\n    W->>W: Capture ExecutionResult\n    W->>Q: queue.put(exec_result)\n    C->>C: Process exits\n    P->>Q: result = queue.get()\n    P->>P: Return ExecutionResult\n```\n\n**Sources**: [src/runtime/python_executor.py:104-130]()\n\n### Testing Isolation Properties\n\nWhen testing subprocess execution, validate these isolation properties:\n\n1. **No Shared State**: Modifications in the child process do not affect parent process variables\n2. **Serialization Boundary**: Only picklable objects can be passed through the queue (see [Workspace State Management](#5.5) for filtering details)\n3. **Independent Environment**: Child process has its own `sys.stdout`, `sys.stderr`, and working directory\n\n### Test Case: Import Isolation\n\nThe test at [tests/unit/runtime/test_python_executor.py:89-97]() validates that imports in the subprocess work correctly:\n\n```python\nresult = run_structured(\"\"\"\nimport time\nc = 10\nimport scipy\n\"\"\", my_globals, None, timeout=20000)\n\nassert result.exit_status == ExecutionStatus.SUCCESS\n# scipy import does not pollute parent process\n```\n\n**Sources**: [tests/unit/runtime/test_python_executor.py:79-97](), [src/runtime/python_executor.py:106-116]()\n\n---\n\n## Understanding Function Definition Behavior\n\nA common point of confusion when testing `exec()` is where function definitions end up. This has significant implications for code that defines and calls functions.\n\n### The Function Placement Problem\n\n```mermaid\ngraph TB\n    subgraph \"Scenario 1: Only globals provided\"\n        S1A[\"exec(code, my_globals)\"]\n        S1B[\"locals defaults to globals\"]\n        S1C[\"Functions written to my_globals\"]\n    end\n    \n    subgraph \"Scenario 2: Both globals and locals provided\"\n        S2A[\"exec(code, my_globals, my_locals)\"]\n        S2B[\"Separate namespace objects\"]\n        S2C[\"Functions written to my_locals\"]\n        S2D[\"my_globals unchanged\"]\n    end\n    \n    S1A --> S1B --> S1C\n    S2A --> S2B --> S2C\n    S2B --> S2D\n```\n\n**Sources**: [docs/python.design.md:428-534]()\n\n### Test Case: Function Accessibility\n\nWhen testing code that defines functions, you must account for this behavior:\n\n```python\ncompiled_code = compile(\"\"\"\ndef myadd(a, b):\n    return a + b\n    \nresult = myadd(5, 3)\nprint(result)\n\"\"\", \"<string>\", \"exec\")\n\n# Scenario 1: Only globals\nmy_globals = {'__name__': '__main__'}\nexec(compiled_code, my_globals)\nassert 'myadd' in my_globals  # â True\n\n# Scenario 2: Separate locals\nmy_globals = {'__name__': '__main__'}\nmy_locals = {}\nexec(compiled_code, my_globals, my_locals)\nassert 'myadd' not in my_globals  # â True - functions in locals only\nassert 'myadd' in my_locals       # â True\n```\n\n### Recommended Testing Pattern\n\nFor tests that need functions in the global namespace (as the execution system does), pass only the `globals` parameter:\n\n```python\n# Recommended for tool execution\nresult = run_structured(code, my_globals)  # locals defaults to globals\n\n# Avoid for function definitions\nresult = run_structured(code, my_globals, {})  # Separate namespaces\n```\n\n**Sources**: [docs/python.design.md:441-534](), [src/runtime/python_executor.py:92-130]()\n\n---\n\n## Testing the exec_runner Module\n\nThe `exec_runner` module provides a higher-level interface for executing code with pre-injected modules. Tests for this system are in `tests/unit/runtime/test_exec_runner.py`.\n\n### Module Injection Test Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Test Configuration\"\n        TC[\"test_exec_runner.py:main()\"]\n        CONF[\"config.CODE_TO_EXEC\"]\n        FLAGS[\"Module inclusion flags\"]\n    end\n    \n    subgraph \"Execution Pipeline\"\n        ER[\"exec_runner.run_exec_with_modules()\"]\n        INJ[\"Module injection logic\"]\n        EXEC[\"exec() with enriched globals\"]\n    end\n    \n    subgraph \"Module Categories\"\n        BLT[\"include_builtin\"]\n        STD[\"include_stdlib\"]\n        TPP[\"include_third_party\"]\n        ALL[\"include_all_installed\"]\n        CST[\"include_custom\"]\n    end\n    \n    TC --> CONF\n    TC --> FLAGS\n    TC --> ER\n    ER --> INJ\n    INJ --> BLT\n    INJ --> STD\n    INJ --> TPP\n    INJ --> ALL\n    INJ --> CST\n    INJ --> EXEC\n```\n\n**Sources**: [tests/unit/runtime/test_exec_runner.py:1-29]()\n\n### Test Configuration\n\nThe test at [tests/unit/runtime/test_exec_runner.py:5-26]() demonstrates how to test module injection:\n\n```python\nexec_runner.run_exec_with_modules(\n    code=config.CODE_TO_EXEC,\n    include_builtin=True,      # __builtins__ injection\n    include_stdlib=True,        # os, sys, json, etc.\n    include_third_party=True,   # numpy, pandas, etc.\n    include_all_installed=False,\n    include_custom=config.CUSTOM_PACKAGES,\n    print_mod_names=config.PRINT_SOURCES  # Debug output\n)\n```\n\nThis tests that the execution environment has access to the specified module categories without requiring explicit import statements in the executed code.\n\n**Sources**: [tests/unit/runtime/test_exec_runner.py:5-26]()\n\n---\n\n## Common Testing Patterns\n\n### Pattern 1: Testing State Persistence\n\n```python\ndef test_state_persistence():\n    globals_dict = {}\n    \n    # First execution\n    result1 = run_structured(\"x = 42\", globals_dict)\n    assert result1.exit_status == ExecutionStatus.SUCCESS\n    \n    # Second execution uses modified globals\n    result2 = run_structured(\"print(x)\", globals_dict)\n    assert \"42\" in result2.stdout\n```\n\n### Pattern 2: Testing Exception Information\n\n```python\ndef test_exception_details():\n    result = run_structured(\"1 / 0\", {})\n    \n    assert result.exit_status == ExecutionStatus.FAILURE\n    assert result.exception_type == \"ZeroDivisionError\"\n    assert result.exception_value is not None\n    assert result.exception_traceback is not None\n```\n\n### Pattern 3: Testing Timeout with Buffer\n\n```python\ndef test_timeout_with_margin():\n    # Use timeout > expected duration to account for startup overhead\n    result = run_structured(\n        \"import time; time.sleep(0.1)\",\n        {},\n        timeout=5  # 50x the actual sleep time\n    )\n    \n    assert result.exit_status == ExecutionStatus.SUCCESS\n```\n\n### Pattern 4: Testing Output Capture\n\n```python\ndef test_stdout_capture():\n    result = run_structured(\"\"\"\nimport sys\nprint(\"Line 1\", file=sys.stdout)\nprint(\"Line 2\", file=sys.stdout)\n\"\"\", {})\n    \n    assert result.exit_status == ExecutionStatus.SUCCESS\n    assert \"Line 1\" in result.stdout\n    assert \"Line 2\" in result.stdout\n```\n\n**Sources**: [tests/unit/runtime/test_python_executor.py:10-105](), [src/runtime/python_executor.py:28-71]()\n\n---\n\n## Debugging Failed Tests\n\nWhen tests fail, the execution system provides multiple debugging mechanisms:\n\n| Information Source | Location | Content |\n|-------------------|----------|---------|\n| `ExecutionResult.exception_type` | Returned by `run_structured()` | Exception class name |\n| `ExecutionResult.exception_value` | Returned by `run_structured()` | Exception `repr()` output |\n| `ExecutionResult.exception_traceback` | Returned by `run_structured()` | Full traceback string |\n| `ExecutionResult.stdout` | Returned by `run_structured()` | Captured print output |\n| Log files | `logs/global.log`, `logs/trace.log` | Function call traces with `@traceable` decorator |\n\nThe traceback includes line numbers relative to the executed code string, making it easier to identify the exact location of failures.\n\n**Sources**: [src/runtime/python_executor.py:46-67](), [src/runtime/source_code.py]()\n\n---\n\n## Summary\n\nTesting the execution system requires understanding:\n\n1. **Namespace Semantics**: How `exec()` handles `globals` vs `locals` and when to use each\n2. **Process Isolation**: How subprocess execution prevents memory sharing and requires serialization\n3. **Result Structures**: Using `ExecutionResult` to programmatically validate execution outcomes\n4. **Timeout Handling**: How to test time-limited execution and detect hangs\n5. **Function Definitions**: Where functions are stored depends on whether separate `locals` is provided\n\nKey test files:\n- [tests/unit/runtime/test_python_executor.py](): Core execution system tests\n- [tests/unit/runtime/test_exec_runner.py](): Module injection tests\n- [docs/python.design.md](): Detailed namespace behavior documentation\n\n**Sources**: [tests/unit/runtime/test_python_executor.py:1-105](), [src/runtime/python_executor.py:1-164](), [docs/python.design.md:1-645]()\n\n---\n\n# Page: Creating New Tools\n\n# Creating New Tools\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/functino_call_err.design.md](docs/functino_call_err.design.md)\n- [src/agent/action.py](src/agent/action.py)\n- [src/agent/memory.py](src/agent/memory.py)\n- [src/agent/tool/base_tool.py](src/agent/tool/base_tool.py)\n- [src/agent/tool/python_tool.py](src/agent/tool/python_tool.py)\n- [src/agent/tool/todo_tool.py](src/agent/tool/todo_tool.py)\n- [src/memory/tree_todo/schemas.py](src/memory/tree_todo/schemas.py)\n- [src/memory/tree_todo/todo_track.py](src/memory/tree_todo/todo_track.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page provides a step-by-step guide for developers who want to extend the algo_agent system by implementing custom tools. You will learn how to:\n\n- Extend the `BaseTool` abstract class with proper Pydantic schemas\n- Implement tool execution logic in the `run()` method\n- Integrate your tool with the agent's action dispatcher\n- Follow best practices for parameter definitions and error handling\n\nFor information about the overall tool architecture and existing tools, see [Tool System](#4). For details on the BaseTool interface specification, see [BaseTool Interface](#4.1).\n\n---\n\n## Understanding the BaseTool Interface\n\nThe `BaseTool` class ([src/agent/tool/base_tool.py:7-76]()) defines the contract that all tools must implement. It uses Pydantic for automatic schema generation and validation.\n\n### Core Components\n\n```mermaid\nclassDiagram\n    class BaseTool {\n        <<abstract>>\n        +str tool_call_purpose\n        +tool_name() str\n        +tool_description() str\n        +get_parameter_schema() dict\n        +get_tool_schema() dict\n        +run() str*\n    }\n    \n    class ExecutePythonCodeTool {\n        +str python_code_snippet\n        +int timeout\n        +run() str\n    }\n    \n    class RecursivePlanTreeTodoTool {\n        +RecursivePlanTree recursive_plan_tree\n        +run() Dict~str,str~\n    }\n    \n    class YourCustomTool {\n        +CustomType your_parameter\n        +run() str\n    }\n    \n    BaseTool <|-- ExecutePythonCodeTool\n    BaseTool <|-- RecursivePlanTreeTodoTool\n    BaseTool <|-- YourCustomTool\n    \n    note for BaseTool \"All methods are classmethods except run()\\nExtends pydantic.BaseModel\"\n```\n\n**Sources:** [src/agent/tool/base_tool.py:7-76]()\n\n### Required Methods\n\n| Method | Type | Purpose | Implementation |\n|--------|------|---------|----------------|\n| `tool_name()` | `@classmethod` | Returns unique identifier | Auto-generated from class name |\n| `tool_description()` | `@classmethod` | Returns tool description | Extracted from class docstring |\n| `get_parameter_schema()` | `@classmethod` | Returns JSON schema | Auto-generated by Pydantic |\n| `get_tool_schema()` | `@classmethod` | Returns full tool schema | Combines above into OpenAI format |\n| `run()` | instance method | Executes tool logic | **Must be implemented** |\n\n**Sources:** [src/agent/tool/base_tool.py:13-76]()\n\n---\n\n## Step-by-Step Implementation Guide\n\n### Step 1: Create Tool Class with Pydantic Fields\n\nCreate a new file in `src/agent/tool/` directory. Your tool must extend `BaseTool` and define parameters as Pydantic fields:\n\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Dict, Any\nfrom src.agent.tool.base_tool import BaseTool\n\nclass MyCustomTool(BaseTool):\n    \"\"\"\n    Brief description of what this tool does.\n    \n    **Functionality:**\n    - List key capabilities\n    - Explain when to use this tool\n    \n    **Important Rules:**\n    1. List any constraints or requirements\n    2. Specify expected input/output formats\n    \"\"\"\n    \n    # Define parameters as Pydantic fields\n    input_parameter: str = Field(\n        ...,  # Required field\n        description=\"Clear description for the LLM to understand\",\n        examples=[\"example value\"]\n    )\n    \n    optional_timeout: int = Field(\n        default=30,\n        description=\"Optional parameter with default value\"\n    )\n```\n\n**Sources:** [src/agent/tool/python_tool.py:13-40](), [src/agent/tool/todo_tool.py:10-26]()\n\n### Tool Naming Convention\n\nThe `tool_name()` classmethod automatically generates the tool identifier from your class name using the `inflection` library:\n\n```mermaid\ngraph LR\n    A[\"MyCustomTool\"] --> B[\"inflection.underscore()\"]\n    B --> C[\"Remove 'Tool' suffix\"]\n    C --> D[\"my_custom\"]\n    \n    E[\"ExecutePythonCodeTool\"] --> F[\"inflection.underscore()\"]\n    F --> G[\"Remove 'Tool' suffix\"]\n    G --> H[\"execute_python_code\"]\n```\n\n**Sources:** [src/agent/tool/base_tool.py:13-18]()\n\n### Step 2: Implement the run() Method\n\nThe `run()` method contains your tool's core logic. It receives parameters via `self` (Pydantic model instance) and must return a string result:\n\n```python\ndef run(self) -> str:\n    \"\"\"Execute the tool logic\"\"\"\n    try:\n        # Access parameters via self\n        user_input = self.input_parameter\n        timeout = self.optional_timeout\n        \n        # Perform your tool's logic\n        result = perform_operation(user_input, timeout)\n        \n        # Return formatted string for LLM\n        return f\"Operation completed: {result}\"\n        \n    except Exception as e:\n        # Return error information\n        return f\"Tool execution failed: {str(e)}\"\n```\n\n**Return Value Guidelines:**\n- Always return a string (LLM will read this)\n- Include relevant execution results\n- For errors, provide diagnostic information\n- Format output to be LLM-readable\n\n**Sources:** [src/agent/tool/python_tool.py:41-51](), [src/agent/tool/todo_tool.py:28-36]()\n\n---\n\n## Schema Generation and LLM Integration\n\n### How Schema Generation Works\n\nThe `get_tool_schema()` method combines class metadata into OpenAI function calling format:\n\n```mermaid\nflowchart TB\n    subgraph \"Class Definition\"\n        A[\"MyCustomTool class\"]\n        B[\"Docstring\"]\n        C[\"Pydantic Fields\"]\n    end\n    \n    subgraph \"Schema Generation\"\n        D[\"tool_name()\"]\n        E[\"tool_description()\"]\n        F[\"get_parameter_schema()\"]\n    end\n    \n    subgraph \"Output Schema\"\n        G[\"type: function\"]\n        H[\"function.name\"]\n        I[\"function.description\"]\n        J[\"function.parameters\"]\n    end\n    \n    A --> D\n    B --> E\n    C --> F\n    \n    D --> H\n    E --> I\n    F --> J\n    \n    H --> G\n    I --> G\n    J --> G\n```\n\n**Sources:** [src/agent/tool/base_tool.py:31-71]()\n\n### Generated Schema Example\n\nFor the `ExecutePythonCodeTool`, the schema becomes:\n\n| Schema Field | Source | Example Value |\n|--------------|--------|---------------|\n| `function.name` | Class name â `tool_name()` | `\"execute_python_code\"` |\n| `function.description` | Docstring â `tool_description()` | Chinese description from [src/agent/tool/python_tool.py:14-28]() |\n| `function.parameters.properties.python_code_snippet` | Field definition | `{\"type\": \"string\", \"description\": \"...\", \"examples\": [...]}` |\n| `function.parameters.properties.timeout` | Field definition | `{\"type\": \"integer\", \"default\": 30, \"description\": \"...\"}` |\n\n**Sources:** [src/agent/tool/python_tool.py:13-40](), [src/agent/tool/base_tool.py:31-71]()\n\n---\n\n## Tool Registration and Dispatch\n\n### Step 3: Register Tool in Action Dispatcher\n\nTo make your tool available to the agent, add it to the `call_tools_safely()` function in `src/agent/action.py`:\n\n```mermaid\nsequenceDiagram\n    participant Agent as \"Deep Research Agent\"\n    participant Dispatcher as \"call_tools_safely()\"\n    participant YourTool as \"YourCustomTool\"\n    \n    Agent->>Dispatcher: tool_info dict\n    Note over Dispatcher: tool_call_name<br/>tool_call_arguments\n    \n    Dispatcher->>Dispatcher: Parse function_name<br/>and arguments\n    \n    alt function_name == \"your_custom\"\n        Dispatcher->>YourTool: Instantiate(**arguments)\n        YourTool->>YourTool: run()\n        YourTool-->>Dispatcher: result string\n    end\n    \n    Dispatcher->>Dispatcher: Set tool_info[\"content\"]\n    Dispatcher-->>Agent: Updated tool_info\n```\n\n**Implementation:**\n\nAdd your tool to the dispatch chain in [src/agent/action.py:10-48]():\n\n```python\nfrom src.agent.tool.your_module import YourCustomTool\n\n@traceable\ndef call_tools_safely(tool_info: dict):\n    def call_tools(tool_info: dict):\n        function_name = tool_info[\"tool_call_name\"]\n        arguments = json.loads(tool_info[\"tool_call_arguments\"])\n        \n        if False: pass\n        elif function_name == ExecutePythonCodeTool.tool_name():\n            execute_python_code_tool = ExecutePythonCodeTool(**arguments)\n            tool_info[\"content\"] = execute_python_code_tool.run()\n        elif function_name == RecursivePlanTreeTodoTool.tool_name():\n            recursive_plan_tree_todo_tool = RecursivePlanTreeTodoTool(**arguments)\n            tool_info[\"content\"] = recursive_plan_tree_todo_tool.run()\n        # Add your tool here\n        elif function_name == YourCustomTool.tool_name():\n            your_custom_tool = YourCustomTool(**arguments)\n            tool_info[\"content\"] = your_custom_tool.run()\n        \n        return tool_info\n```\n\n**Sources:** [src/agent/action.py:10-48]()\n\n### Step 4: Register Tool Schema for LLM\n\nEnsure the tool schema is included when calling the LLM. This typically happens in the agent's query processing loop where tools are gathered:\n\n```python\n# In your agent initialization or query loop\ntools_list = [\n    ExecutePythonCodeTool.get_tool_schema(),\n    RecursivePlanTreeTodoTool.get_tool_schema(),\n    YourCustomTool.get_tool_schema(),  # Add your tool\n]\n```\n\n**Sources:** [src/agent/tool/base_tool.py:31-71]()\n\n---\n\n## Best Practices\n\n### Field Definitions\n\n| Practice | Example | Rationale |\n|----------|---------|-----------|\n| **Use descriptive names** | `python_code_snippet` instead of `code` | LLM needs clear parameter names |\n| **Provide detailed descriptions** | Include purpose, format, constraints | Helps LLM construct valid arguments |\n| **Add examples** | `examples=[\"print('Hello')\"]` | Guides LLM parameter generation |\n| **Use appropriate defaults** | `timeout: int = Field(default=30)` | Makes parameters optional where sensible |\n| **Specify types precisely** | Use `List[str]`, `Dict[str, Any]`, etc. | Enables validation and better schemas |\n\n**Sources:** [src/agent/tool/python_tool.py:29-39]()\n\n### Docstring Guidelines\n\nThe class docstring becomes the tool's description for the LLM. Write it in a clear, instructional format:\n\n```python\nclass MyTool(BaseTool):\n    \"\"\"\n    [One-line summary of purpose]\n    \n    **Functionality:**\n    - Key feature 1\n    - Key feature 2\n    \n    **Important Rules:**\n    1. Constraint or requirement\n    2. Expected behavior\n    3. Common pitfalls to avoid\n    \"\"\"\n```\n\n**Sources:** [src/agent/tool/python_tool.py:14-28](), [src/agent/tool/todo_tool.py:11-17]()\n\n### Error Handling\n\nAlways catch and format exceptions appropriately:\n\n```python\ndef run(self) -> str:\n    try:\n        # Tool logic\n        result = self.execute_logic()\n        return f\"Success: {result}\"\n    except ValidationError as e:\n        return f\"Validation failed: {e}\"\n    except Exception as e:\n        return f\"Execution error: {str(e)}\"\n```\n\nThe `call_tools_safely()` function provides an outer error handler, but internal error handling improves user experience.\n\n**Sources:** [src/agent/action.py:40-47]()\n\n### State Management Considerations\n\nIf your tool needs to maintain state across invocations:\n\n1. **Use workspace module** - Like `ExecutePythonCodeTool` uses `workspace.get_arg_globals()` and `workspace.append_out_globals()` ([src/agent/tool/python_tool.py:42-49]())\n2. **Use module-level variables** - Like `RecursivePlanTreeTodoTool` uses `arg_todo_list` in `todo_track.py` ([src/memory/tree_todo/todo_track.py:6-18]())\n3. **Document state behavior** - Clearly explain in docstring how state persists\n\n**Sources:** [src/agent/tool/python_tool.py:42-49](), [src/memory/tree_todo/todo_track.py:6-18]()\n\n---\n\n## Complete Example: File Reader Tool\n\nHere's a complete example of a simple tool that reads and summarizes files:\n\n```python\n# src/agent/tool/file_reader_tool.py\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\nimport os\nfrom src.agent.tool.base_tool import BaseTool\nfrom src.utils import global_logger\n\nclass FileReaderTool(BaseTool):\n    \"\"\"\n    Reads text files and returns their contents or statistics.\n    \n    **Functionality:**\n    - Read file contents with encoding support\n    - Return file statistics (size, line count)\n    - Handle common file reading errors\n    \n    **Important Rules:**\n    1. File paths must be relative to working directory\n    2. Only text files are supported\n    3. Large files (>1MB) return statistics instead of full content\n    \"\"\"\n    \n    file_path: str = Field(\n        ...,\n        description=\"Path to the file to read, relative to working directory\",\n        examples=[\"data/input.txt\", \"logs/output.log\"]\n    )\n    \n    encoding: str = Field(\n        default=\"utf-8\",\n        description=\"Character encoding of the file\"\n    )\n    \n    max_size_mb: int = Field(\n        default=1,\n        description=\"Maximum file size in MB to read fully\"\n    )\n    \n    def run(self) -> str:\n        try:\n            # Validate file exists\n            if not os.path.exists(self.file_path):\n                return f\"Error: File '{self.file_path}' does not exist\"\n            \n            # Check file size\n            file_size = os.path.getsize(self.file_path)\n            file_size_mb = file_size / (1024 * 1024)\n            \n            if file_size_mb > self.max_size_mb:\n                # Return statistics for large files\n                with open(self.file_path, 'r', encoding=self.encoding) as f:\n                    line_count = sum(1 for _ in f)\n                return (\n                    f\"File '{self.file_path}' is too large ({file_size_mb:.2f}MB).\\n\"\n                    f\"Statistics: {line_count} lines, {file_size} bytes\"\n                )\n            \n            # Read file contents\n            with open(self.file_path, 'r', encoding=self.encoding) as f:\n                content = f.read()\n            \n            global_logger.info(f\"Read file: {self.file_path}\")\n            return f\"File contents ({len(content)} chars):\\n{content}\"\n            \n        except UnicodeDecodeError as e:\n            return f\"Encoding error: Cannot decode file with {self.encoding}. Try a different encoding.\"\n        except Exception as e:\n            global_logger.error(f\"FileReaderTool failed: {e}\", exc_info=True)\n            return f\"Error reading file: {str(e)}\"\n```\n\n### Integration Steps\n\n```mermaid\ngraph TB\n    subgraph \"1. Tool Definition\"\n        A[\"Define FileReaderTool class\"]\n        B[\"Add Pydantic fields\"]\n        C[\"Implement run() method\"]\n    end\n    \n    subgraph \"2. Action Registration\"\n        D[\"Import in action.py\"]\n        E[\"Add elif branch\"]\n        F[\"Instantiate and call\"]\n    end\n    \n    subgraph \"3. Schema Registration\"\n        G[\"Add to tools_list\"]\n        H[\"Pass to LLM\"]\n    end\n    \n    subgraph \"4. Testing\"\n        I[\"Test standalone\"]\n        J[\"Test via agent\"]\n    end\n    \n    A --> B --> C --> D\n    D --> E --> F --> G\n    G --> H --> I\n    I --> J\n```\n\nThen register in `action.py`:\n\n```python\nfrom src.agent.tool.file_reader_tool import FileReaderTool\n\nelif function_name == FileReaderTool.tool_name():\n    file_reader_tool = FileReaderTool(**arguments)\n    tool_info[\"content\"] = file_reader_tool.run()\n```\n\n**Sources:** [src/agent/tool/python_tool.py:13-51](), [src/agent/tool/base_tool.py:7-76](), [src/agent/action.py:10-48]()\n\n---\n\n## Debugging and Testing\n\n### Testing Your Tool Standalone\n\nBefore integrating with the agent, test your tool directly:\n\n```python\nif __name__ == \"__main__\":\n    # Test with valid input\n    tool = FileReaderTool(\n        tool_call_purpose=\"Testing file reader\",\n        file_path=\"test.txt\",\n        encoding=\"utf-8\"\n    )\n    result = tool.run()\n    print(result)\n    \n    # Test error handling\n    tool_invalid = FileReaderTool(\n        tool_call_purpose=\"Testing error case\",\n        file_path=\"nonexistent.txt\"\n    )\n    error_result = tool_invalid.run()\n    print(error_result)\n```\n\n### Validating Schema Generation\n\nCheck that your tool generates the expected schema:\n\n```python\nimport json\n\nschema = FileReaderTool.get_tool_schema()\nprint(json.dumps(schema, indent=2))\n```\n\nThis should output the OpenAI function calling format with all your parameters correctly specified.\n\n**Sources:** [src/agent/tool/todo_tool.py:41-133](), [src/agent/tool/base_tool.py:31-71]()\n\n---\n\n## Common Patterns and Anti-Patterns\n\n### â Good Patterns\n\n| Pattern | Example Tool | Benefit |\n|---------|--------------|---------|\n| **Stateless when possible** | Simple calculations | Easier to reason about |\n| **Clear parameter validation** | Check file paths exist | Fail fast with good errors |\n| **Descriptive return values** | Include context in results | LLM can understand outcomes |\n| **Leverage Pydantic validation** | Use `Field` constraints | Automatic validation |\n| **Log important operations** | `global_logger.info()` | Debugging and tracing |\n\n**Sources:** [src/agent/tool/python_tool.py:41-51]()\n\n### â Anti-Patterns to Avoid\n\n| Anti-Pattern | Problem | Solution |\n|--------------|---------|----------|\n| **Non-string return values** | LLM expects strings | Always return `str` from `run()` |\n| **Unclear error messages** | \"Error occurred\" | Include details: \"File not found: path.txt\" |\n| **Missing docstrings** | LLM can't understand tool | Write clear, structured docstrings |\n| **Mutable default arguments** | State leakage | Use `Field(default_factory=list)` |\n| **Side effects without logging** | Hard to debug | Log state changes with `global_logger` |\n\n**Sources:** [src/agent/tool/base_tool.py:73-75](), [src/agent/action.py:40-47]()\n\n---\n\n## Tool Integration Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Tool Development\"\n        DEV[\"Developer\"]\n        TOOL[\"MyCustomTool class\"]\n        TEST[\"Standalone Tests\"]\n    end\n    \n    subgraph \"Registration\"\n        ACTION[\"action.py\"]\n        DISPATCH[\"call_tools_safely()\"]\n    end\n    \n    subgraph \"Agent Runtime\"\n        AGENT[\"Deep Research Agent\"]\n        LLM[\"LLM Service\"]\n        SCHEMA[\"Tool Schema List\"]\n    end\n    \n    subgraph \"Execution\"\n        INSTANTIATE[\"Tool Instance\"]\n        RUN[\"run() method\"]\n        RESULT[\"String Result\"]\n    end\n    \n    DEV --> TOOL\n    TOOL --> TEST\n    TEST --> ACTION\n    \n    ACTION --> DISPATCH\n    TOOL --> SCHEMA\n    \n    AGENT --> LLM\n    SCHEMA --> LLM\n    LLM --> DISPATCH\n    \n    DISPATCH --> INSTANTIATE\n    INSTANTIATE --> RUN\n    RUN --> RESULT\n    RESULT --> AGENT\n```\n\n**Sources:** [src/agent/action.py:10-48](), [src/agent/tool/base_tool.py:7-76]()\n\n---\n\n## Summary Checklist\n\nWhen creating a new tool, ensure you have:\n\n- [ ] Extended `BaseTool` class\n- [ ] Written clear docstring with functionality and rules\n- [ ] Defined all parameters as Pydantic fields with descriptions\n- [ ] Implemented `run()` method returning a string\n- [ ] Added error handling in `run()`\n- [ ] Registered tool in `call_tools_safely()` dispatcher\n- [ ] Added tool schema to LLM tools list\n- [ ] Written standalone tests\n- [ ] Tested integration with agent\n- [ ] Added logging for important operations\n- [ ] Documented any state management behavior\n\n**Sources:** [src/agent/tool/base_tool.py:7-76](), [src/agent/action.py:10-48](), [src/agent/tool/python_tool.py:13-51](), [src/agent/tool/todo_tool.py:10-36]()\n\n---\n\n# Page: Database Setup for Examples\n\n# Database Setup for Examples\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.vscode/settings.json](.vscode/settings.json)\n- [docs/gen.md](docs/gen.md)\n- [docs/pg.md](docs/pg.md)\n- [docs/sql.md](docs/sql.md)\n- [pyproject.toml](pyproject.toml)\n- [uv.lock](uv.lock)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document provides instructions for setting up database environments required to run data processing examples in the algo_agent system. The examples in sections [7.2](#7.2) and [7.3](#7.3) demonstrate data processing capabilities using relational databases (PostgreSQL, MySQL) and vector databases (pgvector).\n\nThis page covers Docker-based database installation, configuration, and verification. For information about the actual data processing examples and use cases, see [Use Cases and Examples](#7). For general testing strategies, see [Testing the Execution System](#8.1).\n\n---\n\n## Database Requirements Overview\n\nThe algo_agent system's data processing examples require database backends for:\n\n| Database Type | Use Case | Required For |\n|--------------|----------|--------------|\n| **PostgreSQL 16** | Relational data storage and SQL query examples | Geographic data processing, structured data analysis |\n| **MySQL 8.0** | Cross-database compatibility testing | SQL portability examples, continuous login analysis |\n| **pgvector (PostgreSQL 17)** | Vector similarity search and embeddings storage | AI/ML feature storage, semantic search examples |\n\nAll databases are deployed via Docker containers to ensure consistent, reproducible development environments without local installation conflicts.\n\n**Sources:** [docs/pg.md:1-257]()\n\n---\n\n## Docker Environment Prerequisites\n\n### System Requirements\n\n```mermaid\ngraph LR\n    DEV[\"Developer Machine\"] --> DOCKER[\"Docker Desktop/Engine\"]\n    DOCKER --> PG16[\"PostgreSQL 16 Container\"]\n    DOCKER --> MYSQL[\"MySQL 8.0 Container\"]\n    DOCKER --> PGVEC[\"pgvector:pg17 Container\"]\n    \n    PG16 --> PORT1[\"Port 5432\"]\n    MYSQL --> PORT2[\"Port 3306\"]\n    PGVEC --> PORT3[\"Port 5433\"]\n    \n    style DOCKER fill:#f9f9f9\n    style PG16 fill:#f9f9f9\n    style MYSQL fill:#f9f9f9\n    style PGVEC fill:#f9f9f9\n```\n\n**Diagram: Database Container Architecture**\n\n### Prerequisites Checklist\n\n1. **Docker Installation**: Docker Desktop (Windows/Mac) or Docker Engine (Linux) must be running\n2. **Port Availability**: Ensure ports 3306, 5432, 5433 are not in use by existing services\n3. **Disk Space**: At least 2GB free for container images and data volumes\n4. **Network Access**: Internet connection required for initial image download\n\n**Verification Commands:**\n\n```bash\n# Check Docker is running\ndocker --version\n\n# Check port availability (Linux/Mac)\nnetstat -an | grep -E '3306|5432|5433'\n\n# Check port availability (Windows PowerShell)\nnetstat -an | findstr \"3306 5432 5433\"\n```\n\n**Sources:** [docs/pg.md:258-262]()\n\n---\n\n## PostgreSQL 16 Setup\n\n### Container Deployment\n\nPostgreSQL 16 serves as the primary relational database for structured data examples.\n\n```bash\n# Create data persistence directory\nmkdir -p /docker/postgres16/data\n\n# Start PostgreSQL 16 container with data volume\ndocker run -d \\\n  --name postgres16 \\\n  --restart always \\\n  -p 5432:5432 \\\n  -e POSTGRES_PASSWORD=123456 \\\n  -e POSTGRES_USER=postgres \\\n  -e POSTGRES_DB=test_db \\\n  -v /docker/postgres16/data:/var/lib/postgresql/data \\\n  postgres:16\n\n# Verify container is running\ndocker ps | grep postgres16\n```\n\n**Sources:** [docs/pg.md:19-32]()\n\n### Configuration Parameters\n\n| Parameter | Value | Description |\n|-----------|-------|-------------|\n| `--name` | `postgres16` | Container identifier for management commands |\n| `-p` | `5432:5432` | Port mapping: host 5432 â container 5432 |\n| `-e POSTGRES_PASSWORD` | `123456` | Superuser `postgres` password |\n| `-e POSTGRES_DB` | `test_db` | Auto-created default database |\n| `-v` | `/docker/postgres16/data:/var/lib/postgresql/data` | Data persistence volume mount |\n| `--restart` | `always` | Auto-restart on system reboot |\n\n**Sources:** [docs/pg.md:7-32]()\n\n### Connection Verification\n\n```mermaid\nsequenceDiagram\n    participant DEV as \"Developer Terminal\"\n    participant DOCKER as \"Docker Engine\"\n    participant PG16 as \"postgres16 Container\"\n    participant PSQL as \"psql Client\"\n    participant DB as \"test_db Database\"\n    \n    DEV->>DOCKER: \"docker exec -it postgres16 psql\"\n    DOCKER->>PG16: \"Execute psql command\"\n    PG16->>PSQL: \"Start psql client\"\n    PSQL->>DB: \"Connect to test_db\"\n    DB-->>PSQL: \"Connection established\"\n    PSQL-->>DEV: \"postgres=# prompt\"\n    DEV->>PSQL: \"CREATE TABLE / SELECT queries\"\n    PSQL->>DB: \"Execute SQL\"\n    DB-->>PSQL: \"Query results\"\n    PSQL-->>DEV: \"Display results\"\n```\n\n**Diagram: PostgreSQL Connection Flow**\n\n```bash\n# Connect to PostgreSQL via container\ndocker exec -it postgres16 psql -U postgres -d test_db\n\n# Test SQL operations\nCREATE TABLE test_connection (\n    id SERIAL PRIMARY KEY,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nINSERT INTO test_connection DEFAULT VALUES;\nSELECT * FROM test_connection;\n\n# Exit psql\n\\q\n```\n\n**Sources:** [docs/pg.md:35-48]()\n\n### External Client Connection\n\nFor connecting via external tools (Navicat, DBeaver, DataGrip):\n\n| Connection Parameter | Value |\n|---------------------|-------|\n| Host | `127.0.0.1` (or server IP) |\n| Port | `5432` |\n| Database | `test_db` |\n| Username | `postgres` |\n| Password | `123456` |\n| SSL Mode | `prefer` (or `disable` for local) |\n\n**Sources:** [docs/pg.md:50-56]()\n\n---\n\n## MySQL 8.0 Setup\n\n### Container Deployment\n\nMySQL 8.0 is used for cross-database SQL compatibility testing and specific MySQL examples.\n\n```bash\n# Start MySQL 8.0 container\ndocker run -d \\\n  --name mysql-test \\\n  --restart always \\\n  -p 3306:3306 \\\n  -e MYSQL_ROOT_PASSWORD=123456 \\\n  -e MYSQL_DATABASE=test_db \\\n  mysql:8.0\n\n# Verify container is running\ndocker ps | grep mysql-test\n```\n\n**Sources:** [docs/pg.md:266-275]()\n\n### Configuration Parameters\n\n| Parameter | Value | Description |\n|-----------|-------|-------------|\n| `--name` | `mysql-test` | Container identifier |\n| `-p` | `3306:3306` | Port mapping: host 3306 â container 3306 |\n| `-e MYSQL_ROOT_PASSWORD` | `123456` | Root user password |\n| `-e MYSQL_DATABASE` | `test_db` | Auto-created database |\n\n**Note:** For production environments with data persistence, add volume mount:\n```bash\n-v /docker/mysql8/data:/var/lib/mysql\n```\n\n**Sources:** [docs/pg.md:266-280]()\n\n### Connection Verification\n\n```bash\n# Connect to MySQL via container\ndocker exec -it mysql-test mysql -uroot -p123456 test_db\n\n# Test SQL operations\nCREATE TABLE test_connection (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nINSERT INTO test_connection () VALUES ();\nSELECT * FROM test_connection;\n\n# Exit MySQL\nexit;\n```\n\n**Sources:** [docs/pg.md:283-299]()\n\n### External Client Connection\n\n| Connection Parameter | Value |\n|---------------------|-------|\n| Host | `127.0.0.1` |\n| Port | `3306` |\n| Database | `test_db` |\n| Username | `root` |\n| Password | `123456` |\n\n**Sources:** [docs/pg.md:266-280]()\n\n---\n\n## pgvector Setup for Vector Storage\n\n### Container Deployment\n\nThe `pgvector` extension adds vector similarity search capabilities to PostgreSQL, required for AI/ML examples with embeddings storage.\n\n```bash\n# Create data persistence directory\nmkdir -p /docker/pgvector17/data\n\n# Start pgvector container (PostgreSQL 17 with vector extension)\ndocker run -d \\\n  --name pgvector17 \\\n  --restart always \\\n  -p 5433:5432 \\\n  -e POSTGRES_PASSWORD=123456 \\\n  -v /docker/pgvector17/data:/var/lib/postgresql/data \\\n  pgvector/pgvector:pg17\n\n# Verify container is running\ndocker ps | grep pgvector17\n```\n\n**Important:** Port 5433 is used on the host to avoid conflicts with the standard PostgreSQL 16 container on port 5432.\n\n**Sources:** [docs/pg.md:61-74]()\n\n### Vector Extension Activation\n\nThe `vector` extension must be explicitly enabled in each database:\n\n```bash\n# Connect to pgvector container\ndocker exec -it pgvector17 psql -U postgres\n\n# Create database and enable vector extension\nCREATE DATABASE vector_db;\n\\c vector_db;\n\nCREATE EXTENSION vector;\n\n# Verify extension installation\n\\dx;\n```\n\nExpected output should show the `vector` extension listed.\n\n**Sources:** [docs/pg.md:77-93]()\n\n### Vector Data Type Usage\n\n```mermaid\ngraph TB\n    subgraph \"Vector Storage Architecture\"\n        TABLE[\"embeddings Table\"]\n        VEC[\"vec vector(N)\"]\n        DATA[\"content TEXT\"]\n        \n        TABLE --> VEC\n        TABLE --> DATA\n    end\n    \n    subgraph \"Vector Operations\"\n        INSERT[\"INSERT vector data\"]\n        SEARCH[\"Similarity search\"]\n        DISTANCE[\"Distance calculation\"]\n        \n        VEC --> INSERT\n        VEC --> SEARCH\n        VEC --> DISTANCE\n    end\n    \n    INSERT --> OP1[\"L2 distance: <->\"]\n    SEARCH --> OP1\n    INSERT --> OP2[\"Cosine distance: <=>\"]\n    SEARCH --> OP2\n    INSERT --> OP3[\"Inner product: <#>\"]\n    SEARCH --> OP3\n    \n    style TABLE fill:#f9f9f9\n    style VEC fill:#f9f9f9\n```\n\n**Diagram: pgvector Data Type and Operations**\n\n```sql\n-- Create table with vector column (example: 3-dimensional vectors)\nCREATE TABLE embeddings (\n  id SERIAL PRIMARY KEY,\n  vec vector(3),        -- vector(N) where N is dimension count\n  content TEXT\n);\n\n-- Insert vector data (array notation)\nINSERT INTO embeddings (vec, content)\nVALUES \n  ('[1.1, 2.2, 3.3]', 'Sample text 1'),\n  ('[4.4, 5.5, 6.6]', 'Sample text 2'),\n  ('[0.9, 2.1, 3.2]', 'Sample text 3');\n\n-- Similarity search using L2 distance (Euclidean)\nSELECT \n  id, \n  content, \n  vec <-> '[1.0, 2.0, 3.0]' AS distance\nFROM embeddings\nORDER BY distance ASC\nLIMIT 5;\n\n-- Alternative distance metrics\n-- Cosine distance: vec <=> '[1.0, 2.0, 3.0]'\n-- Inner product: vec <#> '[1.0, 2.0, 3.0]'\n```\n\n**Sources:** [docs/pg.md:95-116]()\n\n### Vector Dimension Configuration\n\n| Use Case | Typical Dimensions | Example Model |\n|----------|-------------------|---------------|\n| Small text embeddings | 384 | `sentence-transformers/all-MiniLM-L6-v2` |\n| Standard embeddings | 768 | `bert-base-uncased` |\n| Large embeddings | 1536 | `text-embedding-ada-002` (OpenAI) |\n| Custom embeddings | Variable | Domain-specific models |\n\n**Dimension Declaration Syntax:**\n- 3D vector: `vector(3)`\n- 768D vector: `vector(768)`\n- 1536D vector: `vector(1536)`\n\n**Sources:** [docs/pg.md:95-116]()\n\n---\n\n## Database Connection from Python Code\n\n### Connection Architecture\n\n```mermaid\ngraph LR\n    subgraph \"algo_agent Runtime\"\n        AGENT[\"Agent Execution\"]\n        PYTOOL[\"ExecutePythonCodeTool\"]\n        EXEC[\"Subprocess Executor\"]\n    end\n    \n    subgraph \"Python Code Snippet\"\n        CODE[\"User SQL Code\"]\n        LIB1[\"psycopg2\"]\n        LIB2[\"mysql-connector\"]\n        LIB3[\"sqlalchemy\"]\n    end\n    \n    subgraph \"Docker Containers\"\n        PG[\"postgres16:5432\"]\n        MYSQL[\"mysql-test:3306\"]\n        PGVEC[\"pgvector17:5433\"]\n    end\n    \n    AGENT --> PYTOOL\n    PYTOOL --> EXEC\n    EXEC --> CODE\n    \n    CODE --> LIB1\n    CODE --> LIB2\n    CODE --> LIB3\n    \n    LIB1 --> PG\n    LIB1 --> PGVEC\n    LIB2 --> MYSQL\n    LIB3 --> PG\n    LIB3 --> MYSQL\n    LIB3 --> PGVEC\n    \n    style AGENT fill:#f9f9f9\n    style PYTOOL fill:#f9f9f9\n    style EXEC fill:#f9f9f9\n```\n\n**Diagram: Database Connection Flow from Agent Code Execution**\n\n### PostgreSQL Connection Example\n\n```python\n# Example code that can be executed via ExecutePythonCodeTool\nimport psycopg2\n\n# Connect to PostgreSQL 16 container\nconn = psycopg2.connect(\n    host=\"localhost\",\n    port=5432,\n    database=\"test_db\",\n    user=\"postgres\",\n    password=\"123456\"\n)\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT version();\")\nversion = cursor.fetchone()\nprint(f\"PostgreSQL version: {version[0]}\")\n\ncursor.close()\nconn.close()\n```\n\n### MySQL Connection Example\n\n```python\n# Example code that can be executed via ExecutePythonCodeTool\nimport mysql.connector\n\n# Connect to MySQL 8.0 container\nconn = mysql.connector.connect(\n    host=\"localhost\",\n    port=3306,\n    database=\"test_db\",\n    user=\"root\",\n    password=\"123456\"\n)\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT VERSION();\")\nversion = cursor.fetchone()\nprint(f\"MySQL version: {version[0]}\")\n\ncursor.close()\nconn.close()\n```\n\n### pgvector Connection Example\n\n```python\n# Example code that can be executed via ExecutePythonCodeTool\nimport psycopg2\n\n# Connect to pgvector container\nconn = psycopg2.connect(\n    host=\"localhost\",\n    port=5433,  # Note: Different port than standard PostgreSQL\n    database=\"vector_db\",\n    user=\"postgres\",\n    password=\"123456\"\n)\n\ncursor = conn.cursor()\n\n# Create table with vector column\ncursor.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS doc_embeddings (\n        id SERIAL PRIMARY KEY,\n        doc_text TEXT,\n        embedding vector(768)\n    )\n\"\"\")\n\n# Insert sample vector (768 dimensions)\nsample_vector = [0.1] * 768  # Placeholder embedding\ncursor.execute(\n    \"INSERT INTO doc_embeddings (doc_text, embedding) VALUES (%s, %s)\",\n    (\"Sample document\", sample_vector)\n)\n\nconn.commit()\ncursor.close()\nconn.close()\n```\n\n**Note:** Database client libraries (`psycopg2`, `mysql-connector-python`) are not listed in [pyproject.toml:7-20]() and must be installed separately or added to project dependencies for these examples to work.\n\n**Sources:** [pyproject.toml:1-21](), [docs/pg.md:283-299]()\n\n---\n\n## SQL Example: Continuous Login Analysis\n\nThis example demonstrates window function usage for analyzing user login patterns, runnable in both PostgreSQL and MySQL.\n\n### Test Data Setup\n\n```sql\n-- Create test table (PostgreSQL/MySQL compatible)\nCREATE TABLE user_login (\n    id SERIAL PRIMARY KEY,        -- PostgreSQL\n    -- id INT AUTO_INCREMENT PRIMARY KEY,  -- MySQL alternative\n    user_id VARCHAR(50) NOT NULL,\n    login_date DATE NOT NULL\n);\n\n-- Insert test data\nINSERT INTO user_login (user_id, login_date) VALUES\n('u001', '2025-01-01'), ('u001', '2025-01-01'),  -- Duplicate login same day\n('u001', '2025-01-02'), ('u001', '2025-01-03'), ('u001', '2025-01-04'),\n('u002', '2025-01-01'), ('u002', '2025-01-03'), ('u002', '2025-01-04'),\n('u003', '2025-01-01'), ('u003', '2025-01-02'), ('u003', '2025-01-03');\n```\n\n**Sources:** [docs/pg.md:144-158](), [docs/pg.md:288-299]()\n\n### Continuous Login Query (PostgreSQL)\n\n```sql\nWITH \n-- Step 1: Deduplicate login records (one record per user per day)\ndistinct_login AS (\n    SELECT DISTINCT user_id, login_date \n    FROM user_login\n),\n-- Step 2: Calculate grouping key using window function\nlogin_rn AS (\n    SELECT \n        user_id,\n        login_date,\n        -- Consecutive dates will have same group_key\n        login_date - ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY login_date) * INTERVAL '1 day' AS group_key\n    FROM distinct_login\n),\n-- Step 3: Aggregate continuous login periods\ncontinuous_login AS (\n    SELECT \n        user_id,\n        MIN(login_date) AS start_date,\n        MAX(login_date) AS end_date,\n        COUNT(*) AS continuous_days\n    FROM login_rn\n    GROUP BY user_id, group_key\n)\n-- Step 4: Filter for 3+ consecutive days\nSELECT \n    user_id,\n    start_date,\n    end_date,\n    continuous_days\nFROM continuous_login\nWHERE continuous_days >= 3\nORDER BY user_id, start_date;\n```\n\n**Expected Result:**\n\n| user_id | start_date | end_date   | continuous_days |\n|---------|------------|------------|-----------------|\n| u001    | 2025-01-01 | 2025-01-04 | 4               |\n| u003    | 2025-01-01 | 2025-01-03 | 3               |\n\n**Sources:** [docs/pg.md:208-242](), [docs/pg.md:372-397]()\n\n### Continuous Login Query (MySQL)\n\n```sql\nWITH \ndistinct_login AS (\n    SELECT DISTINCT user_id, login_date \n    FROM user_login\n),\nlogin_rn AS (\n    SELECT \n        user_id,\n        login_date,\n        -- MySQL date arithmetic\n        DATE_SUB(login_date, INTERVAL ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY login_date) DAY) AS group_key\n    FROM distinct_login\n),\ncontinuous_login AS (\n    SELECT \n        user_id,\n        MIN(login_date) AS start_date,\n        MAX(login_date) AS end_date,\n        COUNT(*) AS continuous_days\n    FROM login_rn\n    GROUP BY user_id, group_key\n)\nSELECT \n    user_id,\n    start_date,\n    end_date,\n    continuous_days\nFROM continuous_login\nWHERE continuous_days >= 3\nORDER BY user_id, start_date;\n```\n\n**Key Difference:** MySQL uses `DATE_SUB(date, INTERVAL n DAY)` while PostgreSQL uses `date - n * INTERVAL '1 day'`.\n\n**Sources:** [docs/pg.md:161-196](), [docs/pg.md:300-326]()\n\n---\n\n## Container Management Commands\n\n### Lifecycle Operations\n\n```bash\n# Start containers\ndocker start postgres16\ndocker start mysql-test\ndocker start pgvector17\n\n# Stop containers\ndocker stop postgres16\ndocker stop mysql-test\ndocker stop pgvector17\n\n# Restart containers\ndocker restart postgres16\ndocker restart mysql-test\ndocker restart pgvector17\n\n# View container logs\ndocker logs postgres16\ndocker logs -f mysql-test     # Follow mode (real-time)\ndocker logs --tail 100 pgvector17  # Last 100 lines\n```\n\n**Sources:** [docs/pg.md:122-129]()\n\n### Data Persistence Management\n\n```bash\n# Remove container (data preserved in volume)\ndocker stop postgres16\ndocker rm postgres16\n\n# Remove container and data\ndocker stop postgres16\ndocker rm postgres16\nrm -rf /docker/postgres16/data\n\n# Backup database (PostgreSQL example)\ndocker exec postgres16 pg_dump -U postgres test_db > backup.sql\n\n# Restore database (PostgreSQL example)\ndocker exec -i postgres16 psql -U postgres test_db < backup.sql\n```\n\n**Sources:** [docs/pg.md:420-427]()\n\n### Port Conflict Resolution\n\nIf default ports are occupied:\n\n```bash\n# Use alternative ports (host:container mapping)\ndocker run -d \\\n  --name postgres16 \\\n  -p 5434:5432 \\    # Map to host port 5434\n  -e POSTGRES_PASSWORD=123456 \\\n  postgres:16\n\ndocker run -d \\\n  --name mysql-test \\\n  -p 3307:3306 \\    # Map to host port 3307\n  -e MYSQL_ROOT_PASSWORD=123456 \\\n  mysql:8.0\n```\n\nConnection parameters must be updated accordingly:\n- PostgreSQL: `host=localhost, port=5434`\n- MySQL: `host=localhost, port=3307`\n\n**Sources:** [docs/pg.md:119-120](), [docs/pg.md:429]()\n\n---\n\n## Troubleshooting\n\n### Common Issues and Solutions\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| \"Port already in use\" | Local database service running | Stop local service or use different port mapping (`-p 5433:5432`) |\n| \"Permission denied\" on volume mount | Directory ownership mismatch | `chmod 777 /docker/postgres16/data` or use named volumes |\n| Container exits immediately | Invalid configuration | Check logs: `docker logs postgres16` |\n| \"Connection refused\" | Container not started or wrong port | Verify: `docker ps` and check port mapping |\n| Vector extension not available | Extension not enabled | Run `CREATE EXTENSION vector;` in target database |\n\n**Sources:** [docs/pg.md:130-134](), [docs/pg.md:429-432]()\n\n### Container Health Verification\n\n```bash\n# Check container status\ndocker ps -a | grep -E 'postgres16|mysql-test|pgvector17'\n\n# Inspect container configuration\ndocker inspect postgres16\n\n# Test network connectivity\ndocker exec postgres16 pg_isready -U postgres\n\n# Check resource usage\ndocker stats postgres16 mysql-test pgvector17\n```\n\n### Database Connection Testing\n\n```bash\n# PostgreSQL connection test\ndocker exec postgres16 psql -U postgres -c \"SELECT 1\"\n\n# MySQL connection test\ndocker exec mysql-test mysql -uroot -p123456 -e \"SELECT 1\"\n\n# pgvector extension test\ndocker exec pgvector17 psql -U postgres -c \"SELECT extname FROM pg_extension WHERE extname='vector'\"\n```\n\n**Sources:** [docs/pg.md:407-418]()\n\n---\n\n## Integration with Example Data Generation\n\nThe database setups work in conjunction with the data generation utilities for creating test datasets.\n\n```mermaid\ngraph TB\n    subgraph \"Data Generation\"\n        GEN[\"docs/gen.md\"]\n        PYDANTIC[\"Pydantic Models\"]\n        RANDOMDATA[\"generate_random_data()\"]\n    end\n    \n    subgraph \"Database Storage\"\n        PG[\"PostgreSQL 16\"]\n        MYSQL[\"MySQL 8.0\"]\n        PGVEC[\"pgvector:pg17\"]\n    end\n    \n    subgraph \"Agent Execution\"\n        AGENT[\"user_query()\"]\n        TOOL[\"ExecutePythonCodeTool\"]\n        SQL[\"SQL INSERT/SELECT\"]\n    end\n    \n    GEN --> PYDANTIC\n    PYDANTIC --> RANDOMDATA\n    RANDOMDATA --> JSON[\"JSON Test Data\"]\n    \n    JSON --> AGENT\n    AGENT --> TOOL\n    TOOL --> SQL\n    \n    SQL --> PG\n    SQL --> MYSQL\n    SQL --> PGVEC\n    \n    style GEN fill:#f9f9f9\n    style PYDANTIC fill:#f9f9f9\n    style AGENT fill:#f9f9f9\n```\n\n**Diagram: Data Generation to Database Storage Flow**\n\nThe [docs/gen.md:1-137]() example demonstrates Pydantic-based data structure generation for emergency response planning scenarios. These generated structures can be stored in PostgreSQL for:\n\n1. **Structured Query Testing**: Geographic point/edge data in relational tables\n2. **Vector Storage**: Embedding representations of materials/locations in pgvector\n3. **Cross-Database Validation**: Comparing query results across PostgreSQL and MySQL\n\nExample integration:\n```python\n# From docs/gen.md example - generate test data\nfrom docs.gen import generate_all_random_data\nimport json\nimport psycopg2\n\n# Generate random emergency response data\ndata = generate_all_random_data()\n\n# Store in PostgreSQL\nconn = psycopg2.connect(\n    host=\"localhost\", port=5432,\n    database=\"test_db\", user=\"postgres\", password=\"123456\"\n)\ncursor = conn.cursor()\n\n# Store map points\nfor point in data.map_data.points:\n    cursor.execute(\n        \"INSERT INTO map_points (x, y) VALUES (%s, %s)\",\n        (point.x, point.y)\n    )\n\nconn.commit()\n```\n\n**Sources:** [docs/gen.md:1-137](), [docs/pg.md:283-299]()\n\n---\n\n## Summary\n\nThis page documented the Docker-based database setup for running algo_agent data processing examples:\n\n- **PostgreSQL 16**: Primary relational database on port 5432\n- **MySQL 8.0**: Cross-database testing on port 3306  \n- **pgvector (PostgreSQL 17)**: Vector storage with similarity search on port 5433\n\nAll containers use persistent data volumes and can be managed via standard Docker commands. Database connections from Python code executed via `ExecutePythonCodeTool` use standard client libraries (`psycopg2`, `mysql-connector-python`).\n\nFor actual implementation of data processing algorithms and examples, see [Data Processing and Visualization](#7.2) and [Geographic Data Processing](#7.3).\n\n**Sources:** [docs/pg.md:1-432](), [docs/gen.md:1-187](), [pyproject.toml:1-21]()\n\n---\n\n# Page: Architecture Reference\n\n# Architecture Reference\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [docs/crashed.design.md](docs/crashed.design.md)\n- [docs/error correction.design.md](docs/error correction.design.md)\n- [docs/log.md](docs/log.md)\n- [src/agent/tool/base_tool.py](src/agent/tool/base_tool.py)\n- [src/memory/tree_todo/schemas.py](src/memory/tree_todo/schemas.py)\n- [src/memory/tree_todo/todo_track.py](src/memory/tree_todo/todo_track.py)\n- [src/runtime/schemas.py](src/runtime/schemas.py)\n- [src/runtime/workspace.py](src/runtime/workspace.py)\n- [src/utils/__pycache__/__init__.cpython-312.pyc](src/utils/__pycache__/__init__.cpython-312.pyc)\n- [tests/playground/crashed.py](tests/playground/crashed.py)\n- [tests/playground/subprocess_output.py](tests/playground/subprocess_output.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document provides a comprehensive technical reference for the core architectural components, data models, and schemas in the algo_agent system. It focuses on the structure and relationships of the fundamental data types that flow through the system, including execution results, task management structures, and tool interfaces.\n\nThis reference is intended for developers who need to understand:\n- The data models used for code execution results and state management\n- The schemas for hierarchical task planning and tracking\n- The tool system's interface contracts and schema generation\n- How data flows between components and is serialized/deserialized\n\nFor implementation details of specific subsystems, see:\n- Code execution strategies: [Execution Runtime](#5)\n- Tool implementations: [Tool System](#4)\n- State persistence mechanisms: [Workspace State Management](#5.5)\n\n---\n\n## Core Data Model Overview\n\nThe system is built around three primary schema families that enable structured communication between the agent, execution runtime, and task management subsystems:\n\n```mermaid\ngraph TB\n    subgraph \"Execution Schemas (runtime/schemas.py)\"\n        ES[ExecutionStatus enum]\n        ER[ExecutionResult model]\n    end\n    \n    subgraph \"Task Management Schemas (memory/tree_todo/schemas.py)\"\n        TS[TaskStatus enum]\n        TN[RecursivePlanTreeNode model]\n        TR[RecursivePlanTree model]\n    end\n    \n    subgraph \"Tool Interface (agent/tool/base_tool.py)\"\n        BT[BaseTool abstract class]\n        TSCHEMA[\"Tool schema dict\"]\n    end\n    \n    subgraph \"Workspace State (runtime/workspace.py)\"\n        ARGLIST[\"arg_globals_list: list[dict]\"]\n        OUTLIST[\"out_globals_list: list[dict]\"]\n    end\n    \n    ER --> ES\n    ER --> ARGLIST\n    ER --> OUTLIST\n    \n    TR --> TN\n    TN --> TS\n    TN --> TN\n    \n    BT --> TSCHEMA\n    \n    TSCHEMA -..\"consumed by\".-> Agent[\"Agent Decision Loop\"]\n    ER -..\"returned to\".-> Agent\n    TR -..\"tracked by\".-> Agent\n    \n    style ES fill:#f9f9f9\n    style TS fill:#f9f9f9\n    style ER fill:#e8f4f8\n    style TR fill:#e8f4f8\n    style BT fill:#f0f8e8\n```\n\n**Sources:** [src/runtime/schemas.py:1-111](), [src/memory/tree_todo/schemas.py:1-81](), [src/agent/tool/base_tool.py:1-76](), [src/runtime/workspace.py:1-108]()\n\n---\n\n## Execution Result Schema Architecture\n\nThe `ExecutionResult` model is the primary data structure for capturing code execution outcomes. It encapsulates both input parameters and output results, supporting multiple execution states and providing structured error information.\n\n### ExecutionStatus Enum\n\n```mermaid\ngraph LR\n    START[\"Code Execution\"] --> SUCCESS\n    START --> FAILURE\n    START --> TIMEOUT\n    START --> CRASHED\n    \n    SUCCESS[\"ExecutionStatus.SUCCESS<br/>exit_code=0<br/>No exceptions\"]\n    FAILURE[\"ExecutionStatus.FAILURE<br/>exit_code=0<br/>Python exception caught\"]\n    TIMEOUT[\"ExecutionStatus.TIMEOUT<br/>exit_code=-15 (SIGTERM)<br/>Exceeded timeout limit\"]\n    CRASHED[\"ExecutionStatus.CRASHED<br/>exit_code varies<br/>Process terminated abnormally\"]\n    \n    SUCCESS --> RET[\"ret_stdout captured<br/>arg_globals preserved\"]\n    FAILURE --> RET\n    TIMEOUT --> RET\n    CRASHED --> RET\n```\n\nThe `ExecutionStatus` enum defines four mutually exclusive execution outcomes:\n\n| Status | Value | Exit Code | Description | Typical Causes |\n|--------|-------|-----------|-------------|----------------|\n| `SUCCESS` | `\"success\"` | 0 | Code executed without errors | Normal completion |\n| `FAILURE` | `\"failure\"` | 0 | Python exception raised | Syntax errors, runtime exceptions, logic errors |\n| `TIMEOUT` | `\"timeout\"` | -15 | Execution exceeded time limit | Infinite loops, blocking operations |\n| `CRASHED` | `\"crashed\"` | varies | Process terminated abnormally | SegFault, OOM, signal termination |\n\n**Sources:** [src/runtime/schemas.py:10-16]()\n\n### ExecutionStatus.get_return_llm Method\n\nThis class method generates human-readable descriptions for the LLM based on execution status:\n\n```mermaid\ngraph TB\n    STATUS[\"ExecutionStatus + ExecutionResult\"] --> METHOD[\"get_return_llm(status, result)\"]\n    \n    METHOD --> SUCCESS_DESC[\"SUCCESS:<br/>ä»£ç æ§è¡æåï¼è¾åºç»æå®æ´<br/>+ ret_stdout\"]\n    METHOD --> FAILURE_DESC[\"FAILURE:<br/>ä»£ç æ§è¡å¤±è´¥ï¼æ ¹æ®æ¥éä¿¡æ¯è°è¯<br/>+ ret_stdout + arg_command with line numbers<br/>+ exception_traceback\"]\n    METHOD --> TIMEOUT_DESC[\"TIMEOUT:<br/>ä»£ç æ§è¡è¶æ¶ï¼è°æ´è¶æ¶æ¶é´<br/>+ ret_stdout + arg_timeout\"]\n    METHOD --> CRASHED_DESC[\"CRASHED:<br/>è¿ç¨å¼å¸¸éåº<br/>+ ret_stdout + exit_code\"]\n    \n    SUCCESS_DESC --> LLMINPUT[\"ret_tool2llm field\"]\n    FAILURE_DESC --> LLMINPUT\n    TIMEOUT_DESC --> LLMINPUT\n    CRASHED_DESC --> LLMINPUT\n```\n\n**Sources:** [src/runtime/schemas.py:18-47]()\n\n### ExecutionResult Model Fields\n\nThe `ExecutionResult` Pydantic model contains three categories of fields:\n\n**Input Parameters (set by caller):**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `arg_command` | `str` | The Python code string executed |\n| `arg_timeout` | `int` | Timeout limit in seconds |\n| `arg_globals` | `Dict[str, Any]` | Filtered and deep-copied global variables (validated by `field_validate_globals`) |\n\n**Execution Results (set by subprocess):**\n\n| Field | Type | Optional | Description |\n|-------|------|----------|-------------|\n| `exit_status` | `ExecutionStatus` | No | Execution outcome enum |\n| `exit_code` | `int` | Yes | Process exit code (set by parent process) |\n| `exception_repr` | `str` | Yes | Exception repr string (FAILURE only) |\n| `exception_type` | `str` | Yes | Exception class name (FAILURE only) |\n| `exception_value` | `str` | Yes | Exception message (FAILURE only) |\n| `exception_traceback` | `str` | Yes | Full traceback string (FAILURE only) |\n\n**Post-Processing Fields (set by parent process):**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `ret_stdout` | `str` | Captured stdout/stderr from execution |\n| `ret_tool2llm` | `str` | Formatted message for LLM (generated by `get_return_llm`) |\n\n**Sources:** [src/runtime/schemas.py:56-84]()\n\n### ExecutionResult Validation\n\nThe `arg_globals` field uses a Pydantic validator that automatically filters and deep-copies globals:\n\n```python\n@field_validator('arg_globals')\n@classmethod        \ndef field_validate_globals(cls, value: Dict[str, Any]) -> Dict[str, Any]:\n    if value is None:\n        return {}\n    return workspace.filter_and_deepcopy_globals(value)\n```\n\nThis validator calls `workspace.filter_and_deepcopy_globals()` which:\n1. Removes `__builtins__` and module objects\n2. Tests pickle serializability to ensure IPC compatibility\n3. Creates deep copies to prevent reference sharing\n\n**Sources:** [src/runtime/schemas.py:77-83](), [src/runtime/workspace.py:38-78]()\n\n---\n\n## Subprocess Execution Implementation\n\nThe `run_structured_in_subprocess` function implements the subprocess-based execution strategy, demonstrating how `ExecutionResult` is constructed:\n\n```mermaid\nsequenceDiagram\n    participant Parent as \"Parent Process\"\n    participant Worker as \"_worker_with_pipe<br/>(subprocess)\"\n    participant Reader as \"_reader<br/>(thread)\"\n    participant Pipe as \"multiprocessing.Pipe\"\n    \n    Parent->>+Pipe: Create pipe (parent_conn, child_conn)\n    Parent->>+Worker: Start subprocess with child_conn\n    Parent->>Reader: Start reader thread\n    \n    Worker->>Worker: Redirect stdout/stderr to PipeWriter\n    Worker->>Worker: exec(command, _globals, _locals)\n    \n    alt Execution Success\n        Worker->>Pipe: Send (_PipeType.STDOUT, output)\n        Worker->>Pipe: Send (_PipeType.RESULT, ExecutionResult<br/>exit_status=SUCCESS)\n        Pipe->>Reader: Receive messages\n    else Execution Failure\n        Worker->>Worker: Catch exception\n        Worker->>Pipe: Send (_PipeType.RESULT, ExecutionResult<br/>exit_status=FAILURE<br/>+ exception details)\n        Pipe->>Reader: Receive messages\n    end\n    \n    Worker->>-Pipe: Close child_conn\n    \n    alt Timeout\n        Parent->>Worker: p.join(timeout) returns with process alive\n        Parent->>Worker: p.terminate()\n        Parent->>Parent: Build ExecutionResult<br/>exit_status=TIMEOUT\n    else Normal/Abnormal Exit\n        Reader->>-Reader: EOFError (pipe closed)\n        Parent->>Parent: Check subprocess_result_container\n        alt Result Received\n            Parent->>Parent: Use ExecutionResult from subprocess\n        else No Result (Crashed)\n            Parent->>Parent: Build ExecutionResult<br/>exit_status=CRASHED\n        end\n    end\n    \n    Parent->>Parent: Set exit_code = p.exitcode\n    Parent->>Parent: Set ret_stdout = joined buffer\n    Parent->>Parent: Set ret_tool2llm = get_return_llm()\n    Parent->>-Parent: Return final ExecutionResult\n```\n\n**Key implementation details:**\n\n1. **Pipe Communication Protocol:** Two message types distinguished by `_PipeType` enum\n   - `STDOUT`: Real-time output strings\n   - `RESULT`: Final `ExecutionResult` object\n\n2. **Timeout Handling:** Parent process checks `p.is_alive()` after `p.join(timeout)`. If alive, terminates and builds TIMEOUT result.\n\n3. **Crash Detection:** If subprocess exits without sending RESULT message, parent detects empty `subprocess_result_container` and builds CRASHED result.\n\n4. **Exit Code Interpretation:**\n   - `0`: Normal exit\n   - `> 0`: Error exit (caught by Python)\n   - `< 0`: Signal termination (`-15` = SIGTERM, `139` = SegFault)\n\n**Sources:** [tests/playground/subprocess_output.py:68-156](), [tests/playground/subprocess_output.py:18-66]()\n\n---\n\n## Task Management Schema Architecture\n\nThe task management system uses a recursive tree structure to represent hierarchical task plans with status tracking.\n\n### TaskStatus Enum\n\n```mermaid\ngraph LR\n    PENDING[\"TaskStatus.PENDING<br/>â³ å¾æ§è¡\"]\n    PROCESSING[\"TaskStatus.PROCESSING<br/>â¡ï¸ æ­£å¨æ§è¡\"]\n    COMPLETED[\"TaskStatus.COMPLETED<br/>â æ§è¡æå\"]\n    FAILED[\"TaskStatus.FAILED<br/>â æ§è¡å¤±è´¥\"]\n    RETRY[\"TaskStatus.RETRY<br/>â»ï¸ éè¯\"]\n    SKIPPED[\"TaskStatus.SKIPPED<br/>â å·²è·³è¿\"]\n    \n    PENDING --> PROCESSING\n    PROCESSING --> COMPLETED\n    PROCESSING --> FAILED\n    FAILED --> RETRY\n    RETRY --> PROCESSING\n    PENDING --> SKIPPED\n```\n\nEach status has associated properties:\n\n| Status | Value | Symbol | Display Description | Usage |\n|--------|-------|--------|---------------------|-------|\n| `PENDING` | `\"pending\"` | `[â³]` | å¾æ§è¡ | Task not yet started |\n| `PROCESSING` | `\"processing\"` | `[â¡ï¸]` | æ­£å¨æ§è¡ | Task currently running |\n| `COMPLETED` | `\"completed\"` | `[â]` | æ§è¡æå | Task finished successfully |\n| `FAILED` | `\"failed\"` | `[â]` | æ§è¡å¤±è´¥ | Task encountered error |\n| `RETRY` | `\"retry\"` | `[â»ï¸]` | éè¯ | Task will be retried |\n| `SKIPPED` | `\"skipped\"` | `[â]` | å·²è·³è¿ | Task skipped due to changed conditions |\n\n**Sources:** [src/memory/tree_todo/schemas.py:6-41]()\n\n### RecursivePlanTreeNode Model\n\nThe `RecursivePlanTreeNode` represents a single task in the hierarchical plan:\n\n```mermaid\ngraph TB\n    NODE[\"RecursivePlanTreeNode\"]\n    \n    NODE --> ID[\"task_id: str<br/>auto-generated UUID\"]\n    NODE --> NAME[\"task_name: str<br/>globally unique name\"]\n    NODE --> DESC[\"description: str<br/>detailed requirements\"]\n    NODE --> STATUS[\"status: TaskStatus<br/>current state\"]\n    NODE --> OUTPUT[\"output: str<br/>execution result\"]\n    NODE --> DEPS[\"dependencies: Optional[List[str]]<br/>task_name references\"]\n    NODE --> RESEARCH[\"research_directions: Optional[List[str]]<br/>complex task analysis\"]\n    NODE --> CHILDREN[\"children: Optional[List[RecursivePlanTreeNode]]<br/>recursive subtasks\"]\n    \n    CHILDREN -.-> NODE\n    \n    style CHILDREN fill:#f9f9f9\n```\n\n**Field specifications:**\n\n| Field | Type | Default | Description |\n|-------|------|---------|-------------|\n| `task_id` | `str` | auto-generated | Unique ID in format `TASK-{uuid4()}` |\n| `task_name` | `str` | required | Unique name for dependency references |\n| `description` | `str` | `\"\"` | Optional detailed task requirements |\n| `status` | `TaskStatus` | `PENDING` | Current execution state |\n| `output` | `str` | `\"\"` | Result when completed/failed |\n| `dependencies` | `Optional[List[str]]` | `None` | List of prerequisite `task_name` values |\n| `research_directions` | `Optional[List[str]]` | `None` | Deep research topics for complex tasks |\n| `children` | `Optional[List[RecursivePlanTreeNode]]` | `None` | Nested subtasks (recursive) |\n\n**Validation:** The `empty_children_to_none` validator ensures empty child lists are normalized to `None`.\n\n**Sources:** [src/memory/tree_todo/schemas.py:43-64]()\n\n### RecursivePlanTree Model\n\nThe `RecursivePlanTree` is the top-level container for the complete task hierarchy:\n\n```mermaid\ngraph TB\n    TREE[\"RecursivePlanTree\"]\n    \n    TREE --> GOAL[\"core_goal: str<br/>ultimate objective\"]\n    TREE --> NODES[\"tree_nodes: List[RecursivePlanTreeNode]<br/>root task list\"]\n    TREE --> ACTION[\"next_action: Dict[str, Any]<br/>suggested next step\"]\n    TREE --> REFS[\"references: Optional[List[str]]<br/>resource links\"]\n    \n    NODES --> NODE1[\"RecursivePlanTreeNode\"]\n    NODES --> NODE2[\"RecursivePlanTreeNode\"]\n    NODES --> NODE3[\"RecursivePlanTreeNode\"]\n    \n    NODE1 --> CHILD1[\"children\"]\n    NODE2 --> CHILD2[\"children\"]\n```\n\n**Field specifications:**\n\n| Field | Type | Default | Description |\n|-------|------|---------|-------------|\n| `core_goal` | `str` | required | The ultimate objective of the plan |\n| `tree_nodes` | `List[RecursivePlanTreeNode]` | `[]` | Root-level tasks (may have nested children) |\n| `next_action` | `Dict[str, Any]` | `{}` | Agent's suggested next execution step |\n| `references` | `Optional[List[str]]` | `None` | Documentation links, data sources |\n\n**Sources:** [src/memory/tree_todo/schemas.py:67-81]()\n\n---\n\n## Task Tree Version Tracking\n\nThe `todo_track.py` module maintains historical versions of task trees and analyzes changes:\n\n```mermaid\ngraph TB\n    subgraph \"Global State\"\n        ARGLIST[\"arg_todo_list: List[RecursivePlanTree]<br/>Historical versions\"]\n        OUTLIST[\"out_todo_list: List[RecursivePlanTree]<br/>(unused in current impl)\"]\n        DIFFLIST[\"track_diff_result_list: List[str]<br/>Change summaries\"]\n    end\n    \n    subgraph \"Change Analysis\"\n        RUN[\"run(current_plan_tree)\"]\n        RUN --> SAVE[\"Append to arg_todo_list\"]\n        RUN --> ANALYZE[\"_analyze_changes(last, current)\"]\n        RUN --> RENDER[\"_render_plan_tree_markdown(nodes)\"]\n        RUN --> STATS[\"_calculate_status_statistics(tree)\"]\n        \n        ANALYZE --> NEWTASK[\"ð New tasks\"]\n        ANALYZE --> DELTASK[\"ðï¸ Deleted tasks\"]\n        ANALYZE --> STATUSCHG[\"ð Status changes\"]\n        ANALYZE --> LEVELCHG[\"ð Level adjustments\"]\n    end\n    \n    subgraph \"Return Value\"\n        RESULT[\"Dict[str, str]\"]\n        RESULT --> CHANGES[\"changes_summary: str\"]\n        RESULT --> MARKDOWN[\"markdown_todo_list: str\"]\n        RESULT --> STATUSSTATS[\"status_statistics: dict\"]\n    end\n    \n    SAVE --> ARGLIST\n    ANALYZE --> CHANGES\n    RENDER --> MARKDOWN\n    STATS --> STATUSSTATS\n```\n\n**Key functions:**\n\n- `run(current_plan_tree)`: Main entry point that saves, analyzes, and renders\n- `_analyze_changes(last_plan, current_plan)`: Identifies new/deleted/modified tasks\n- `_render_plan_tree_markdown(nodes, indent_level)`: Recursive Markdown generation\n- `_calculate_status_statistics(tree)`: Counts tasks by status with completion rate\n\n**Sources:** [src/memory/tree_todo/todo_track.py:1-201]()\n\n---\n\n## Tool Schema Format\n\nThe `BaseTool` abstract class defines the interface all tools must implement, including schema generation for LLM function calling.\n\n### BaseTool Interface\n\n```mermaid\nclassDiagram\n    class BaseTool {\n        +str tool_call_purpose\n        +tool_name() str$\n        +tool_description() str$\n        +get_parameter_schema() dict$\n        +get_tool_schema() dict$\n        +run() str\n    }\n    \n    class ExecutePythonCodeTool {\n        +str code\n        +int timeout\n        +run() str\n    }\n    \n    class RecursivePlanTreeTodoTool {\n        +RecursivePlanTree current_plan_tree\n        +run() str\n    }\n    \n    BaseTool <|-- ExecutePythonCodeTool\n    BaseTool <|-- RecursivePlanTreeTodoTool\n    \n    note for BaseTool \"All class methods are auto-generated<br/>from Pydantic model metadata\"\n```\n\n**Key methods:**\n\n| Method | Type | Returns | Description |\n|--------|------|---------|-------------|\n| `tool_name()` | classmethod | `str` | Underscore name from class (e.g., `\"execute_python_code\"`) |\n| `tool_description()` | classmethod | `str` | Extracted from class docstring |\n| `get_parameter_schema()` | classmethod | `dict` | Pydantic JSON Schema for fields |\n| `get_tool_schema()` | classmethod | `dict` | OpenAI function calling format |\n| `run()` | instance | `str` | Execute tool logic (must override) |\n\n**Sources:** [src/agent/tool/base_tool.py:6-76]()\n\n### Tool Schema Structure\n\nThe `get_tool_schema()` method returns a dictionary in OpenAI function calling format:\n\n```python\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"type\": \"function\",\n        \"name\": \"<tool_name>\",\n        \"description\": \"<tool_description>\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"field_name\": {\n                    \"type\": \"string\",\n                    \"description\": \"Field description from Pydantic Field\"\n                },\n                ...\n            },\n            \"required\": [\"field1\", \"field2\"]\n        },\n        \"strict\": True\n    }\n}\n```\n\nThe `parameters` section is automatically generated from the Pydantic model's `model_json_schema()` output, which includes:\n- Field types (`string`, `integer`, `array`, etc.)\n- Field descriptions from `Field(..., description=\"...\")`\n- Required field list from `Field(...)` vs `Field(default=...)`\n- Nested object schemas for complex types\n\n**Sources:** [src/agent/tool/base_tool.py:31-71]()\n\n---\n\n## Workspace State Management\n\nThe `workspace.py` module manages global variable persistence across multiple code executions.\n\n### Global State Lists\n\n```mermaid\ngraph LR\n    EXEC1[\"Execution 1\"] --> ARG1[\"arg_globals_list[0]<br/>Initial workspace\"]\n    EXEC1 --> OUT1[\"out_globals_list[0]<br/>After exec 1\"]\n    \n    EXEC2[\"Execution 2\"] --> ARG2[\"arg_globals_list[1]<br/>= filtered copy of out_globals_list[0]\"]\n    EXEC2 --> OUT2[\"out_globals_list[1]<br/>After exec 2\"]\n    \n    EXEC3[\"Execution 3\"] --> ARG3[\"arg_globals_list[2]<br/>= filtered copy of out_globals_list[1]\"]\n    EXEC3 --> OUT3[\"out_globals_list[2]<br/>After exec 3\"]\n    \n    OUT1 --> ARG2\n    OUT2 --> ARG3\n    \n    style ARG1 fill:#e8f4f8\n    style ARG2 fill:#e8f4f8\n    style ARG3 fill:#e8f4f8\n    style OUT1 fill:#f0f8e8\n    style OUT2 fill:#f0f8e8\n    style OUT3 fill:#f0f8e8\n```\n\n**Key data structures:**\n\n| Variable | Type | Purpose |\n|----------|------|---------|\n| `arg_globals_list` | `list[dict]` | Input globals for each execution (filtered) |\n| `out_globals_list` | `list[dict]` | Output globals after each execution (filtered) |\n\n**Sources:** [src/runtime/workspace.py:10-11]()\n\n### Filtering and Serialization\n\nThe `filter_and_deepcopy_globals()` function ensures only valid, serializable objects are preserved:\n\n```mermaid\ngraph TB\n    INPUT[\"original_globals dict\"] --> FILTER[\"filter_and_deepcopy_globals()\"]\n    \n    FILTER --> CHECK1[\"Exclude __builtins__\"]\n    FILTER --> CHECK2[\"Exclude module objects<br/>(isinstance(value, type(sys)))\"]\n    FILTER --> CHECK3[\"Test pickle.dumps(value)<br/>Catch PicklingError, TypeError, etc.\"]\n    FILTER --> COPY[\"Deep copy valid objects<br/>copy.deepcopy(value)\"]\n    \n    CHECK1 --> FILTERED[\"filtered_dict\"]\n    CHECK2 --> FILTERED\n    CHECK3 --> FILTERED\n    COPY --> FILTERED\n    \n    FILTERED --> OUTPUT[\"Dict[str, Any]<br/>Safe for IPC/serialization\"]\n```\n\n**Filtering rules:**\n\n1. **Exclude `__builtins__`**: Standard library globals are not persisted\n2. **Exclude modules**: Module objects (e.g., `import numpy`) cannot be pickled\n3. **Test serializability**: Uses `pickle.dumps()` to validate picklability\n4. **Deep copy**: Creates independent copies to prevent reference sharing\n\n**Exception types caught during serialization test:**\n- `pickle.PicklingError`\n- `TypeError`\n- `AttributeError`\n- `RecursionError`\n- `MemoryError`\n\n**Sources:** [src/runtime/workspace.py:38-78]()\n\n### Workspace Initialization\n\n```python\ndef initialize_workspace() -> dict:\n    workspace: dict = __create_workspace()\n    instance = workspace.update({'__name__': '__main__'})\n    return workspace\n```\n\nThe `initialize_workspace()` function creates a new execution namespace by:\n1. Calling `exec(\"\", workspace)` to populate default builtins\n2. Setting `__name__` to `'__main__'` for script-like execution context\n\n**Sources:** [src/runtime/workspace.py:14-23]()\n\n---\n\n## Data Flow Relationships\n\nThe following diagram shows how schemas and data structures relate across the system:\n\n```mermaid\ngraph TB\n    subgraph \"Agent Layer\"\n        USER[\"User Query\"]\n        MESSAGES[\"messages list<br/>(OpenAI format)\"]\n        LLM[\"LLM Response<br/>tool_calls\"]\n    end\n    \n    subgraph \"Tool Invocation\"\n        TOOLSCHEMA[\"BaseTool.get_tool_schema()<br/>Function signature\"]\n        TOOLCALL[\"Tool instantiation<br/>with parsed arguments\"]\n    end\n    \n    subgraph \"Python Execution Tool\"\n        PYTOOL[\"ExecutePythonCodeTool.run()\"]\n        GETGLOBALS[\"workspace.get_arg_globals()\"]\n        SUBPROCESS[\"run_structured_in_subprocess()\"]\n        APPENDGLOBALS[\"workspace.append_out_globals()\"]\n    end\n    \n    subgraph \"Execution Runtime\"\n        WORKER[\"_worker_with_pipe\"]\n        EXECRESULT[\"ExecutionResult construction\"]\n        FILTER[\"filter_and_deepcopy_globals\"]\n    end\n    \n    subgraph \"Task Management Tool\"\n        TODOTOOL[\"RecursivePlanTreeTodoTool.run()\"]\n        TRACK[\"todo_track.run()\"]\n        ARGLIST[\"arg_todo_list\"]\n        ANALYZE[\"_analyze_changes\"]\n    end\n    \n    USER --> MESSAGES\n    MESSAGES --> LLM\n    LLM --> TOOLSCHEMA\n    TOOLSCHEMA --> TOOLCALL\n    \n    TOOLCALL --> PYTOOL\n    TOOLCALL --> TODOTOOL\n    \n    PYTOOL --> GETGLOBALS\n    GETGLOBALS --> SUBPROCESS\n    SUBPROCESS --> WORKER\n    WORKER --> EXECRESULT\n    EXECRESULT --> FILTER\n    FILTER --> APPENDGLOBALS\n    EXECRESULT --> MESSAGES\n    \n    TODOTOOL --> TRACK\n    TRACK --> ARGLIST\n    TRACK --> ANALYZE\n    ANALYZE --> MESSAGES\n```\n\n**Sources:** [src/runtime/schemas.py:1-111](), [src/memory/tree_todo/schemas.py:1-81](), [src/agent/tool/base_tool.py:1-76](), [src/runtime/workspace.py:1-108](), [tests/playground/subprocess_output.py:1-706]()\n\n---\n\n## Summary of Key Architectural Patterns\n\n### 1. Structured Error Handling\n\nAll execution outcomes are captured in strongly-typed enums (`ExecutionStatus`, `TaskStatus`) with associated metadata. This enables:\n- Predictable error handling logic\n- Clear state transitions\n- Informative LLM feedback generation\n\n### 2. Recursive Data Structures\n\nBoth `RecursivePlanTreeNode` and nested children support arbitrary depth hierarchies, enabling:\n- Complex task decomposition\n- Dependency graph representation\n- Incremental status tracking\n\n### 3. Serialization-First Design\n\nAll data models use Pydantic validation with explicit serialization checks (`filter_and_deepcopy_globals`), ensuring:\n- IPC compatibility for multiprocessing\n- State persistence across executions\n- Type safety at runtime\n\n### 4. Schema Auto-Generation\n\nTool schemas are derived from Pydantic models via `model_json_schema()`, providing:\n- Single source of truth for parameters\n- Automatic OpenAI function calling format\n- Type-safe tool invocations\n\n**Sources:** [src/runtime/schemas.py:1-111](), [src/memory/tree_todo/schemas.py:1-81](), [src/agent/tool/base_tool.py:1-76](), [src/runtime/workspace.py:1-108]()\n\n---\n\n# Page: ExecutionResult Schema Reference\n\n# ExecutionResult Schema Reference\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [docs/crashed.design.md](docs/crashed.design.md)\n- [src/runtime/cwd.py](src/runtime/cwd.py)\n- [src/runtime/schemas.py](src/runtime/schemas.py)\n- [src/runtime/subprocess_python_executor.py](src/runtime/subprocess_python_executor.py)\n- [tests/playground/crashed.py](tests/playground/crashed.py)\n- [tests/playground/subprocess_output.py](tests/playground/subprocess_output.py)\n\n</details>\n\n\n\nThis page provides a complete technical reference for the `ExecutionResult` and `ExecutionStatus` data structures that represent the outcome of Python code execution across all execution strategies (subprocess, subthread, and direct execution).\n\nFor information about how ExecutionResult is used in practice during code execution, see [ExecutionResult and Status Handling](#5.4). For implementation details of the subprocess executor that produces these results, see [Subprocess Execution](#5.1).\n\n---\n\n## Overview\n\nThe `ExecutionResult` schema is a Pydantic model that captures comprehensive information about Python code execution outcomes. It serves as the unified return type for all execution strategies, providing structured data about success, failure, timeout, or crash scenarios along with associated metadata.\n\nThe schema is designed to:\n- Provide consistent execution results across different executor implementations\n- Capture detailed error information for debugging\n- Preserve global variable state for workspace persistence\n- Generate formatted feedback for the LLM agent\n- Support timeout and crash detection\n\nSources: [src/runtime/schemas.py:56-70]()\n\n---\n\n## ExecutionStatus Enum\n\n### Status Values\n\nThe `ExecutionStatus` enum defines four possible execution outcomes:\n\n| Status | String Value | Description | Exit Code Range |\n|--------|-------------|-------------|-----------------|\n| `SUCCESS` | `\"success\"` | Code executed without exceptions | `0` |\n| `FAILURE` | `\"failure\"` | Code raised an exception (e.g., ZeroDivisionError, TypeError) | `0` (exception caught) |\n| `TIMEOUT` | `\"timeout\"` | Execution exceeded timeout limit and was terminated | `-15` (SIGTERM) |\n| `CRASHED` | `\"crashed\"` | Process crashed abnormally (e.g., SegFault, OOM) | `139` (SIGSEGV), `1` (other errors) |\n\nSources: [src/runtime/schemas.py:11-16]()\n\n### Status Determination Flow\n\n```mermaid\ngraph TD\n    START[\"Process Execution Begins\"]\n    EXEC[\"exec(command, globals, locals)\"]\n    WAIT[\"p.join(timeout)\"]\n    \n    START --> EXEC\n    EXEC --> WAIT\n    \n    WAIT --> TIMEOUT_CHECK{\"p.is_alive()?\"}\n    TIMEOUT_CHECK -->|Yes| TIMEOUT[\"Status: TIMEOUT<br/>exit_code: -15\"]\n    TIMEOUT_CHECK -->|No| NORMAL[\"Process Exited\"]\n    \n    NORMAL --> RESULT_CHECK{\"ExecutionResult<br/>received from<br/>subprocess?\"}\n    \n    RESULT_CHECK -->|Yes| EXEC_RESULT{\"Exception<br/>occurred?\"}\n    EXEC_RESULT -->|No| SUCCESS[\"Status: SUCCESS<br/>exit_code: 0\"]\n    EXEC_RESULT -->|Yes| FAILURE[\"Status: FAILURE<br/>exit_code: 0\"]\n    \n    RESULT_CHECK -->|No| CRASHED[\"Status: CRASHED<br/>exit_code: 139 or 1\"]\n    \n    style SUCCESS fill:#f0fff0\n    style FAILURE fill:#fff0f0\n    style TIMEOUT fill:#fff8f0\n    style CRASHED fill:#f8f0ff\n```\n\n**Status Determination Logic:**\n1. **Parent process waits** with timeout via `p.join(timeout)`\n2. **If process still alive** after timeout â `TIMEOUT`\n3. **If process exited normally:**\n   - **If ExecutionResult received** from pipe â check exception fields\n     - No exception â `SUCCESS`\n     - Exception present â `FAILURE`\n   - **If no ExecutionResult received** â `CRASHED`\n\nSources: [src/runtime/subprocess_python_executor.py:128-159](), [tests/playground/subprocess_output.py:120-155]()\n\n---\n\n## ExecutionResult Schema\n\n### Schema Structure\n\n```mermaid\nclassDiagram\n    class ExecutionResult {\n        +arg_command: str\n        +arg_timeout: int\n        +arg_globals: Dict[str, Any]\n        +exit_status: ExecutionStatus\n        +exit_code: Optional[int]\n        +exception_repr: Optional[str]\n        +exception_type: Optional[str]\n        +exception_value: Optional[str]\n        +exception_traceback: Optional[str]\n        +ret_stdout: str\n        +ret_tool2llm: Optional[str]\n        +field_validate_globals(value) Dict\n    }\n    \n    class ExecutionStatus {\n        <<enumeration>>\n        SUCCESS\n        FAILURE\n        TIMEOUT\n        CRASHED\n        +get_return_llm(status, result) str\n    }\n    \n    ExecutionResult --> ExecutionStatus : exit_status\n    ExecutionResult ..> \"workspace.filter_and_deepcopy_globals\" : validates arg_globals\n    ExecutionStatus ..> ExecutionResult : formats ret_tool2llm\n```\n\nSources: [src/runtime/schemas.py:56-83]()\n\n### Field Reference\n\n#### Input Arguments (Populated by Caller)\n\n| Field | Type | Required | Description | Population Point |\n|-------|------|----------|-------------|------------------|\n| `arg_command` | `str` | Yes | The Python code snippet that was executed | Caller passes to executor |\n| `arg_timeout` | `int` | Yes | Maximum allowed execution time in seconds | Caller passes to executor |\n| `arg_globals` | `Dict[str, Any]` | Yes | Global variables after execution, filtered and deep-copied | Subprocess filters during construction |\n\n**`arg_globals` Validation:**\nThe `field_validate_globals` validator automatically processes this field by calling `workspace.filter_and_deepcopy_globals()`, which:\n- Removes built-in objects (`__builtins__`)\n- Removes module references\n- Excludes non-picklable objects\n- Deep copies remaining values to prevent mutation\n\nSources: [src/runtime/schemas.py:77-83](), [src/runtime/schemas.py:58-60]()\n\n#### Execution Results (Populated by Subprocess or Parent)\n\n| Field | Type | Required | Description | Population Point |\n|-------|------|----------|-------------|------------------|\n| `exit_status` | `ExecutionStatus` | Yes | Execution outcome: SUCCESS, FAILURE, TIMEOUT, or CRASHED | Set in subprocess (SUCCESS/FAILURE) or parent (TIMEOUT/CRASHED) |\n| `exit_code` | `Optional[int]` | No | Process exit code from OS | Parent process via `p.exitcode` |\n\nSources: [src/runtime/schemas.py:62-63]()\n\n#### Exception Details (Populated on FAILURE)\n\n| Field | Type | Required | Description | Example |\n|-------|------|----------|-------------|---------|\n| `exception_repr` | `Optional[str]` | No | String representation of exception | `\"ZeroDivisionError('division by zero')\"` |\n| `exception_type` | `Optional[str]` | No | Exception class name | `\"ZeroDivisionError\"` |\n| `exception_value` | `Optional[str]` | No | Exception message | `\"division by zero\"` |\n| `exception_traceback` | `Optional[str]` | No | Full traceback from `traceback.format_exc()` | Multi-line traceback string |\n\nThese fields are populated only when `exit_status == FAILURE`, captured in the subprocess via `except Exception as e:` block.\n\nSources: [src/runtime/schemas.py:64-67](), [src/runtime/subprocess_python_executor.py:58-69]()\n\n#### Generated Output (Populated by Parent Process)\n\n| Field | Type | Required | Description | Population Point |\n|-------|------|----------|-------------|------------------|\n| `ret_stdout` | `str` | No (defaults to `\"\"`) | Captured stdout/stderr output | Parent accumulates from pipe reader thread |\n| `ret_tool2llm` | `Optional[str]` | No | Formatted message for LLM based on status | Parent calls `ExecutionStatus.get_return_llm()` |\n\nSources: [src/runtime/schemas.py:69-70](), [src/runtime/subprocess_python_executor.py:161-162]()\n\n---\n\n## Field Population Lifecycle\n\n### Execution Timeline\n\n```mermaid\nsequenceDiagram\n    participant Caller\n    participant Parent as \"Parent Process\"\n    participant Subprocess as \"Child Process\"\n    participant Pipe as \"Pipe Communication\"\n    \n    Caller->>Parent: run_structured_in_subprocess(command, globals, timeout)\n    Parent->>Parent: Create multiprocessing.Pipe()\n    Parent->>Subprocess: spawn/fork with _worker_with_pipe()\n    Parent->>Parent: Start reader thread\n    \n    Subprocess->>Subprocess: Redirect stdout to _PipeWriter\n    \n    alt Normal Execution\n        Subprocess->>Subprocess: exec(command, globals, locals)\n        Subprocess->>Subprocess: Create ExecutionResult(SUCCESS)\n        Subprocess->>Pipe: send(('result', ExecutionResult))\n        Pipe->>Parent: subprocess_result_container[0] = result\n    end\n    \n    alt Exception During Execution\n        Subprocess->>Subprocess: exec() raises exception\n        Subprocess->>Subprocess: Capture traceback.format_exc()\n        Subprocess->>Subprocess: Create ExecutionResult(FAILURE)\n        Subprocess->>Pipe: send(('result', ExecutionResult))\n        Pipe->>Parent: subprocess_result_container[0] = result\n    end\n    \n    alt Timeout\n        Parent->>Parent: p.join(timeout) - still alive\n        Parent->>Subprocess: p.terminate() [SIGTERM]\n        Parent->>Parent: Create ExecutionResult(TIMEOUT)\n    end\n    \n    alt Crash\n        Subprocess->>Subprocess: SegFault / OOM / os._exit()\n        Subprocess--xPipe: No result sent\n        Parent->>Parent: subprocess_result_container is empty\n        Parent->>Parent: Create ExecutionResult(CRASHED)\n    end\n    \n    Parent->>Parent: final_res.exit_code = p.exitcode\n    Parent->>Parent: final_res.ret_stdout = \"\".join(buffer)\n    Parent->>Parent: final_res.ret_tool2llm = get_return_llm()\n    Parent->>Caller: return final_res\n```\n\n**Lifecycle Stages:**\n\n1. **Initialization** (Caller â Parent):\n   - `arg_command`, `arg_timeout`, `arg_globals` provided\n   \n2. **Subprocess Execution** (Child Process):\n   - Redirects stdout/stderr to pipe\n   - Executes code with `exec()`\n   - **On success:** Creates `ExecutionResult(SUCCESS)` with filtered globals\n   - **On exception:** Creates `ExecutionResult(FAILURE)` with exception details\n   - Sends result through pipe\n   \n3. **Parent Process Monitoring**:\n   - Reader thread accumulates stdout messages\n   - **If timeout:** Creates `ExecutionResult(TIMEOUT)` without subprocess data\n   - **If crashed:** Creates `ExecutionResult(CRASHED)` when no result received\n   \n4. **Finalization** (Parent):\n   - Sets `exit_code` from `p.exitcode`\n   - Joins `ret_stdout` from accumulated buffer\n   - Generates `ret_tool2llm` via `get_return_llm()`\n\nSources: [src/runtime/subprocess_python_executor.py:76-163](), [tests/playground/subprocess_output.py:68-155]()\n\n---\n\n## Status-Based LLM Formatting\n\n### get_return_llm Method\n\nThe `ExecutionStatus.get_return_llm()` static method generates formatted messages for the LLM agent based on execution status. This method is called by the parent process after all fields are populated.\n\n```mermaid\ngraph LR\n    RESULT[\"ExecutionResult<br/>with exit_status\"]\n    METHOD[\"ExecutionStatus.get_return_llm(status, result)\"]\n    \n    SUCCESS_MSG[\"## ä»£ç æ§è¡æåï¼è¾åºç»æå®æ´ï¼ä»»å¡å®æ<br/>### ç»ç«¯è¾åºï¼<br/>{stdout}\"]\n    FAILURE_MSG[\"## ä»£ç æ§è¡å¤±è´¥ï¼ä»£ç æåºå¼å¸¸ï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯<br/>### ç»ç«¯è¾åºï¼<br/>{stdout}<br/>### åå§ä»£ç ï¼<br/>{numbered_code}<br/>### æ¥éä¿¡æ¯ï¼<br/>{traceback}\"]\n    TIMEOUT_MSG[\"## ä»£ç æ§è¡è¶æ¶ï¼å¼ºå¶éåºæ§è¡ï¼è°æ´è¶æ¶æ¶é´åéè¯<br/>### ç»ç«¯è¾åºï¼<br/>{stdout}<br/>### è¶åºéå¶çæ¶é´ï¼{timeout} ç§\"]\n    CRASHED_MSG[\"## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯<br/>### ç»ç«¯è¾åºï¼<br/>{stdout}<br/>### éåºç¶æç ï¼{exit_code}\"]\n    \n    RESULT --> METHOD\n    METHOD -->|SUCCESS| SUCCESS_MSG\n    METHOD -->|FAILURE| FAILURE_MSG\n    METHOD -->|TIMEOUT| TIMEOUT_MSG\n    METHOD -->|CRASHED| CRASHED_MSG\n```\n\nSources: [src/runtime/schemas.py:18-47]()\n\n### Format Templates\n\n#### SUCCESS Format\n```markdown\n## ä»£ç æ§è¡æåï¼è¾åºç»æå®æ´ï¼ä»»å¡å®æ\n### ç»ç«¯è¾åºï¼\n{result.ret_stdout}\n```\n\n#### FAILURE Format\n```markdown\n## ä»£ç æ§è¡å¤±è´¥ï¼ä»£ç æåºå¼å¸¸ï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\n{result.ret_stdout}\n### åå§ä»£ç ï¼\n{source_code.add_line_numbers(result.arg_command)}\n### æ¥éä¿¡æ¯ï¼\n{result.exception_traceback}\n```\n\n#### TIMEOUT Format\n```markdown\n## ä»£ç æ§è¡è¶æ¶ï¼å¼ºå¶éåºæ§è¡ï¼è°æ´è¶æ¶æ¶é´åéè¯\n### ç»ç«¯è¾åºï¼\n{result.ret_stdout}\n### è¶åºéå¶çæ¶é´ï¼{result.arg_timeout} ç§\n```\n\n#### CRASHED Format\n```markdown\n## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\n{result.ret_stdout}\n### éåºç¶æç ï¼{result.exit_code}\n```\n\nSources: [src/runtime/schemas.py:23-46]()\n\n---\n\n## Exit Code Interpretation\n\n### Unix Exit Code Semantics\n\nThe `exit_code` field follows standard Unix conventions:\n\n| Exit Code | Meaning | Example Scenario |\n|-----------|---------|------------------|\n| `0` | Normal exit | Successful execution or caught exception |\n| `> 0` | Error exit | Logic errors, caught and re-raised exceptions |\n| `< 0` | Signal termination | `-15` (SIGTERM), `-9` (SIGKILL) |\n| `139` (`128+11`) | SIGSEGV | Segmentation fault |\n\n**Code Comment Reference:**\n```python\n\"\"\"\nexitcode = 0ï¼è¿ç¨æ­£å¸¸éåºï¼\nexitcode > 0ï¼è¿ç¨å éè¯¯éåºï¼å¦ä»£ç é»è¾éè¯¯ãå½ä»¤æ§è¡å¤±è´¥ï¼ï¼\nexitcode < 0ï¼è¿ç¨è¢«ä¿¡å·ç»æ­¢ï¼-ä¿¡å·ç¼å·ï¼å¦ -15 å¯¹åº SIGTERMï¼-9 å¯¹åº SIGKILL å¼ºå¶ç»æ­¢ï¼\n             -11=139=128+11 å¯¹åº SIGSEGV segmentation faultï¼ã\n\"\"\"\n```\n\nSources: [src/runtime/schemas.py:50-54]()\n\n---\n\n## Example Outputs by Status\n\n### SUCCESS Example\n\n```python\n{\n    'arg_command': 'import time\\nc = 10\\nimport scipy\\nprint(\"scipy imported\")\\n',\n    'arg_globals': {'a': 123, 'b': [1, 2, 3], 'c': 10},\n    'arg_timeout': 20000,\n    'exception_repr': None,\n    'exception_traceback': None,\n    'exception_type': None,\n    'exception_value': None,\n    'exit_code': 0,\n    'exit_status': <ExecutionStatus.SUCCESS: 'success'>,\n    'ret_stdout': 'scipy imported\\n',\n    'ret_tool2llm': '## ä»£ç æ§è¡æåï¼è¾åºç»æå®æ´ï¼ä»»å¡å®æ\\n### ç»ç«¯è¾åºï¼\\nscipy imported\\n'\n}\n```\n\nSources: [tests/playground/subprocess_output.py:362-372]()\n\n### FAILURE Example\n\n```python\n{\n    'arg_command': '\\na = 123\\nb = 0\\nc = a/b\\n',\n    'arg_globals': {'a': 123, 'b': 0},\n    'arg_timeout': 20000,\n    'exception_repr': \"ZeroDivisionError('division by zero')\",\n    'exception_traceback': 'Traceback (most recent call last):\\n'\n                           '  File \"...subprocess_output.py\", line 42, in _worker_with_pipe\\n'\n                           '    exec(command, _globals, _locals)\\n'\n                           '  File \"<string>\", line 4, in <module>\\n'\n                           'ZeroDivisionError: division by zero\\n',\n    'exception_type': 'ZeroDivisionError',\n    'exception_value': 'division by zero',\n    'exit_code': 0,\n    'exit_status': <ExecutionStatus.FAILURE: 'failure'>,\n    'ret_stdout': '',\n    'ret_tool2llm': '## ä»£ç æ§è¡å¤±è´¥ï¼ä»£ç æåºå¼å¸¸ï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\\n...'\n}\n```\n\nSources: [tests/playground/subprocess_output.py:397-427]()\n\n### TIMEOUT Example\n\n```python\n{\n    'arg_command': '\\nimport time\\nprint(\"Start sleeping...\", flush=True)\\ntime.sleep(10)\\n...',\n    'arg_globals': {'a': 123, 'b': [1, 2, 3]},\n    'arg_timeout': 3,\n    'exception_repr': None,\n    'exception_traceback': None,\n    'exception_type': None,\n    'exception_value': None,\n    'exit_code': -15,  # SIGTERM\n    'exit_status': <ExecutionStatus.TIMEOUT: 'timeout'>,\n    'ret_stdout': 'Start sleeping...\\n',\n    'ret_tool2llm': '## ä»£ç æ§è¡è¶æ¶ï¼å¼ºå¶éåºæ§è¡ï¼è°æ´è¶æ¶æ¶é´åéè¯\\n...'\n}\n```\n\nSources: [tests/playground/subprocess_output.py:320-337]()\n\n### CRASHED Example (SegFault)\n\n```python\n{\n    'arg_command': '\\nimport os\\nos._exit(139)  # Trigger SegFault\\n',\n    'arg_globals': {'a': 123, 'b': [1, 2, 3]},\n    'arg_timeout': 3,\n    'exception_repr': None,\n    'exception_traceback': None,\n    'exception_type': None,\n    'exception_value': None,\n    'exit_code': 139,  # SIGSEGV\n    'exit_status': <ExecutionStatus.CRASHED: 'crashed'>,\n    'ret_stdout': '',\n    'ret_tool2llm': '## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\\n...'\n}\n```\n\nSources: [tests/playground/subprocess_output.py:451-468]()\n\n### CRASHED Example (RecursionError with Pickle Failure)\n\n```python\n{\n    'arg_command': '\\ndef recursive_crash(depth=0):\\n    recursive_crash(depth + 1)\\nrecursive_crash()\\n',\n    'arg_globals': {'a': 123, 'b': [1, 2, 3]},\n    'arg_timeout': 3,\n    'exception_repr': None,\n    'exception_traceback': None,\n    'exception_type': None,\n    'exception_value': None,\n    'exit_code': 1,\n    'exit_status': <ExecutionStatus.CRASHED: 'crashed'>,\n    'ret_stdout': 'å½åéå½æ·±åº¦ï¼0\\n...\\n_pickle.PicklingError: Can\\'t pickle <function recursive_crash>...\\n',\n    'ret_tool2llm': '## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\\n...'\n}\n```\n\n**Note:** This crashes because the subprocess attempts to send the `ExecutionResult` containing a local function through the pipe, but the function cannot be pickled. The process exits before successfully transmitting the result, triggering `CRASHED` status.\n\nSources: [tests/playground/subprocess_output.py:493-593]()\n\n---\n\n## Pydantic Configuration\n\nThe `ExecutionResult` model uses the following Pydantic configuration:\n\n```python\nmodel_config = ConfigDict(\n    # use_enum_values=True,  # Commented out - keeps enum objects rather than strings\n    arbitrary_types_allowed=True  # Allows Dict[str, Any] with non-JSON types\n)\n```\n\n**Configuration Details:**\n- `arbitrary_types_allowed=True`: Required to support arbitrary Python objects in `arg_globals` (functions, custom classes, etc.)\n- `use_enum_values` is disabled: `exit_status` remains as `ExecutionStatus` enum object rather than converting to string value\n\nSources: [src/runtime/schemas.py:72-75]()\n\n---\n\n## Integration with Workspace Management\n\nThe `arg_globals` field integrates with the workspace system to enable state persistence across multiple code executions:\n\n```mermaid\ngraph LR\n    EXEC1[\"Execution 1:<br/>ExecutionResult\"]\n    FILTER1[\"workspace.filter_and_deepcopy_globals()\"]\n    GLOBALS1[\"arg_globals filtered\"]\n    \n    WORKSPACE[\"workspace.append_out_globals()\"]\n    \n    EXEC2[\"Execution 2:<br/>receives globals\"]\n    \n    EXEC1 --> FILTER1\n    FILTER1 --> GLOBALS1\n    GLOBALS1 --> WORKSPACE\n    WORKSPACE --> EXEC2\n```\n\n**Filtering Process:**\n1. **Automatic validation**: Triggered by `@field_validator('arg_globals')` decorator\n2. **Filter operation**: `workspace.filter_and_deepcopy_globals(value)` removes:\n   - `__builtins__` objects\n   - Module references (identified by `type(v).__name__ == 'module'`)\n   - Non-picklable objects (tested via `pickle.dumps()`)\n3. **Deep copy**: Prevents mutations to original objects\n4. **Workspace append**: Filtered globals added to persistent workspace state\n\nFor more details on workspace management, see [Workspace State Management](#5.5).\n\nSources: [src/runtime/schemas.py:77-83](), [tests/playground/subprocess_output.py:88-99]()\n\n---\n\n## Common Patterns and Edge Cases\n\n### Pattern: Checking for Successful Execution\n\n```python\nif result.exit_status == ExecutionStatus.SUCCESS:\n    # Safe to use arg_globals for subsequent executions\n    workspace.append_out_globals(result.arg_globals)\nelse:\n    # Log error details\n    logger.error(f\"Execution failed: {result.ret_tool2llm}\")\n```\n\n### Edge Case: Empty Exception Fields on CRASHED\n\nWhen status is `CRASHED`, exception fields (`exception_type`, `exception_value`, etc.) are `None` because the subprocess terminated before sending the result. The crash details may be available in `ret_stdout` (stderr output before crash).\n\n### Edge Case: Exit Code 0 with FAILURE Status\n\nWhen code raises an exception that is caught by the subprocess wrapper, the process exits normally (`exit_code=0`) but `exit_status=FAILURE` because the exception details are captured.\n\n### Edge Case: Pickle Errors During Result Transmission\n\nIf `arg_globals` contains unpicklable objects (functions defined in `__main__`, certain class instances), the subprocess may fail to send the result through the pipe, resulting in `CRASHED` status with pickle-related errors in `ret_stdout`.\n\nSources: [tests/playground/subprocess_output.py:215-260](), [tests/playground/subprocess_output.py:469-593]()\n\n---\n\n# Page: Task Tree Schema Reference\n\n# Task Tree Schema Reference\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [src/agent/tool/base_tool.py](src/agent/tool/base_tool.py)\n- [src/agent/tool/python_tool.py](src/agent/tool/python_tool.py)\n- [src/agent/tool/todo_tool.py](src/agent/tool/todo_tool.py)\n- [src/memory/tree_todo/schemas.py](src/memory/tree_todo/schemas.py)\n- [src/memory/tree_todo/todo_track.py](src/memory/tree_todo/todo_track.py)\n\n</details>\n\n\n\nThis document provides a complete technical reference for the task tree data structures used by the `RecursivePlanTreeTodoTool`. These schemas define hierarchical task planning, status tracking, and change analysis capabilities in the algo_agent system.\n\nFor information about how these schemas are used in practice, see [Recursive Task Planning Tool](#4.3). For the execution result schemas, see [ExecutionResult Schema Reference](#9.1).\n\n---\n\n## Overview\n\nThe task tree system provides structured hierarchical task planning through three primary components:\n\n| Component | Type | Purpose | File Location |\n|-----------|------|---------|---------------|\n| `TaskStatus` | Enum | Task lifecycle state enumeration | [src/memory/tree_todo/schemas.py:8-40]() |\n| `RecursivePlanTreeNode` | Pydantic Model | Individual task unit with recursive children | [src/memory/tree_todo/schemas.py:44-64]() |\n| `RecursivePlanTree` | Pydantic Model | Complete task tree with metadata | [src/memory/tree_todo/schemas.py:68-80]() |\n\nAll schemas use Pydantic V2 for validation and serialization. The tree structure supports unlimited nesting depth through recursive `children` fields.\n\n**Sources:** [src/memory/tree_todo/schemas.py:1-80]()\n\n---\n\n## Schema Architecture\n\n### Task Tree Structure Diagram\n\n```mermaid\ngraph TB\n    RecursivePlanTree[\"RecursivePlanTree<br/>(Root Container)\"]\n    Node1[\"RecursivePlanTreeNode<br/>(task_id, task_name, status)\"]\n    Node2[\"RecursivePlanTreeNode<br/>(task_id, task_name, status)\"]\n    Node3[\"RecursivePlanTreeNode<br/>(task_id, task_name, status)\"]\n    Node4[\"RecursivePlanTreeNode<br/>(task_id, task_name, status)\"]\n    \n    RecursivePlanTree -->|\"tree_nodes: List\"| Node1\n    RecursivePlanTree -->|\"tree_nodes: List\"| Node2\n    Node1 -->|\"children: Optional[List]\"| Node3\n    Node1 -->|\"children: Optional[List]\"| Node4\n    Node3 -->|\"children: Optional[List]\"| Node5[\"RecursivePlanTreeNode<br/>(recursive)\"]\n    \n    RecursivePlanTree -.->|\"core_goal: str\"| Goal[\"'Complete Python project'\"]\n    Node1 -.->|\"status: TaskStatus\"| Status1[\"PROCESSING\"]\n    Node2 -.->|\"status: TaskStatus\"| Status2[\"PENDING\"]\n    Node3 -.->|\"status: TaskStatus\"| Status3[\"COMPLETED\"]\n    \n    RecursivePlanTree -.->|\"references: Optional[List[str]]\"| Refs[\"['doc_url', 'data_source']\"]\n    RecursivePlanTree -.->|\"next_action: Dict[str, Any]\"| NextAction[\"{'priority': 'high'}\"]\n```\n\n**Sources:** [src/memory/tree_todo/schemas.py:44-80]()\n\n---\n\n## TaskStatus Enum\n\nThe `TaskStatus` enum defines six lifecycle states for tasks. Each status has an associated display symbol and Chinese description used for rendering.\n\n### Enum Definition\n\n```python\nclass TaskStatus(str, Enum):\n    PENDING = \"pending\"        # Waiting to execute\n    PROCESSING = \"processing\"  # Currently executing\n    COMPLETED = \"completed\"    # Successfully completed\n    FAILED = \"failed\"          # Execution failed\n    RETRY = \"retry\"            # Retry required\n    SKIPPED = \"skipped\"        # Intentionally skipped\n```\n\n### Status Properties\n\n| Status | Value | Symbol | Description (Chinese) | Use Case |\n|--------|-------|--------|----------------------|----------|\n| `PENDING` | `\"pending\"` | `[â³]` | å¾æ§è¡ | Task not yet started |\n| `PROCESSING` | `\"processing\"` | `[â¡ï¸]` | æ­£å¨æ§è¡ | Task currently executing |\n| `COMPLETED` | `\"completed\"` | `[â]` | æ§è¡æå | Task finished successfully |\n| `FAILED` | `\"failed\"` | `[â]` | æ§è¡å¤±è´¥ | Task encountered error |\n| `RETRY` | `\"retry\"` | `[â»ï¸]` | éè¯ | Task requires retry |\n| `SKIPPED` | `\"skipped\"` | `[â]` | å·²è·³è¿ | Task intentionally bypassed |\n\n### Property Methods\n\nThe enum provides two property methods for display purposes:\n\n- **`display_symbol`** - Returns the emoji/symbol for visual rendering (e.g., `[â]`)\n- **`display_desc`** - Returns the Chinese language description (e.g., `\"æ§è¡æå\"`)\n\n**Implementation:**\n```python\n@property\ndef display_symbol(self) -> str:\n    \"\"\"ç¶æå¯¹åºçå¯è§åç¬¦å·\"\"\"\n    symbol_map = {\n        self.PENDING: \"[â³]\",\n        self.PROCESSING: \"[â¡ï¸]\",\n        # ... (additional mappings)\n    }\n    return symbol_map[self]\n```\n\n**Sources:** [src/memory/tree_todo/schemas.py:8-40]()\n\n---\n\n## RecursivePlanTreeNode Schema\n\n`RecursivePlanTreeNode` represents an individual task unit with support for hierarchical nesting through the `children` field.\n\n### Field Reference\n\n| Field Name | Type | Required | Default | Description |\n|------------|------|----------|---------|-------------|\n| `task_id` | `str` | No | `TASK-{uuid4()}` | Unique task identifier, auto-generated if not provided |\n| `task_name` | `str` | **Yes** | - | Task name (must be globally unique, used in dependencies) |\n| `description` | `str` | No | `\"\"` | Detailed task explanation or requirements |\n| `status` | `TaskStatus` | No | `PENDING` | Current task status (enum value) |\n| `output` | `str` | No | `\"\"` | Execution result or completion notes |\n| `dependencies` | `Optional[List[str]]` | No | `None` | List of prerequisite task names (references `task_name`) |\n| `research_directions` | `Optional[List[str]]` | No | `None` | Deep research directions for complex tasks |\n| `children` | `Optional[List[RecursivePlanTreeNode]]` | No | `None` | Nested child tasks (recursive structure) |\n\n### Field Validation\n\nThe schema includes a validator for the `children` field that converts empty lists to `None`:\n\n```python\n@field_validator(\"children\")\ndef empty_children_to_none(cls, v: Optional[List[\"RecursivePlanTreeNode\"]]) -> Optional[List[\"RecursivePlanTreeNode\"]]:\n    return v if v and len(v) > 0 else None\n```\n\nThis ensures that tasks without children have `children=None` rather than `children=[]`.\n\n**Sources:** [src/memory/tree_todo/schemas.py:44-64]()\n\n### Task ID Generation\n\nTask IDs are auto-generated using the pattern `TASK-{uuid4()}` if not explicitly provided:\n\n```python\ntask_id: str = Field(\n    default_factory=lambda: f\"TASK-{str(uuid.uuid4())}\", \n    description=\"ä»»å¡å¯ä¸IDï¼æ¬æ¬¡å·¥å·è°ç¨å¿é¡»å¯ä¸ï¼åç»­æ¨çå¦ææ¯æä»£åä¸ä¸ªä»»å¡ç´æ¥å¼ç¨ï¼\"\n)\n```\n\n**Sources:** [src/memory/tree_todo/schemas.py:46]()\n\n### Dependency System\n\nThe `dependencies` field allows tasks to declare prerequisites using task names:\n\n```python\ndependencies: Optional[List[str]] = Field(\n    default=None, \n    description=\"ä¾èµçä»»å¡åç§°çåè¡¨ï¼ä»»å¡åç§°å¿é¡»æ¯task_name\"\n)\n```\n\n**Important:** Dependencies reference `task_name` values, not `task_id` values. Task names must be globally unique to support this reference mechanism.\n\n**Sources:** [src/memory/tree_todo/schemas.py:51]()\n\n### Self-Reference Resolution\n\nThe schema uses self-referential typing for the `children` field. Pydantic V2 requires explicit model rebuilding to resolve this forward reference:\n\n```python\nRecursivePlanTreeNode.model_rebuild()\n```\n\n**Sources:** [src/memory/tree_todo/schemas.py:64]()\n\n---\n\n## RecursivePlanTree Schema\n\n`RecursivePlanTree` is the root container for the entire task hierarchy, including metadata and context.\n\n### Field Reference\n\n| Field Name | Type | Required | Default | Description |\n|------------|------|----------|---------|-------------|\n| `core_goal` | `str` | **Yes** | - | Primary objective of the entire plan tree |\n| `tree_nodes` | `List[RecursivePlanTreeNode]` | No | `[]` | Root-level task list |\n| `next_action` | `Dict[str, Any]` | No | `{}` | Suggested next steps or metadata |\n| `references` | `Optional[List[str]]` | No | `None` | External references (URLs, data sources) |\n\n### Schema Definition\n\n```python\nclass RecursivePlanTree(BaseModel):\n    \"\"\"å®æ´éå½è®¡åæ ï¼åå«å±çº§ä»»å¡æ ãæ ¸å¿ç®æ ãç¶æç»è®¡ç­\"\"\"\n    core_goal: str = Field(..., description=\"æ ¸å¿ç®æ ï¼è®¡åæ è¦è¾¾æçæç»ç®çï¼\")\n    tree_nodes: List[RecursivePlanTreeNode] = Field(default_factory=list, description=\"è®¡åæ æ ¹ä»»å¡åè¡¨\")\n    next_action: Dict[str, Any] = Field(default_factory=dict, description=\"ä¸ä¸æ­¥å»ºè®®å¨ä½ï¼å¯éï¼\")\n    references: Optional[List[str]] = Field(default=None, description=\"åèèµæºåè¡¨ï¼å¯éï¼å¦ææ¡£é¾æ¥ãæ°æ®æ¥æºï¼\")\n```\n\n**Sources:** [src/memory/tree_todo/schemas.py:68-80]()\n\n### Metadata Fields\n\nThe `RecursivePlanTree` contains two optional metadata fields:\n\n1. **`next_action`** - A flexible dictionary for storing suggested next steps, priorities, or other action metadata\n2. **`references`** - A list of URLs, file paths, or data source identifiers for documentation\n\nThese fields are not processed by the tracking system but are available for tool-specific logic.\n\n**Sources:** [src/memory/tree_todo/schemas.py:74-75]()\n\n---\n\n## Schema Integration with Tools\n\n### Tool Usage Pattern\n\n```mermaid\nsequenceDiagram\n    participant LLM[\"LLM (qwen-plus)\"]\n    participant Tool[\"RecursivePlanTreeTodoTool\"]\n    participant Validator[\"Pydantic Validator\"]\n    participant Tracker[\"todo_track.run()\"]\n    participant Storage[\"arg_todo_list\"]\n    \n    LLM->>Tool: \"tool_calls with recursive_plan_tree\"\n    Tool->>Validator: \"Validate RecursivePlanTree schema\"\n    Validator->>Validator: \"Validate each RecursivePlanTreeNode\"\n    Validator->>Validator: \"Check TaskStatus enum values\"\n    Validator->>Validator: \"Apply empty_children_to_none validator\"\n    Validator-->>Tool: \"Validated RecursivePlanTree instance\"\n    Tool->>Tracker: \"run(recursive_plan_tree)\"\n    Tracker->>Storage: \"Append to arg_todo_list\"\n    Tracker->>Tracker: \"Compare with last version\"\n    Tracker-->>Tool: \"changes_summary, markdown_todo_list\"\n    Tool-->>LLM: \"Return formatted result string\"\n```\n\n**Sources:** [src/agent/tool/todo_tool.py:10-36](), [src/memory/tree_todo/todo_track.py:21-49]()\n\n### Tool Parameter Schema\n\nThe `RecursivePlanTreeTodoTool` declares the tree schema as a required parameter:\n\n```python\nclass RecursivePlanTreeTodoTool(BaseTool):\n    recursive_plan_tree: RecursivePlanTree = Field(\n        ..., \n        description=\"è¦ç®¡ççéå½è®¡åæ å¯¹è±¡ï¼åå«ä»»å¡èç¹ãç¶æåå­ä»»å¡ãåªå¢å ãä¸ä¿®æ¹ãä¸å é¤ä»»å¡èç¹ã\"\n    )\n```\n\nWhen the LLM invokes this tool, it must provide a complete `RecursivePlanTree` object serialized as JSON in the `tool_calls` parameters.\n\n**Sources:** [src/agent/tool/todo_tool.py:19-25]()\n\n---\n\n## Version Tracking and Change Analysis\n\n### Version Storage\n\nThe tracking system maintains a history of all plan tree versions in the `arg_todo_list`:\n\n```python\narg_todo_list: List[RecursivePlanTree] = [\n    RecursivePlanTree(\n        core_goal=\"ç©ºè®¡åæ ç­å¾åå§å\",\n    )\n]\n```\n\nEach time the tool runs, the current tree is appended:\n\n```python\nlast_plan = arg_todo_list[-1] if arg_todo_list else None\narg_todo_list.append(current_plan_tree.model_copy(deep=True))\n```\n\n**Sources:** [src/memory/tree_todo/todo_track.py:6-32]()\n\n### Change Detection\n\nThe `_analyze_changes()` function compares consecutive versions to detect:\n\n| Change Type | Detection Method | Example Output |\n|-------------|------------------|----------------|\n| New tasks | `set(current_ids) - set(last_ids)` | `\"ð æ°å¢ä»»å¡ï¼æ¡æ¶å¯¹æ¯, é¡¹ç®åå§å\"` |\n| Deleted tasks | `set(last_ids) - set(current_ids)` | `\"ðï¸ å é¤ä»»å¡ï¼æ§éæ±\"` |\n| Status changes | Compare status of common task IDs | `\"ð ç¶æåæ´ï¼éæ±åæï¼å¾æ§è¡ â æ§è¡æåï¼\"` |\n| Hierarchy changes | Compare parent tasks | `\"ð å±çº§è°æ´ï¼æ¡æ¶å¯¹æ¯ï¼ç¶ä»»å¡ï¼æ ¹èç¹ â ææ¯éåï¼\"` |\n\n**Implementation:**\n```python\ndef _analyze_changes(last_plan: RecursivePlanTree, current_plan: RecursivePlanTree) -> str:\n    \"\"\"å¯¹æ¯ä¸¤ä¸ªè®¡åæ ï¼åæåæ´åå®¹\"\"\"\n    changes = []\n    # Collect all task IDs recursively\n    last_task_ids = collect_all_task_ids(last_plan.tree_nodes)\n    current_task_ids = collect_all_task_ids(current_plan.tree_nodes)\n    # Detect new, deleted, status-changed, and level-changed tasks\n    # ...\n    return \"\\n\".join(changes) if changes else \"â¹ï¸ è®¡åæ æ ææ¾åæ´\"\n```\n\n**Sources:** [src/memory/tree_todo/todo_track.py:62-121]()\n\n---\n\n## Markdown Rendering\n\n### Rendering Algorithm\n\nThe `_render_plan_tree_markdown()` function recursively generates a hierarchical Todo list:\n\n```python\ndef _render_plan_tree_markdown(nodes: List[RecursivePlanTreeNode], indent_level: int = 0) -> str:\n    \"\"\"éå½æ¸²æè®¡åæ ä¸ºMarkdown Todoåè¡¨\"\"\"\n    markdown_lines = []\n    indent = \"  \" * indent_level  # 2 spaces per level\n    \n    for node in nodes:\n        status_symbol = node.status.display_symbol\n        task_line = f\"{indent}- {status_symbol} **{node.task_name}**ï¼IDï¼{node.task_id}ï¼\"\n        \n        if node.description:\n            task_line += f\"\\n{indent}  > è¯´æï¼{node.description}\"\n        \n        if node.output:\n            task_line += f\"\\n{indent}  > ç»æï¼{node.output}\"\n        \n        markdown_lines.append(task_line)\n        \n        if node.children:\n            child_lines = _render_plan_tree_markdown(node.children, indent_level + 1)\n            markdown_lines.append(child_lines)\n    \n    return \"\\n\".join(markdown_lines)\n```\n\n**Sources:** [src/memory/tree_todo/todo_track.py:137-170]()\n\n### Rendering Example\n\nFor a node with children:\n```\n- [â¡ï¸] **éæ±åæ**ï¼IDï¼TASK-abc123ï¼\n  > è¯´æï¼æ¢³çæ ¸å¿åè½åéåè½éæ±\n  - [â] **æ¶éç¨æ·éæ±**ï¼IDï¼TASK-def456ï¼\n    > ç»æï¼å·²æ¶é3ç±»æ ¸å¿éæ±\n  - [â³] **æ°åéæ±ææ¡£**ï¼IDï¼TASK-ghi789ï¼\n```\n\n**Sources:** [src/memory/tree_todo/todo_track.py:137-170]()\n\n---\n\n## Status Statistics\n\n### Calculation Logic\n\nThe `_calculate_status_statistics()` function aggregates task counts by status:\n\n```python\ndef _calculate_status_statistics(recursive_plan_tree: RecursivePlanTree) -> Dict[str, int]:\n    \"\"\"æ ¹æ®ææä»»å¡ç¶æèªå¨çæç»è®¡ä¿¡æ¯\"\"\"\n    status_count = {status.value: 0 for status in TaskStatus}\n    \n    def count_status(nodes: List[RecursivePlanTreeNode]):\n        for node in nodes:\n            status_count[node.status.value] += 1\n            if node.children:\n                count_status(node.children)\n    \n    count_status(recursive_plan_tree.tree_nodes)\n    \n    total_tasks = sum(status_count.values())\n    statistics = {\n        \"__total\": total_tasks,\n        \"__completion_rate\": round(status_count[TaskStatus.COMPLETED.value] / total_tasks, 2) if total_tasks > 0 else 0.0,\n        \"__pending_rate\": round(status_count[TaskStatus.PENDING.value] / total_tasks, 2) if total_tasks > 0 else 0.0,\n    }\n    status_count.update(statistics)\n    return status_count\n```\n\n**Sources:** [src/memory/tree_todo/todo_track.py:173-200]()\n\n### Statistics Output Format\n\nThe function returns a dictionary with the following structure:\n\n| Key | Type | Description |\n|-----|------|-------------|\n| `\"pending\"` | `int` | Count of PENDING tasks |\n| `\"processing\"` | `int` | Count of PROCESSING tasks |\n| `\"completed\"` | `int` | Count of COMPLETED tasks |\n| `\"failed\"` | `int` | Count of FAILED tasks |\n| `\"retry\"` | `int` | Count of RETRY tasks |\n| `\"skipped\"` | `int` | Count of SKIPPED tasks |\n| `\"__total\"` | `int` | Total task count |\n| `\"__completion_rate\"` | `float` | Completion rate (0.0-1.0) |\n| `\"__pending_rate\"` | `float` | Pending task rate (0.0-1.0) |\n\n**Sources:** [src/memory/tree_todo/todo_track.py:173-200]()\n\n---\n\n## Configuration and Validation\n\n### Pydantic Configuration\n\nBoth models use the following configuration:\n\n```python\nclass Config:\n    arbitrary_types_allowed = True  # Allow arbitrary nested types\n```\n\nThe commented-out `use_enum_values = True` option would serialize enums as their string values rather than enum objects. The current implementation preserves enum objects for type safety.\n\n**Sources:** [src/memory/tree_todo/schemas.py:59-61](), [src/memory/tree_todo/schemas.py:77-79]()\n\n### Serialization Methods\n\nPydantic V2 model serialization:\n\n| Method | Purpose | Example |\n|--------|---------|---------|\n| `model_dump()` | Serialize to dictionary | `tree.model_dump()` |\n| `model_dump_json()` | Serialize to JSON string | `tree.model_dump_json()` |\n| `model_copy(deep=True)` | Deep copy the model | `tree.model_copy(deep=True)` |\n\n**Note:** Pydantic V1's `dict()` and `copy()` methods are replaced by `model_dump()` and `model_copy()` in V2.\n\n**Sources:** [src/memory/tree_todo/todo_track.py:32](), [src/agent/tool/todo_tool.py:70]()\n\n---\n\n## Complete Schema Diagram\n\n```mermaid\nclassDiagram\n    class TaskStatus {\n        <<enumeration>>\n        +PENDING: \"pending\"\n        +PROCESSING: \"processing\"\n        +COMPLETED: \"completed\"\n        +FAILED: \"failed\"\n        +RETRY: \"retry\"\n        +SKIPPED: \"skipped\"\n        +display_symbol() str\n        +display_desc() str\n    }\n    \n    class RecursivePlanTreeNode {\n        +task_id: str\n        +task_name: str\n        +description: str\n        +status: TaskStatus\n        +output: str\n        +dependencies: Optional~List~str~~\n        +research_directions: Optional~List~str~~\n        +children: Optional~List~RecursivePlanTreeNode~~\n        +empty_children_to_none() validator\n    }\n    \n    class RecursivePlanTree {\n        +core_goal: str\n        +tree_nodes: List~RecursivePlanTreeNode~\n        +next_action: Dict~str,Any~\n        +references: Optional~List~str~~\n    }\n    \n    class RecursivePlanTreeTodoTool {\n        +tool_call_purpose: str\n        +recursive_plan_tree: RecursivePlanTree\n        +run() Dict~str,str~\n    }\n    \n    class todo_track {\n        +arg_todo_list: List~RecursivePlanTree~\n        +run(current_plan_tree) Dict\n        +_analyze_changes(last, current) str\n        +_render_plan_tree_markdown(nodes) str\n        +_calculate_status_statistics(tree) Dict\n    }\n    \n    RecursivePlanTreeNode --> TaskStatus : uses\n    RecursivePlanTreeNode --> RecursivePlanTreeNode : recursive children\n    RecursivePlanTree --> RecursivePlanTreeNode : contains\n    RecursivePlanTreeTodoTool --> RecursivePlanTree : parameter\n    RecursivePlanTreeTodoTool --> todo_track : calls\n    todo_track --> RecursivePlanTree : stores history\n```\n\n**Sources:** [src/memory/tree_todo/schemas.py:1-80](), [src/agent/tool/todo_tool.py:10-37](), [src/memory/tree_todo/todo_track.py:1-201]()\n\n---\n\n## Usage Example Structure\n\nA minimal valid `RecursivePlanTree` requires only the `core_goal` and at least one root task:\n\n```json\n{\n  \"core_goal\": \"Complete Python project development\",\n  \"tree_nodes\": [\n    {\n      \"task_name\": \"Requirements analysis\",\n      \"status\": \"processing\",\n      \"children\": [\n        {\n          \"task_name\": \"Collect user requirements\",\n          \"status\": \"completed\",\n          \"output\": \"Collected 3 core requirements\"\n        },\n        {\n          \"task_name\": \"Write requirements document\",\n          \"status\": \"pending\"\n        }\n      ]\n    }\n  ]\n}\n```\n\nThe LLM generates this structure in the `tool_calls` parameters when invoking `RecursivePlanTreeTodoTool`.\n\n**Sources:** [src/agent/tool/todo_tool.py:42-69]()\n\n---\n\n# Page: Tool Schema Format\n\n# Tool Schema Format\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [src/agent/llm.py](src/agent/llm.py)\n- [src/agent/tool/base_tool.py](src/agent/tool/base_tool.py)\n- [src/memory/tree_todo/schemas.py](src/memory/tree_todo/schemas.py)\n- [src/memory/tree_todo/todo_track.py](src/memory/tree_todo/todo_track.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page documents the JSON schema format used to describe tools to the LLM, including the OpenAI function calling convention, parameter definitions, and how schemas are automatically generated from Pydantic models. This reference focuses on the **schema structure itself** rather than tool implementation.\n\nFor information about implementing new tools, see [Creating New Tools](#8.2). For the base tool interface that generates these schemas, see [BaseTool Interface](#4.1). For how the agent dispatches tool calls, see [Action Coordination](#3.4).\n\n---\n\n## Schema Generation Pipeline\n\nThe following diagram shows how tool schemas are generated from Python classes and consumed by the LLM:\n\n**Tool Schema Generation Flow**\n\n```mermaid\ngraph TB\n    subgraph \"Tool Definition\"\n        CLASS[\"Tool Class<br/>(extends BaseTool)\"]\n        DOCSTRING[\"Class Docstring\"]\n        PYDANTIC[\"Pydantic Field Definitions\"]\n    end\n    \n    subgraph \"Schema Generation Methods\"\n        TOOLNAME[\"tool_name()<br/>inflection.underscore()\"]\n        TOOLDESC[\"tool_description()<br/>inspect.getdoc()\"]\n        PARAMSCHEMA[\"get_parameter_schema()<br/>model_json_schema()\"]\n        TOOLSCHEMA[\"get_tool_schema()\"]\n    end\n    \n    subgraph \"Generated Schema\"\n        JSONSCHEMA[\"JSON Schema<br/>OpenAI Function Format\"]\n        NAME[\"function.name\"]\n        DESC[\"function.description\"]\n        PARAMS[\"function.parameters\"]\n        STRICT[\"function.strict: True\"]\n    end\n    \n    subgraph \"LLM Integration\"\n        TOOLSLIST[\"tools_schema_list\"]\n        LLMCALL[\"_generate_chat_completion()\"]\n        RESPONSE[\"assistant_output.tool_calls\"]\n    end\n    \n    CLASS --> TOOLNAME\n    DOCSTRING --> TOOLDESC\n    PYDANTIC --> PARAMSCHEMA\n    \n    TOOLNAME --> TOOLSCHEMA\n    TOOLDESC --> TOOLSCHEMA\n    PARAMSCHEMA --> TOOLSCHEMA\n    \n    TOOLSCHEMA --> JSONSCHEMA\n    JSONSCHEMA --> NAME\n    JSONSCHEMA --> DESC\n    JSONSCHEMA --> PARAMS\n    JSONSCHEMA --> STRICT\n    \n    JSONSCHEMA --> TOOLSLIST\n    TOOLSLIST --> LLMCALL\n    LLMCALL --> RESPONSE\n```\n\n**Sources:** [src/agent/tool/base_tool.py:13-71](), [src/agent/llm.py:14-24]()\n\n---\n\n## OpenAI Function Calling Format\n\nTool schemas follow the OpenAI function calling format, which structures tools as function objects with typed parameters. The schema is a nested dictionary with the following structure:\n\n### Schema Structure\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `type` | `\"function\"` | Top-level type identifier |\n| `function` | `object` | Function definition container |\n| `function.type` | `\"function\"` | Redundant type field |\n| `function.name` | `string` | Unique tool identifier |\n| `function.description` | `string` | Tool purpose and usage |\n| `function.parameters` | `object` | JSON Schema for parameters |\n| `function.strict` | `boolean` | Always `True` for strict validation |\n\n### Example Schema Structure\n\n```json\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"type\": \"function\",\n        \"name\": \"execute_python_code\",\n        \"description\": \"Executes Python code with state persistence\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {...},\n            \"required\": [...],\n            \"additionalProperties\": false\n        },\n        \"strict\": true\n    }\n}\n```\n\nThe `strict: true` flag enforces that the LLM must provide exactly the parameters defined in the schema with correct types.\n\n**Sources:** [src/agent/tool/base_tool.py:31-71]()\n\n---\n\n## Tool Naming Convention\n\nTool names are automatically generated from class names using a consistent convention that removes \"Tool\" suffixes and converts to snake_case.\n\n### Name Generation Process\n\n**Tool Name Generation Logic**\n\n```mermaid\ngraph LR\n    CLASSNAME[\"Class Name<br/>'ExecutePythonCodeTool'\"]\n    REMOVE[\"Remove 'Tool' / 'tool'<br/>'ExecutePythonCode'\"]\n    UNDERSCORE[\"inflection.underscore()<br/>'execute_python_code'\"]\n    \n    CLASSNAME --> REMOVE\n    REMOVE --> UNDERSCORE\n```\n\nThe `tool_name()` class method implements this at [src/agent/tool/base_tool.py:13-18]():\n\n```python\n@classmethod\ndef tool_name(cls) -> str:\n    \"\"\"å·¥å·å¯ä¸æ è¯åï¼ç¨äºè·¯ç±å¹éï¼å¦ \"weatherquery\"ï¼\"\"\"\n    return inflection.underscore(\n        cls.__name__.replace(\"Tool\", \"\").replace(\"tool\", \"\")\n    )\n```\n\n### Naming Examples\n\n| Class Name | Generated Tool Name |\n|------------|---------------------|\n| `ExecutePythonCodeTool` | `execute_python_code` |\n| `RecursivePlanTreeTodoTool` | `recursive_plan_tree_todo` |\n| `WeatherQueryTool` | `weather_query` |\n| `DataProcessingTool` | `data_processing` |\n\nTool names must be unique across all available tools since they are used for routing tool calls from the LLM.\n\n**Sources:** [src/agent/tool/base_tool.py:13-18]()\n\n---\n\n## Tool Description\n\nTool descriptions are extracted from the class docstring and provide natural language guidance to the LLM about when and how to use the tool.\n\n### Description Extraction\n\nThe `tool_description()` method uses Python's `inspect.getdoc()` to extract the docstring at [src/agent/tool/base_tool.py:20-23]():\n\n```python\n@classmethod\ndef tool_description(cls) -> str:\n    \"\"\"å·¥å·æè¿°ï¼ä¾ Agent çè§£ç¨éï¼\"\"\"\n    return inspect.getdoc(cls) or \"æ å·¥å·æè¿°\"\n```\n\n### Description Best Practices\n\n| Guideline | Rationale |\n|-----------|-----------|\n| Start with a verb phrase | Describes what the tool does |\n| Mention key capabilities | Helps LLM understand when to use it |\n| Include limitations | Prevents inappropriate usage |\n| Keep under 200 characters | Concise for LLM token efficiency |\n\n### Example Descriptions\n\n```python\nclass ExecutePythonCodeTool(BaseTool):\n    \"\"\"\n    Executes arbitrary Python code in an isolated environment with \n    state persistence across calls. Supports timeout control and \n    error handling. Use for data processing, calculations, and \n    algorithm implementation.\n    \"\"\"\n```\n\nThe description appears in the generated schema as `function.description` and influences the LLM's decision to invoke the tool.\n\n**Sources:** [src/agent/tool/base_tool.py:20-23]()\n\n---\n\n## Parameter Schema Generation\n\nParameter schemas are automatically generated from Pydantic model definitions using `model_json_schema()`, which produces JSON Schema compatible with OpenAI's function calling format.\n\n### Schema Generation Method\n\nThe `get_parameter_schema()` method at [src/agent/tool/base_tool.py:26-28]() delegates to Pydantic:\n\n```python\n@classmethod\ndef get_parameter_schema(cls) -> dict:\n    \"\"\"è·ååæ° JSON Schemaï¼ä¾ Agent æé åæ°ï¼\"\"\"\n    return cls.model_json_schema()\n```\n\n### Pydantic to JSON Schema Mapping\n\n**Field Type Conversion**\n\n```mermaid\ngraph TB\n    subgraph \"Pydantic Field Types\"\n        STR[\"str\"]\n        INT[\"int\"]\n        FLOAT[\"float\"]\n        BOOL[\"bool\"]\n        LIST[\"List[T]\"]\n        DICT[\"Dict[K,V]\"]\n        ENUM[\"Enum\"]\n        OPTIONAL[\"Optional[T]\"]\n        NESTED[\"BaseModel\"]\n    end\n    \n    subgraph \"JSON Schema Types\"\n        JSTR[\"type: string\"]\n        JINT[\"type: integer\"]\n        JFLOAT[\"type: number\"]\n        JBOOL[\"type: boolean\"]\n        JARRAY[\"type: array<br/>items: T schema\"]\n        JOBJECT[\"type: object<br/>properties: {...}\"]\n        JENUM[\"type: string<br/>enum: [values]\"]\n        JOPTIONAL[\"Not in required[]\"]\n        JNESTED[\"type: object<br/>$ref or inline\"]\n    end\n    \n    STR --> JSTR\n    INT --> JINT\n    FLOAT --> JFLOAT\n    BOOL --> JBOOL\n    LIST --> JARRAY\n    DICT --> JOBJECT\n    ENUM --> JENUM\n    OPTIONAL --> JOPTIONAL\n    NESTED --> JNESTED\n```\n\n### Field Definition Components\n\nPydantic `Field()` objects provide schema metadata:\n\n| Field Parameter | JSON Schema Output | Example |\n|----------------|-------------------|---------|\n| `description` | `description` | `\"The Python code to execute\"` |\n| `default` | `default` value | `\"pending\"` |\n| `default_factory` | `default` computed | `lambda: str(uuid.uuid4())` |\n| `...` (Ellipsis) | Added to `required[]` | Mandatory field |\n\n### Example Parameter Schema\n\nGiven a Pydantic model definition from [src/memory/tree_todo/schemas.py:44-53]():\n\n```python\nclass RecursivePlanTreeNode(BaseModel):\n    task_id: str = Field(\n        default_factory=lambda: f\"TASK-{str(uuid.uuid4())}\", \n        description=\"ä»»å¡å¯ä¸IDï¼æ¬æ¬¡å·¥å·è°ç¨å¿é¡»å¯ä¸ï¼åç»­æ¨çå¦ææ¯æä»£åä¸ä¸ªä»»å¡ç´æ¥å¼ç¨ï¼\"\n    )\n    task_name: str = Field(\n        ..., \n        description=\"ä»»å¡åç§°ï¼ç®æ´æè¿°æ ¸å¿å¨ä½ï¼ï¼å¤§è¯­è¨æ¨¡åçæï¼å¿é¡»å¨å±å¯ä¸ï¼ä¼è¢«dependenciesåè¡¨å¼ç¨\"\n    )\n    status: TaskStatus = Field(\n        default=TaskStatus.PENDING, \n        description=f\"ä»»å¡ç¶ææä¸¾ï¼{[status.value for status in TaskStatus]}\"\n    )\n```\n\nThe generated parameter schema includes:\n\n```json\n{\n    \"type\": \"object\",\n    \"properties\": {\n        \"task_id\": {\n            \"type\": \"string\",\n            \"description\": \"ä»»å¡å¯ä¸IDï¼æ¬æ¬¡å·¥å·è°ç¨å¿é¡»å¯ä¸ï¼åç»­æ¨çå¦ææ¯æä»£åä¸ä¸ªä»»å¡ç´æ¥å¼ç¨ï¼\",\n            \"default\": \"TASK-<uuid>\"\n        },\n        \"task_name\": {\n            \"type\": \"string\",\n            \"description\": \"ä»»å¡åç§°ï¼ç®æ´æè¿°æ ¸å¿å¨ä½ï¼ï¼å¤§è¯­è¨æ¨¡åçæï¼å¿é¡»å¨å±å¯ä¸ï¼ä¼è¢«dependenciesåè¡¨å¼ç¨\"\n        },\n        \"status\": {\n            \"type\": \"string\",\n            \"enum\": [\"pending\", \"processing\", \"completed\", \"failed\", \"retry\", \"skipped\"],\n            \"default\": \"pending\",\n            \"description\": \"ä»»å¡ç¶ææä¸¾ï¼['pending', 'processing', 'completed', 'failed', 'retry', 'skipped']\"\n        }\n    },\n    \"required\": [\"task_name\"],\n    \"additionalProperties\": false\n}\n```\n\n**Sources:** [src/agent/tool/base_tool.py:26-28](), [src/memory/tree_todo/schemas.py:44-53]()\n\n---\n\n## Nested and Recursive Schemas\n\nPydantic models with nested or self-referential structures generate complex JSON schemas with `$ref` references or inline definitions.\n\n### Recursive Schema Example\n\nThe `RecursivePlanTreeNode` model demonstrates self-reference at [src/memory/tree_todo/schemas.py:53]():\n\n```python\nchildren: Optional[List[\"RecursivePlanTreeNode\"]] = Field(\n    default=None, \n    description=\"å­ä»»å¡åè¡¨ï¼å±çº§åµå¥ï¼\"\n)\n```\n\nAfter calling `RecursivePlanTreeNode.model_rebuild()` at [src/memory/tree_todo/schemas.py:64](), the generated schema includes:\n\n```json\n{\n    \"properties\": {\n        \"children\": {\n            \"anyOf\": [\n                {\n                    \"type\": \"array\",\n                    \"items\": {\"$ref\": \"#/$defs/RecursivePlanTreeNode\"}\n                },\n                {\"type\": \"null\"}\n            ],\n            \"default\": null,\n            \"description\": \"å­ä»»å¡åè¡¨ï¼å±çº§åµå¥ï¼\"\n        }\n    },\n    \"$defs\": {\n        \"RecursivePlanTreeNode\": {\n            \"type\": \"object\",\n            \"properties\": {...}\n        }\n    }\n}\n```\n\n### Handling Optional Types\n\nThe `Optional[T]` type generates an `anyOf` schema with null support:\n\n```json\n{\n    \"anyOf\": [\n        {\"type\": \"array\", \"items\": {...}},\n        {\"type\": \"null\"}\n    ]\n}\n```\n\nThis allows the LLM to either provide the value or omit it entirely.\n\n**Sources:** [src/memory/tree_todo/schemas.py:44-64]()\n\n---\n\n## Complete Schema Examples\n\n### Example 1: BaseTool with tool_call_purpose\n\nAll tools inherit a `tool_call_purpose` field from `BaseTool` at [src/agent/tool/base_tool.py:9-12]():\n\n```python\ntool_call_purpose: str = Field(\n    ..., \n    description=\"å·¥å·è°ç¨çç®çï¼å³è°ç¨è¯¥å·¥å·çå·ä½åºæ¯æé®é¢ã\"\n)\n```\n\nThis field appears in every tool's parameter schema:\n\n```json\n{\n    \"type\": \"object\",\n    \"properties\": {\n        \"tool_call_purpose\": {\n            \"type\": \"string\",\n            \"description\": \"å·¥å·è°ç¨çç®çï¼å³è°ç¨è¯¥å·¥å·çå·ä½åºæ¯æé®é¢ã\"\n        },\n        ...\n    },\n    \"required\": [\"tool_call_purpose\", ...]\n}\n```\n\n### Example 2: RecursivePlanTree Schema\n\nThe `RecursivePlanTree` model at [src/memory/tree_todo/schemas.py:68-79]() generates:\n\n```json\n{\n    \"type\": \"object\",\n    \"properties\": {\n        \"core_goal\": {\n            \"type\": \"string\",\n            \"description\": \"æ ¸å¿ç®æ ï¼è®¡åæ è¦è¾¾æçæç»ç®çï¼\"\n        },\n        \"tree_nodes\": {\n            \"type\": \"array\",\n            \"items\": {\"$ref\": \"#/$defs/RecursivePlanTreeNode\"},\n            \"default\": [],\n            \"description\": \"è®¡åæ æ ¹ä»»å¡åè¡¨\"\n        },\n        \"next_action\": {\n            \"type\": \"object\",\n            \"default\": {},\n            \"description\": \"ä¸ä¸æ­¥å»ºè®®å¨ä½ï¼å¯éï¼\"\n        },\n        \"references\": {\n            \"anyOf\": [\n                {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                {\"type\": \"null\"}\n            ],\n            \"default\": null,\n            \"description\": \"åèèµæºåè¡¨ï¼å¯éï¼å¦ææ¡£é¾æ¥ãæ°æ®æ¥æºï¼\"\n        }\n    },\n    \"required\": [\"core_goal\"]\n}\n```\n\n**Sources:** [src/agent/tool/base_tool.py:9-12](), [src/memory/tree_todo/schemas.py:68-79]()\n\n---\n\n## Integration with LLM\n\nThe generated schemas are passed to the LLM as a list through the `tools` parameter in the chat completion API.\n\n### Schema List Construction\n\n**Schema Flow to LLM**\n\n```mermaid\ngraph LR\n    subgraph \"Tool Classes\"\n        TOOL1[\"ExecutePythonCodeTool\"]\n        TOOL2[\"RecursivePlanTreeTodoTool\"]\n        TOOLN[\"...\"]\n    end\n    \n    subgraph \"Schema Generation\"\n        SCHEMA1[\"tool1.get_tool_schema()\"]\n        SCHEMA2[\"tool2.get_tool_schema()\"]\n        SCHEMAN[\"...\"]\n    end\n    \n    subgraph \"API Call\"\n        LIST[\"tools_schema_list:<br/>[schema1, schema2, ...]\"]\n        API[\"client.chat.completions.create()\"]\n        PARAM[\"tools=tools_schema_list\"]\n    end\n    \n    TOOL1 --> SCHEMA1\n    TOOL2 --> SCHEMA2\n    TOOLN --> SCHEMAN\n    \n    SCHEMA1 --> LIST\n    SCHEMA2 --> LIST\n    SCHEMAN --> LIST\n    \n    LIST --> PARAM\n    PARAM --> API\n```\n\n### LLM API Integration\n\nThe `_generate_chat_completion()` function at [src/agent/llm.py:14-24]() passes the schema list:\n\n```python\n@traceable\ndef _generate_chat_completion(messages: list[dict], tools_schema_list=None) -> ChatCompletion:\n    completion: ChatCompletion = client.chat.completions.create(\n        model=\"qwen-plus\",\n        messages=messages,\n        tools=tools_schema_list,\n        function_call=None,\n        parallel_tool_calls=True,\n    )\n    return completion\n```\n\n### API Parameters\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `tools` | `tools_schema_list` | List of available tool schemas |\n| `function_call` | `None` | Let LLM decide when to call tools |\n| `parallel_tool_calls` | `True` | Allow multiple tool calls in one response |\n\n### Tool Call Response Format\n\nWhen the LLM decides to invoke a tool, it returns `tool_calls` in the response at [src/agent/llm.py:29]():\n\n```python\nassistant_output: ChatCompletionMessage = completion.choices[0].message\n# assistant_output.tool_calls is a list of tool call objects\n```\n\nEach tool call object contains:\n- `id`: Unique identifier for tracking\n- `type`: Always `\"function\"`\n- `function.name`: Matches a tool's `tool_name()`\n- `function.arguments`: JSON string of parameters matching the schema\n\n**Sources:** [src/agent/llm.py:14-31](), [src/agent/llm.py:44-49]()\n\n---\n\n## Schema Validation and Strict Mode\n\nThe `strict: true` flag in tool schemas enforces strict validation of LLM-generated parameters.\n\n### Strict Mode Behavior\n\n| Validation | Without Strict | With Strict |\n|------------|---------------|-------------|\n| Missing required field | May use default | Error returned to LLM |\n| Extra field | Ignored | Error returned to LLM |\n| Wrong type | Coerced if possible | Error returned to LLM |\n| Invalid enum value | May accept | Error returned to LLM |\n\n### additionalProperties: false\n\nPydantic's `model_json_schema()` automatically sets `additionalProperties: false` in the generated schema, ensuring the LLM cannot add unexpected fields. This works in conjunction with `strict: true` to guarantee parameter conformance.\n\n### Validation Errors\n\nWhen the LLM provides parameters that violate the schema, the API returns an error that includes:\n- The specific validation failure\n- The field path that failed\n- The expected vs. actual value\n\nThe agent can then retry with corrected parameters or provide feedback to the user.\n\n**Sources:** [src/agent/tool/base_tool.py:68]()\n\n---\n\n## Summary\n\nThe tool schema format provides a bridge between Pydantic-based Python tool definitions and the OpenAI function calling format consumed by the LLM. Key aspects:\n\n1. **Automatic Generation**: Schemas are generated from class names, docstrings, and Pydantic field definitions\n2. **Strict Validation**: `strict: true` and `additionalProperties: false` ensure parameter conformance\n3. **Recursive Support**: Nested and self-referential models are supported via `$ref` definitions\n4. **Standard Format**: Follows OpenAI function calling conventions for compatibility\n5. **Type Safety**: Pydantic types map directly to JSON Schema types with full validation\n\nThis architecture allows tool developers to focus on Pydantic model definitions while the schema generation system handles the conversion to LLM-compatible formats automatically.\n\n**Sources:** [src/agent/tool/base_tool.py:1-76](), [src/agent/llm.py:14-24](), [src/memory/tree_todo/schemas.py:1-81]()\n\n---\n\n# Page: Troubleshooting\n\n# Troubleshooting\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [docs/crashed.design.md](docs/crashed.design.md)\n- [docs/error correction.design.md](docs/error correction.design.md)\n- [docs/log.md](docs/log.md)\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/runtime/schemas.py](src/runtime/schemas.py)\n- [src/runtime/workspace.py](src/runtime/workspace.py)\n- [src/utils/__pycache__/__init__.cpython-312.pyc](src/utils/__pycache__/__init__.cpython-312.pyc)\n- [tests/playground/crashed.py](tests/playground/crashed.py)\n- [tests/playground/subprocess_output.py](tests/playground/subprocess_output.py)\n\n</details>\n\n\n\nThis page provides a comprehensive guide to identifying, diagnosing, and resolving issues that occur when running the algo_agent system. It covers the error handling architecture, common failure patterns, and debugging strategies using the system's built-in observability features.\n\nFor specific error types and detailed solutions, see:\n- **Execution-related errors** (UnicodeDecodeError, PickleError, timeouts): [Common Execution Errors](#10.1)\n- **LLM and tool invocation issues**: [Tool Call Errors](#10.2)\n\nThis page focuses on understanding the overall error handling system and general debugging approaches that apply across different error categories.\n\n---\n\n## Error Handling Architecture\n\nThe algo_agent system implements a structured error handling mechanism centered around the `ExecutionResult` schema and `ExecutionStatus` enum. All code executions return standardized result objects that capture success states, failures, timeouts, and crashes.\n\n### Execution Status Flow\n\n```mermaid\nflowchart TD\n    Start[\"Code Execution Request\"] --> Exec[\"Execute in Subprocess/Thread\"]\n    \n    Exec --> Check{Execution Outcome}\n    \n    Check -->|\"Normal Completion\"| Success[\"ExecutionStatus.SUCCESS\"]\n    Check -->|\"Exception Caught\"| Failure[\"ExecutionStatus.FAILURE\"]\n    Check -->|\"Timeout Exceeded\"| Timeout[\"ExecutionStatus.TIMEOUT\"]\n    Check -->|\"Process Crashed\"| Crashed[\"ExecutionStatus.CRASHED\"]\n    \n    Success --> BuildResult[\"Build ExecutionResult\"]\n    Failure --> BuildResult\n    Timeout --> BuildResult\n    Crashed --> BuildResult\n    \n    BuildResult --> Validate[\"field_validate_globals\"]\n    Validate --> Filter[\"filter_and_deepcopy_globals\"]\n    Filter --> Return[\"Return to Agent\"]\n    \n    Return --> Format[\"get_return_llm\"]\n    Format --> LLM[\"Send to LLM with Context\"]\n```\n\n**Sources:** [src/runtime/schemas.py:10-47](), [src/runtime/workspace.py:38-78]()\n\nThe `ExecutionStatus` enum defines four possible outcomes, each with specific handling:\n\n| Status | Exit Code | Description | Captured Information |\n|--------|-----------|-------------|---------------------|\n| `SUCCESS` | 0 | Code executed without errors | stdout, modified globals |\n| `FAILURE` | 0 or 1 | Python exception raised | exception type, traceback, stdout |\n| `TIMEOUT` | -15 (SIGTERM) | Execution exceeded timeout limit | partial stdout, timeout duration |\n| `CRASHED` | 1, 139, or negative | Process terminated abnormally | stdout, exit code, crash signal |\n\n**Sources:** [src/runtime/schemas.py:10-47](), [tests/playground/subprocess_output.py:296-705]()\n\n### ExecutionResult Schema\n\nThe `ExecutionResult` data structure captures all execution metadata:\n\n```mermaid\nclassDiagram\n    class ExecutionResult {\n        +str arg_command\n        +int arg_timeout\n        +Dict arg_globals\n        +ExecutionStatus exit_status\n        +Optional[int] exit_code\n        +Optional[str] exception_repr\n        +Optional[str] exception_type\n        +Optional[str] exception_value\n        +Optional[str] exception_traceback\n        +str ret_stdout\n        +Optional[str] ret_tool2llm\n        +field_validate_globals()\n    }\n    \n    class ExecutionStatus {\n        <<enumeration>>\n        SUCCESS\n        FAILURE\n        TIMEOUT\n        CRASHED\n        +get_return_llm()\n    }\n    \n    class workspace {\n        +filter_and_deepcopy_globals()\n        +arg_globals_list\n        +out_globals_list\n    }\n    \n    ExecutionResult --> ExecutionStatus\n    ExecutionResult --> workspace : validates globals\n```\n\n**Sources:** [src/runtime/schemas.py:56-83](), [src/runtime/workspace.py:38-98]()\n\n---\n\n## Common Error Categories\n\nThe system encounters errors in three primary layers:\n\n### 1. Subprocess Execution Layer\n\nErrors occurring during isolated code execution in subprocess:\n\n```mermaid\ngraph TB\n    subgraph \"Subprocess Execution Flow\"\n        Worker[\"_worker_with_pipe\"] --> ExecCode[\"exec(command, _globals)\"]\n        \n        ExecCode -->|Success| BuildSuccess[\"ExecutionResult(SUCCESS)\"]\n        ExecCode -->|Exception| CatchExc[\"except Exception\"]\n        CatchExc --> BuildFailure[\"ExecutionResult(FAILURE)\"]\n        \n        BuildSuccess --> SendPipe[\"child_conn.send(RESULT)\"]\n        BuildFailure --> SendPipe\n        \n        SendPipe --> PickleAttempt{Picklable?}\n        PickleAttempt -->|Yes| ParentRecv[\"Parent Receives\"]\n        PickleAttempt -->|No| PickleError[\"PicklingError\"]\n        PickleError --> ProcessCrash[\"Process Exits with Error\"]\n        \n        ParentRecv --> ParentJoin[\"p.join(timeout)\"]\n        ProcessCrash --> ParentJoin\n        \n        ParentJoin -->|\"p.is_alive() == False\"| CheckContainer{Result Container Empty?}\n        ParentJoin -->|\"p.is_alive() == True\"| TimeoutHandle[\"Timeout Handler\"]\n        \n        CheckContainer -->|Has Result| FinalSuccess[\"Return SUCCESS/FAILURE\"]\n        CheckContainer -->|Empty| FinalCrash[\"Return CRASHED\"]\n        TimeoutHandle --> FinalTimeout[\"Return TIMEOUT\"]\n    end\n```\n\n**Sources:** [tests/playground/subprocess_output.py:18-155](), [src/runtime/subprocess_python_executor.py:18-155]()\n\n### 2. Serialization Layer\n\nThe workspace management system filters and deep-copies globals to ensure only serializable objects persist:\n\n**Common serialization failures:**\n- File handles (`TextIOWrapper`) - lines 166, 219, 272 in logs/utils.log\n- Module objects - filtered by [src/runtime/workspace.py:71-73]()\n- Lambda functions - cannot be pickled across processes\n- Thread locks - not serializable\n\n**Sources:** [src/runtime/workspace.py:38-78](), [logs/utils.log:139-287]()\n\n### 3. Encoding Layer\n\nFile I/O operations without explicit encoding specification:\n\n**Pattern:**\n```python\n# Problematic code (defaults to 'gbk' on Windows)\nwith open('data.json', 'r') as f:  \n    data = json.load(f)  # UnicodeDecodeError\n\n# Correct code\nwith open('data.json', 'r', encoding='utf-8') as f:\n    data = json.load(f)\n```\n\n**Sources:** [logs/utils.log:146-252](), [logs/utils.log:289-290]()\n\n---\n\n## Debugging Strategy\n\n### Using Log Files\n\nThe system provides four specialized log files for different debugging needs:\n\n| Log File | Purpose | Key Information |\n|----------|---------|-----------------|\n| `logs/global.log` | Function call traces | Stack paths, timing, arguments, return values |\n| `logs/utils.log` | User operations | Query inputs, tool outputs, LLM responses |\n| `logs/trace.log` | Detailed execution | Stack traces, performance metrics |\n| `logs/all.log` | Comprehensive | All system activity |\n\n**Example log entry pattern:**\n```\n[timestamp] ãè°ç¨å¼å§ã æ è·¯å¾ï¼ module.class.function | å¼å§æ¶é´ï¼ ... | ä½ç½®åæ°ï¼ (...) | å³é®å­åæ°ï¼ {}\n[timestamp] ãè°ç¨æåã æ è·¯å¾ï¼ module.class.function | èæ¶ï¼ Xms | è¿åå¼ï¼ ...\n```\n\n**Sources:** [src/utils/log_decorator.py](), [logs/global.log:1-700]()\n\n### Analyzing ExecutionResult\n\nWhen code execution fails, the `ExecutionResult.ret_tool2llm` field contains formatted error information:\n\n```mermaid\nflowchart LR\n    subgraph \"Error Information Flow\"\n        ExecResult[\"ExecutionResult\"] --> GetReturnLLM[\"ExecutionStatus.get_return_llm()\"]\n        \n        GetReturnLLM --> FormatSuccess[\"## ä»£ç æ§è¡æå\\n### ç»ç«¯è¾åº:\\nstdout\"]\n        GetReturnLLM --> FormatFailure[\"## ä»£ç æ§è¡å¤±è´¥\\n### ç»ç«¯è¾åº\\n### åå§ä»£ç \\n### æ¥éä¿¡æ¯:\\ntraceback\"]\n        GetReturnLLM --> FormatTimeout[\"## ä»£ç æ§è¡è¶æ¶\\n### ç»ç«¯è¾åº\\n### è¶åºéå¶çæ¶é´\"]\n        GetReturnLLM --> FormatCrashed[\"## ä»£ç æ§è¡å´©æº\\n### ç»ç«¯è¾åº\\n### éåºç¶æç \"]\n        \n        FormatSuccess --> AgentMemory[\"Appended to messages\"]\n        FormatFailure --> AgentMemory\n        FormatTimeout --> AgentMemory\n        FormatCrashed --> AgentMemory\n        \n        AgentMemory --> LLMContext[\"LLM sees full context\"]\n    end\n```\n\n**Sources:** [src/runtime/schemas.py:19-47](), [logs/utils.log:136-287]()\n\n### Process Exit Code Interpretation\n\nExit codes provide clues about crash causes:\n\n| Exit Code | Signal | Meaning |\n|-----------|--------|---------|\n| 0 | - | Normal termination (may still have FAILURE status if exception was caught) |\n| 1 | - | General error (uncaught exception, PicklingError) |\n| -15 | SIGTERM | Timeout termination by parent process |\n| 139 | SIGSEGV (11) | Segmentation fault (memory access violation) |\n| -11 | SIGSEGV | Segmentation fault (alternative representation) |\n\n**Exit code formula:** Negative exit code = -(signal number), e.g., -15 = SIGTERM (15)\n\n**Sources:** [src/runtime/schemas.py:50-54](), [tests/playground/subprocess_output.py:428-702]()\n\n---\n\n## Diagnostic Checklist\n\nWhen encountering an error, follow this systematic approach:\n\n### Step 1: Identify Error Category\n\n```mermaid\nflowchart TD\n    Start{Error Occurred} --> CheckLogs[\"Check logs/utils.log\"]\n    \n    CheckLogs --> Category{Error Type?}\n    \n    Category -->|\"UnicodeDecodeError\"| Encoding[\"Encoding Issue\"]\n    Category -->|\"PicklingError\\nTypeError: cannot pickle\"| Pickle[\"Serialization Issue\"]\n    Category -->|\"TimeoutError\\nexit_code=-15\"| Timeout[\"Timeout Issue\"]\n    Category -->|\"exit_code=139\\nexit_code=1\"| Crash[\"Process Crash\"]\n    Category -->|\"Tool call format\"| ToolError[\"Tool Call Issue\"]\n    \n    Encoding --> Page101[\"See section 10.1\"]\n    Pickle --> Page101\n    Timeout --> Page101\n    Crash --> Page101\n    ToolError --> Page102[\"See section 10.2\"]\n```\n\n**Sources:** [logs/utils.log:136-287](), [src/runtime/schemas.py:10-47]()\n\n### Step 2: Examine ExecutionResult\n\nFor execution errors, the `ExecutionResult` object contains:\n- `exit_status`: Which of the four states occurred\n- `exception_traceback`: Full Python traceback (FAILURE only)\n- `ret_stdout`: Captured output before error\n- `exit_code`: Process exit code\n- `arg_command`: The code that was executed\n\n**Sources:** [src/runtime/schemas.py:56-70]()\n\n### Step 3: Check Workspace State\n\nIf serialization errors occur, inspect what globals are being persisted:\n\n**Filtering logic:**\n1. Excludes `__builtins__` key\n2. Excludes module type objects\n3. Tests picklability with `pickle.dumps()`\n4. Deep copies remaining objects\n\n**Sources:** [src/runtime/workspace.py:38-78]()\n\n---\n\n## Quick Reference: Error to Solution Map\n\n| Error Pattern | Root Cause | Solution | Page Reference |\n|---------------|------------|----------|----------------|\n| `'gbk' codec can't decode` | File opened without UTF-8 encoding | Add `encoding='utf-8'` to `open()` | [#10.1](#10.1) |\n| `cannot pickle 'TextIOWrapper'` | File handle in globals | Close file in `with` block, don't expose | [#10.1](#10.1) |\n| `UnboundLocalError: cannot access local variable 'res'` | Nested exception during error handling | Subprocess crash during pickle attempt | [#10.1](#10.1) |\n| `exit_status=TIMEOUT, exit_code=-15` | Execution exceeded timeout | Increase timeout or optimize code | [#10.1](#10.1) |\n| `exit_status=CRASHED, exit_code=139` | Segmentation fault | Memory access violation, check C extensions | [#10.1](#10.1) |\n| `exit_status=CRASHED, exit_code=1` | Process terminated during error handling | Often PicklingError in result transmission | [#10.1](#10.1) |\n| `function_call is not None but tool_calls is None` | LLM response format mismatch | Model returned old function_call format | [#10.2](#10.2) |\n| Missing tool schema | Tool not registered | Check `get_tools_schema()` includes tool | [#10.2](#10.2) |\n\n**Sources:** [logs/utils.log:136-287](), [src/runtime/schemas.py:10-54]()\n\n---\n\n## Recovery Patterns\n\nThe agent implements automatic recovery for certain error types:\n\n### LLM Self-Correction Loop\n\n```mermaid\nsequenceDiagram\n    participant Agent\n    participant LLM\n    participant Executor\n    participant Memory\n    \n    Agent->>LLM: Generate code\n    LLM->>Executor: execute_python_code\n    Executor->>Executor: Run in subprocess\n    \n    alt Execution Failed\n        Executor->>Memory: Store ExecutionResult(FAILURE)\n        Memory->>LLM: Send error context with traceback\n        LLM->>LLM: Analyze error, adjust code\n        LLM->>Executor: execute_python_code (retry)\n    else Execution Timeout\n        Executor->>Memory: Store ExecutionResult(TIMEOUT)\n        Memory->>LLM: Send timeout info\n        LLM->>LLM: Optimize or increase timeout\n        LLM->>Executor: execute_python_code (retry)\n    else Execution Crashed\n        Executor->>Memory: Store ExecutionResult(CRASHED)\n        Memory->>LLM: Send crash info with stdout\n        LLM->>LLM: Diagnose crash cause\n        Note over LLM: May need human intervention\n    end\n```\n\n**Sources:** [src/agent/deep_research.py](), [src/runtime/schemas.py:19-47]()\n\n---\n\n## System Error Message Format\n\nThe system generates structured error messages that the LLM can interpret:\n\n**SUCCESS format:**\n```\n## ä»£ç æ§è¡æåï¼è¾åºç»æå®æ´ï¼ä»»å¡å®æ\n### ç»ç«¯è¾åºï¼\n{stdout}\n```\n\n**FAILURE format:**\n```\n## ä»£ç æ§è¡å¤±è´¥ï¼ä»£ç æåºå¼å¸¸ï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\n{stdout}\n### åå§ä»£ç ï¼\n{numbered_code}\n### æ¥éä¿¡æ¯ï¼\n{exception_traceback}\n```\n\n**TIMEOUT format:**\n```\n## ä»£ç æ§è¡è¶æ¶ï¼å¼ºå¶éåºæ§è¡ï¼è°æ´è¶æ¶æ¶é´åéè¯\n### ç»ç«¯è¾åºï¼\n{stdout}\n### è¶åºéå¶çæ¶é´ï¼{timeout} ç§\n```\n\n**CRASHED format:**\n```\n## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\n{stdout}\n### éåºç¶æç ï¼{exit_code}\n```\n\n**Sources:** [src/runtime/schemas.py:23-47]()\n\n---\n\n## Next Steps\n\nFor detailed solutions to specific error types:\n- **UnicodeDecodeError, PickleError, timeout issues**: See [Common Execution Errors](#10.1)\n- **Tool call format errors, LLM integration problems**: See [Tool Call Errors](#10.2)\n\nFor understanding the logging system architecture: See [Observability and Logging](#6)\n\nFor details on execution strategies and isolation: See [Execution Runtime](#5)\n\n---\n\n# Page: Common Execution Errors\n\n# Common Execution Errors\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/runtime/cwd.py](src/runtime/cwd.py)\n- [src/runtime/subprocess_python_executor.py](src/runtime/subprocess_python_executor.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document catalogs the most frequently encountered errors during Python code execution in the algo_agent system, their root causes, and detailed resolution strategies. It focuses on errors that occur within the execution runtime layer when the system attempts to run user-provided or LLM-generated Python code snippets.\n\nFor errors related to tool calling and LLM integration (such as malformed tool schemas or function call format issues), see [Tool Call Errors](#10.2). For general system architecture and execution flow, see [Execution Runtime](#5).\n\n---\n\n## Overview of Execution Error Types\n\nThe execution system can produce several distinct error categories, each represented by a specific `ExecutionStatus`:\n\n| Status | Description | Recovery Difficulty |\n|--------|-------------|---------------------|\n| `SUCCESS` | Code executed without exceptions | N/A |\n| `FAILURE` | Python exception caught during execution | Easy - retry with fixes |\n| `TIMEOUT` | Execution exceeded time limit | Medium - optimize code |\n| `CRASHED` | Process terminated abnormally (SegFault, unhandled exceptions in result construction) | Hard - system-level issue |\n\nSources: [src/runtime/schemas.py:20-30](), [logs/utils.log:136-180]()\n\n---\n\n## Error Flow Architecture\n\n```mermaid\ngraph TB\n    Code[\"User/LLM Code Snippet\"]\n    Subprocess[\"Subprocess Worker<br/>_worker_with_pipe()\"]\n    Exec[\"exec(command, globals, locals)\"]\n    Result[\"ExecutionResult Construction\"]\n    Pipe[\"Pipe Communication<br/>child_conn.send()\"]\n    Parent[\"Parent Process<br/>run_structured_in_subprocess()\"]\n    \n    Code --> Subprocess\n    Subprocess --> Exec\n    \n    Exec -->|Success| Result\n    Exec -->|Python Exception| ResultFail[\"ExecutionResult<br/>status=FAILURE\"]\n    \n    Result -->|deepcopy globals| Serialize[\"filter_and_deepcopy_globals()\"]\n    ResultFail --> Serialize\n    \n    Serialize -->|Success| Pipe\n    Serialize -->|PickleError| Crash[\"Process Crash<br/>UnboundLocalError\"]\n    \n    Pipe -->|Success| Parent\n    Crash -->|EOFError| ParentCrashed[\"Parent detects CRASHED\"]\n    \n    Parent -->|p.join(timeout)| TimeoutCheck{Timeout?}\n    TimeoutCheck -->|Yes| Terminate[\"p.terminate()<br/>status=TIMEOUT\"]\n    TimeoutCheck -->|No| CheckResult{Result received?}\n    CheckResult -->|Yes| ReturnResult[\"Return ExecutionResult\"]\n    CheckResult -->|No| ReturnCrashed[\"Return CRASHED result\"]\n    \n    style Crash fill:#fee\n    style ParentCrashed fill:#fee\n    style Terminate fill:#ffe\n    style Serialize fill:#eff\n```\n\n**Diagram: Execution Error Flow and Status Determination**\n\nThis diagram shows how different error conditions lead to different ExecutionStatus values. Critical failure points include serialization (PickleError) and timeout detection.\n\nSources: [src/runtime/subprocess_python_executor.py:19-163](), [src/runtime/schemas.py:15-40]()\n\n---\n\n## 1. UnicodeDecodeError: Encoding Mismatch\n\n### Error Signature\n\n```\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265: illegal multibyte sequence\n```\n\n### Root Cause\n\n**Platform-specific default encoding mismatch**: \n- JSON files are typically saved with UTF-8 encoding\n- Windows systems default to locale-specific encodings (e.g., `gbk` in Chinese locales, `cp1252` in Western locales)\n- Python's `open()` without explicit encoding uses the system default\n\n**Code pattern causing the error**:\n```python\n# â WRONG: Uses system default encoding (gbk on Windows)\nwith open('schema.json', 'r') as file:\n    schema = json.load(file)\n```\n\nSources: [logs/utils.log:143-146](), [logs/utils.log:196-199](), [logs/utils.log:249-252]()\n\n### Resolution Strategy\n\n**Always specify UTF-8 encoding explicitly**:\n\n```python\n# â CORRECT: Explicit UTF-8 encoding\nwith open('schema.json', 'r', encoding='utf-8') as file:\n    schema = json.load(file)\n```\n\n### Detection in Logs\n\nLook for these patterns in [logs/utils.log]():\n- `UnicodeDecodeError: 'gbk' codec can't decode byte`\n- `illegal multibyte sequence`\n- Accompanied by file I/O operations (lines 143-146, 196-199, 249-252)\n\n### System Response\n\nWhen UnicodeDecodeError occurs:\n1. Python raises the exception during `file.read()` or `json.load()`\n2. Subprocess catches it in the `try-except` block at [src/runtime/subprocess_python_executor.py:58-69]()\n3. System attempts to construct `ExecutionResult` with `status=FAILURE`\n4. **Secondary failure**: If the exception leaves unpicklable objects in `_globals`, see next section\n\nSources: [logs/utils.log:139-180](), [src/runtime/subprocess_python_executor.py:58-69]()\n\n---\n\n## 2. PickleError: Unpicklable Objects in Globals\n\n### Error Signature\n\n```\nTypeError: cannot pickle 'TextIOWrapper' instances\n```\n\n### Root Cause Cascade\n\n```mermaid\ngraph LR\n    FileOpen[\"open('file.json', 'r')\"]\n    GlobalVar[\"file handle stored<br/>in _globals namespace\"]\n    Exception[\"UnicodeDecodeError<br/>during json.load()\"]\n    ResultConstruct[\"ExecutionResult(...,<br/>arg_globals=_globals)\"]\n    Validate[\"Pydantic validator<br/>field_validate_globals()\"]\n    Filter[\"filter_and_deepcopy_globals()\"]\n    Deepcopy[\"copy.deepcopy(value)\"]\n    PickleErr[\"TypeError: cannot pickle<br/>'TextIOWrapper'\"]\n    Crash[\"UnboundLocalError:<br/>res not defined\"]\n    \n    FileOpen --> GlobalVar\n    GlobalVar --> Exception\n    Exception --> ResultConstruct\n    ResultConstruct --> Validate\n    Validate --> Filter\n    Filter --> Deepcopy\n    Deepcopy --> PickleErr\n    PickleErr --> Crash\n    \n    style PickleErr fill:#fee\n    style Crash fill:#fcc\n```\n\n**Diagram: PickleError Cascade Leading to Process Crash**\n\nSources: [logs/utils.log:148-178](), [src/runtime/workspace.py:55](), [src/runtime/schemas.py:83]()\n\n### Detailed Explanation\n\n**The chain of events**:\n\n1. **File handle creation**: `open('file.json', 'r')` creates a `TextIOWrapper` object\n2. **Implicit global storage**: The `file` variable enters the `_globals` namespace due to `exec()` semantics\n3. **Primary exception**: `json.load(file)` raises `UnicodeDecodeError`\n4. **Secondary failure attempt**: Code at [src/runtime/subprocess_python_executor.py:58-69]() tries to construct `ExecutionResult` with `arg_globals=_globals`\n5. **Pydantic validation**: [src/runtime/schemas.py:83]() invokes `filter_and_deepcopy_globals()`\n6. **Serialization failure**: [src/runtime/workspace.py:55]() attempts `copy.deepcopy(value)` on the file handle\n7. **Pickle error**: `TextIOWrapper` is not serializable\n8. **Unhandled exception**: The except block at [src/runtime/subprocess_python_executor.py:66]() tries to send undefined `res`\n9. **Process crash**: `UnboundLocalError: cannot access local variable 'res'`\n\n### Objects That Cannot Be Pickled\n\n| Object Type | Example | Common Source |\n|-------------|---------|---------------|\n| File handles | `open('file')` | File I/O without context manager |\n| Thread locks | `threading.Lock()` | Concurrent programming |\n| Database connections | `sqlite3.connect()` | Database operations |\n| Socket objects | `socket.socket()` | Network programming |\n| Lambda functions with closures | `lambda x: nonlocal_var + x` | Functional programming |\n| Module-level imports | `import numpy as np` | Import statements (these are filtered) |\n\nSources: [src/runtime/workspace.py:40-68]()\n\n### Resolution Strategies\n\n#### Strategy 1: Use Context Managers (Recommended)\n\n```python\n# â CORRECT: File handle automatically closed, not stored in globals\nwith open('schema.json', 'r', encoding='utf-8') as f:\n    schema = json.load(f)\n# f is out of scope here, not in _globals\n```\n\n#### Strategy 2: Explicit Cleanup\n\n```python\n# â CORRECT: Explicit cleanup before execution ends\nfile = open('schema.json', 'r', encoding='utf-8')\nschema = json.load(file)\nfile.close()\ndel file  # Remove from namespace\n```\n\n#### Strategy 3: Local Scope Isolation\n\n```python\n# â CORRECT: Use a function to isolate file handle\ndef load_schema():\n    with open('schema.json', 'r', encoding='utf-8') as f:\n        return json.load(f)\n\nschema = load_schema()\n# File handle never enters global namespace\n```\n\n### System Filtering Mechanisms\n\nThe system attempts to filter out unpicklable objects at [src/runtime/workspace.py:40-68]():\n\n```python\n# Simplified filtering logic\nfiltered_dict = {}\nfor key, value in _globals.items():\n    if key.startswith('_'):  # Skip private variables\n        continue\n    if isinstance(value, type(sys)):  # Skip modules\n        continue\n    try:\n        filtered_dict[key] = copy.deepcopy(value)  # Test serializability\n    except Exception:\n        # Skip unpicklable objects\n        continue\n```\n\n**However**, this filtering runs **during ExecutionResult construction**, which is **after** the primary exception. If the primary exception handler itself fails due to unpicklable objects, the process crashes.\n\nSources: [src/runtime/workspace.py:40-68](), [src/runtime/schemas.py:80-85]()\n\n---\n\n## 3. Process Crashes and UnboundLocalError\n\n### Error Signature\n\n```\nUnboundLocalError: cannot access local variable 'res' where it is not associated with a value\n```\n\n### Root Cause\n\nThis error occurs in [src/runtime/subprocess_python_executor.py:66]() when:\n1. The code in the `except Exception as e:` block (lines 58-69) raises a **secondary exception**\n2. The `finally` block (line 72) executes and tries to send `res` via pipe\n3. Variable `res` was never successfully assigned due to the secondary exception\n\n### Code Analysis\n\n```python\n# src/runtime/subprocess_python_executor.py:49-73\ntry:\n    exec(command, _globals, _locals)\n    res = ExecutionResult(...)  # Line 52: Success path\nexcept Exception as e:\n    res = ExecutionResult(...)  # Line 60: Failure path - may raise!\nfinally:\n    child_conn.send((_PipeType.RESULT, res))  # Line 72: res may be undefined\n    child_conn.close()\n```\n\n**The problem**: If line 60's `ExecutionResult(...)` constructor raises an exception (e.g., during Pydantic validation or `filter_and_deepcopy_globals`), `res` is never assigned, causing the `finally` block to fail.\n\nSources: [src/runtime/subprocess_python_executor.py:49-73](), [logs/utils.log:170-178]()\n\n### Parent Process Detection\n\nWhen the subprocess crashes:\n1. Pipe reader receives `EOFError` at [src/runtime/subprocess_python_executor.py:108]()\n2. `subprocess_result_container` remains empty\n3. Parent process detects missing result at [src/runtime/subprocess_python_executor.py:149-159]()\n4. Returns `ExecutionResult` with `status=CRASHED`\n\n```python\n# src/runtime/subprocess_python_executor.py:149-159\nif subprocess_result_container:\n    final_res: ExecutionResult = subprocess_result_container[0]\nelse:\n    # No result received - process crashed\n    final_res = ExecutionResult(\n        arg_command=command,\n        arg_timeout=timeout,\n        arg_globals=_globals or {},\n        exit_status=ExecutionStatus.CRASHED,\n    )\n```\n\nSources: [src/runtime/subprocess_python_executor.py:149-159](), [logs/utils.log:133]()\n\n### Error Message to User\n\nWhen `CRASHED` status is returned, the tool output shows:\n\n```\n## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\nProcess Process-1:\nTraceback (most recent call last):\n  ...\nUnboundLocalError: cannot access local variable 'res' where it is not associated with a value\n### éåºç¶æç ï¼1\n```\n\nSources: [logs/utils.log:136-180]()\n\n---\n\n## 4. Timeout Errors\n\n### Error Signature\n\n```\nExecutionStatus.TIMEOUT\n```\n\n### Root Cause\n\nCode execution exceeds the specified timeout duration (default 30 seconds). Common causes:\n\n1. **Infinite loops**:\n```python\nwhile True:  # Never terminates\n    pass\n```\n\n2. **Long-running computations**:\n```python\n# Processing millions of records\nfor i in range(10_000_000):\n    complex_calculation(i)\n```\n\n3. **Blocking I/O operations**:\n```python\nimport requests\nresponse = requests.get('http://slow-server.com')  # Hangs indefinitely\n```\n\n### Detection Mechanism\n\n```python\n# src/runtime/subprocess_python_executor.py:129-144\np.join(timeout)  # Wait for subprocess with timeout\nif p.is_alive():\n    # Timeout occurred\n    p.terminate()\n    p.join()\n    parent_conn.close()\n    t.join()\n    final_res = ExecutionResult(\n        arg_command=command,\n        arg_timeout=timeout,\n        arg_globals=_globals or {},\n        exit_status=ExecutionStatus.TIMEOUT,\n    )\n```\n\nThe parent process uses `multiprocessing.Process.join(timeout)` to wait. If the process is still alive after timeout seconds, it forcibly terminates the subprocess using `p.terminate()`.\n\nSources: [src/runtime/subprocess_python_executor.py:129-144]()\n\n### Resolution Strategies\n\n#### Strategy 1: Increase Timeout\n\n```python\n# Pass larger timeout value to the tool\n{\n    \"tool_call_purpose\": \"Long-running analysis\",\n    \"python_code_snippet\": \"...\",\n    \"timeout\": 300  # 5 minutes instead of default 30 seconds\n}\n```\n\n#### Strategy 2: Optimize Code\n\n- Add early termination conditions to loops\n- Use generators and lazy evaluation\n- Process data in chunks\n- Add progress indicators to verify execution is proceeding\n\n#### Strategy 3: Asynchronous Execution\n\nFor truly long-running tasks, consider:\n- Breaking into smaller sub-tasks\n- Implementing checkpoints to resume interrupted work\n- Moving computation outside the agent loop\n\nSources: [src/tool/python_tool.py]() (tool parameter definition)\n\n---\n\n## 5. Debugging Strategies\n\n### Log File Analysis\n\n```mermaid\ngraph TB\n    Error[\"Execution Error Occurs\"]\n    \n    CheckLog{\"Which log file?\"}\n    \n    Error --> CheckLog\n    \n    CheckLog -->|Global| GlobalLog[\"logs/global.log<br/>- Function call traces<br/>- Execution timing<br/>- System-level events\"]\n    \n    CheckLog -->|Utils| UtilsLog[\"logs/utils.log<br/>- User queries<br/>- Tool outputs<br/>- Error messages<br/>- LLM responses\"]\n    \n    CheckLog -->|Trace| TraceLog[\"logs/trace.log<br/>- Detailed stack traces<br/>- Parameter values<br/>- Exception details\"]\n    \n    GlobalLog --> Pattern1[\"Search for:<br/>'å­è¿ç¨å¼å¸¸ç»æ'<br/>'å­è¿ç¨å´©æºéåº'<br/>'Pipe reader error'\"]\n    \n    UtilsLog --> Pattern2[\"Search for:<br/>'ä»£ç æ§è¡å´©æº'<br/>'UnicodeDecodeError'<br/>'TypeError: cannot pickle'\"]\n    \n    TraceLog --> Pattern3[\"Search for:<br/>Full tracebacks<br/>Variable states<br/>Call chains\"]\n    \n    Pattern1 --> Action1[\"Identify crash location<br/>Check exitcode\"]\n    Pattern2 --> Action2[\"Extract full error message<br/>Identify error type\"]\n    Pattern3 --> Action3[\"Reconstruct execution state<br/>Trace root cause\"]\n```\n\n**Diagram: Log File Navigation for Error Debugging**\n\nSources: [logs/global.log:1-620](), [logs/utils.log:1-296]()\n\n### Error Pattern Recognition\n\n| Log Pattern | Error Type | Next Step |\n|-------------|------------|-----------|\n| `'gbk' codec can't decode` | UnicodeDecodeError | Add `encoding='utf-8'` |\n| `cannot pickle 'TextIOWrapper'` | PickleError | Use context managers |\n| `UnboundLocalError: cannot access local variable 'res'` | Process crash | Fix unpicklable objects |\n| `Pipe reader error: type=EOFError` | Subprocess crash | Check subprocess_result_container |\n| `å­è¿ç¨è¶æ¶éåºï¼è¶æ¶ X ç§` | Timeout | Increase timeout or optimize code |\n\n### Subprocess Working Directory\n\nThe system changes the working directory at [src/runtime/cwd.py:9-28]() before executing code. Check log messages:\n\n```\nå­è¿ç¨ PID: 11336 è¦å°å·¥ä½ç®å½æ´æ¹ä¸º: D:\\zyt\\git_ln\\algo_agent\\wsm\\1\\g4-1\nå­è¿ç¨ PID: 11336 å·²å°å·¥ä½ç®å½æ´æ¹ä¸º: D:\\zyt\\git_ln\\algo_agent\\wsm\\1\\g4-1\n```\n\nFile paths in errors are relative to this working directory, not the project root.\n\nSources: [src/runtime/cwd.py:9-28](), [logs/utils.log:130-131]()\n\n### Step-by-Step Debugging Process\n\n1. **Identify the error type** from exit status:\n   - Check `exit_status` field in `ExecutionResult`\n   - Look for corresponding log patterns\n\n2. **Locate the error in logs**:\n   - Search for the timestamp or subprocess PID\n   - Find the full traceback in `exception_traceback` field\n\n3. **Analyze the code snippet**:\n   - Review `arg_command` field\n   - Identify problematic constructs (file I/O, imports, etc.)\n\n4. **Check global state**:\n   - Examine `arg_globals` for unpicklable objects\n   - Verify `out_globals` was successfully serialized\n\n5. **Apply the fix**:\n   - Follow resolution strategies for specific error type\n   - Re-run with corrected code\n\nSources: [src/runtime/schemas.py:40-100]()\n\n---\n\n## 6. Prevention Best Practices\n\n### Code Generation Guidelines for LLMs\n\nWhen generating code snippets, the LLM should follow these patterns:\n\n```python\n# â SAFE PATTERN: Context managers + explicit encoding\ndef safe_file_operation():\n    with open('data.json', 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\n\n# â SAFE PATTERN: Minimal global scope pollution\nresult = safe_file_operation()\nprint(result)\n\n# â UNSAFE PATTERN: File handle in global scope\nf = open('data.json', 'r')  # No encoding specified\ndata = json.load(f)  # May crash on non-ASCII\n# f remains in _globals, cannot be pickled\n```\n\n### Error Recovery in Agent Loop\n\nThe agent system should implement retry logic at [src/agent/deep_research.py]():\n\n1. **Detect error type** from `ExecutionResult.exit_status`\n2. **Analyze error message** from `exception_traceback`\n3. **Generate corrected code** based on error type\n4. **Retry execution** with fix applied\n5. **Update task tree** to record retry attempts\n\nThis prevents the agent from repeatedly making the same mistake.\n\nSources: [src/agent/deep_research.py]() (agent orchestration)\n\n### System-Level Protections\n\nThe execution system provides multiple safety layers:\n\n| Layer | Location | Protection |\n|-------|----------|------------|\n| Process isolation | [subprocess_python_executor.py]() | SegFault doesn't crash agent |\n| Timeout enforcement | [subprocess_python_executor.py:129]() | Infinite loops auto-terminate |\n| Global filtering | [workspace.py:40-68]() | Removes unpicklable objects |\n| Exception capture | [subprocess_python_executor.py:58]() | All exceptions logged |\n\nSources: [src/runtime/subprocess_python_executor.py:1-205](), [src/runtime/workspace.py:40-68]()\n\n---\n\n## Summary Table: Quick Error Reference\n\n| Error | Log Indicator | Primary Cause | Quick Fix | See Section |\n|-------|---------------|---------------|-----------|-------------|\n| UnicodeDecodeError | `'gbk' codec can't decode` | Missing UTF-8 encoding | Add `encoding='utf-8'` to `open()` | Â§1 |\n| PickleError | `cannot pickle 'TextIOWrapper'` | File handle in globals | Use `with` statements | Â§2 |\n| Process Crash | `UnboundLocalError` + `EOFError` | Secondary exception during error handling | Fix picklability issues | Â§3 |\n| Timeout | `å­è¿ç¨è¶æ¶éåº` | Code runs too long | Increase timeout or optimize | Â§4 |\n| CRASHED status | `subprocess_result_container` empty | Various system-level failures | Check subprocess logs | Â§3 |\n\nSources: All sections above\n\n---\n\n# Page: Tool Call Errors\n\n# Tool Call Errors\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/functino_call_err.design.md](docs/functino_call_err.design.md)\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/agent/action.py](src/agent/action.py)\n- [src/agent/llm.py](src/agent/llm.py)\n- [src/agent/memory.py](src/agent/memory.py)\n\n</details>\n\n\n\nThis document covers errors that occur during LLM tool calling and function invocation, including format mismatches, message structure issues, and argument parsing failures. These errors typically manifest when the LLM attempts to invoke tools but the request or response format is invalid.\n\nFor errors occurring during code execution within tools (like timeout or pickle errors), see [Common Execution Errors](#10.1). For general tool system architecture, see [Tool System](#4).\n\n---\n\n## Overview of Tool Call Error Types\n\nTool call errors occur at the interface between the LLM and the tool execution system. The system uses OpenAI's function calling protocol, which requires strict adherence to message formats and structure.\n\n**Common Error Categories:**\n\n| Error Type | Trigger Point | Impact |\n|------------|---------------|--------|\n| Format Mismatch (`function_call` vs `tool_calls`) | LLM response parsing | Agent loop terminates |\n| Message Structure Validation | LLM API request | BadRequestError from API |\n| Argument Parsing | Tool dispatch | Tool execution skipped |\n| Tool Execution Failure | Inside tool `run()` method | Graceful error message returned |\n\nSources: [src/agent/llm.py:1-51](), [src/agent/action.py:1-49](), [logs/utils.log:1-296]()\n\n---\n\n## Function Call Format Mismatch\n\n### Problem Description\n\nThe most common tool call error occurs when the LLM returns a deprecated `function_call` field instead of the modern `tool_calls` array. This happens despite explicitly setting `function_call=None` in the API request.\n\n```mermaid\nsequenceDiagram\n    participant Agent as \"deep_research.user_query\"\n    participant LLM as \"llm.generate_assistant_output_append\"\n    participant API as \"DashScope API\"\n    participant Checker as \"has_function_call()\"\n    \n    Agent->>LLM: \"messages + tools_schema_list\"\n    Note over LLM: \"function_call=None<br/>parallel_tool_calls=True\"\n    LLM->>API: \"POST /chat/completions\"\n    API-->>LLM: \"ChatCompletionMessage with function_call\"\n    Note over LLM: \"Deprecated format returned!\"\n    LLM-->>Agent: \"assistant_output\"\n    Agent->>Checker: \"has_function_call(assistant_output)\"\n    Checker-->>Agent: \"True\"\n    Note over Agent: \"Error: function_call not supported\"\n    Agent->>Agent: \"Append error message to messages\"\n```\n\n**Example Error from Logs:**\n\n```\nChatCompletionMessage(\n    content='', \n    role='assistant',\n    function_call=FunctionCall(\n        arguments='{\"file_path\": \"schema.json\"}', \n        name='file_read'\n    ), \n    tool_calls=None\n)\n```\n\n### Detection Logic\n\nThe system detects this error using helper functions in [src/agent/llm.py:44-49]():\n\n```python\ndef has_tool_call(assistant_output: ChatCompletionMessage) -> bool:\n    return assistant_output.tool_calls is not None\n\ndef has_function_call(assistant_output: ChatCompletionMessage) -> bool:\n    return assistant_output.function_call is not None\n```\n\nWhen `has_function_call()` returns `True`, the system rejects the response because no handler is registered for `function_call`.\n\n### Recovery Strategy\n\nThe system appends an error message to the conversation and retries:\n\n```\næ²¡æå®ä¹function_callå·¥å·è°ç¨ï¼æ æ³æ§è¡function_callï¼è¯·ä½¿ç¨tool_callsè°ç¨å·¥å·ã\n```\n\nThis feedback message is added to the message history, prompting the LLM to retry with the correct `tool_calls` format.\n\nSources: [src/agent/llm.py:44-49](), [logs/utils.log:62-64](), [docs/functino_call_err.design.md:1-3]()\n\n---\n\n## Message Structure Validation Errors\n\n### The tool_call_id Response Requirement\n\nWhen the LLM generates a message with `tool_calls`, the OpenAI API requires that each `tool_call_id` in the assistant message must have a corresponding tool response message. Failure to provide responses causes a BadRequestError.\n\n**Error Structure:**\n\n```mermaid\ngraph TB\n    AssistantMsg[\"Assistant Message<br/>(role: assistant)\"]\n    ToolCall1[\"tool_calls[0]<br/>id: call_abc123\"]\n    ToolCall2[\"tool_calls[1]<br/>id: call_def456\"]\n    ToolCall3[\"tool_calls[2]<br/>id: call_ghi789\"]\n    \n    ToolResp1[\"Tool Message<br/>(role: tool)<br/>tool_call_id: call_abc123\"]\n    ToolResp2[\"Tool Message<br/>(role: tool)<br/>tool_call_id: call_def456\"]\n    Missing[\"â Missing Response<br/>for call_ghi789\"]\n    \n    AssistantMsg --> ToolCall1\n    AssistantMsg --> ToolCall2\n    AssistantMsg --> ToolCall3\n    \n    ToolCall1 --> ToolResp1\n    ToolCall2 --> ToolResp2\n    ToolCall3 --> Missing\n    \n    style Missing fill:#fee,stroke:#f00\n```\n\n**Actual Error from Logs:**\n\n```\nBadRequestError: Error code: 400 - {\n  'error': {\n    'message': '<400> InternalError.Algo.InvalidParameter: \n                An assistant message with \"tool_calls\" must be followed \n                by tool messages responding to each \"tool_call_id\". \n                The following tool_call_ids did not have response messages: \n                message[7].role',\n    'type': 'invalid_request_error',\n    'code': 'invalid_parameter_error'\n  }\n}\n```\n\n### Message Array Structure\n\nThe conversation messages array must maintain this strict ordering:\n\n```mermaid\ngraph LR\n    System[\"message[0]<br/>role: system\"]\n    User1[\"message[1]<br/>role: user\"]\n    Asst1[\"message[2]<br/>role: assistant<br/>tool_calls: [...]\"]\n    Tool1[\"message[3]<br/>role: tool<br/>tool_call_id: ...\"]\n    Tool2[\"message[4]<br/>role: tool<br/>tool_call_id: ...\"]\n    Asst2[\"message[5]<br/>role: assistant\"]\n    \n    System --> User1\n    User1 --> Asst1\n    Asst1 --> Tool1\n    Asst1 --> Tool2\n    Tool1 --> Asst2\n    Tool2 --> Asst2\n    \n    style Asst1 fill:#fef,stroke:#333\n    style Tool1 fill:#efe,stroke:#333\n    style Tool2 fill:#efe,stroke:#333\n```\n\n### Root Causes\n\nThis error occurs when:\n\n1. **Missing tool response messages**: Some `tool_call_id` values are not followed by corresponding tool messages\n2. **Incorrect message role**: Using `role: \"user\"` instead of `role: \"tool\"` for tool responses\n3. **Malformed tool_call_id**: The `tool_call_id` in the tool message doesn't match any `tool_calls[i].id`\n\nSources: [docs/functino_call_err.design.md:1-51](), [logs/global.log:1-10000]()\n\n---\n\n## Parallel Tool Calls and Response Mapping\n\n### Configuration\n\nThe system enables parallel tool calls in [src/agent/llm.py:22]():\n\n```python\nparallel_tool_calls=True\n```\n\nThis allows the LLM to request multiple tool executions in a single response.\n\n### Tool Call Processing Flow\n\n```mermaid\ngraph TB\n    LLMResponse[\"LLM Response<br/>assistant_output.tool_calls\"]\n    \n    subgraph \"Tool Call Array\"\n        TC1[\"tool_calls[0]<br/>id: call_3085f1f75d534390a7c2b7<br/>name: execute_python_code<br/>arguments: {...}\"]\n        TC2[\"tool_calls[1]<br/>id: call_ab025bb2bb3e430aaefd92<br/>name: execute_python_code<br/>arguments: {...}\"]\n        TC3[\"tool_calls[2]<br/>id: call_92fd80e050c34c78a18ea9<br/>name: execute_python_code<br/>arguments: {...}\"]\n    end\n    \n    subgraph \"Tool Execution\"\n        Exec1[\"call_tools_safely(tool_info)<br/>Parse JSON arguments<br/>Dispatch to tool\"]\n        Exec2[\"call_tools_safely(tool_info)<br/>Parse JSON arguments<br/>Dispatch to tool\"]\n        Exec3[\"call_tools_safely(tool_info)<br/>Parse JSON arguments<br/>Dispatch to tool\"]\n    end\n    \n    subgraph \"Tool Response Messages\"\n        TM1[\"role: tool<br/>tool_call_id: call_3085...<br/>content: execution result\"]\n        TM2[\"role: tool<br/>tool_call_id: call_ab02...<br/>content: execution result\"]\n        TM3[\"role: tool<br/>tool_call_id: call_92fd...<br/>content: execution result\"]\n    end\n    \n    LLMResponse --> TC1\n    LLMResponse --> TC2\n    LLMResponse --> TC3\n    \n    TC1 --> Exec1\n    TC2 --> Exec2\n    TC3 --> Exec3\n    \n    Exec1 --> TM1\n    Exec2 --> TM2\n    Exec3 --> TM3\n```\n\n**Example from Logs:**\n\nThe system correctly processes three parallel tool calls:\n\n```python\ntool_calls=[\n    ChatCompletionMessageFunctionToolCall(\n        id='call_3085f1f75d534390a7c2b7',\n        function=Function(\n            arguments='{\"tool_call_purpose\": \"...\", \"python_code_snippet\": \"...\"}',\n            name='execute_python_code'\n        ),\n        type='function',\n        index=0\n    ),\n    ChatCompletionMessageFunctionToolCall(id='call_ab025bb2bb3e430aaefd92', ...),\n    ChatCompletionMessageFunctionToolCall(id='call_92fd80e050c34c78a18ea9', ...)\n]\n```\n\nSources: [logs/utils.log:128-129](), [src/agent/llm.py:14-24]()\n\n---\n\n## Argument Parsing Errors\n\n### JSON Deserialization\n\nTool arguments are passed as JSON strings that must be parsed in [src/agent/action.py:13]():\n\n```python\narguments = json.loads(tool_info[\"tool_call_arguments\"])\n```\n\n### Common Parsing Failures\n\n| Error Type | Cause | Example |\n|------------|-------|---------|\n| `JSONDecodeError` | Malformed JSON syntax | Missing quotes, trailing commas |\n| `KeyError` | Missing required field | Tool schema requires `python_code_snippet` but not provided |\n| `TypeError` | Wrong type for parameter | String provided for integer `timeout` parameter |\n| `ValidationError` | Pydantic validation failure | Invalid enum value for `TaskStatus` |\n\n### Error Handling\n\nThe `call_tools_safely` wrapper catches parsing errors in [src/agent/action.py:40-47]():\n\n```python\ntry:\n    return call_tools(tool_info)\nexcept Exception as e:\n    error_msg = traceback.format_exc()\n    global_logger.error(f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\", exc_info=True)\n    tool_info[\"content\"] = f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\"\n    return tool_info\n```\n\nWhen parsing fails, the error message is:\n1. Logged to `logs/global.log` with full traceback\n2. Returned as the tool's \"content\" field\n3. Appended to the message history for LLM to see\n\nThis allows the LLM to adjust its approach based on the specific parsing error.\n\nSources: [src/agent/action.py:10-47]()\n\n---\n\n## Tool Dispatch Errors\n\n### Tool Name Resolution\n\nThe system maps tool names to implementations using a series of if-elif checks in [src/agent/action.py:14-20]():\n\n```mermaid\ngraph TD\n    Start[\"tool_info['tool_call_name']\"]\n    \n    Check1{\"name ==<br/>'execute_python_code'?\"}\n    Check2{\"name ==<br/>'recursive_plan_tree_todo'?\"}\n    Fallback[\"No handler found<br/>(implicit else)\"]\n    \n    Tool1[\"ExecutePythonCodeTool(**arguments)<br/>tool.run()\"]\n    Tool2[\"RecursivePlanTreeTodoTool(**arguments)<br/>tool.run()\"]\n    \n    Start --> Check1\n    Check1 -->|Yes| Tool1\n    Check1 -->|No| Check2\n    Check2 -->|Yes| Tool2\n    Check2 -->|No| Fallback\n    \n    style Fallback fill:#fee,stroke:#f00\n```\n\n### Unrecognized Tool Names\n\nIf the LLM requests a tool name that doesn't match any registered tool:\n- The dispatch logic falls through all checks\n- No tool is executed\n- The function returns without setting `tool_info[\"content\"]`\n- This leaves the content as the original (likely empty or error state)\n\n**Prevention:** The tool schema provided to the LLM should only include available tools. The schema is generated by [tool/schema.py:get_tools_schema]() and includes only:\n- `ExecutePythonCodeTool`\n- `RecursivePlanTreeTodoTool`\n\nSources: [src/agent/action.py:11-21]()\n\n---\n\n## Tool Execution Failures\n\n### Execution Errors vs Tool Call Errors\n\nTool execution errors occur **after** successful tool call parsing and dispatch. These are distinct from tool call errors:\n\n- **Tool Call Error**: Format/parsing issues before tool execution begins\n- **Tool Execution Error**: Issues during the tool's `run()` method (covered in [Common Execution Errors](#10.1))\n\n### Error Propagation\n\nWhen a tool's `run()` method fails:\n\n```mermaid\nsequenceDiagram\n    participant Action as \"call_tools_safely\"\n    participant Tool as \"ExecutePythonCodeTool.run\"\n    participant Executor as \"subprocess_python_executor\"\n    \n    Action->>Tool: \"tool.run()\"\n    Tool->>Executor: \"run_structured_in_subprocess(...)\"\n    Executor-->>Tool: \"ExecutionResult(status=CRASHED, ...)\"\n    Tool-->>Action: \"Formatted error message\"\n    Note over Action: \"Error caught by try-except\"\n    Action->>Action: \"tool_info['content'] = error_msg\"\n    Action-->>Agent: \"Return tool_info with error\"\n    Agent->>Messages: \"Append as tool message\"\n    Agent->>LLM: \"Send updated messages\"\n```\n\nThe error is wrapped and returned as a string in `tool_info[\"content\"]`, which becomes the tool response message content. The LLM can then read this error and adjust its strategy.\n\nSources: [src/agent/action.py:40-47]()\n\n---\n\n## Debugging Tool Call Errors\n\n### Log File Analysis\n\nTool call errors are logged to multiple files:\n\n| Log File | Contents | Location |\n|----------|----------|----------|\n| `logs/utils.log` | User inputs, LLM outputs, tool results | [logs/utils.log:1-296]() |\n| `logs/global.log` | Function call traces with arguments | [logs/global.log:1-10000]() |\n| `logs/trace.log` | Detailed execution traces | (Not provided in files) |\n\n### Key Log Patterns\n\n**1. Function Call Format Error:**\n```\nChatCompletionMessage(..., function_call=FunctionCall(...), tool_calls=None)\n```\n\n**2. Missing Tool Response:**\n```\nBadRequestError: ... tool_call_ids did not have response messages: message[7].role\n```\n\n**3. Argument Parsing Failure:**\n```\nå·¥å·å½æ°è°ç¨å¤±è´¥..., éè¯¯ä¿¡æ¯: JSONDecodeError: ...\n```\n\n### Inspection Points\n\n```mermaid\ngraph LR\n    P1[\"llm.py:36<br/>assistant_output inspection\"]\n    P2[\"llm.py:44-49<br/>has_tool_call() check\"]\n    P3[\"action.py:13<br/>JSON argument parsing\"]\n    P4[\"action.py:40<br/>Exception catching\"]\n    \n    P1 --> P2\n    P2 --> P3\n    P3 --> P4\n    \n    style P1 fill:#eff,stroke:#333\n    style P2 fill:#eff,stroke:#333\n    style P3 fill:#eff,stroke:#333\n    style P4 fill:#eff,stroke:#333\n```\n\nSources: [logs/utils.log:1-296](), [logs/global.log:1-10000]()\n\n---\n\n## Recovery Patterns\n\n### Pattern 1: Format Correction Feedback\n\nWhen `function_call` is detected instead of `tool_calls`:\n\n1. **Detect:** [src/agent/llm.py:48]() returns `True`\n2. **Feedback:** Append correction message to conversation\n3. **Retry:** LLM generates new response with `tool_calls` format\n\n### Pattern 2: Message Structure Fix\n\nWhen message validation fails:\n\n1. **Identify:** Parse error message to find missing `tool_call_id`\n2. **Reconstruct:** Ensure all tool calls have response messages\n3. **Validate:** Check message array has correct role sequence\n\n### Pattern 3: Argument Adjustment\n\nWhen argument parsing fails:\n\n1. **Extract:** Error details from exception traceback\n2. **Explain:** Return detailed error to LLM as tool response\n3. **Adjust:** LLM modifies arguments based on error feedback\n\n### Pattern 4: Tool Fallback\n\nWhen tool execution fails repeatedly:\n\n1. **Track:** Count consecutive failures for same tool\n2. **Escalate:** After threshold, suggest alternative approaches\n3. **Skip:** Mark problematic subtask as \"skipped\" in task tree\n\nSources: [src/agent/action.py:40-47](), [docs/functino_call_err.design.md:1-51]()\n\n---\n\n## System Prompt Guidance\n\nThe system prompt in [prompt.py:react_system_prompt]() includes error handling instructions:\n\n```\nå¦æéå°å·¥å·è°ç¨éè¯¯ï¼\n1. è®°å½éè¯¯ä¿¡æ¯ï¼åæéè¯¯åå ã\n2. å°è¯è°æ´éè¯ï¼æèè°æ´åæè®¡åæ ï¼éæ°è§åè·¯å¾ã\n3. è®°å½éè¯æ¬¡æ°ï¼é¿åæ éå¾ªç¯ã\n4. å¦æå¤æ¬¡éè¯ä»ç¶å¤±è´¥ï¼èèæ¾å¼è¯¥è·¯å¾ï¼è½¬åå¶ä»åææ¹åã\n```\n\nThis guidance helps the LLM:\n- Recognize when tool calls fail\n- Analyze error messages in tool responses\n- Adjust tool arguments or approach\n- Prevent infinite retry loops\n- Gracefully abandon failing paths\n\nSources: [src/agent/memory.py:1-17](), [logs/global.log:115-126]()\n\n---\n\n## Common Error Messages Reference\n\n| Error Message | Root Cause | Fix |\n|---------------|------------|-----|\n| \"æ²¡æå®ä¹function_callå·¥å·è°ç¨\" | LLM used deprecated `function_call` | Wait for LLM to retry with `tool_calls` |\n| \"tool_call_ids did not have response messages\" | Missing tool response in message array | Ensure each `tool_call_id` has response |\n| \"JSONDecodeError: Expecting value\" | Malformed JSON in arguments | Check for syntax errors in LLM output |\n| \"KeyError: 'python_code_snippet'\" | Missing required tool parameter | Review tool schema and ensure LLM provides all required fields |\n| \"å·¥å·å½æ°è°ç¨å¤±è´¥\" | Exception during tool execution | Check tool execution logs for details |\n\nSources: [logs/utils.log:64](), [docs/functino_call_err.design.md:1-51]()"
            }
        ],
        "structuredContent": {
            "result": "# Page: Overview\n\n# Overview\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.vscode/settings.json](.vscode/settings.json)\n- [docs/sql.md](docs/sql.md)\n- [pyproject.toml](pyproject.toml)\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/agent/llm.py](src/agent/llm.py)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n- [uv.lock](uv.lock)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document provides a high-level introduction to the **algo_agent** system, an AI agent framework that combines Large Language Model (LLM) reasoning with safe Python code execution capabilities. The system enables users to submit natural language queries that are automatically translated into executable Python code, run in isolated environments, and iteratively refined based on execution results.\n\n**For detailed information on specific subsystems:**\n- Agent query processing and decision loops: see [Query Processing Loop](#3.1)\n- LLM integration details: see [LLM Integration](#3.2)\n- Code execution strategies: see [Execution Runtime](#5)\n- Tool system architecture: see [Tool System](#4)\n- Logging and debugging: see [Observability and Logging](#6)\n\n**Sources:** [src/agent/deep_research.py:1-129](), [pyproject.toml:1-21]()\n\n---\n\n## System Architecture Overview\n\nThe algo_agent system is organized into five primary layers that work together to process user queries and execute code:\n\n```mermaid\ngraph TB\n    User[\"User Query<br/>(Natural Language)\"]\n    \n    Agent[\"deep_research.py<br/>user_query function<br/>Agent Orchestration\"]\n    \n    LLM[\"llm.py<br/>qwen-plus model<br/>DashScope API\"]\n    \n    Memory[\"memory.py<br/>init_messages_with_system_prompt<br/>Message History\"]\n    \n    Action[\"action.py<br/>call_tools_safely<br/>Tool Dispatcher\"]\n    \n    ToolInterface[\"tool/base.py<br/>BaseTool<br/>Abstract Interface\"]\n    \n    PythonTool[\"tool/python_tool.py<br/>ExecutePythonCodeTool\"]\n    \n    TodoTool[\"tool/todo_tool.py<br/>RecursivePlanTreeTodoTool\"]\n    \n    SubprocessExec[\"executor/run_in_subprocess.py<br/>run_structured_in_subprocess\"]\n    \n    SubthreadExec[\"executor/run_in_subthread.py<br/>run_structured_in_subthread\"]\n    \n    DirectExec[\"executor/run_in_direct.py<br/>run_structured_direct_exec\"]\n    \n    Workspace[\"executor/workspace.py<br/>arg_globals_list<br/>out_globals_list\"]\n    \n    Logger[\"utils/log_decorator.py<br/>global_logger<br/>traceable\"]\n    \n    User --> Agent\n    Agent --> Memory\n    Agent --> LLM\n    LLM --> Action\n    Action --> ToolInterface\n    ToolInterface --> PythonTool\n    ToolInterface --> TodoTool\n    PythonTool --> SubprocessExec\n    PythonTool --> SubthreadExec\n    PythonTool --> DirectExec\n    PythonTool --> Workspace\n    SubprocessExec --> Workspace\n    SubthreadExec --> Workspace\n    DirectExec --> Workspace\n    \n    Agent -.logs.-> Logger\n    Action -.logs.-> Logger\n    PythonTool -.logs.-> Logger\n```\n\n**Core Design Principles:**\n\n| Principle | Implementation |\n|-----------|---------------|\n| **Isolation** | Python code runs in subprocess/subthread environments with timeout enforcement |\n| **Safety** | Workspace globals are filtered and serialized; crash detection prevents system failures |\n| **Observability** | Comprehensive logging via `@traceable` and `@log_function` decorators |\n| **Extensibility** | Tool system uses `BaseTool` interface for easy addition of new capabilities |\n| **Iterative Refinement** | Agent loops until LLM produces a final answer without tool calls |\n\n**Sources:** [src/agent/deep_research.py:1-74](), [src/agent/llm.py:1-51](), [src/utils/log_decorator.py:19-64]()\n\n---\n\n## Key Components\n\n### Agent Orchestration Layer\n\nThe **agent orchestration layer** manages the end-to-end query processing workflow through the `user_query` function in [src/agent/deep_research.py:15-73]().\n\n```mermaid\ngraph LR\n    UserQuery[\"user_query(user_input)\"]\n    InitMem[\"memory.init_messages_with_system_prompt\"]\n    GetSchema[\"tool.schema.get_tools_schema\"]\n    GenOutput[\"llm.generate_assistant_output_append\"]\n    CallTools[\"action.call_tools_safely\"]\n    \n    UserQuery --> InitMem\n    UserQuery --> GetSchema\n    InitMem --> GenOutput\n    GetSchema --> GenOutput\n    GenOutput --> CallTools\n    CallTools --> GenOutput\n```\n\n**Key functions:**\n- `user_query(user_input)` - Entry point for all queries [src/agent/deep_research.py:15]()\n- `init_messages_with_system_prompt(user_input)` - Initializes conversation with system prompt\n- `generate_assistant_output_append(messages, tools_schema_list)` - Queries LLM and appends response [src/agent/llm.py:34-41]()\n- `call_tools_safely(tool_info)` - Dispatches tool execution with error handling\n\n**Sources:** [src/agent/deep_research.py:15-73](), [src/agent/llm.py:27-50]()\n\n---\n\n### LLM Integration\n\nThe system integrates with **Alibaba's qwen-plus model** via the DashScope API using OpenAI-compatible client.\n\n| Configuration | Value |\n|--------------|-------|\n| **Model** | `qwen-plus` |\n| **API Base URL** | `https://dashscope.aliyuncs.com/compatible-mode/v1` |\n| **Client Library** | `openai>=2.7.2` |\n| **Tool Calling** | Parallel tool calls enabled |\n\n**Core functions:**\n- `_generate_chat_completion(messages, tools_schema_list)` - Creates OpenAI client and calls API [src/agent/llm.py:14-24]()\n- `has_tool_call(assistant_output)` - Checks if response contains tool calls [src/agent/llm.py:44-45]()\n- `has_function_call(assistant_output)` - Checks for legacy function call format [src/agent/llm.py:48-49]()\n\n**Message format:** Follows OpenAI chat completion format with roles: `system`, `user`, `assistant`, `tool`, `function`.\n\n**Sources:** [src/agent/llm.py:1-51](), [pyproject.toml:14]()\n\n---\n\n### Tool Execution System\n\nThe **tool system** provides extensible capabilities through two concrete implementations:\n\n```mermaid\ngraph TB\n    BaseTool[\"tool/base.py<br/>BaseTool<br/>Abstract Base Class\"]\n    \n    PythonTool[\"tool/python_tool.py<br/>ExecutePythonCodeTool<br/>Code Execution\"]\n    \n    TodoTool[\"tool/todo_tool.py<br/>RecursivePlanTreeTodoTool<br/>Task Management\"]\n    \n    RunMethod[\"run(tool_call_arguments)<br/>Execute tool logic\"]\n    \n    Schema[\"get_tool_schema()<br/>JSON schema for LLM\"]\n    \n    BaseTool --> PythonTool\n    BaseTool --> TodoTool\n    BaseTool --> RunMethod\n    BaseTool --> Schema\n```\n\n**BaseTool interface requirements:**\n- `tool_name` - String identifier\n- `tool_description` - Natural language description for LLM\n- `get_parameter_schema()` - JSON schema of input parameters\n- `get_tool_schema()` - Complete tool schema in OpenAI format\n- `run(tool_call_arguments)` - Execution logic returning string result\n\n**Available tools (configured in [src/agent/deep_research.py:20-23]()):**\n- `ExecutePythonCodeTool` - Executes Python code snippets with state persistence\n- `RecursivePlanTreeTodoTool` - Manages hierarchical task structures (currently commented out)\n\n**Sources:** [src/agent/deep_research.py:20-23]()\n\n---\n\n### Python Execution Runtime\n\nThe **execution runtime** provides three strategies for running Python code with varying isolation levels:\n\n| Strategy | Module | Isolation Level | Timeout Support | Use Case |\n|----------|--------|-----------------|-----------------|----------|\n| **Subprocess** | `executor/run_in_subprocess.py` | Full process isolation | â | Production code execution |\n| **Subthread** | `executor/run_in_subthread.py` | Thread-based isolation | â | Faster execution, shared memory |\n| **Direct** | `executor/run_in_direct.py` | In-process | â | Testing, minimal overhead |\n\n**Common execution flow:**\n\n```mermaid\ngraph LR\n    Input[\"Python Code<br/>+ arg_globals_list<br/>+ timeout\"]\n    \n    Filter[\"workspace.filter_and_deepcopy_globals<br/>Serialize safe objects\"]\n    \n    Execute[\"Executor Strategy<br/>subprocess/subthread/direct\"]\n    \n    Result[\"ExecutionResult<br/>status + stdout + stderr<br/>+ exception + out_globals\"]\n    \n    Append[\"workspace.append_out_globals<br/>Persist variables\"]\n    \n    Input --> Filter\n    Filter --> Execute\n    Execute --> Result\n    Result --> Append\n```\n\n**ExecutionResult schema:**\n- `status` - Enum: `SUCCESS`, `FAILURE`, `TIMEOUT`, `CRASHED`\n- `stdout` - Captured standard output\n- `stderr` - Captured standard error\n- `exception_type`, `exception_value`, `traceback` - Error details\n- `out_globals` - Dictionary of output variables\n\n**Sources:** [src/agent/deep_research.py:1-129]()\n\n---\n\n### Observability Layer\n\nThe **logging system** provides comprehensive tracing through decorators and multiple log files:\n\n```mermaid\ngraph TB\n    Decorator[\"@traceable<br/>@log_function<br/>utils/log_decorator.py\"]\n    \n    Setup[\"setup_logger<br/>Configure handlers\"]\n    \n    GlobalLogger[\"global_logger<br/>logs/print.log\"]\n    \n    TraceLogger[\"traceable<br/>logs/trace.log\"]\n    \n    AllLogger[\"all_logger<br/>logs/all.log\"]\n    \n    Decorator --> Setup\n    Setup --> GlobalLogger\n    Setup --> TraceLogger\n    Setup --> AllLogger\n```\n\n**Logger instances:**\n- `global_logger` - User-facing operations and query processing [src/utils/log_decorator.py:305-306]()\n- `traceable` - Function call tracing with timing [src/utils/log_decorator.py:297-302]()\n- `all_logger` - Comprehensive system logs [src/utils/log_decorator.py:293-294]()\n\n**Logging features:**\n- Automatic parameter/return value capture\n- Execution timing (milliseconds)\n- Stack trace recording with project-only filtering [src/utils/log_decorator.py:208-217]()\n- Exception handling with traceback [src/utils/log_decorator.py:233-247]()\n- Rotating file handlers (10MB max, 5 backups) [src/utils/log_decorator.py:53-61]()\n\n**Sources:** [src/utils/log_decorator.py:19-307]()\n\n---\n\n## Execution Flow: Query to Response\n\nThe following sequence diagram shows how a user query flows through the system:\n\n```mermaid\nsequenceDiagram\n    participant U as User\n    participant DRA as deep_research.py<br/>user_query\n    participant MEM as memory.py<br/>init_messages\n    participant LLM as llm.py<br/>qwen-plus\n    participant ACT as action.py<br/>call_tools_safely\n    participant PYT as ExecutePythonCodeTool\n    participant EXE as Subprocess Executor\n    participant LOG as global_logger\n    \n    U->>DRA: user_query(user_input)\n    DRA->>LOG: Log user input\n    DRA->>MEM: init_messages_with_system_prompt\n    MEM-->>DRA: messages list\n    \n    loop Until no tool calls\n        DRA->>LLM: generate_assistant_output_append\n        LLM-->>DRA: ChatCompletionMessage\n        \n        alt has tool_calls or function_call\n            DRA->>ACT: call_tools_safely(tool_info)\n            ACT->>PYT: run(tool_call_arguments)\n            PYT->>EXE: run_structured_in_subprocess\n            EXE-->>PYT: ExecutionResult\n            PYT-->>ACT: formatted result\n            ACT->>LOG: Log tool output\n            ACT-->>DRA: tool result\n            DRA->>MEM: Append tool result to messages\n        else No tool call\n            DRA->>LOG: Log final answer\n            DRA-->>U: Return response\n        end\n    end\n```\n\n**Key decision points:**\n1. **Line 27**: Check `has_tool_call()` or `has_function_call()` [src/agent/deep_research.py:27]()\n2. **Line 32**: Loop continues while tools are called [src/agent/deep_research.py:32]()\n3. **Line 33-46**: Handle legacy `function_call` format [src/agent/deep_research.py:33-46]()\n4. **Line 47-64**: Handle modern `tool_calls` array [src/agent/deep_research.py:47-64]()\n5. **Line 73**: Final answer logged and returned [src/agent/deep_research.py:73]()\n\n**Sources:** [src/agent/deep_research.py:15-73](), [src/agent/llm.py:34-50]()\n\n---\n\n## Technology Stack\n\nThe system is built on the following dependencies:\n\n| Category | Package | Version | Purpose |\n|----------|---------|---------|---------|\n| **LLM Client** | `openai` | â¥2.7.2 | DashScope API communication |\n| **Data Processing** | `pandas` | â¥2.3.3 | DataFrame operations |\n| **Data Processing** | `numpy` | â¥2.3.4 | Numerical computing |\n| **Visualization** | `matplotlib` | â¥3.10.7 | Plotting and charts |\n| **Graph Processing** | `networkx` | â¥3.5 | Network/graph algorithms |\n| **ML/Analysis** | `scikit-learn` | â¥1.7.2 | Machine learning utilities |\n| **Utilities** | `deepdiff` | â¥8.6.1 | Deep comparison of data structures |\n| **Utilities** | `decorator` | â¥5.2.1 | Decorator utilities |\n| **Utilities** | `inflection` | â¥0.5.1 | String transformations |\n| **Web Interface** | `streamlit` | â¥1.51.0 | Interactive web UI (optional) |\n| **HTTP** | `requests` | â¥2.32.5 | HTTP client |\n| **Visualization** | `wordcloud` | â¥1.9.4 | Word cloud generation |\n\n**Python version requirement:** â¥3.12\n\n**Configuration files:**\n- [pyproject.toml:1-21]() - Project metadata and dependencies\n- [uv.lock:1-302]() - Locked dependency versions\n- [.vscode/settings.json:1-34]() - IDE configuration\n\n**Sources:** [pyproject.toml:1-21](), [uv.lock:1-302]()\n\n---\n\n## System Characteristics\n\nThe algo_agent system is characterized by:\n\n**Strengths:**\n- **Safety-first design**: Multiple isolation levels prevent code crashes from affecting the system\n- **Comprehensive observability**: Every function call, execution, and error is logged\n- **Extensible architecture**: New tools can be added by implementing `BaseTool` interface\n- **State persistence**: Global variables maintained across executions via workspace management\n- **Flexible execution**: Three strategies (subprocess/subthread/direct) for different use cases\n\n**Typical use cases (from logs):**\n- Emergency response planning with data schema validation [src/agent/deep_research.py:77-128]()\n- Geographic data processing and visualization\n- Multi-day travel route optimization with metro/walking constraints\n- Pydantic data modeling and validation\n- Algorithm development and testing\n\n**For more detailed information:**\n- Installation and setup: see [Getting Started](#2)\n- Adding new tools: see [Creating New Tools](#8.2)\n- Debugging execution issues: see [Troubleshooting](#10)\n\n**Sources:** [src/agent/deep_research.py:76-129](), [pyproject.toml:1-21]()\n\n---\n\n# Page: Getting Started\n\n# Getting Started\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.vscode/settings.json](.vscode/settings.json)\n- [docs/sql.md](docs/sql.md)\n- [pyproject.toml](pyproject.toml)\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n- [uv.lock](uv.lock)\n\n</details>\n\n\n\nThis page provides step-by-step instructions for installing, configuring, and running your first query with the algo_agent system. It covers dependency installation, API configuration, and executing a basic code generation task. For detailed information about the agent's decision loop and tool execution mechanics, see [Agent Orchestration](#3). For information about the execution strategies and isolation modes, see [Execution Runtime](#5).\n\n---\n\n## Prerequisites\n\nThe algo_agent system requires:\n\n- **Python 3.12 or higher** - The project specifies `requires-python = \">=3.12\"` in its configuration\n- **Git** - For cloning the repository\n- **uv** (recommended) or pip - For dependency management\n- **DashScope API access** - The system uses Alibaba's qwen-plus LLM model\n\nSources: [pyproject.toml:6]()\n\n---\n\n## Installation\n\n### Step 1: Clone the Repository\n\n```bash\ngit clone https://github.com/1850298154/algo_agent\ncd algo_agent\n```\n\n### Step 2: Install Dependencies\n\nThe project uses `uv` for dependency management. Install all required packages:\n\n```bash\n# Using uv (recommended)\nuv sync\n\n# Or using pip\npip install -e .\n```\n\nThis installs the following core dependencies:\n\n| Package | Version | Purpose |\n|---------|---------|---------|\n| `openai` | â¥2.7.2 | LLM client interface |\n| `matplotlib` | â¥3.10.7 | Data visualization |\n| `pandas` | â¥2.3.3 | Data manipulation |\n| `numpy` | â¥2.3.4 | Numerical computing |\n| `networkx` | â¥3.5 | Graph algorithms |\n| `streamlit` | â¥1.51.0 | Web interface |\n| `deepdiff` | â¥8.6.1 | Deep object comparison |\n| `scikit-learn` | â¥1.7.2 | Machine learning utilities |\n\nSources: [pyproject.toml:7-20](), [uv.lock:1-43]()\n\n### Step 3: Verify Installation\n\nVerify the installation by checking the Python environment:\n\n```bash\npython -c \"import openai, matplotlib, pandas; print('Dependencies OK')\"\n```\n\n---\n\n## Configuration\n\n### API Key Setup\n\nThe system uses Alibaba's DashScope API for LLM access. You need to configure your API key:\n\n**Option 1: Environment Variable**\n```bash\nexport DASHSCOPE_API_KEY=\"your-api-key-here\"\n```\n\n**Option 2: Configuration File**\nCreate a configuration file in your project with the API credentials. The LLM client will read from the environment.\n\n### Working Directory Configuration\n\nThe system creates a `logs/` directory for storing execution logs. This is automatically created by the logging system:\n\n```mermaid\ngraph LR\n    START[\"System Startup\"] --> CHECK_DIR[\"setup_logger checks log directory\"]\n    CHECK_DIR --> CREATE[\"os.makedirs(log_dir, exist_ok=True)\"]\n    CREATE --> LOGGERS[\"Initialize loggers:<br/>- global.log<br/>- trace.log<br/>- print.log<br/>- all.log\"]\n    LOGGERS --> READY[\"System Ready\"]\n```\n\n**Diagram: Log Directory Initialization**\n\nThe logging system automatically creates necessary directories on first run.\n\nSources: [src/utils/log_decorator.py:19-28](), [src/utils/log_decorator.py:288-306]()\n\n### VS Code Configuration (Optional)\n\nIf using VS Code, the repository includes workspace settings that set `PYTHONPATH`:\n\n```json\n{\n    \"terminal.integrated.env.windows\": {\n        \"PYTHONPATH\": \"${workspaceFolder};${env:PYTHONPATH}\"\n    },\n    \"terminal.integrated.env.linux\": {\n        \"PYTHONPATH\": \"${workspaceFolder}:${env:PYTHONPATH}\"\n    }\n}\n```\n\nThis ensures the `src/` module can be imported correctly.\n\nSources: [.vscode/settings.json:2-7]()\n\n---\n\n## Running Your First Query\n\n### Basic Query Example\n\nThe primary entry point is the `user_query()` function in the deep research agent module:\n\n```python\nfrom src.agent.deep_research import user_query\n\n# Simple data analysis query\nquery = \"\"\"\nGenerate a list of prime numbers up to 100 using Python,\nthen create a histogram showing their distribution.\n\"\"\"\n\nuser_query(query)\n```\n\n### Query Execution Flow\n\nWhen you call `user_query()`, the following sequence occurs:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant user_query[\"user_query()<br/>(deep_research.py:15)\"]\n    participant memory[\"init_messages_with_system_prompt()<br/>(memory module)\"]\n    participant llm[\"generate_assistant_output_append()<br/>(llm module)\"]\n    participant tools[\"get_tools_schema()<br/>(tool.schema)\"]\n    participant action[\"call_tools_safely()<br/>(action module)\"]\n    participant ExecutePythonCodeTool[\"ExecutePythonCodeTool.run()\"]\n    participant global_logger[\"global_logger<br/>(logs/print.log)\"]\n\n    User->>user_query: \"query string\"\n    user_query->>global_logger: Log user input\n    user_query->>memory: Initialize message history\n    memory-->>user_query: messages list\n    \n    user_query->>tools: Get tool schemas\n    tools-->>user_query: [ExecutePythonCodeTool schema]\n    \n    user_query->>llm: Call qwen-plus with messages + tools\n    llm-->>user_query: assistant_output with tool_calls\n    \n    user_query->>action: call_tools_safely(tool_info)\n    action->>ExecutePythonCodeTool: Execute Python code\n    ExecutePythonCodeTool-->>action: ExecutionResult\n    action-->>user_query: tool output\n    \n    user_query->>global_logger: Log tool output\n    user_query->>llm: Call LLM with tool results\n    llm-->>user_query: Final answer\n    user_query->>global_logger: Log final answer\n    user_query-->>User: Complete\n```\n\n**Diagram: First Query Execution Sequence**\n\nThis shows how a user query flows through the agent, LLM, and tool execution system.\n\nSources: [src/agent/deep_research.py:15-73]()\n\n### Understanding the Output\n\nThe system produces multiple types of output:\n\n#### 1. Console Output\n\nThe agent logs its progress to the console via `global_logger`:\n\n```\n[2025-01-15 10:30:15]  ç¨æ·è¾å¥ï¼ Generate prime numbers...\n[2025-01-15 10:30:16]  å·¥å· tool call è¾åºä¿¡æ¯ï¼ Execution completed successfully\n[2025-01-15 10:30:17]  æç»ç­æ¡ï¼ Here are the prime numbers...\n```\n\nSources: [src/agent/deep_research.py:17](), [src/agent/deep_research.py:62](), [src/agent/deep_research.py:73]()\n\n#### 2. Log Files\n\nThe system writes detailed logs to multiple files in the `logs/` directory:\n\n| Log File | Content | Logger Name |\n|----------|---------|-------------|\n| `print.log` | User-facing messages and results | `root.all.print` |\n| `trace.log` | Detailed function traces with timing | `root.all.trace` |\n| `global.log` | System-level operation logs | `root.all` |\n| `all.log` | Comprehensive combined logs | `root.all` |\n\nSources: [src/utils/log_decorator.py:292-306]()\n\n#### 3. Execution Artifacts\n\nIf the query involves code execution with file output (plots, data files), these are saved to the working directory. The system outputs absolute paths:\n\n```\nå¾ççç»å¯¹è·¯å¾ï¼/path/to/workspace/prime_histogram_1.png\n```\n\nSources: [src/agent/deep_research.py:108]()\n\n---\n\n## Directory Structure After Setup\n\nAfter installation and first run, your directory structure will look like:\n\n```\nalgo_agent/\nâââ src/\nâ   âââ agent/\nâ   â   âââ deep_research.py      # Main entry point\nâ   âââ llm/                       # LLM integration\nâ   âââ action/                    # Tool dispatcher\nâ   âââ memory/                    # Context management\nâ   âââ tool/                      # Tool implementations\nâ   âââ utils/                     # Logging and utilities\nâââ logs/                          # Auto-created log directory\nâ   âââ print.log\nâ   âââ trace.log\nâ   âââ global.log\nâ   âââ all.log\nâââ pyproject.toml                 # Project dependencies\nâââ uv.lock                        # Locked dependencies\n```\n\n---\n\n## Tool Configuration\n\nBy default, the system loads the following tools:\n\n```python\ntools_schema_list = tool.schema.get_tools_schema([\n    tool.python_tool.ExecutePythonCodeTool,\n    # tool.todo_tool.RecursivePlanTreeTodoTool,  # Commented out\n])\n```\n\nThe `ExecutePythonCodeTool` is enabled by default, allowing the agent to write and execute Python code. The task planning tool (`RecursivePlanTreeTodoTool`) is commented out in the example but can be enabled for hierarchical task management.\n\nSources: [src/agent/deep_research.py:20-23]()\n\n---\n\n## Execution Mode Selection\n\nThe Python code execution tool supports three isolation strategies. The system automatically selects the appropriate mode based on the execution requirements:\n\n```mermaid\ngraph TB\n    CODE[\"Python Code Snippet\"] --> TOOL[\"ExecutePythonCodeTool\"]\n    TOOL --> DECISION{\"Select Execution Mode\"}\n    \n    DECISION -->|\"Full Isolation\"| SUBPROCESS[\"run_structured_in_subprocess()<br/>Process isolation<br/>Pipe-based IPC<br/>Crash protection\"]\n    \n    DECISION -->|\"Balanced\"| SUBTHREAD[\"run_structured_in_subthread()<br/>Thread isolation<br/>Shared memory<br/>Timeout support\"]\n    \n    DECISION -->|\"Fast\"| DIRECT[\"Direct execution<br/>In-process<br/>No isolation<br/>Fastest\"]\n    \n    SUBPROCESS --> RESULT[\"ExecutionResult\"]\n    SUBTHREAD --> RESULT\n    DIRECT --> RESULT\n    \n    RESULT --> STATUS[\"status: SUCCESS/FAILURE/TIMEOUT/CRASHED\"]\n    RESULT --> OUTPUT[\"stdout/stderr capture\"]\n    RESULT --> GLOBALS[\"out_globals_list<br/>Persisted variables\"]\n```\n\n**Diagram: Execution Mode Selection**\n\nThe tool selects execution modes based on isolation requirements and performance needs.\n\nSources: Inferred from architecture diagrams in the provided context\n\n---\n\n## Testing Your Setup\n\n### Test 1: Basic Code Execution\n\n```python\nfrom src.agent.deep_research import user_query\n\nuser_query(\"Write Python code to print 'Hello, algo_agent!'\")\n```\n\nExpected output:\n```\nç¨æ·è¾å¥ï¼ Write Python code to print 'Hello, algo_agent!'\nå·¥å· tool call è¾åºä¿¡æ¯ï¼ Execution completed successfully\næç»ç­æ¡ï¼ The code has been executed successfully...\n```\n\n### Test 2: Data Processing\n\n```python\nfrom src.agent.deep_research import user_query\n\nuser_query(\"\"\"\nCreate a pandas DataFrame with 10 random numbers,\ncalculate their mean and standard deviation,\nand display the results.\n\"\"\")\n```\n\nThis tests:\n- Python code generation\n- Library imports (pandas, numpy)\n- Statistical computation\n- Result formatting\n\n---\n\n## Common Setup Issues\n\n### Issue 1: Import Errors\n\n**Problem**: `ModuleNotFoundError: No module named 'src'`\n\n**Solution**: Ensure `PYTHONPATH` includes the project root:\n```bash\nexport PYTHONPATH=\"${PWD}:${PYTHONPATH}\"\n```\n\nOr install the package in development mode:\n```bash\npip install -e .\n```\n\n### Issue 2: API Key Not Found\n\n**Problem**: LLM calls fail with authentication errors\n\n**Solution**: Verify the `DASHSCOPE_API_KEY` environment variable is set:\n```bash\necho $DASHSCOPE_API_KEY\n```\n\n### Issue 3: Log Directory Permissions\n\n**Problem**: `PermissionError` when creating log files\n\n**Solution**: The logging system automatically creates the directory with `exist_ok=True`, but ensure write permissions:\n```bash\nchmod -R u+w logs/\n```\n\nSources: [src/utils/log_decorator.py:19-28]()\n\n---\n\n## Next Steps\n\nAfter successfully running your first query, you can:\n\n1. **Understand Agent Behavior** - Read [Agent Orchestration](#3) to learn how the agent processes queries and makes tool decisions\n2. **Explore Tool System** - See [Tool System](#4) for details on creating custom tools\n3. **Review Execution Modes** - Study [Execution Runtime](#5) to understand code isolation strategies\n4. **Analyze Logs** - Check [Observability and Logging](#6) for debugging and performance analysis\n5. **Try Complex Examples** - Explore [Use Cases and Examples](#7) for real-world scenarios\n\n---\n\n## Quick Reference: Key Code Entities\n\n| Entity | Location | Purpose |\n|--------|----------|---------|\n| `user_query()` | [src/agent/deep_research.py:15]() | Main entry point for queries |\n| `init_messages_with_system_prompt()` | memory module | Initialize conversation context |\n| `generate_assistant_output_append()` | llm module | Call LLM with tool schemas |\n| `call_tools_safely()` | action module | Dispatch and execute tools |\n| `ExecutePythonCodeTool` | tool.python_tool module | Execute Python code snippets |\n| `get_tools_schema()` | tool.schema module | Generate tool schemas for LLM |\n| `global_logger` | [src/utils/log_decorator.py:305]() | User-facing log output |\n| `setup_logger()` | [src/utils/log_decorator.py:19]() | Configure logging system |\n\nSources: [src/agent/deep_research.py:15-73](), [src/utils/log_decorator.py:19-63](), [src/utils/log_decorator.py:292-306]()\n\n---\n\n# Page: Agent Orchestration\n\n# Agent Orchestration\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/functino_call_err.design.md](docs/functino_call_err.design.md)\n- [src/agent/action.py](src/agent/action.py)\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/agent/llm.py](src/agent/llm.py)\n- [src/agent/memory.py](src/agent/memory.py)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document describes the core agent orchestration system that coordinates the execution of user queries. The orchestration layer is responsible for managing the conversation loop between the user, the LLM (Large Language Model), and available tools. It handles message flow, tool dispatch, context management, and error recovery.\n\nFor detailed information about specific subsystems:\n- Query processing mechanics: see [Query Processing Loop](#3.1)\n- LLM API integration details: see [LLM Integration](#3.2)\n- Conversation history management: see [Memory and Context Management](#3.3)\n- Tool execution coordination: see [Action Coordination](#3.4)\n- Tool implementations: see [Tool System](#4)\n\n## Architecture Overview\n\nThe agent orchestration system is centered around the `user_query` function in [src/agent/deep_research.py:15-74](), which coordinates four primary components: memory management, LLM integration, action coordination, and logging.\n\n```mermaid\ngraph TB\n    UserInput[\"User Input<br/>(user_query parameter)\"]\n    \n    subgraph \"Agent Orchestration Layer\"\n        UserQuery[\"user_query()<br/>src/agent/deep_research.py:15\"]\n        DecisionLoop[\"Agent Decision Loop<br/>lines 32-65\"]\n    end\n    \n    subgraph \"Core Components\"\n        Memory[\"memory.init_messages_with_system_prompt()<br/>src/agent/memory.py:5\"]\n        LLM[\"llm.generate_assistant_output_append()<br/>src/agent/llm.py:34\"]\n        Action[\"action.call_tools_safely()<br/>src/agent/action.py:10\"]\n        ToolSchema[\"tool.schema.get_tools_schema()<br/>src/agent/tool/schema.py\"]\n    end\n    \n    subgraph \"State Management\"\n        Messages[\"messages: list[dict]<br/>OpenAI format\"]\n        ToolInfo[\"tool_info: dict<br/>role, content, tool_call_id\"]\n    end\n    \n    subgraph \"Tools\"\n        PythonTool[\"ExecutePythonCodeTool\"]\n        TodoTool[\"RecursivePlanTreeTodoTool\"]\n    end\n    \n    subgraph \"Observability\"\n        GlobalLogger[\"global_logger<br/>src/utils/log_decorator.py:305\"]\n    end\n    \n    UserInput --> UserQuery\n    UserQuery --> Memory\n    Memory --> Messages\n    UserQuery --> ToolSchema\n    \n    UserQuery --> DecisionLoop\n    DecisionLoop --> LLM\n    LLM --> Messages\n    Messages --> LLM\n    \n    DecisionLoop --> Action\n    Action --> ToolInfo\n    ToolInfo --> PythonTool\n    ToolInfo --> TodoTool\n    PythonTool --> ToolInfo\n    TodoTool --> ToolInfo\n    ToolInfo --> Messages\n    \n    UserQuery --> GlobalLogger\n    DecisionLoop --> GlobalLogger\n    Action --> GlobalLogger\n    \n    style UserQuery fill:#f9f9f9\n    style DecisionLoop fill:#f9f9f9\n    style Messages fill:#f0f0f0\n```\n\n**Diagram: Agent Orchestration Architecture and Code Entities**\n\nSources: [src/agent/deep_research.py:1-74](), [src/agent/memory.py:1-17](), [src/agent/llm.py:1-51](), [src/agent/action.py:1-49]()\n\n## Core Components\n\nThe orchestration layer integrates four primary components:\n\n| Component | Module | Primary Function | Key Functions |\n|-----------|--------|------------------|---------------|\n| **Deep Research Agent** | `src/agent/deep_research.py` | Entry point and decision loop | `user_query()` |\n| **Memory Management** | `src/agent/memory.py` | Initialize and maintain conversation context | `init_messages_with_system_prompt()` |\n| **LLM Integration** | `src/agent/llm.py` | Interface with qwen-plus model via DashScope | `generate_assistant_output_append()`, `has_tool_call()`, `has_function_call()` |\n| **Action Coordinator** | `src/agent/action.py` | Dispatch tool calls and handle errors | `call_tools_safely()` |\n\n### Deep Research Agent\n\nThe `user_query` function at [src/agent/deep_research.py:15]() serves as the orchestration entry point. It initializes the conversation, manages the agent decision loop, and coordinates all interactions between the LLM and tools.\n\n### Memory Management\n\nMemory management is handled by `init_messages_with_system_prompt()` at [src/agent/memory.py:5-16](). It creates a message list with the system prompt from `prompt.react_system_prompt` and the initial user input, following the OpenAI message format.\n\n### LLM Integration\n\nThe LLM integration layer at [src/agent/llm.py:1-51]() provides three critical functions:\n- `generate_assistant_output_append()`: Calls the LLM and appends the response to messages\n- `has_tool_call()`: Checks if the response contains tool_calls\n- `has_function_call()`: Checks if the response contains function_call (legacy format)\n\n### Action Coordinator\n\nThe action coordinator at [src/agent/action.py:10-48]() provides safe tool execution through `call_tools_safely()`, which wraps tool dispatch in error handling and logs all execution attempts.\n\nSources: [src/agent/deep_research.py:15-74](), [src/agent/memory.py:5-16](), [src/agent/llm.py:34-49](), [src/agent/action.py:10-48]()\n\n## Orchestration Flow\n\nThe agent follows a continuous decision loop until the LLM produces a final answer without requesting tool execution:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant user_query as \"user_query()<br/>deep_research.py:15\"\n    participant memory as \"memory.init_messages_with_system_prompt()<br/>memory.py:5\"\n    participant messages as \"messages: list[dict]\"\n    participant llm as \"llm.generate_assistant_output_append()<br/>llm.py:34\"\n    participant has_tool_call as \"llm.has_tool_call()<br/>llm.py:44\"\n    participant action as \"action.call_tools_safely()<br/>action.py:10\"\n    participant tools as \"Tool Implementations\"\n    \n    User->>user_query: \"user_input string\"\n    user_query->>memory: \"user_input\"\n    memory->>messages: \"[system_prompt, user_input]\"\n    memory-->>user_query: \"messages\"\n    \n    user_query->>llm: \"(messages, tools_schema_list)\"\n    llm-->>messages: \"append assistant_output\"\n    llm-->>user_query: \"assistant_output: ChatCompletionMessage\"\n    \n    user_query->>has_tool_call: \"assistant_output\"\n    \n    alt No tool calls\n        has_tool_call-->>user_query: \"False\"\n        user_query->>User: \"assistant_output.content\"\n    else Has tool calls\n        has_tool_call-->>user_query: \"True\"\n        \n        loop For each tool_call\n            user_query->>action: \"tool_info dict\"\n            action->>tools: \"Dispatch by tool_call_name\"\n            tools-->>action: \"result or error\"\n            action-->>user_query: \"tool_info with content\"\n            user_query->>messages: \"append tool_info\"\n        end\n        \n        user_query->>llm: \"(messages, tools_schema_list)\"\n        Note over user_query,llm: Loop continues until no tool calls\n    end\n```\n\n**Diagram: Agent Decision Loop Execution Flow**\n\nThe flow consists of these stages:\n\n1. **Initialization** [src/agent/deep_research.py:19-23](): Initialize messages with system prompt and obtain tool schemas\n2. **First LLM Call** [src/agent/deep_research.py:26-29](): Generate initial assistant response\n3. **Decision Loop** [src/agent/deep_research.py:32-65](): \n   - Check for `tool_calls` or `function_call` in assistant output\n   - If present, execute tools and append results to messages\n   - Call LLM again with updated context\n   - Repeat until no tool calls are requested\n4. **Final Response** [src/agent/deep_research.py:73](): Log and return the final answer\n\nSources: [src/agent/deep_research.py:15-74](), [src/agent/llm.py:34-49](), [src/agent/action.py:10-21]()\n\n## Message Protocol\n\nThe agent uses OpenAI's message format to maintain conversation state. Messages are stored in a `list[dict]` structure where each message has specific required fields based on its role.\n\n### Message Structure\n\n```mermaid\ngraph LR\n    subgraph \"Message Types\"\n        System[\"role: 'system'<br/>content: react_system_prompt\"]\n        User[\"role: 'user'<br/>content: user_input\"]\n        Assistant[\"role: 'assistant'<br/>content: text response<br/>tool_calls: list or None\"]\n        Tool[\"role: 'tool'<br/>content: tool output<br/>tool_call_id: tracking id\"]\n        Function[\"role: 'function'<br/>content: tool output<br/>(legacy format)\"]\n    end\n    \n    Messages[\"messages: list[dict]\"]\n    \n    Messages --> System\n    Messages --> User\n    Messages --> Assistant\n    Messages --> Tool\n    Messages --> Function\n```\n\n**Diagram: Message Format and Role Types**\n\n### Message Roles\n\n| Role | Fields | Purpose | Code Location |\n|------|--------|---------|---------------|\n| `system` | `content`, `role` | System prompt defining agent behavior | [src/agent/memory.py:7-10]() |\n| `user` | `content`, `role` | User input or query | [src/agent/memory.py:11-14]() |\n| `assistant` | `content`, `role`, `tool_calls` (optional) | LLM response with potential tool requests | [src/agent/llm.py:36-40]() |\n| `tool` | `content`, `role`, `tool_call_id` | Tool execution result | [src/agent/deep_research.py:50-57]() |\n| `function` | `content`, `role` | Legacy function call result | [src/agent/deep_research.py:34-41]() |\n\n### Tool Call Structure\n\nWhen the LLM requests tool execution, the assistant message contains a `tool_calls` field. The agent extracts this information into a `tool_info` dictionary at [src/agent/deep_research.py:50-57]():\n\n```python\ntool_info = {\n    \"content\": \"\",  # Populated by tool execution\n    \"role\": \"tool\",\n    \"tool_call_id\": assistant_output.tool_calls[i].id,\n    # Non-required fields for dispatch\n    \"tool_call_name\": assistant_output.tool_calls[i].function.name,\n    \"tool_call_arguments\": assistant_output.tool_calls[i].function.arguments,\n}\n```\n\nThe system also supports the legacy `function_call` format at [src/agent/deep_research.py:34-41]() for backward compatibility.\n\nSources: [src/agent/deep_research.py:34-64](), [src/agent/memory.py:5-16](), [src/agent/llm.py:36-40]()\n\n## Tool Dispatch Mechanism\n\nThe agent determines which tool to execute by examining the `tool_call_name` field in the `tool_info` dictionary. The dispatch logic at [src/agent/action.py:11-21]() uses a conditional chain:\n\n```mermaid\ngraph TB\n    CallToolsSafely[\"call_tools_safely(tool_info)<br/>action.py:10\"]\n    ParseArgs[\"json.loads(tool_call_arguments)<br/>action.py:13\"]\n    CheckName[\"Check tool_call_name\"]\n    \n    ExecutePythonCode[\"ExecutePythonCodeTool(**arguments)<br/>action.py:15-17\"]\n    RecursiveTodo[\"RecursivePlanTreeTodoTool(**arguments)<br/>action.py:18-20\"]\n    \n    SetContent[\"tool_info['content'] = result\"]\n    ErrorHandler[\"Exception Handler<br/>traceback.format_exc()<br/>action.py:42-47\"]\n    \n    CallToolsSafely --> ParseArgs\n    ParseArgs --> CheckName\n    CheckName -->|\"== 'execute_python_code'\"| ExecutePythonCode\n    CheckName -->|\"== 'recursive_plan_tree_todo'\"| RecursiveTodo\n    \n    ExecutePythonCode --> SetContent\n    RecursiveTodo --> SetContent\n    \n    SetContent -.Exception.-> ErrorHandler\n    ErrorHandler --> SetContent\n```\n\n**Diagram: Tool Dispatch and Error Handling Logic**\n\nThe dispatch mechanism:\n1. Parses JSON arguments from `tool_call_arguments` [src/agent/action.py:13]()\n2. Matches `tool_call_name` against known tool names [src/agent/action.py:15-20]()\n3. Instantiates the tool class with parsed arguments\n4. Calls the tool's `run()` method and stores the result in `tool_info[\"content\"]`\n5. Returns the modified `tool_info` dictionary\n\nTool registration is performed at [src/agent/deep_research.py:20-23]() where tool schemas are provided to the LLM:\n\n```python\ntools_schema_list = tool.schema.get_tools_schema([\n    tool.python_tool.ExecutePythonCodeTool,\n    # tool.todo_tool.RecursivePlanTreeTodoTool,  # Can be enabled\n])\n```\n\nSources: [src/agent/action.py:10-48](), [src/agent/deep_research.py:20-23]()\n\n## Error Handling Strategy\n\nThe orchestration layer implements multiple levels of error handling to ensure robustness:\n\n### Tool Execution Errors\n\nThe `call_tools_safely()` function at [src/agent/action.py:40-47]() wraps all tool dispatch in a try-except block:\n\n```python\ntry:\n    return call_tools(tool_info)\nexcept Exception as e:\n    error_msg = traceback.format_exc()\n    global_logger.error(f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\", exc_info=True)\n    tool_info[\"content\"] = f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\"\n    return tool_info\n```\n\nWhen a tool fails:\n1. The full traceback is captured using `traceback.format_exc()`\n2. The error is logged to `global_logger`\n3. The error message is placed in `tool_info[\"content\"]`\n4. The modified `tool_info` is returned to the agent\n5. The error message becomes part of the conversation context\n6. The LLM receives the error and can attempt recovery\n\n### LLM Integration Errors\n\nLLM API errors can occur due to invalid message formats, as documented in [docs/functino_call_err.design.md:8-27](). The most common error is:\n\n```\nBadRequestError: An assistant message with \"tool_calls\" must be followed by \ntool messages responding to each \"tool_call_id\"\n```\n\nThis occurs when the message sequence is malformed. The agent ensures correctness by:\n1. Always appending tool results immediately after tool calls [src/agent/deep_research.py:46, 64]()\n2. Maintaining proper role sequencing (assistant â tool â assistant)\n3. Including the correct `tool_call_id` for each tool response [src/agent/deep_research.py:53]()\n\n### Logging Infrastructure\n\nAll orchestration activities are logged through `global_logger` defined at [src/utils/log_decorator.py:305-306](). Key logging points:\n\n| Event | Location | Log Level |\n|-------|----------|-----------|\n| User input received | [src/agent/deep_research.py:17]() | INFO |\n| No tool call needed | [src/agent/deep_research.py:28]() | INFO |\n| Tool output received | [src/agent/deep_research.py:44, 62]() | INFO |\n| LLM round output | [src/agent/deep_research.py:68-72]() | INFO |\n| Final answer | [src/agent/deep_research.py:73]() | INFO |\n\nThe `@traceable` decorator from [src/utils/log_decorator.py:297-302]() is applied to key functions (`init_messages_with_system_prompt`, `generate_assistant_output_append`, `call_tools_safely`) to capture detailed execution traces including arguments, return values, timing, and stack traces.\n\nSources: [src/agent/action.py:40-47](), [src/agent/deep_research.py:17-73](), [src/utils/log_decorator.py:297-306](), [docs/functino_call_err.design.md:8-27]()\n\n## Configuration\n\nThe orchestration system is configured through two primary mechanisms:\n\n### Tool Registration\n\nTools are registered at [src/agent/deep_research.py:20-23]() by passing tool classes to `get_tools_schema()`:\n\n```python\ntools_schema_list = tool.schema.get_tools_schema([\n    tool.python_tool.ExecutePythonCodeTool,\n    # tool.todo_tool.RecursivePlanTreeTodoTool,\n])\n```\n\nTools can be enabled or disabled by commenting/uncommenting lines in this list.\n\n### LLM Configuration\n\nThe LLM client is configured at [src/agent/llm.py:8-11]():\n\n```python\nclient = openai.OpenAI(\n    api_key=model_api_key,\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\n```\n\nModel selection occurs at [src/agent/llm.py:16]():\n\n```python\nmodel=\"qwen-plus\"\n```\n\nAdditional LLM parameters are set at [src/agent/llm.py:19-22]():\n- `tools`: Tool schema list for function calling\n- `function_call`: None (disabled)\n- `parallel_tool_calls`: True (enables parallel tool execution)\n\nSources: [src/agent/deep_research.py:20-23](), [src/agent/llm.py:8-22]()\n\n---\n\n# Page: Query Processing Loop\n\n# Query Processing Loop\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document explains the core agent decision loop implemented in the `user_query` function that processes user queries from start to finish. The loop orchestrates the interaction between the user, the LLM, and available tools until a final answer is produced.\n\nFor information about how the LLM is called and responses are parsed, see [LLM Integration](#3.2). For details on memory and context management, see [Memory and Context Management](#3.3). For tool execution specifics, see [Action Coordination](#3.4).\n\n## Architecture Overview\n\nThe query processing loop operates as a state machine that alternates between LLM reasoning and tool execution phases. The loop maintains conversational state in a `messages` list and continues until the LLM decides no further tool calls are needed.\n\n```mermaid\ngraph TB\n    START[\"user_query(user_input)\"]\n    INIT[\"Initialize messages<br/>memory.init_messages_with_system_prompt()\"]\n    SCHEMA[\"Get tool schemas<br/>tool.schema.get_tools_schema()\"]\n    LLM[\"Call LLM<br/>llm.generate_assistant_output_append()\"]\n    CHECK{\"Check tool calls<br/>llm.has_tool_call() or<br/>llm.has_function_call()\"}\n    DISPATCH[\"Extract tool_info dict<br/>action.call_tools_safely()\"]\n    APPEND[\"Append tool result<br/>to messages list\"]\n    RETURN[\"Return final answer<br/>assistant_output.content\"]\n    \n    START --> INIT\n    INIT --> SCHEMA\n    SCHEMA --> LLM\n    LLM --> CHECK\n    CHECK -->|\"No tool calls\"| RETURN\n    CHECK -->|\"Has tool calls\"| DISPATCH\n    DISPATCH --> APPEND\n    APPEND --> LLM\n```\n\n**Diagram: Main Query Processing Flow**\n\nSources: [src/agent/deep_research.py:15-74]()\n\n## Entry Point and Initialization\n\nThe `user_query` function serves as the main entry point for all query processing. It accepts a `user_input` string and orchestrates the entire interaction lifecycle.\n\n### Function Signature\n\n```python\ndef user_query(user_input):\n```\n\n### Initialization Steps\n\nThe function performs three critical initialization steps before entering the main loop:\n\n| Step | Function Call | Purpose |\n|------|--------------|---------|\n| 1. Log user input | `global_logger.info()` | Record incoming query for observability |\n| 2. Initialize messages | `memory.init_messages_with_system_prompt(user_input)` | Create message list with system prompt and user query |\n| 3. Prepare tool schemas | `tool.schema.get_tools_schema([...])` | Generate JSON schemas for available tools |\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant user_query\n    participant memory\n    participant tool_schema\n    participant global_logger\n    \n    User->>user_query: user_input string\n    user_query->>global_logger: Log user input\n    user_query->>memory: init_messages_with_system_prompt(user_input)\n    memory-->>user_query: messages list\n    user_query->>tool_schema: get_tools_schema([ExecutePythonCodeTool, ...])\n    tool_schema-->>user_query: tools_schema_list\n```\n\n**Diagram: Initialization Sequence**\n\nThe `messages` list follows OpenAI's chat format with roles: `system`, `user`, `assistant`, `tool`, and `function`. The initial list contains two messages:\n\n1. **System message**: Contains `react_system_prompt` defining agent behavior\n2. **User message**: Contains the actual user query\n\nSources: [src/agent/deep_research.py:15-24](), [logs/global.log:57-181]()\n\n## Main Decision Loop Structure\n\nThe core of the query processing system is a `while` loop that continues executing until the LLM produces a response without tool calls. This represents the agent's iterative reasoning process.\n\n### Loop Condition\n\n```python\nwhile llm.has_tool_call(assistant_output) or llm.has_function_call(assistant_output):\n```\n\nThe loop checks two conditions because the system supports both:\n- **tool_calls** format (OpenAI's newer function calling format)\n- **function_call** format (legacy format for backward compatibility)\n\n### First LLM Call (Outside Loop)\n\nBefore entering the loop, one LLM call is made to determine if tools are needed at all:\n\n```python\nassistant_output: ChatCompletionMessage = llm.generate_assistant_output_append(messages, tools_schema_list)\nif not llm.has_tool_call(assistant_output) and not llm.has_function_call(assistant_output):\n    global_logger.info(f\"æ éè°ç¨å·¥å·ï¼æå¯ä»¥ç´æ¥åå¤ï¼{assistant_output.content}\")\n    return\n```\n\nThis early exit optimization avoids entering the loop for queries that don't require tool usage.\n\n```mermaid\nstateDiagram-v2\n    [*] --> InitialLLMCall\n    InitialLLMCall --> CheckToolCalls\n    CheckToolCalls --> DirectAnswer: No tool calls\n    CheckToolCalls --> EnterLoop: Has tool calls\n    DirectAnswer --> [*]\n    EnterLoop --> ProcessToolCall\n    ProcessToolCall --> SubsequentLLMCall\n    SubsequentLLMCall --> CheckLoopCondition\n    CheckLoopCondition --> ProcessToolCall: Still has tool calls\n    CheckLoopCondition --> FinalAnswer: No more tool calls\n    FinalAnswer --> [*]\n```\n\n**Diagram: Loop State Machine**\n\nSources: [src/agent/deep_research.py:25-32](), [logs/global.log:893-900]()\n\n## Tool Call Detection and Extraction\n\nThe system handles two different tool calling formats from the LLM. Each format requires different extraction logic.\n\n### Format 1: function_call (Legacy)\n\n```python\nif llm.has_function_call(assistant_output):\n    tool_info = {\n        \"content\": \"\",\n        \"role\": \"function\",\n        \"tool_call_id\": \"\",\n        \"tool_call_name\": assistant_output.function_call.name,\n        \"tool_call_arguments\": assistant_output.function_call.arguments,\n    }\n```\n\n### Format 2: tool_calls (Current)\n\n```python\nif llm.has_tool_call(assistant_output):\n    for i in range(len(assistant_output.tool_calls)):\n        tool_info = {\n            \"content\": \"\",\n            \"role\": \"tool\",\n            \"tool_call_id\": assistant_output.tool_calls[i].id,\n            \"tool_call_name\": assistant_output.tool_calls[i].function.name,\n            \"tool_call_arguments\": assistant_output.tool_calls[i].function.arguments,\n        }\n```\n\n### tool_info Dictionary Structure\n\n| Field | Type | Purpose |\n|-------|------|---------|\n| `content` | string | Initially empty, filled with tool output after execution |\n| `role` | string | Either `\"tool\"` or `\"function\"` depending on format |\n| `tool_call_id` | string | Unique identifier for tracking tool calls |\n| `tool_call_name` | string | Name of the tool to invoke (e.g., `\"execute_python_code\"`) |\n| `tool_call_arguments` | string | JSON string containing tool parameters |\n\n```mermaid\ngraph LR\n    ASST[\"assistant_output<br/>ChatCompletionMessage\"]\n    CHECK_FC{\"has_function_call()\"}\n    CHECK_TC{\"has_tool_call()\"}\n    EXTRACT_FC[\"Extract from<br/>function_call field\"]\n    EXTRACT_TC[\"Extract from<br/>tool_calls list\"]\n    TOOL_INFO[\"tool_info dict\"]\n    \n    ASST --> CHECK_FC\n    ASST --> CHECK_TC\n    CHECK_FC -->|True| EXTRACT_FC\n    CHECK_TC -->|True| EXTRACT_TC\n    EXTRACT_FC --> TOOL_INFO\n    EXTRACT_TC --> TOOL_INFO\n```\n\n**Diagram: Tool Call Extraction Logic**\n\nSources: [src/agent/deep_research.py:33-57](), [logs/global.log:908-1125]()\n\n## Tool Execution Path\n\nOnce a `tool_info` dictionary is constructed, it is passed to the action coordinator for safe execution. The coordinator handles dispatch, error handling, and result capture.\n\n### Execution Flow\n\n```mermaid\nsequenceDiagram\n    participant Loop as \"Query Loop\"\n    participant action as \"action.call_tools_safely()\"\n    participant tool as \"Tool Instance\"\n    participant global_logger\n    \n    Loop->>action: call_tools_safely(tool_info)\n    action->>tool: Dispatch to specific tool.run()\n    tool-->>action: Execution result\n    action->>tool_info: Update content field\n    action-->>Loop: Modified tool_info\n    Loop->>global_logger: Log tool output\n    Loop->>messages: Append tool_info to list\n```\n\n**Diagram: Tool Execution Sequence**\n\n### Code Implementation\n\nThe loop processes both `function_call` and `tool_calls` formats:\n\n**For function_call format:**\n```python\naction.call_tools_safely(tool_info)\ntool_output = tool_info[\"content\"]\nglobal_logger.info(f\"å·¥å· function call è¾åºä¿¡æ¯ï¼ {tool_output}\\n\")\nglobal_logger.info(\"-\" * 60)\nmessages.append(tool_info)\n```\n\n**For tool_calls format:**\n```python\nfor i in range(len(assistant_output.tool_calls)):\n    # ... extract tool_info ...\n    action.call_tools_safely(tool_info)\n    tool_output = tool_info[\"content\"]\n    global_logger.info(f\"å·¥å· tool call è¾åºä¿¡æ¯ï¼ {tool_output}\\n\")\n    global_logger.info(\"-\" * 60)\n    messages.append(tool_info)\n```\n\nThe key insight is that `action.call_tools_safely()` **mutates** the `tool_info` dictionary by filling its `\"content\"` field with the execution result. This modified dictionary is then appended to the `messages` list.\n\nSources: [src/agent/deep_research.py:42-64](), [logs/utils.log:59-62]()\n\n## Message Accumulation and Context\n\nThe `messages` list serves as the cumulative conversation history that provides context to the LLM on each iteration. Understanding its growth pattern is crucial.\n\n### Message List Growth Pattern\n\n```mermaid\ngraph TD\n    INIT[\"messages = [<br/>  {role: 'system', content: ...},<br/>  {role: 'user', content: user_input}<br/>]\"]\n    \n    LLM1[\"LLM Call 1\"]\n    ASST1[\"Append assistant message<br/>(via generate_assistant_output_append)\"]\n    TOOL1[\"Append tool result message<br/>{role: 'tool', content: result, ...}\"]\n    \n    LLM2[\"LLM Call 2\"]\n    ASST2[\"Append assistant message\"]\n    TOOL2[\"Append tool result message\"]\n    \n    LLMN[\"LLM Call N\"]\n    ASSTN[\"Append assistant message<br/>(no tool_calls)\"]\n    FINAL[\"Final messages list\"]\n    \n    INIT --> LLM1\n    LLM1 --> ASST1\n    ASST1 --> TOOL1\n    TOOL1 --> LLM2\n    LLM2 --> ASST2\n    ASST2 --> TOOL2\n    TOOL2 --> LLLM[\"...\"]\n    LLLM --> LLMN\n    LLMN --> ASSTN\n    ASSTN --> FINAL\n```\n\n**Diagram: Message List Evolution**\n\n### Message Roles and Structure\n\nEach message in the list has a specific role that determines its purpose:\n\n| Role | When Added | Content |\n|------|-----------|---------|\n| `system` | Initialization | System prompt defining agent behavior |\n| `user` | Initialization | User's original query |\n| `assistant` | After each LLM call | LLM response (may include `tool_calls` field) |\n| `tool` | After tool execution | Tool output and metadata |\n| `function` | After tool execution (legacy) | Tool output for function_call format |\n\n### Example Message Sequence\n\nFrom the logs, we can see a typical message sequence:\n\n**Iteration 1:**\n```\nRound 1:\n- messages[0]: {role: \"system\", content: \"ä½ æ¯ä¸ä¸ªèµæ·±çæ·±åº¦ç ç©¶ä¸å®¶...\"}\n- messages[1]: {role: \"user\", content: \"ä½ æå¨çå·¥ä½è·¯å¾ä¸é¢...\"}\n- messages[2]: {role: \"assistant\", tool_calls: [...]}  # Added by generate_assistant_output_append\n- messages[3]: {role: \"tool\", content: \"...\", tool_call_id: \"...\"}  # Added after tool execution\n```\n\n**Iteration 2:**\n```\nRound 2:\n- messages[0-3]: (previous messages)\n- messages[4]: {role: \"assistant\", content: \"...\", tool_calls: [...]}  # Next LLM call\n- messages[5]: {role: \"tool\", content: \"...\", tool_call_id: \"...\"}  # Next tool result\n```\n\nThe function `llm.generate_assistant_output_append()` automatically appends the assistant's message to the list before returning it.\n\nSources: [src/agent/deep_research.py:19-65](), [logs/global.log:115-181](), [logs/utils.log:56-63]()\n\n## Loop Termination and Final Answer\n\nThe loop terminates when the LLM produces an `assistant_output` that contains no `tool_calls` and no `function_call`. This signals that the LLM has determined it has sufficient information to provide a final answer.\n\n### Termination Logic\n\n```python\n# After each LLM call in the loop\nassistant_output = llm.generate_assistant_output_append(messages, tools_schema_list)\n\n# Null content safety check\nif assistant_output.content is None:\n    assistant_output.content = \"\"\n\n# Loop condition check happens automatically\n# If no tool calls, loop exits\n\n# After loop exits\nglobal_logger.info(f\"æç»ç­æ¡ï¼ {assistant_output.content}\")\n```\n\n### Logging and Output\n\nThroughout the loop, the system logs extensive information for debugging:\n\n**During loop iterations:**\n```python\nglobal_logger.info(\n    f\"\"\"ç¬¬{len(messages) // 2}è½®å¤§æ¨¡åè¾åºä¿¡æ¯ï¼ \n\\n\\nassistant_output.content:: \\n\\n {pprint.pformat(assistant_output.content)}\n\\n\\nassistant_output.tool_calls::\\n\\n {pprint.pformat([toolcall.model_dump() for toolcall in assistant_output.tool_calls] if assistant_output.tool_calls else [])}\\n\"\"\"\n)\n```\n\nThis log entry includes:\n- Round number (estimated as `len(messages) // 2`)\n- Full assistant content\n- Any tool calls present\n\n```mermaid\nflowchart TD\n    LOOP_ITER[\"Loop Iteration\"]\n    CALL_LLM[\"llm.generate_assistant_output_append()\"]\n    CHECK_NULL{\"assistant_output.content<br/>is None?\"}\n    SET_EMPTY[\"Set content = ''\"]\n    LOG_ROUND[\"Log round number and output\"]\n    CHECK_TOOLS{\"has_tool_call() or<br/>has_function_call()?\"}\n    EXEC_TOOLS[\"Execute tools\"]\n    LOG_FINAL[\"Log final answer\"]\n    EXIT[\"Return from user_query()\"]\n    \n    LOOP_ITER --> CALL_LLM\n    CALL_LLM --> CHECK_NULL\n    CHECK_NULL -->|Yes| SET_EMPTY\n    CHECK_NULL -->|No| LOG_ROUND\n    SET_EMPTY --> LOG_ROUND\n    LOG_ROUND --> CHECK_TOOLS\n    CHECK_TOOLS -->|Yes| EXEC_TOOLS\n    CHECK_TOOLS -->|No| LOG_FINAL\n    EXEC_TOOLS --> LOOP_ITER\n    LOG_FINAL --> EXIT\n```\n\n**Diagram: Loop Termination Flow**\n\nSources: [src/agent/deep_research.py:65-74](), [logs/utils.log:62-64]()\n\n## Complete Loop Walkthrough\n\nTo solidify understanding, here's a complete trace of the loop with actual code references:\n\n### Phase 1: Setup\n- [Line 15-17](): Log user input\n- [Line 19](): Call `memory.init_messages_with_system_prompt(user_input)` â returns `messages` list\n- [Line 20-23](): Call `tool.schema.get_tools_schema([tool classes])` â returns `tools_schema_list`\n\n### Phase 2: First LLM Call\n- [Line 26](): Call `llm.generate_assistant_output_append(messages, tools_schema_list)` â returns `assistant_output`\n  - This function appends the assistant's message to `messages` automatically\n- [Line 27-29](): Check if no tool calls, if true, log and return early\n\n### Phase 3: Tool Execution Loop\n- [Line 32](): While `has_tool_call()` or `has_function_call()`:\n  - [Line 33-46](): If `function_call` format:\n    - Extract `tool_info` dictionary\n    - Call `action.call_tools_safely(tool_info)` (mutates `tool_info[\"content\"]`)\n    - Log tool output\n    - Append `tool_info` to `messages`\n  - [Line 47-64](): If `tool_calls` format:\n    - Iterate through `assistant_output.tool_calls` list\n    - For each tool call, extract `tool_info` dictionary\n    - Call `action.call_tools_safely(tool_info)` (mutates `tool_info[\"content\"]`)\n    - Log tool output\n    - Append `tool_info` to `messages`\n  - [Line 65](): Call `llm.generate_assistant_output_append(messages, tools_schema_list)` again\n  - [Line 66-67](): Safety check for null content\n  - [Line 68-72](): Log this round's output\n  - Loop continues or exits based on tool call presence\n\n### Phase 4: Completion\n- [Line 73](): Log final answer from `assistant_output.content`\n- Function returns (implicitly)\n\n```mermaid\ngraph TB\n    subgraph \"Phase 1: Setup\"\n        S1[\"Log input\"]\n        S2[\"init_messages_with_system_prompt()\"]\n        S3[\"get_tools_schema()\"]\n        S1 --> S2 --> S3\n    end\n    \n    subgraph \"Phase 2: First LLM Call\"\n        L1[\"generate_assistant_output_append()\"]\n        L2{\"No tool calls?\"}\n        L3[\"Return early\"]\n        L1 --> L2\n        L2 -->|Yes| L3\n    end\n    \n    subgraph \"Phase 3: Loop (while tool calls exist)\"\n        LOOP1[\"Extract tool_info<br/>(function_call or tool_calls)\"]\n        LOOP2[\"call_tools_safely()<br/>(mutates tool_info)\"]\n        LOOP3[\"Append tool_info to messages\"]\n        LOOP4[\"generate_assistant_output_append()\"]\n        LOOP5[\"Log round output\"]\n        LOOP1 --> LOOP2 --> LOOP3 --> LOOP4 --> LOOP5\n        LOOP5 -.->|has tool calls| LOOP1\n    end\n    \n    subgraph \"Phase 4: Completion\"\n        F1[\"Log final answer\"]\n        F2[\"Return\"]\n        F1 --> F2\n    end\n    \n    S3 --> L1\n    L2 -->|No| LOOP1\n    LOOP5 -.->|no tool calls| F1\n```\n\n**Diagram: Complete Loop Phases**\n\nSources: [src/agent/deep_research.py:15-74]()\n\n## Error Handling and Edge Cases\n\nWhile the main loop structure is straightforward, several edge cases are handled:\n\n### Null Content Handling\n\n```python\nif assistant_output.content is None:\n    assistant_output.content = \"\"\n```\n\nThis prevents errors when the LLM returns no textual content (e.g., when only tool calls are present).\n\n### Both Formats Support\n\nThe loop explicitly checks and handles both `function_call` and `tool_calls` formats within the same iteration, supporting mixed-format responses if the LLM provider sends them.\n\n### Multiple Tool Calls per Round\n\nThe `tool_calls` format allows multiple tools to be called in a single LLM response. The loop handles this with:\n\n```python\nfor i in range(len(assistant_output.tool_calls)):\n```\n\nEach tool result is appended to `messages` separately, maintaining proper ordering for the LLM to understand.\n\nSources: [src/agent/deep_research.py:33-74]()\n\n## Integration with Other Components\n\nThe query processing loop integrates with several other system components:\n\n| Component | Integration Point | Purpose |\n|-----------|-------------------|---------|\n| Memory module | `memory.init_messages_with_system_prompt()` | Initialize conversation context |\n| LLM module | `llm.generate_assistant_output_append()` | Generate responses |\n| LLM module | `llm.has_tool_call()` / `llm.has_function_call()` | Detect tool usage |\n| Tool schema | `tool.schema.get_tools_schema()` | Provide tool definitions to LLM |\n| Action coordinator | `action.call_tools_safely()` | Execute tools with error handling |\n| Logging | `global_logger.info()` | Record execution trace |\n\nFor detailed information about each integration:\n- LLM integration specifics: [LLM Integration](#3.2)\n- Message initialization and context: [Memory and Context Management](#3.3)\n- Tool execution and error handling: [Action Coordination](#3.4)\n- Individual tool implementations: [Tool System](#4)\n\nSources: [src/agent/deep_research.py:1-13](), [src/agent/deep_research.py:15-74]()\n\n---\n\n# Page: LLM Integration\n\n# LLM Integration\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/agent/llm.py](src/agent/llm.py)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document describes how the algo_agent system integrates with Large Language Models (LLMs) to generate intelligent responses and coordinate tool executions. The LLM serves as the reasoning engine that interprets user queries, decides which tools to invoke, and synthesizes final answers.\n\nThe system uses the **qwen-plus** model hosted on Alibaba's DashScope platform, accessed through OpenAI-compatible APIs. This integration handles message formatting, tool schema injection, response parsing, and function calling protocols.\n\nFor information about how the agent processes queries and manages the decision loop, see [Query Processing Loop](#3.1). For details on conversation state management, see [Memory and Context Management](#3.3). For tool execution orchestration, see [Action Coordination](#3.4).\n\n---\n\n## API Configuration\n\nThe LLM client is initialized in [src/agent/llm.py:8-11]() using the OpenAI SDK with DashScope-specific configuration:\n\n```python\nclient = openai.OpenAI(\n    api_key=model_api_key,    \n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\n```\n\n| Configuration | Value | Purpose |\n|--------------|-------|---------|\n| **Model** | `qwen-plus` | Primary reasoning model |\n| **Base URL** | `https://dashscope.aliyuncs.com/compatible-mode/v1` | DashScope OpenAI-compatible endpoint |\n| **API Key** | From `secret.model_api_key` | Authentication token |\n| **Parallel Tool Calls** | `True` | Enables multiple tool invocations in single response |\n\nThe OpenAI-compatible interface allows seamless integration while using Alibaba's infrastructure.\n\n**Sources:** [src/agent/llm.py:1-11]()\n\n---\n\n## Core LLM Functions\n\nThe LLM integration module provides a layered abstraction with three primary functions:\n\n```mermaid\ngraph TB\n    Agent[\"Deep Research Agent<br/>deep_research.py\"]\n    \n    GenAppend[\"generate_assistant_output_append()<br/>Public Interface\"]\n    Extract[\"_extract_assistant_output_from_chat()<br/>Response Extraction\"]\n    GenCompletion[\"_generate_chat_completion()<br/>API Call\"]\n    \n    CheckTool[\"has_tool_call()<br/>Tool Detection\"]\n    CheckFunc[\"has_function_call()<br/>Function Detection\"]\n    \n    API[\"DashScope API<br/>qwen-plus model\"]\n    \n    Agent --> GenAppend\n    GenAppend --> Extract\n    Extract --> GenCompletion\n    GenCompletion --> API\n    \n    GenAppend --> CheckTool\n    GenAppend --> CheckFunc\n    \n    style GenAppend fill:#f9f9f9\n    style GenCompletion fill:#e8e8e8\n```\n\n### Function Hierarchy\n\n1. **`_generate_chat_completion(messages, tools_schema_list)`** [src/agent/llm.py:14-24]()\n   - Lowest level: Makes actual API call to DashScope\n   - Returns full `ChatCompletion` object\n   - Decorated with `@traceable` for detailed logging\n\n2. **`_extract_assistant_output_from_chat(messages, tools_schema_list)`** [src/agent/llm.py:27-31]()\n   - Extracts `ChatCompletionMessage` from completion response\n   - Returns only the assistant's message content\n\n3. **`generate_assistant_output_append(messages, tools_schema_list)`** [src/agent/llm.py:34-41]()\n   - Public interface used by agent\n   - Appends assistant response to messages list\n   - Ensures `content` field is never `None` (defaults to empty string)\n\n4. **`has_tool_call(assistant_output)`** [src/agent/llm.py:44-45]()\n   - Checks if response contains `tool_calls` field\n\n5. **`has_function_call(assistant_output)`** [src/agent/llm.py:48-49]()\n   - Checks if response contains legacy `function_call` field\n\n**Sources:** [src/agent/llm.py:14-49]()\n\n---\n\n## Message Format and Structure\n\nThe system uses the OpenAI chat completion message format. Messages maintain conversation state and include multiple roles:\n\n```mermaid\ngraph LR\n    subgraph \"Message Roles\"\n        System[\"system<br/>System prompt and instructions\"]\n        User[\"user<br/>User queries\"]\n        Assistant[\"assistant<br/>LLM responses + tool_calls\"]\n        Tool[\"tool<br/>Tool execution results\"]\n        Function[\"function<br/>Legacy function results\"]\n    end\n    \n    subgraph \"Message Flow\"\n        Init[\"init_messages_with_system_prompt()\"]\n        Messages[\"messages: list[dict]\"]\n        LLM[\"LLM Processing\"]\n        Append[\"append to messages\"]\n    end\n    \n    System --> Messages\n    User --> Messages\n    Messages --> LLM\n    LLM --> Assistant\n    Assistant --> Append\n    Tool --> Append\n    Function --> Append\n    Append --> Messages\n```\n\n### Message Structure\n\nEach message in the list follows this schema:\n\n| Field | Type | Description | Example Roles |\n|-------|------|-------------|---------------|\n| `role` | `str` | Message sender | `system`, `user`, `assistant`, `tool`, `function` |\n| `content` | `str` | Text content | User queries, assistant responses, tool outputs |\n| `tool_calls` | `list` | Tool invocation requests (optional) | Only in `assistant` messages |\n| `tool_call_id` | `str` | Unique identifier linking tool results (optional) | Only in `tool` messages |\n| `function_call` | `dict` | Legacy function call format (optional) | Only in `assistant` messages |\n\n### Example Message Sequence\n\nFrom [src/agent/deep_research.py:19-46](), a typical conversation flow:\n\n1. **System Message** - Initialized via `memory.init_messages_with_system_prompt(user_input)`\n2. **User Message** - Contains the original query\n3. **Assistant Message** - LLM response, may contain `tool_calls`\n4. **Tool Messages** - Results from tool executions, linked via `tool_call_id`\n5. **Assistant Message** - Final response after tool use\n\n**Sources:** [src/agent/deep_research.py:19-46](), [src/agent/llm.py:14-41]()\n\n---\n\n## LLM Call Flow\n\nThe following diagram shows the complete flow from agent query to LLM response:\n\n```mermaid\nsequenceDiagram\n    participant Agent as \"deep_research.py<br/>user_query()\"\n    participant GenAppend as \"llm.py<br/>generate_assistant_output_append()\"\n    participant Extract as \"llm.py<br/>_extract_assistant_output_from_chat()\"\n    participant GenComp as \"llm.py<br/>_generate_chat_completion()\"\n    participant API as \"DashScope API<br/>qwen-plus\"\n    \n    Agent->>Agent: \"init_messages_with_system_prompt()\"\n    Agent->>Agent: \"get_tools_schema()\"\n    \n    Agent->>GenAppend: \"generate_assistant_output_append(messages, tools_schema_list)\"\n    GenAppend->>GenAppend: \"Log separator (60 dashes)\"\n    GenAppend->>Extract: \"_extract_assistant_output_from_chat(messages, tools_schema_list)\"\n    \n    Extract->>GenComp: \"_generate_chat_completion(messages, tools_schema_list)\"\n    Note over GenComp: \"@traceable decorator<br/>logs all parameters\"\n    \n    GenComp->>API: \"client.chat.completions.create()\"\n    Note over GenComp,API: \"model='qwen-plus'<br/>tools=tools_schema_list<br/>parallel_tool_calls=True\"\n    \n    API-->>GenComp: \"ChatCompletion object\"\n    GenComp-->>Extract: \"completion\"\n    \n    Extract->>Extract: \"completion.choices[0].message\"\n    Extract-->>GenAppend: \"ChatCompletionMessage\"\n    \n    GenAppend->>GenAppend: \"Ensure content != None\"\n    GenAppend->>GenAppend: \"messages.append(assistant_output)\"\n    GenAppend-->>Agent: \"assistant_output\"\n    \n    Agent->>Agent: \"Check has_tool_call() or has_function_call()\"\n```\n\n### Key Parameters\n\nFrom [src/agent/llm.py:15-23](), the `_generate_chat_completion` function passes these parameters:\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `model` | `\"qwen-plus\"` | Specifies the LLM model to use |\n| `messages` | `list[dict]` | Complete conversation history |\n| `tools` | `tools_schema_list` | Available tool schemas in OpenAI format |\n| `function_call` | `None` | Disables legacy function calling |\n| `parallel_tool_calls` | `True` | Allows multiple simultaneous tool invocations |\n\n**Sources:** [src/agent/llm.py:14-24](), [src/agent/deep_research.py:26-29]()\n\n---\n\n## Tool Calling Mechanism\n\nThe LLM can request tool executions through two formats: **modern tool_calls** and **legacy function_call**.\n\n### Tool Calls Format (Modern)\n\nWhen the LLM decides to use tools, it returns a `ChatCompletionMessage` with a `tool_calls` field:\n\n```mermaid\ngraph TB\n    subgraph \"Assistant Response\"\n        AssistMsg[\"ChatCompletionMessage<br/>role='assistant'\"]\n        ToolCalls[\"tool_calls: list\"]\n        Content[\"content: str or None\"]\n    end\n    \n    subgraph \"Tool Call Structure\"\n        TC1[\"ToolCall[0]\"]\n        TC2[\"ToolCall[1]\"]\n        TCN[\"ToolCall[n]\"]\n    end\n    \n    subgraph \"ToolCall Fields\"\n        ID[\"id: str<br/>'call_abc123'\"]\n        Type[\"type: 'function'\"]\n        Func[\"function\"]\n        FuncName[\"name: 'ExecutePythonCodeTool'\"]\n        FuncArgs[\"arguments: JSON string\"]\n    end\n    \n    AssistMsg --> ToolCalls\n    AssistMsg --> Content\n    \n    ToolCalls --> TC1\n    ToolCalls --> TC2\n    ToolCalls --> TCN\n    \n    TC1 --> ID\n    TC1 --> Type\n    TC1 --> Func\n    \n    Func --> FuncName\n    Func --> FuncArgs\n```\n\nFrom [src/agent/deep_research.py:47-64](), the agent processes tool_calls:\n\n```python\nif llm.has_tool_call(assistant_output):\n    for i in range(len(assistant_output.tool_calls)):\n        tool_info = {\n            \"content\": \"\",\n            \"role\": \"tool\",\n            \"tool_call_id\": assistant_output.tool_calls[i].id,\n            # Non-required fields\n            \"tool_call_name\": assistant_output.tool_calls[i].function.name,\n            \"tool_call_arguments\": assistant_output.tool_calls[i].function.arguments,\n        }\n        action.call_tools_safely(tool_info)\n        messages.append(tool_info)\n```\n\n### Function Call Format (Legacy)\n\nThe system also supports the older `function_call` format [src/agent/deep_research.py:33-46]():\n\n```python\nif llm.has_function_call(assistant_output):\n    tool_info = {\n        \"content\": \"\",\n        \"role\": \"function\",\n        \"tool_call_id\": \"\",\n        # Non-required fields\n        \"tool_call_name\": assistant_output.function_call.name,\n        \"tool_call_arguments\": assistant_output.function_call.arguments,\n    }\n```\n\n### Tool Invocation Decision Logic\n\nThe agent checks both formats in its decision loop [src/agent/deep_research.py:32-65]():\n\n```mermaid\ngraph TD\n    Start[\"Assistant Response Received\"]\n    \n    CheckToolCall{\"has_tool_call()?\"}\n    CheckFuncCall{\"has_function_call()?\"}\n    \n    ProcessTool[\"Process tool_calls array<br/>Multiple tools possible\"]\n    ProcessFunc[\"Process function_call<br/>Single function\"]\n    \n    NoTools[\"No tool execution needed<br/>Return final answer\"]\n    \n    CallAction[\"action.call_tools_safely(tool_info)\"]\n    AppendMsg[\"messages.append(tool_info)\"]\n    NextRound[\"Next LLM round\"]\n    \n    Start --> CheckToolCall\n    Start --> CheckFuncCall\n    \n    CheckToolCall -->|Yes| ProcessTool\n    CheckFuncCall -->|Yes| ProcessFunc\n    \n    CheckToolCall -->|No| CheckFuncCall\n    CheckFuncCall -->|No| NoTools\n    \n    ProcessTool --> CallAction\n    ProcessFunc --> CallAction\n    \n    CallAction --> AppendMsg\n    AppendMsg --> NextRound\n```\n\n**Sources:** [src/agent/deep_research.py:27-74](), [src/agent/llm.py:44-49]()\n\n---\n\n## Response Handling and Content Management\n\n### Null Content Handling\n\nThe system ensures that `assistant_output.content` is never `None` to prevent downstream errors:\n\nFrom [src/agent/llm.py:38-40]():\n```python\nif assistant_output.content is None:\n    assistant_output.content = \"\"\nmessages.append(assistant_output)\n```\n\nThis is also enforced in the agent loop [src/agent/deep_research.py:66-68]():\n```python\nif assistant_output.content is None:\n    assistant_output.content = \"\"\n```\n\n### Message Appending Strategy\n\nThe `generate_assistant_output_append` function automatically appends the assistant's response to the messages list [src/agent/llm.py:40](). This maintains conversation continuity and ensures the LLM has full context for subsequent rounds.\n\n### Multi-Round Conversation Flow\n\n```mermaid\ngraph TB\n    Start[\"User Query\"]\n    InitMsg[\"Initialize messages with<br/>system prompt + user query\"]\n    \n    LLMCall[\"LLM generates response\"]\n    \n    CheckDone{\"has_tool_call OR<br/>has_function_call?\"}\n    \n    ExecuteTools[\"Execute requested tools<br/>Append tool results to messages\"]\n    \n    FinalAnswer[\"Return final answer to user\"]\n    \n    Start --> InitMsg\n    InitMsg --> LLMCall\n    \n    LLMCall --> CheckDone\n    \n    CheckDone -->|Yes| ExecuteTools\n    CheckDone -->|No| FinalAnswer\n    \n    ExecuteTools --> LLMCall\n```\n\nThe loop continues [src/agent/deep_research.py:32-65]() until the LLM generates a response without tool calls, indicated by the final log statement [src/agent/deep_research.py:73]():\n```python\nglobal_logger.info(f\"æç»ç­æ¡ï¼ {assistant_output.content}\")\n```\n\n**Sources:** [src/agent/llm.py:34-41](), [src/agent/deep_research.py:26-73]()\n\n---\n\n## Observability and Tracing\n\n### Function Tracing\n\nThe `_generate_chat_completion` function is decorated with `@traceable` [src/agent/llm.py:13](), which provides comprehensive logging via the log_function decorator system [src/utils/log_decorator.py:297-302]().\n\nThis captures:\n- Function call parameters (messages, tools_schema_list)\n- Execution timing\n- Return values\n- Exception traces\n\nTraces are written to `logs/trace.log` [src/utils/log_decorator.py:296]().\n\n### Agent-Level Logging\n\nThe agent logs key events using `global_logger` [src/agent/deep_research.py:4]():\n\n| Log Point | Location | Purpose |\n|-----------|----------|---------|\n| User input | [src/agent/deep_research.py:17]() | Records incoming query |\n| No tool needed | [src/agent/deep_research.py:28]() | Direct response without tools |\n| Tool output | [src/agent/deep_research.py:44,62]() | Tool execution results |\n| Round info | [src/agent/deep_research.py:68-72]() | Assistant response + tool_calls details |\n| Final answer | [src/agent/deep_research.py:73]() | Conversation conclusion |\n\n### Log Separator Pattern\n\nThe LLM module uses visual separators [src/agent/llm.py:35]():\n```python\nglobal_logger.info(\"-\" * 60)\n```\n\nThis creates clear boundaries in log files, making it easier to trace individual LLM calls.\n\n**Sources:** [src/agent/llm.py:13-35](), [src/agent/deep_research.py:4,17,28,44,62,68-73](), [src/utils/log_decorator.py:296-306]()\n\n---\n\n## Tool Schema Injection\n\nThe agent provides available tools to the LLM via the `tools_schema_list` parameter. From [src/agent/deep_research.py:20-23]():\n\n```python\ntools_schema_list = tool.schema.get_tools_schema([\n    tool.python_tool.ExecutePythonCodeTool,\n    # tool.todo_tool.RecursivePlanTreeTodoTool,\n])\n```\n\nThis schema list is passed to every LLM call [src/agent/deep_research.py:26,65](), enabling the model to understand available capabilities and invoke them appropriately.\n\nThe schema format follows OpenAI's function calling specification, including:\n- Tool name\n- Tool description\n- Parameter schemas with types and descriptions\n- Required vs. optional parameters\n\nFor details on schema generation, see [Tool Schema Format](#9.3).\n\n**Sources:** [src/agent/deep_research.py:20-26]()\n\n---\n\n## Error Handling\n\n### API Call Tracing\n\nThe `@traceable` decorator on `_generate_chat_completion` [src/agent/llm.py:13]() automatically captures and logs exceptions from the DashScope API, including:\n- Network errors\n- Authentication failures\n- Rate limiting\n- Invalid request formats\n\nFrom [src/utils/log_decorator.py:233-257](), when exceptions occur:\n```python\nexcept Exception as e:\n    # Record exception with full traceback\n    logger.error(\n        f\"ãè°ç¨å¤±è´¥ã æ è·¯å¾ï¼ {stack_full_path} | èæ¶ï¼ {elapsed_time:.3f}ms \"\n        f\"| å¼å¸¸ä½ç½®ï¼ {module_name}.{class_name}.{func_name}:{exc_lineno} \"\n        f\"| å¼å¸¸ç±»åï¼ {exception_type} | å¼å¸¸ä¿¡æ¯ï¼ {exception_msg} | å æ ä¿¡æ¯ï¼ {traceback_str}\",\n        exc_info=True\n    )\n    raise  # Exception is re-raised\n```\n\n### Null Safety\n\nThe system implements defensive null checking:\n- `assistant_output.content` is never `None` [src/agent/llm.py:38-39]()\n- This prevents `NoneType` errors when processing responses\n- Empty string `\"\"` is used as the default value\n\n### Tool Call Validation\n\nBefore processing tool calls, the agent validates their presence:\n- `has_tool_call(assistant_output)` [src/agent/llm.py:44-45]()\n- `has_function_call(assistant_output)` [src/agent/llm.py:48-49]()\n\nThese simple boolean checks prevent attribute errors when accessing `tool_calls` or `function_call` fields that may be `None`.\n\n**Sources:** [src/agent/llm.py:38-49](), [src/utils/log_decorator.py:233-257]()\n\n---\n\n## Integration with Agent Components\n\nThe LLM module serves as the central reasoning component, interacting with multiple subsystems:\n\n```mermaid\ngraph TB\n    subgraph \"llm.py Module\"\n        LLMGen[\"generate_assistant_output_append()\"]\n        LLMCheck[\"has_tool_call()<br/>has_function_call()\"]\n    end\n    \n    subgraph \"Agent Components\"\n        Agent[\"deep_research.py<br/>user_query()\"]\n        Memory[\"memory.py<br/>init_messages_with_system_prompt()\"]\n        Action[\"action.py<br/>call_tools_safely()\"]\n        Schema[\"tool.schema.py<br/>get_tools_schema()\"]\n    end\n    \n    subgraph \"External Services\"\n        DashScope[\"DashScope API<br/>qwen-plus model\"]\n    end\n    \n    subgraph \"Logging\"\n        GlobalLog[\"global_logger<br/>logs/print.log\"]\n        TraceLog[\"traceable_logger<br/>logs/trace.log\"]\n    end\n    \n    Agent --> Memory\n    Agent --> Schema\n    \n    Memory --> Agent\n    Schema --> Agent\n    \n    Agent --> LLMGen\n    LLMGen --> DashScope\n    DashScope --> LLMGen\n    \n    LLMGen --> Agent\n    Agent --> LLMCheck\n    LLMCheck --> Agent\n    \n    Agent --> Action\n    \n    LLMGen --> GlobalLog\n    LLMGen --> TraceLog\n```\n\n### Component Interactions\n\n1. **Memory Management** - `memory.init_messages_with_system_prompt()` initializes conversation state (see [Memory and Context Management](#3.3))\n\n2. **Tool Schema** - `tool.schema.get_tools_schema()` provides tool definitions to the LLM (see [Tool Schema Format](#9.3))\n\n3. **Action Coordination** - `action.call_tools_safely()` executes tools requested by the LLM (see [Action Coordination](#3.4))\n\n4. **Logging** - Both `global_logger` and `@traceable` decorator provide comprehensive observability (see [Logging System Architecture](#6.1))\n\n**Sources:** [src/agent/deep_research.py:1-74](), [src/agent/llm.py:1-49]()\n\n---\n\n## Summary\n\nThe LLM Integration layer provides a clean abstraction over the DashScope API with these key features:\n\n| Feature | Implementation | Benefit |\n|---------|---------------|---------|\n| **OpenAI Compatibility** | Uses OpenAI SDK with DashScope endpoint | Standard interface, easy to swap models |\n| **Tool Calling** | Supports both `tool_calls` and `function_call` | Flexible integration, backward compatible |\n| **Parallel Execution** | `parallel_tool_calls=True` | Multiple tools in single round |\n| **Null Safety** | Automatic `None` to `\"\"` conversion | Prevents downstream errors |\n| **Comprehensive Logging** | `@traceable` + `global_logger` | Full observability of LLM interactions |\n| **Stateless Design** | Messages passed explicitly | Clear data flow, easy to debug |\n\nThe module's simplicity and clear separation of concerns make it easy to understand, maintain, and extend.\n\n**Sources:** [src/agent/llm.py:1-49](), [src/agent/deep_research.py:1-74]()\n\n---\n\n# Page: Memory and Context Management\n\n# Memory and Context Management\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/functino_call_err.design.md](docs/functino_call_err.design.md)\n- [src/agent/action.py](src/agent/action.py)\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/agent/memory.py](src/agent/memory.py)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page describes how the algo_agent system maintains conversation history, initializes system prompts, and manages context that is passed to the LLM. The memory system uses a messages list following OpenAI's chat completion format to track the entire conversation flow including user queries, assistant responses, and tool execution results.\n\nFor information about the broader query processing loop that uses this memory system, see [Query Processing Loop](#3.1). For details on how the LLM consumes these messages, see [LLM Integration](#3.2). For tool execution result handling, see [Action Coordination](#3.4).\n\n---\n\n## Core Concepts\n\nThe memory system is built around a single, append-only **messages list** that serves as the complete conversation history. This list is initialized at the start of each query and grows as the agent interacts with the LLM and tools. The system follows OpenAI's message format specification to ensure compatibility with the underlying LLM API.\n\n### Message List Structure\n\nThe `messages` list is a Python list of dictionaries, where each dictionary represents a single message in the conversation. The list maintains strict orderingâmessages appear in chronological order, and the LLM processes them sequentially to build context.\n\n**Core Message Roles:**\n\n| Role | Purpose | Required Fields | Created By |\n|------|---------|----------------|------------|\n| `system` | Sets the agent's behavior and instructions | `role`, `content` | `init_messages_with_system_prompt` |\n| `user` | Contains the user's query or input | `role`, `content` | `init_messages_with_system_prompt`, user input |\n| `assistant` | LLM's response, may include tool calls | `role`, `content`, `tool_calls` (optional) | `generate_assistant_output_append` |\n| `tool` | Result from tool execution (modern format) | `role`, `content`, `tool_call_id` | `call_tools_safely` |\n| `function` | Result from tool execution (legacy format) | `role`, `content`, `tool_call_id` | `call_tools_safely` |\n\nSources: [src/agent/deep_research.py:1-74](), [src/agent/memory.py:1-17]()\n\n---\n\n## System Initialization\n\n### Message Initialization with System Prompt\n\nThe entry point for memory management is the `init_messages_with_system_prompt` function, which creates the initial messages list with exactly two messages: a system message and a user message.\n\n```mermaid\ngraph TB\n    INPUT[\"user_input<br/>(user query string)\"]\n    FUNC[\"init_messages_with_system_prompt()\"]\n    SYSTEMPROMPT[\"react_system_prompt<br/>(imported from prompt module)\"]\n    \n    MSG1[\"Message 1<br/>role: system<br/>content: react_system_prompt\"]\n    MSG2[\"Message 2<br/>role: user<br/>content: user_input\"]\n    \n    OUTPUT[\"messages list<br/>[system_msg, user_msg]\"]\n    \n    INPUT --> FUNC\n    SYSTEMPROMPT --> FUNC\n    FUNC --> MSG1\n    FUNC --> MSG2\n    MSG1 --> OUTPUT\n    MSG2 --> OUTPUT\n    \n    style FUNC fill:#f9f9f9\n    style OUTPUT fill:#f9f9f9\n```\n\n**Function Implementation:**\n\nThe `init_messages_with_system_prompt` function is defined in [src/agent/memory.py:4-16]() as:\n\n```python\ndef init_messages_with_system_prompt(user_input: str) -> list[dict[str, str]]:\n    messages = [\n        {\n            \"content\": react_system_prompt,\n            \"role\": \"system\",\n        },\n        {\n            \"content\": user_input,\n            \"role\": \"user\",\n        }\n    ]\n    return messages\n```\n\n**Key Characteristics:**\n\n- **System Prompt Source**: Imported from `prompt.react_system_prompt` module (not shown in provided files)\n- **Return Type**: Returns a list of dictionaries with string keys and values\n- **Immutability**: Creates a new list each time; does not maintain global state\n- **Logging**: Decorated with `@traceable` for execution tracking\n\nSources: [src/agent/memory.py:1-17]()\n\n---\n\n## Message Flow and Lifecycle\n\n### Message Appending Pattern\n\nThroughout the agent's execution loop, messages are appended to the list using Python's `list.append()` method. The system follows a strict alternating pattern to maintain LLM API compatibility.\n\n```mermaid\nsequenceDiagram\n    participant Init as init_messages_with_system_prompt\n    participant MsgList as messages list\n    participant LLM as generate_assistant_output_append\n    participant Action as call_tools_safely\n    \n    Note over Init,MsgList: Initialization Phase\n    Init->>MsgList: Append system message (role=system)\n    Init->>MsgList: Append user message (role=user)\n    \n    Note over MsgList,LLM: First LLM Call\n    LLM->>MsgList: Append assistant message<br/>(role=assistant, may have tool_calls)\n    \n    Note over MsgList,Action: Tool Execution Phase (if tool_calls present)\n    loop For each tool_call\n        Action->>MsgList: Append tool result<br/>(role=tool, tool_call_id=xxx)\n    end\n    \n    Note over MsgList,LLM: Subsequent LLM Calls\n    LLM->>MsgList: Append assistant message\n    \n    alt More tool calls\n        loop For each tool_call\n            Action->>MsgList: Append tool result\n        end\n        LLM->>MsgList: Append assistant message\n    else No tool calls\n        Note over LLM: Loop terminates\n    end\n```\n\n**Append Locations in Code:**\n\n1. **Assistant Messages**: Appended within `generate_assistant_output_append` in the LLM module (not shown in provided files, but called at [src/agent/deep_research.py:26]())\n\n2. **Tool Result Messages**: Appended in the main loop at [src/agent/deep_research.py:46]() (function call format) and [src/agent/deep_research.py:64]() (tool call format)\n\nSources: [src/agent/deep_research.py:15-74]()\n\n---\n\n## Message Schema and Structure\n\n### Standard Message Fields\n\nEach message dictionary in the list follows a specific schema based on its role. Here's the detailed structure:\n\n#### System Message\n```python\n{\n    \"role\": \"system\",\n    \"content\": \"<system prompt text>\"\n}\n```\n\n#### User Message\n```python\n{\n    \"role\": \"user\",\n    \"content\": \"<user query text>\"\n}\n```\n\n#### Assistant Message (without tool calls)\n```python\n{\n    \"role\": \"assistant\",\n    \"content\": \"<assistant response text>\",\n    \"tool_calls\": None  # or not present\n}\n```\n\n#### Assistant Message (with tool calls)\n```python\n{\n    \"role\": \"assistant\",\n    \"content\": \"<optional reasoning text>\",\n    \"tool_calls\": [\n        {\n            \"id\": \"<tool_call_id>\",\n            \"function\": {\n                \"name\": \"<tool_name>\",\n                \"arguments\": \"<json_string>\"\n            }\n        }\n    ]\n}\n```\n\n#### Tool Result Message\n```python\n{\n    \"role\": \"tool\",\n    \"content\": \"<tool execution result>\",\n    \"tool_call_id\": \"<matching_id_from_assistant_message>\",\n    # Additional fields (not required by LLM API):\n    \"tool_call_name\": \"<tool_name>\",\n    \"tool_call_arguments\": \"<json_string>\"\n}\n```\n\nSources: [src/agent/deep_research.py:34-64](), [src/agent/action.py:10-21]()\n\n---\n\n## Tool Information Dictionary\n\n### Tool Info Structure\n\nDuring tool execution, the system uses a `tool_info` dictionary to track tool execution metadata. This structure bridges between the assistant's tool call and the resulting tool message.\n\n```mermaid\ngraph LR\n    subgraph \"Assistant Message\"\n        TOOLCALL[\"tool_calls[i]<br/>id, function.name, function.arguments\"]\n    end\n    \n    subgraph \"Tool Info Dictionary\"\n        TOOLINFO[\"tool_info dict<br/>role, tool_call_id, content<br/>tool_call_name, tool_call_arguments\"]\n    end\n    \n    subgraph \"Action Module\"\n        CALLTOOL[\"call_tools_safely()<br/>Executes tool, updates content\"]\n    end\n    \n    subgraph \"Messages List\"\n        TOOLMSG[\"Tool result message<br/>Appended to messages\"]\n    end\n    \n    TOOLCALL -->|\"Extract fields\"| TOOLINFO\n    TOOLINFO --> CALLTOOL\n    CALLTOOL -->|\"Update content field\"| TOOLINFO\n    TOOLINFO -->|\"Append\"| TOOLMSG\n```\n\n**Tool Info Creation for Modern Format (tool_calls):**\n\nAt [src/agent/deep_research.py:50-57](), the system creates:\n\n```python\ntool_info = {\n    \"content\": \"\",  # Empty initially, filled by tool execution\n    \"role\": \"tool\",\n    \"tool_call_id\": assistant_output.tool_calls[i].id,\n    # Additional tracking fields:\n    \"tool_call_name\": assistant_output.tool_calls[i].function.name,\n    \"tool_call_arguments\": assistant_output.tool_calls[i].function.arguments,\n}\n```\n\n**Tool Info Creation for Legacy Format (function_call):**\n\nAt [src/agent/deep_research.py:34-41](), for backward compatibility:\n\n```python\ntool_info = {\n    \"content\": \"\",\n    \"role\": \"function\",\n    \"tool_call_id\": \"\",  # Empty for function_call format\n    \"tool_call_name\": assistant_output.function_call.name,\n    \"tool_call_arguments\": assistant_output.function_call.arguments,\n}\n```\n\nSources: [src/agent/deep_research.py:32-64](), [src/agent/action.py:1-48]()\n\n---\n\n## Context Management Flow\n\n### Complete Context Pipeline\n\nThe following diagram shows how context flows from initialization through multiple agent loops:\n\n```mermaid\ngraph TB\n    subgraph \"Initialization\"\n        USERINPUT[\"User Input String\"]\n        INITMSG[\"init_messages_with_system_prompt()\"]\n        INITLIST[\"Initial messages list<br/>[system, user]\"]\n    end\n    \n    subgraph \"LLM Context Building\"\n        MSGLIST[\"messages list<br/>(growing over time)\"]\n        TOOLSCHEMA[\"tools_schema_list<br/>(available tools)\"]\n        LLMCONTEXT[\"LLM Context Package<br/>messages + tools\"]\n    end\n    \n    subgraph \"LLM Processing\"\n        LLM[\"generate_assistant_output_append()\"]\n        ASSISTMSG[\"assistant_output<br/>(ChatCompletionMessage)\"]\n    end\n    \n    subgraph \"Tool Execution Context\"\n        TOOLCALL[\"tool_calls extraction\"]\n        TOOLINFO[\"tool_info dict\"]\n        TOOLEXEC[\"call_tools_safely()\"]\n        TOOLRESULT[\"Tool execution result\"]\n    end\n    \n    subgraph \"Context Update\"\n        APPEND[\"messages.append()\"]\n        UPDATEDLIST[\"Updated messages list<br/>(includes new assistant & tool messages)\"]\n    end\n    \n    USERINPUT --> INITMSG\n    INITMSG --> INITLIST\n    INITLIST --> MSGLIST\n    \n    MSGLIST --> LLMCONTEXT\n    TOOLSCHEMA --> LLMCONTEXT\n    LLMCONTEXT --> LLM\n    LLM --> ASSISTMSG\n    \n    ASSISTMSG --> APPEND\n    ASSISTMSG --> TOOLCALL\n    TOOLCALL --> TOOLINFO\n    TOOLINFO --> TOOLEXEC\n    TOOLEXEC --> TOOLRESULT\n    TOOLRESULT --> TOOLINFO\n    TOOLINFO --> APPEND\n    \n    APPEND --> UPDATEDLIST\n    UPDATEDLIST --> MSGLIST\n    \n    style MSGLIST fill:#f9f9f9\n    style LLMCONTEXT fill:#f9f9f9\n```\n\n**Key Flow Points:**\n\n1. **Initialization**: [src/agent/deep_research.py:19]() - `messages = memory.init_messages_with_system_prompt(user_input)`\n2. **Tool Schema Addition**: [src/agent/deep_research.py:20-23]() - Tools schema is passed alongside messages but not stored in messages\n3. **LLM Call**: [src/agent/deep_research.py:26]() - `llm.generate_assistant_output_append(messages, tools_schema_list)`\n4. **Tool Result Append**: [src/agent/deep_research.py:46]() and [src/agent/deep_research.py:64]() - `messages.append(tool_info)`\n5. **Loop Continuation**: [src/agent/deep_research.py:65]() - Next LLM call uses updated messages list\n\nSources: [src/agent/deep_research.py:15-74]()\n\n---\n\n## Context Preservation Across Iterations\n\n### Stateless Function with Stateful Data\n\nThe memory management design follows a **stateless function, stateful data** pattern:\n\n- **Stateless**: The `init_messages_with_system_prompt` function and `call_tools_safely` functions don't maintain internal state\n- **Stateful**: The `messages` list accumulates all conversation history and is passed by reference\n\n```mermaid\ngraph LR\n    subgraph \"Iteration 1\"\n        MSG1[\"messages<br/>[system, user]\"]\n        LLM1[\"LLM Call 1\"]\n        MSG1A[\"messages<br/>[system, user, assistant1]\"]\n        TOOL1[\"Tool Call\"]\n        MSG1B[\"messages<br/>[..., assistant1, tool1]\"]\n    end\n    \n    subgraph \"Iteration 2\"\n        LLM2[\"LLM Call 2\"]\n        MSG2A[\"messages<br/>[..., tool1, assistant2]\"]\n        TOOL2[\"Tool Call\"]\n        MSG2B[\"messages<br/>[..., assistant2, tool2]\"]\n    end\n    \n    subgraph \"Iteration N\"\n        LLMN[\"LLM Call N\"]\n        MSGN[\"messages<br/>[..., assistantN]\"]\n        DONE[\"No tool calls<br/>Done\"]\n    end\n    \n    MSG1 --> LLM1\n    LLM1 --> MSG1A\n    MSG1A --> TOOL1\n    TOOL1 --> MSG1B\n    MSG1B --> LLM2\n    LLM2 --> MSG2A\n    MSG2A --> TOOL2\n    TOOL2 --> MSG2B\n    MSG2B -->|\"Loop continues\"| LLMN\n    LLMN --> MSGN\n    MSGN --> DONE\n    \n    style MSG1 fill:#f9f9f9\n    style MSG1B fill:#f9f9f9\n    style MSG2B fill:#f9f9f9\n    style MSGN fill:#f9f9f9\n```\n\n**Reference Passing:**\n\nThe messages list is passed by reference throughout the execution:\n- LLM module receives the list and appends assistant messages\n- Tool execution results are appended in the main loop\n- Each subsequent LLM call sees the accumulated history\n- No explicit state management or persistence layer is needed\n\nSources: [src/agent/deep_research.py:15-74]()\n\n---\n\n## Error Handling and Context Validation\n\n### Message Format Validation\n\nThe system relies on LLM API validation for message format correctness. When messages are malformed, the API returns specific error codes.\n\n**Common Error: Tool Call Response Mismatch**\n\nA critical error occurs when assistant messages with `tool_calls` are not followed by corresponding tool response messages. This is documented in [docs/functino_call_err.design.md:1-51]():\n\n```\nError code: 400 - An assistant message with \"tool_calls\" must be followed by \ntool messages responding to each \"tool_call_id\". The following tool_call_ids \ndid not have response messages: message[7].role\n```\n\n**Root Cause**: At [src/agent/deep_research.py:32-46](), if a message has `function_call` (legacy format), the system creates a role `\"function\"` message. However, if the LLM uses modern `tool_calls` format, the API expects role `\"tool\"` messages with matching `tool_call_id` values.\n\n**Prevention Strategy:**\n\nThe current code handles both formats:\n- Modern format: [src/agent/deep_research.py:47-64]() properly sets `role: \"tool\"` and `tool_call_id`\n- Legacy format: [src/agent/deep_research.py:32-46]() sets `role: \"function\"` (may cause issues if LLM sends `tool_calls`)\n\nSources: [docs/functino_call_err.design.md:1-51](), [src/agent/deep_research.py:32-64]()\n\n---\n\n## Logging and Observability\n\n### Memory Operations Tracing\n\nThe memory system integrates with the logging architecture to track all memory operations.\n\n**Logged Operations:**\n\n1. **Initialization**: The `init_messages_with_system_prompt` function is decorated with `@traceable` at [src/agent/memory.py:4]()\n2. **User Input**: Logged at [src/agent/deep_research.py:17]() with `global_logger.info`\n3. **Tool Outputs**: Logged at [src/agent/deep_research.py:44]() and [src/agent/deep_research.py:62]()\n4. **Assistant Outputs**: Logged at [src/agent/deep_research.py:68-72]() with detailed content and tool_calls\n\n**Log Decorator Behavior:**\n\nThe `@traceable` decorator (defined at [src/utils/log_decorator.py:297-302]()) automatically logs:\n- Function entry with all arguments (the `user_input` string)\n- Function exit with return value (the initialized messages list)\n- Execution time\n- Call stack for debugging\n\n**Log File Locations:**\n\nPer [src/utils/log_decorator.py:288-306]():\n- `logs/trace.log` - Detailed execution traces including `init_messages_with_system_prompt` calls\n- `logs/print.log` - User-facing logs including tool outputs and final answers\n- `logs/all.log` - Comprehensive logs of all operations\n\nSources: [src/agent/memory.py:1-17](), [src/agent/deep_research.py:15-74](), [src/utils/log_decorator.py:287-307]()\n\n---\n\n## Context Size and Memory Limits\n\n### Unbounded Growth Pattern\n\nThe current implementation uses an **unbounded append-only list**, meaning:\n\n- No automatic truncation or summarization of old messages\n- Full conversation history is sent to the LLM on every call\n- Context window limits are enforced by the LLM API, not the agent\n\n**Implications:**\n\n| Aspect | Behavior |\n|--------|----------|\n| Memory Usage | Grows linearly with conversation length |\n| Context Window | Limited by LLM API (model-specific, typically 8k-32k tokens) |\n| Token Costs | Increases with each iteration (full history resent) |\n| Truncation | Not implemented; will fail when exceeding API limits |\n\n**Potential Improvements** (not currently implemented):\n\n- Implement sliding window context retention\n- Add message summarization for long conversations\n- Implement token counting and automatic truncation\n- Use context compression techniques\n\nSources: [src/agent/deep_research.py:15-74](), [src/agent/memory.py:1-17]()\n\n---\n\n## Integration with Execution State\n\n### Relationship to Workspace State\n\nThe conversation memory (messages list) is separate from but complementary to the execution state (workspace globals). See [Workspace State Management](#5.5) for details on execution state.\n\n**Key Differences:**\n\n| Aspect | Conversation Memory | Execution State |\n|--------|---------------------|-----------------|\n| **Data Structure** | `messages` list (dicts) | `arg_globals_list` / `out_globals_list` |\n| **Purpose** | Track conversation history | Persist Python variables across executions |\n| **Scope** | Entire agent session | Individual tool executions |\n| **Serialization** | JSON-serializable strings | Pickleable Python objects |\n| **Location** | [src/agent/memory.py]() | [src/agent/tool/python_tool.py]() |\n\n**Interaction Points:**\n\n1. Tool execution results (from workspace state) are converted to strings and added to conversation memory\n2. The LLM's reasoning (in conversation memory) determines which code to execute (which updates workspace state)\n3. Both are maintained in parallel but independently\n\nSources: [src/agent/memory.py:1-17](), [src/agent/deep_research.py:15-74]()\n\n---\n\n## Summary\n\nThe memory and context management system in algo_agent is designed around a simple, append-only messages list that follows OpenAI's chat completion format. Key characteristics:\n\n- **Initialization**: `init_messages_with_system_prompt` creates the initial `[system, user]` message pair\n- **Growth Pattern**: Messages are appended using `messages.append()` throughout the agent loop\n- **Message Roles**: System, user, assistant, tool (modern), and function (legacy)\n- **Context Passing**: The full messages list is passed to the LLM on each iteration\n- **State Management**: Stateless functions operating on a stateful data structure (the messages list)\n- **Integration**: Works alongside tool schema and workspace state systems\n- **Observability**: Full logging via `@traceable` decorator and `global_logger`\n\nThe system prioritizes simplicity and API compatibility over optimization, making it easy to understand and debug but potentially inefficient for very long conversations.\n\nSources: [src/agent/memory.py:1-17](), [src/agent/deep_research.py:1-74](), [src/agent/action.py:1-48]()\n\n---\n\n# Page: Action Coordination\n\n# Action Coordination\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/functino_call_err.design.md](docs/functino_call_err.design.md)\n- [src/agent/action.py](src/agent/action.py)\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/agent/memory.py](src/agent/memory.py)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThe Action Coordination subsystem is responsible for safely dispatching tool calls from the LLM to concrete tool implementations. It serves as the bridge between the agent's decision loop and the tool execution layer, providing error handling, logging, and result formatting.\n\nThis page covers the `call_tools_safely` function and the tool dispatch mechanism. For information about the agent's decision loop and how tool calls are generated, see [Query Processing Loop](#3.1). For details on individual tool implementations, see [Tool System](#4).\n\n---\n\n## Core Components\n\n### The `call_tools_safely` Function\n\nThe action coordinator is implemented primarily through the `call_tools_safely` function in [src/agent/action.py:10-47](), which acts as the central dispatch point for all tool executions in the system.\n\n**Function Signature:**\n```python\n@traceable\ndef call_tools_safely(tool_info: dict)\n```\n\nThe function accepts a `tool_info` dictionary that is mutated in-place to store the execution result in the `\"content\"` field. This design allows the agent to pass tool results directly back to the LLM without additional data structure transformations.\n\nSources: [src/agent/action.py:1-49]()\n\n---\n\n### The tool_info Dictionary Structure\n\nThe `tool_info` dictionary serves as the primary data structure for tool coordination, containing both input parameters and execution results:\n\n| Field | Type | Purpose | Required |\n|-------|------|---------|----------|\n| `tool_call_id` | str | Unique identifier for tracking tool calls in conversation | Yes |\n| `role` | str | Either `\"tool\"` or `\"function\"` depending on API format | Yes |\n| `tool_call_name` | str | Name of the tool to invoke (e.g., `\"execute_python_code\"`) | Yes |\n| `tool_call_arguments` | str | JSON-encoded string of tool parameters | Yes |\n| `content` | str | Tool execution result or error message (populated by dispatcher) | Modified |\n\n**Example tool_info Dictionary:**\n```python\ntool_info = {\n    \"content\": \"\",  # Will be populated with result\n    \"role\": \"tool\",\n    \"tool_call_id\": \"call_abc123\",\n    \"tool_call_name\": \"execute_python_code\",\n    \"tool_call_arguments\": '{\"python_code_snippet\": \"print(1+1)\", ...}'\n}\n```\n\nSources: [src/agent/deep_research.py:34-41](), [src/agent/deep_research.py:50-57]()\n\n---\n\n## Tool Dispatch Flow\n\n### Dispatch Mechanism\n\nThe dispatcher uses a simple if-elif chain to route tool calls based on the `tool_call_name` field. This design prioritizes simplicity and debuggability over extensibility.\n\n```mermaid\ngraph TD\n    A[\"call_tools_safely(tool_info)\"] --> B[\"Extract function_name<br/>from tool_info\"]\n    B --> C[\"Parse arguments JSON\"]\n    C --> D{\"Match function_name\"}\n    \n    D -->|\"execute_python_code\"| E[\"Instantiate<br/>ExecutePythonCodeTool\"]\n    D -->|\"update_recursive_plan_tree\"| F[\"Instantiate<br/>RecursivePlanTreeTodoTool\"]\n    D -->|\"Unknown\"| G[\"No match<br/>(falls through)\"]\n    \n    E --> H[\"Call tool.run()\"]\n    F --> H\n    G --> I[\"tool_info['content']<br/>remains empty\"]\n    \n    H --> J[\"Store result in<br/>tool_info['content']\"]\n    J --> K[\"Return tool_info\"]\n    I --> K\n```\n\n**Dispatch Implementation:**\n```python\ndef call_tools(tool_info: dict):\n    function_name = tool_info[\"tool_call_name\"]\n    arguments = json.loads(tool_info[\"tool_call_arguments\"])\n    \n    if function_name == ExecutePythonCodeTool.tool_name():\n        execute_python_code_tool = ExecutePythonCodeTool(**arguments)\n        tool_info[\"content\"] = execute_python_code_tool.run()\n    elif function_name == RecursivePlanTreeTodoTool.tool_name():\n        recursive_plan_tree_todo_tool = RecursivePlanTreeTodoTool(**arguments)\n        tool_info[\"content\"] = recursive_plan_tree_todo_tool.run()\n    \n    return tool_info\n```\n\nSources: [src/agent/action.py:11-21]()\n\n---\n\n### Tool Instantiation Pattern\n\nEach tool is instantiated dynamically using the parsed arguments dictionary. This leverages Python's `**kwargs` syntax to map JSON parameters to tool constructor arguments:\n\n```mermaid\nsequenceDiagram\n    participant Dispatcher as \"call_tools\"\n    participant JSON as \"json.loads\"\n    participant Tool as \"Tool Class\"\n    participant Instance as \"Tool Instance\"\n    \n    Dispatcher->>JSON: Parse tool_call_arguments\n    JSON-->>Dispatcher: arguments dict\n    Dispatcher->>Tool: Tool(**arguments)\n    Tool->>Instance: __init__ with kwargs\n    Instance-->>Dispatcher: tool instance\n    Dispatcher->>Instance: run()\n    Instance-->>Dispatcher: result string\n    Dispatcher->>Dispatcher: tool_info[\"content\"] = result\n```\n\nThis pattern requires that tool classes implement a constructor that accepts all parameters defined in their tool schema. See [BaseTool Interface](#4.1) for details on tool schema definitions.\n\nSources: [src/agent/action.py:15-20]()\n\n---\n\n## Error Handling and Recovery\n\n### Exception Capture Strategy\n\nThe `call_tools_safely` function wraps the entire dispatch logic in a try-except block that catches all exceptions, ensuring the agent loop never crashes due to tool failures:\n\n```mermaid\ngraph TB\n    A[\"call_tools_safely(tool_info)\"] --> B[\"try:\"]\n    B --> C[\"call_tools(tool_info)\"]\n    C --> D{\"Exception?\"}\n    \n    D -->|\"No Exception\"| E[\"Return tool_info<br/>with success result\"]\n    \n    D -->|\"Exception Raised\"| F[\"traceback.format_exc()\"]\n    F --> G[\"Log error with<br/>global_logger.error()\"]\n    G --> H[\"Set tool_info['content']<br/>to error message\"]\n    H --> I[\"Return tool_info<br/>with error\"]\n    \n    style F fill:#ffebee\n    style G fill:#ffebee\n    style H fill:#ffebee\n    style I fill:#ffebee\n```\n\n**Error Handling Implementation:**\n```python\ntry:\n    return call_tools(tool_info)\nexcept Exception as e:\n    error_msg = traceback.format_exc()\n    global_logger.error(\n        f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\", \n        exc_info=True\n    )\n    tool_info[\"content\"] = f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\"\n    return tool_info\n```\n\nSources: [src/agent/action.py:40-47]()\n\n---\n\n### Error Message Format\n\nWhen an exception occurs, the error message is formatted to include:\n\n1. **Contextual prefix**: `\"å·¥å·å½æ°è°ç¨å¤±è´¥\"` (Tool function call failed)\n2. **Original content**: The partial content from `tool_info['content']` (usually empty)\n3. **Full traceback**: Complete stack trace from `traceback.format_exc()`\n\nThis comprehensive error format ensures that:\n- The LLM receives actionable error information to adjust its approach\n- Developers can debug tool failures without accessing logs\n- The conversation history maintains a complete audit trail\n\n**Example Error Message in tool_info:**\n```\nå·¥å·å½æ°è°ç¨å¤±è´¥, éè¯¯ä¿¡æ¯: Traceback (most recent call last):\n  File \"src/agent/action.py\", line 41, in call_tools_safely\n    return call_tools(tool_info)\n  File \"src/agent/action.py\", line 13, in call_tools\n    arguments = json.loads(tool_info[\"tool_call_arguments\"])\n  File \"json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n```\n\nSources: [src/agent/action.py:43-46]()\n\n---\n\n## Integration with Agent Loop\n\n### Call Sites in Agent Decision Loop\n\nThe action coordinator is invoked at two points in the agent's decision loop to handle both legacy `function_call` and modern `tool_calls` API formats:\n\n```mermaid\nsequenceDiagram\n    participant Agent as \"user_query\"\n    participant LLM as \"generate_assistant_output_append\"\n    participant Action as \"call_tools_safely\"\n    participant Memory as \"messages list\"\n    \n    Agent->>LLM: Generate response with tools\n    LLM-->>Agent: assistant_output\n    \n    alt has_function_call (legacy)\n        Agent->>Agent: Build tool_info with role=\"function\"\n        Agent->>Action: call_tools_safely(tool_info)\n        Action-->>Agent: tool_info with content\n        Agent->>Memory: Append tool_info\n    end\n    \n    alt has_tool_call (modern)\n        loop For each tool_call\n            Agent->>Agent: Build tool_info with role=\"tool\"\n            Agent->>Action: call_tools_safely(tool_info)\n            Action-->>Agent: tool_info with content\n            Agent->>Memory: Append tool_info\n        end\n    end\n    \n    Agent->>LLM: Continue with updated messages\n```\n\n**Function Call Handler** [src/agent/deep_research.py:33-46]():\n```python\nif llm.has_function_call(assistant_output):\n    tool_info = {\n        \"content\": \"\",\n        \"role\": \"function\",\n        \"tool_call_id\": \"\",\n        \"tool_call_name\": assistant_output.function_call.name,\n        \"tool_call_arguments\": assistant_output.function_call.arguments,\n    }\n    action.call_tools_safely(tool_info)\n    messages.append(tool_info)\n```\n\n**Tool Calls Handler** [src/agent/deep_research.py:47-64]():\n```python\nif llm.has_tool_call(assistant_output):\n    for i in range(len(assistant_output.tool_calls)):\n        tool_info = {\n            \"content\": \"\",\n            \"role\": \"tool\",\n            \"tool_call_id\": assistant_output.tool_calls[i].id,\n            \"tool_call_name\": assistant_output.tool_calls[i].function.name,\n            \"tool_call_arguments\": assistant_output.tool_calls[i].function.arguments,\n        }\n        action.call_tools_safely(tool_info)\n        messages.append(tool_info)\n```\n\nSources: [src/agent/deep_research.py:32-64]()\n\n---\n\n### Message Flow and State Mutation\n\nThe action coordinator follows a **mutation-based design** where the `tool_info` dictionary is modified in-place rather than returning a new object. This design choice has several implications:\n\n```mermaid\ngraph LR\n    A[\"Agent creates<br/>empty tool_info\"] --> B[\"call_tools_safely\"]\n    B --> C[\"Mutate content field\"]\n    C --> D[\"Return same object\"]\n    D --> E[\"Agent appends<br/>to messages\"]\n    E --> F[\"LLM receives<br/>tool result\"]\n    \n    style C fill:#fff3e0\n```\n\n**Benefits:**\n- Simplifies agent code (no need to reassign variables)\n- Preserves object identity for debugging\n- Maintains all metadata fields unchanged\n\n**Drawbacks:**\n- Less functional/immutable\n- Potential confusion about ownership semantics\n- No explicit return type in function signature\n\nSources: [src/agent/action.py:10-21](), [src/agent/deep_research.py:42-46](), [src/agent/deep_research.py:59-64]()\n\n---\n\n## Observability and Tracing\n\n### The @traceable Decorator\n\nThe `call_tools_safely` function is decorated with `@traceable`, which provides comprehensive execution tracing:\n\n```mermaid\ngraph TB\n    A[\"@traceable decorator\"] --> B[\"Log call start<br/>with parameters\"]\n    B --> C[\"Execute<br/>call_tools_safely\"]\n    C --> D{\"Exception?\"}\n    \n    D -->|\"Success\"| E[\"Log completion<br/>with timing & result\"]\n    D -->|\"Exception\"| F[\"Log failure<br/>with stack trace\"]\n    \n    E --> G[\"Write to trace.log\"]\n    F --> G\n    \n    C --> H[\"Inner exception<br/>handling\"]\n    H --> I[\"Log to global.log\"]\n```\n\n**Traceable Decorator Configuration** [src/utils/log_decorator.py:297-302]():\n```python\ntraceable = lambda func: log_function(\n    logger_name=\"root.all.trace\",\n    log_file=traceable_logger_file_name,  # logs/trace.log\n    exclude_args=[\"password\", \"token\", \"secret\"],\n    level=logging.DEBUG\n)(func)\n```\n\nThis dual-logging approach provides:\n- **trace.log**: Complete parameter and return value logging\n- **global.log**: High-level operation logging with error details\n\nSources: [src/agent/action.py:9-10](), [src/utils/log_decorator.py:287-306]()\n\n---\n\n### Logged Information\n\nFor each tool call, the tracing system captures:\n\n| Information | Source | Log Level |\n|------------|--------|-----------|\n| Function arguments (tool_info dict) | @traceable | DEBUG |\n| Call stack trace | @traceable | DEBUG |\n| Execution start time | @traceable | DEBUG |\n| Tool execution result | Inner logging | INFO |\n| Execution duration | @traceable | DEBUG |\n| Exception type and message | Both | ERROR |\n| Full traceback | Both | ERROR |\n\n**Example Log Entry:**\n```\n[2025-11-26 03:39:31,682] ãè°ç¨å¼å§ã æ è·¯å¾ï¼ action.None.call_tools_safely \n| å¼å§æ¶é´ï¼ 2025-11-26 03:39:31.682000 \n| ä½ç½®åæ°ï¼ {'tool_call_name': 'execute_python_code', ...}\n| å³é®å­åæ°ï¼ {}\n```\n\nSources: [src/utils/log_decorator.py:199-217](), [src/agent/action.py:45]()\n\n---\n\n## Tool Registration Pattern\n\n### Current Implementation\n\nThe current implementation uses **hardcoded if-elif chains** for tool dispatch. This approach is simple but requires code changes to add new tools:\n\n```python\nif function_name == ExecutePythonCodeTool.tool_name():\n    # ExecutePythonCodeTool dispatch\nelif function_name == RecursivePlanTreeTodoTool.tool_name():\n    # RecursivePlanTreeTodoTool dispatch\n```\n\n**Advantages:**\n- Explicit and easy to understand\n- No magic registration mechanisms\n- Excellent IDE support (jump to definition works)\n- Easy to debug (straightforward control flow)\n\n**Disadvantages:**\n- Requires modifying action.py for each new tool\n- Cannot dynamically discover tools\n- Violates Open-Closed Principle\n\nSources: [src/agent/action.py:14-20]()\n\n---\n\n### Tool Discovery\n\nTools are explicitly registered in the agent's initialization [src/agent/deep_research.py:20-23]():\n\n```python\ntools_schema_list = tool.schema.get_tools_schema([\n    tool.python_tool.ExecutePythonCodeTool,\n    # tool.todo_tool.RecursivePlanTreeTodoTool,  # Commented out\n])\n```\n\nThis schema list is passed to the LLM to inform it which tools are available. The action coordinator must maintain consistency with this list, ensuring every tool in the schema has a corresponding dispatch branch.\n\n**Consistency Requirements:**\n1. Tool name in dispatch must match `Tool.tool_name()`\n2. Tool parameters must match schema definition\n3. Tools in schema list must have dispatch handlers\n4. Commented tools should also be commented in dispatcher\n\nSources: [src/agent/deep_research.py:20-23](), [src/agent/action.py:15-20]()\n\n---\n\n## Error Scenarios and Recovery\n\n### Common Error Patterns\n\nThe action coordinator handles several classes of errors:\n\n```mermaid\ngraph TB\n    A[\"Tool Execution Errors\"] --> B[\"JSON Parsing Errors\"]\n    A --> C[\"Tool Instantiation Errors\"]\n    A --> D[\"Tool Runtime Errors\"]\n    \n    B --> B1[\"Invalid JSON in<br/>tool_call_arguments\"]\n    C --> C1[\"Missing required parameters\"]\n    C --> C2[\"Type validation failures\"]\n    D --> D1[\"Code execution timeouts\"]\n    D --> D2[\"Serialization errors\"]\n    D --> D3[\"External dependency failures\"]\n    \n    B1 --> E[\"Captured by<br/>call_tools_safely\"]\n    C1 --> E\n    C2 --> E\n    D1 --> E\n    D2 --> E\n    D3 --> E\n    \n    E --> F[\"Error message returned<br/>to LLM in content field\"]\n```\n\n**JSON Parsing Error Example:**\n```python\n# If tool_call_arguments contains invalid JSON:\narguments = json.loads(tool_info[\"tool_call_arguments\"])\n# Raises: json.decoder.JSONDecodeError\n# Caught by: try-except in call_tools_safely\n# Result: Error message in tool_info[\"content\"]\n```\n\nSources: [src/agent/action.py:13](), [src/agent/action.py:40-47]()\n\n---\n\n### Function Call vs Tool Calls API Mismatch\n\nA critical error pattern occurs when the LLM uses `function_call` format but no handler processes it. This is documented in [docs/functino_call_err.design.md:2-3]():\n\n```\n{'content': 'æ²¡æå®ä¹function_callå·¥å·è°ç¨ï¼æ æ³æ§è¡function_callï¼è¯·ä½¿ç¨tool_callsè°ç¨å·¥å·ã',\n 'role': 'user'}\n```\n\nThis error occurs when:\n1. LLM generates a response with `function_call` field\n2. Agent detects `has_function_call()` but doesn't properly handle it\n3. Empty or malformed message is appended to conversation\n4. LLM API rejects the malformed message sequence\n\n**Mitigation:**\nThe agent properly handles both formats [src/agent/deep_research.py:33-64]() by checking for both `has_function_call()` and `has_tool_call()` in the decision loop.\n\nSources: [docs/functino_call_err.design.md:1-51](), [src/agent/deep_research.py:33-64]()\n\n---\n\n## Adding New Tools\n\nTo add a new tool to the action coordinator:\n\n### Step 1: Implement Tool Class\nCreate a new tool class extending `BaseTool` (see [Creating New Tools](#8.2))\n\n### Step 2: Add Dispatch Branch\nModify [src/agent/action.py:14-20]() to add a new elif branch:\n\n```python\nelif function_name == NewTool.tool_name():\n    new_tool = NewTool(**arguments)\n    tool_info[\"content\"] = new_tool.run()\n```\n\n### Step 3: Register Tool Schema\nAdd tool to schema list in [src/agent/deep_research.py:20-23]():\n\n```python\ntools_schema_list = tool.schema.get_tools_schema([\n    tool.python_tool.ExecutePythonCodeTool,\n    tool.new_tool.NewTool,  # Add here\n])\n```\n\n### Step 4: Test Error Handling\nVerify that:\n- Invalid arguments raise appropriate errors\n- Errors are caught by `call_tools_safely`\n- Error messages are informative for the LLM\n\nSources: [src/agent/action.py:14-20](), [src/agent/deep_research.py:20-23]()\n\n---\n\n## Performance Considerations\n\n### Synchronous Execution Model\n\nThe action coordinator executes tools **synchronously** within the agent loop. This means:\n\n- Each tool call blocks until completion\n- Multiple tool calls are executed sequentially\n- Long-running tools delay the agent response\n\n```mermaid\nsequenceDiagram\n    participant Agent\n    participant Coordinator as \"call_tools_safely\"\n    participant Tool1 as \"Tool 1\"\n    participant Tool2 as \"Tool 2\"\n    \n    Agent->>Coordinator: Call Tool 1\n    Coordinator->>Tool1: run()\n    Note over Tool1: Executes (may take seconds)\n    Tool1-->>Coordinator: Result\n    Coordinator-->>Agent: Updated tool_info\n    \n    Agent->>Coordinator: Call Tool 2\n    Coordinator->>Tool2: run()\n    Note over Tool2: Executes (may take seconds)\n    Tool2-->>Coordinator: Result\n    Coordinator-->>Agent: Updated tool_info\n```\n\nFor Python code execution specifically, timeouts are enforced at the tool level (see [Python Code Execution Tool](#4.2)), not by the action coordinator.\n\nSources: [src/agent/action.py:10-47](), [src/agent/deep_research.py:47-64]()\n\n---\n\n### Memory Overhead\n\nThe action coordinator has minimal memory overhead:\n- No caching of tool instances (created per call)\n- No buffering of results (stored directly in messages list)\n- tool_info dictionaries are small (typically < 1KB)\n\nThe primary memory concerns are:\n1. **Large tool results**: Python code output stored in `content` field\n2. **Message history growth**: All tool calls remain in conversation\n3. **Tool internal state**: Managed by individual tools (see [Workspace State Management](#5.5))\n\nSources: [src/agent/action.py:10-21](), [src/agent/deep_research.py:46](), [src/agent/deep_research.py:64]()\n\n---\n\n## Summary\n\nThe Action Coordination subsystem provides a simple, robust dispatching layer with the following characteristics:\n\n**Design Principles:**\n- **Safety First**: Comprehensive exception handling prevents crashes\n- **Observability**: Dual logging (trace + global) for debugging\n- **Simplicity**: Direct if-elif dispatch over complex registration\n- **LLM-Friendly Errors**: Detailed error messages help the model recover\n\n**Key Components:**\n- `call_tools_safely`: Error-handling wrapper\n- `call_tools`: Core dispatch logic  \n- `tool_info`: Mutable state container\n- `@traceable`: Execution tracing decorator\n\n**Integration Points:**\n- Called by agent loop for each tool invocation\n- Instantiates tool classes dynamically from schemas\n- Returns results via in-place mutation\n- Logs to both trace.log and global.log\n\nSources: [src/agent/action.py:1-49](), [src/agent/deep_research.py:1-129]()\n\n---\n\n# Page: Tool System\n\n# Tool System\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/functino_call_err.design.md](docs/functino_call_err.design.md)\n- [src/agent/action.py](src/agent/action.py)\n- [src/agent/memory.py](src/agent/memory.py)\n- [src/agent/tool/base_tool.py](src/agent/tool/base_tool.py)\n- [src/agent/tool/python_tool.py](src/agent/tool/python_tool.py)\n- [src/agent/tool/todo_tool.py](src/agent/tool/todo_tool.py)\n- [src/memory/tree_todo/schemas.py](src/memory/tree_todo/schemas.py)\n- [src/memory/tree_todo/todo_track.py](src/memory/tree_todo/todo_track.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThe Tool System provides the interface layer between the LLM agent and executable actions. It defines how tools are structured, discovered, invoked, and integrated into the agent's decision loop. This system enables the agent to interact with Python execution environments and task management structures through a unified, type-safe interface.\n\nFor details on how the agent orchestrates tool invocations, see [Agent Orchestration](#3). For the base interface specification, see [BaseTool Interface](#4.1). For individual tool implementations, see [Python Code Execution Tool](#4.2) and [Recursive Task Planning Tool](#4.3).\n\n**Sources:** [src/agent/tool/base_tool.py:1-76](), [src/agent/tool/python_tool.py:1-51](), [src/agent/tool/todo_tool.py:1-37]()\n\n---\n\n## Architecture Overview\n\nThe tool system consists of four primary components: the `BaseTool` abstract interface, concrete tool implementations, the action dispatcher, and the schema generation system that bridges tools to LLM function calling.\n\n### System Component Diagram\n\n```mermaid\ngraph TB\n    subgraph \"LLM Integration Layer\"\n        LLM[\"LLM<br/>(qwen-plus)\"]\n        SCHEMA[\"get_tool_schema()<br/>JSON Schema Generator\"]\n    end\n    \n    subgraph \"Tool Interface Layer\"\n        BASE[\"BaseTool<br/>(Abstract Base Class)\"]\n        METHODS[\"Interface Methods:<br/>- tool_name()<br/>- tool_description()<br/>- get_parameter_schema()<br/>- get_tool_schema()<br/>- run()\"]\n    end\n    \n    subgraph \"Concrete Tool Implementations\"\n        PYTOOL[\"ExecutePythonCodeTool<br/>python_code_snippet: str<br/>timeout: int\"]\n        TODOTOOL[\"RecursivePlanTreeTodoTool<br/>recursive_plan_tree: RecursivePlanTree\"]\n    end\n    \n    subgraph \"Dispatch Layer\"\n        DISPATCHER[\"call_tools_safely()<br/>action.py\"]\n        ERROR[\"Exception Handler<br/>traceback.format_exc()\"]\n    end\n    \n    subgraph \"Execution Backends\"\n        RUNTIME[\"Python Execution<br/>Runtime System\"]\n        TRACK[\"Task Tree<br/>Tracking System\"]\n    end\n    \n    LLM --> SCHEMA\n    SCHEMA --> BASE\n    BASE --> METHODS\n    METHODS --> PYTOOL\n    METHODS --> TODOTOOL\n    \n    LLM --> DISPATCHER\n    DISPATCHER --> PYTOOL\n    DISPATCHER --> TODOTOOL\n    DISPATCHER --> ERROR\n    \n    PYTOOL --> RUNTIME\n    TODOTOOL --> TRACK\n```\n\nThe architecture follows a standard plugin pattern where:\n- `BaseTool` defines the contract all tools must implement\n- Concrete tools extend `BaseTool` and provide specific functionality\n- The dispatcher routes tool calls from the LLM to the appropriate tool instance\n- Schema generation produces JSON schemas compatible with OpenAI's function calling format\n\n**Sources:** [src/agent/tool/base_tool.py:7-76](), [src/agent/action.py:1-48]()\n\n---\n\n## Tool Definition and Schema Generation\n\n### Pydantic-Based Tool Definition\n\nAll tools inherit from `BaseTool`, which is itself a Pydantic `BaseModel`. This provides automatic validation, type checking, and schema generation. Each tool defines its parameters as Pydantic fields with descriptions and type annotations.\n\n| Component | Purpose | Implementation |\n|-----------|---------|----------------|\n| `tool_name()` | Generates unique identifier from class name | [base_tool.py:14-18]() |\n| `tool_description()` | Extracts docstring for LLM context | [base_tool.py:20-23]() |\n| `get_parameter_schema()` | Generates JSON Schema from Pydantic model | [base_tool.py:26-28]() |\n| `get_tool_schema()` | Produces complete function calling schema | [base_tool.py:31-71]() |\n| `run()` | Abstract method for tool execution logic | [base_tool.py:73-75]() |\n\n### Schema Generation Flow\n\n```mermaid\nsequenceDiagram\n    participant Agent as \"Agent System\"\n    participant Tool as \"Tool Class<br/>(ExecutePythonCodeTool)\"\n    participant Pydantic as \"Pydantic<br/>Schema Engine\"\n    participant LLM as \"LLM Service\"\n    \n    Agent->>Tool: get_tool_schema()\n    Tool->>Tool: tool_name()<br/>\"execute_python_code\"\n    Tool->>Tool: tool_description()<br/>Extract docstring\n    Tool->>Pydantic: model_json_schema()\n    Pydantic-->>Tool: Parameter schema dict\n    Tool->>Tool: Construct OpenAI format\n    Tool-->>Agent: Complete tool schema\n    Agent->>LLM: Pass tools parameter\n    LLM-->>Agent: Response with tool_calls\n```\n\nThe `get_tool_schema()` method [base_tool.py:31-71]() produces a structure conforming to OpenAI's function calling format:\n\n```json\n{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"execute_python_code\",\n    \"description\": \"å¿é¡»è°ç¨å¨æ¯ä¸è½®æ¨çä¸­...\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"tool_call_purpose\": {...},\n        \"python_code_snippet\": {...},\n        \"timeout\": {...}\n      },\n      \"required\": [\"tool_call_purpose\", \"python_code_snippet\"]\n    },\n    \"strict\": true\n  }\n}\n```\n\n**Sources:** [src/agent/tool/base_tool.py:26-71](), [src/agent/tool/python_tool.py:13-40]()\n\n---\n\n## Tool Invocation Pipeline\n\n### Dispatcher Architecture\n\nThe `call_tools_safely()` function [action.py:10-47]() serves as the central dispatcher that routes tool calls from the LLM to the appropriate tool implementation. It provides comprehensive error handling and logging.\n\n```mermaid\ngraph LR\n    subgraph \"Input\"\n        TOOLINFO[\"tool_info dict<br/>{tool_call_name,<br/>tool_call_arguments,<br/>content}\"]\n    end\n    \n    subgraph \"call_tools_safely Function\"\n        PARSE[\"Parse Arguments<br/>json.loads()\"]\n        ROUTE[\"Route by Name<br/>function_name match\"]\n        INSTANTIATE[\"Instantiate Tool<br/>**arguments\"]\n        EXECUTE[\"Execute run()<br/>Get result\"]\n        CATCH[\"Exception Handler<br/>traceback.format_exc()\"]\n    end\n    \n    subgraph \"Tool Implementations\"\n        EXEC[\"ExecutePythonCodeTool.run()\"]\n        TODO[\"RecursivePlanTreeTodoTool.run()\"]\n    end\n    \n    subgraph \"Output\"\n        SUCCESS[\"tool_info with content\"]\n        ERROR[\"tool_info with error message\"]\n    end\n    \n    TOOLINFO --> PARSE\n    PARSE --> ROUTE\n    ROUTE --> INSTANTIATE\n    INSTANTIATE --> EXEC\n    INSTANTIATE --> TODO\n    EXEC --> EXECUTE\n    TODO --> EXECUTE\n    EXECUTE --> SUCCESS\n    EXECUTE --> CATCH\n    CATCH --> ERROR\n```\n\n### Call Flow with Code References\n\nThe dispatcher follows this execution path:\n\n1. **Extract function name and arguments** [action.py:12-13]()\n   - `function_name = tool_info[\"tool_call_name\"]`\n   - `arguments = json.loads(tool_info[\"tool_call_arguments\"])`\n\n2. **Route to tool implementation** [action.py:15-20]()\n   - Match against `ExecutePythonCodeTool.tool_name()`\n   - Match against `RecursivePlanTreeTodoTool.tool_name()`\n\n3. **Instantiate and execute** [action.py:16-20]()\n   - Create tool instance with validated parameters\n   - Call `run()` method\n   - Store result in `tool_info[\"content\"]`\n\n4. **Handle exceptions** [action.py:40-47]()\n   - Capture full traceback with `traceback.format_exc()`\n   - Log error with `global_logger.error()`\n   - Return error message in `tool_info[\"content\"]`\n\n**Sources:** [src/agent/action.py:10-47]()\n\n---\n\n## Available Tools\n\nThe system provides two concrete tool implementations:\n\n### ExecutePythonCodeTool\n\nExecutes Python code snippets in a stateful environment with variable persistence across calls. This tool is the primary computational interface for the agent.\n\n**Key Parameters:**\n- `python_code_snippet: str` - The Python code to execute [python_tool.py:29-35]()\n- `timeout: int` - Maximum execution time in seconds (default: 30) [python_tool.py:36-39]()\n\n**Execution Flow:**\n1. Retrieves execution context from `workspace.get_arg_globals()` [python_tool.py:42]()\n2. Runs code using `subthread_python_executor.run_structured_in_thread()` [python_tool.py:44-48]()\n3. Persists output globals with `workspace.append_out_globals()` [python_tool.py:49]()\n4. Returns formatted result to LLM [python_tool.py:50]()\n\nFor detailed implementation, see [Python Code Execution Tool](#4.2).\n\n### RecursivePlanTreeTodoTool\n\nManages hierarchical task structures with status tracking, version history, and change analysis. This tool enables the agent to maintain complex planning state.\n\n**Key Parameters:**\n- `recursive_plan_tree: RecursivePlanTree` - The task tree structure [todo_tool.py:19-25]()\n\n**Execution Flow:**\n1. Delegates to `todo_track.run()` with tree structure [todo_tool.py:31]()\n2. Compares against previous version for change detection\n3. Renders Markdown representation of task tree\n4. Returns change summary and statistics [todo_tool.py:32-36]()\n\nFor detailed implementation, see [Recursive Task Planning Tool](#4.3).\n\n**Sources:** [src/agent/tool/python_tool.py:13-51](), [src/agent/tool/todo_tool.py:10-37]()\n\n---\n\n## Tool Registration and Discovery\n\n### Static Registration\n\nTools are currently registered through explicit imports and conditional logic in the dispatcher. The registration happens in two locations:\n\n1. **Import statements** [action.py:7-8]()\n   ```python\n   from src.agent.tool.python_tool import ExecutePythonCodeTool \n   from src.agent.tool.todo_tool import RecursivePlanTreeTodoTool\n   ```\n\n2. **Dispatch logic** [action.py:15-20]()\n   ```python\n   if function_name == ExecutePythonCodeTool.tool_name():\n       execute_python_code_tool = ExecutePythonCodeTool(**arguments)\n       tool_info[\"content\"] = execute_python_code_tool.run()\n   elif function_name == RecursivePlanTreeTodoTool.tool_name():\n       recursive_plan_tree_todo_tool = RecursivePlanTreeTodoTool(**arguments)\n       tool_info[\"content\"] = recursive_plan_tree_todo_tool.run()\n   ```\n\n### Tool Name Convention\n\nTool names are automatically generated from class names using the `inflection` library [base_tool.py:16-18]():\n- `ExecutePythonCodeTool` â `\"execute_python_code\"`\n- `RecursivePlanTreeTodoTool` â `\"recursive_plan_tree_todo\"`\n\nThis transformation:\n1. Removes \"Tool\" suffix from class name\n2. Converts from CamelCase to snake_case using `inflection.underscore()`\n\n**Sources:** [src/agent/action.py:7-20](), [src/agent/tool/base_tool.py:14-18]()\n\n---\n\n## Integration with Agent System\n\n### Schema Provisioning to LLM\n\nTools are made available to the LLM through the `tools` parameter in chat completion requests. The agent must collect all tool schemas before each LLM call.\n\n```mermaid\nsequenceDiagram\n    participant Agent as \"user_query()<br/>deep_research.py\"\n    participant Tools as \"Tool Classes\"\n    participant Memory as \"Message List\"\n    participant LLM as \"generate_chat_completion()\"\n    \n    Agent->>Tools: ExecutePythonCodeTool.get_tool_schema()\n    Tools-->>Agent: Python tool schema\n    Agent->>Tools: RecursivePlanTreeTodoTool.get_tool_schema()\n    Tools-->>Agent: Todo tool schema\n    Agent->>LLM: messages + tools=[schemas]\n    LLM-->>Agent: assistant_output with tool_calls\n    Agent->>Agent: call_tools_safely(tool_info)\n    Agent->>Memory: Append tool result\n```\n\n### Response Handling\n\nWhen the LLM decides to invoke a tool, it returns a response with `tool_calls` attribute. The agent must:\n\n1. **Extract tool call information** from `assistant_output.tool_calls[0]`\n2. **Build tool_info dictionary**:\n   ```python\n   tool_info = {\n       \"tool_call_name\": function.name,\n       \"tool_call_arguments\": function.arguments,\n       \"content\": \"\"\n   }\n   ```\n3. **Invoke dispatcher** via `call_tools_safely(tool_info)`\n4. **Append result to message history** with role `\"tool\"` and appropriate `tool_call_id`\n\nFor the complete integration flow, see [Action Coordination](#3.4).\n\n**Sources:** [src/agent/action.py:10-47](), [src/agent/tool/base_tool.py:31-71]()\n\n---\n\n## Error Handling and Resilience\n\n### Multi-Layer Error Protection\n\nThe tool system implements error handling at multiple levels:\n\n| Level | Implementation | Purpose |\n|-------|---------------|---------|\n| Pydantic Validation | Automatic on tool instantiation | Type checking and required field validation |\n| Dispatcher Try-Catch | [action.py:40-47]() | Catches tool execution failures |\n| Execution Runtime | See [Execution Runtime](#5) | Handles code crashes, timeouts, and subprocess failures |\n| Logging | `global_logger.error()` [action.py:45]() | Records full exception details with stack traces |\n\n### Error Response Format\n\nWhen a tool fails, the dispatcher captures the complete traceback and returns it in the tool response:\n\n```python\nerror_msg = traceback.format_exc()\nglobal_logger.error(f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\", exc_info=True)\ntool_info[\"content\"] = f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\"\n```\n\nThis allows the LLM to:\n- Understand what went wrong\n- Attempt self-correction\n- Retry with modified parameters\n- Choose alternative approaches\n\n**Sources:** [src/agent/action.py:40-47]()\n\n---\n\n## Tool Execution Context\n\n### State Persistence Across Calls\n\nThe `ExecutePythonCodeTool` maintains execution state through the workspace system:\n\n```mermaid\ngraph LR\n    subgraph \"Call N\"\n        GET1[\"workspace.get_arg_globals()\"]\n        EXEC1[\"Execute Code<br/>Variables: x, y, func()\"]\n        STORE1[\"workspace.append_out_globals()\"]\n    end\n    \n    subgraph \"Global Workspace\"\n        GLOBALS[\"arg_globals_list<br/>Persistent storage\"]\n    end\n    \n    subgraph \"Call N+1\"\n        GET2[\"workspace.get_arg_globals()\"]\n        EXEC2[\"Execute Code<br/>Uses: x, y, func()\"]\n        STORE2[\"workspace.append_out_globals()\"]\n    end\n    \n    GET1 --> GLOBALS\n    EXEC1 --> STORE1\n    STORE1 --> GLOBALS\n    GLOBALS --> GET2\n    GET2 --> EXEC2\n    EXEC2 --> STORE2\n    STORE2 --> GLOBALS\n```\n\n### Context Flow\n\n1. **Context Retrieval** [python_tool.py:42]()\n   - `execution_context = workspace.get_arg_globals()`\n   - Returns the most recent execution state\n\n2. **Execution with Context** [python_tool.py:44-48]()\n   - Passes `_globals=execution_context` to executor\n   - Code runs with previously defined variables available\n\n3. **State Update** [python_tool.py:49]()\n   - `workspace.append_out_globals(exec_result.arg_globals)`\n   - New or modified variables persist for next call\n\nFor detailed workspace management, see [Workspace State Management](#5.5).\n\n**Sources:** [src/agent/tool/python_tool.py:41-50](), [src/runtime/workspace.py]()\n\n---\n\n## Tool Schema Examples\n\n### ExecutePythonCodeTool Schema\n\nThe Python execution tool exposes this interface to the LLM:\n\n```json\n{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"execute_python_code\",\n    \"description\": \"å¿é¡»è°ç¨å¨æ¯ä¸è½®æ¨çä¸­ï¼ä½ä¸ºè®¡ç®å·¥å·ãå¨æç¶æçç¯å¢ä¸­æ§è¡Pythonä»£ç çæ®µ...\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"tool_call_purpose\": {\n          \"type\": \"string\",\n          \"description\": \"å·¥å·è°ç¨çç®çï¼å³è°ç¨è¯¥å·¥å·çå·ä½åºæ¯æé®é¢ã\"\n        },\n        \"python_code_snippet\": {\n          \"type\": \"string\",\n          \"description\": \"è¦æ§è¡çææ Python ä»£ç çæ®µãä¸å¾åå«æ¶æä»£ç ...\",\n          \"examples\": [\"print('Hello, World!')\"]\n        },\n        \"timeout\": {\n          \"type\": \"integer\",\n          \"default\": 30,\n          \"description\": \"æ§è¡ä»£ç çæå¤§æ¶é´ï¼ç§ï¼...\"\n        }\n      },\n      \"required\": [\"tool_call_purpose\", \"python_code_snippet\"]\n    },\n    \"strict\": true\n  }\n}\n```\n\n### RecursivePlanTreeTodoTool Schema\n\nThe task management tool exposes this interface:\n\n```json\n{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"recursive_plan_tree_todo\",\n    \"description\": \"å¿é¡»è°ç¨ä½ä¸ºè®¡åæ¨çè¿ç¨çæèä¸è®°å½...\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"tool_call_purpose\": {\n          \"type\": \"string\",\n          \"description\": \"å·¥å·è°ç¨çç®ç...\"\n        },\n        \"recursive_plan_tree\": {\n          \"type\": \"object\",\n          \"description\": \"è¦ç®¡ççéå½è®¡åæ å¯¹è±¡...\",\n          \"properties\": {\n            \"core_goal\": {\"type\": \"string\"},\n            \"tree_nodes\": {\n              \"type\": \"array\",\n              \"items\": {\"$ref\": \"#/definitions/RecursivePlanTreeNode\"}\n            },\n            \"next_action\": {\"type\": \"object\"},\n            \"references\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n          }\n        }\n      },\n      \"required\": [\"tool_call_purpose\", \"recursive_plan_tree\"]\n    },\n    \"strict\": true\n  }\n}\n```\n\n**Sources:** [src/agent/tool/python_tool.py:13-40](), [src/agent/tool/todo_tool.py:10-26](), [src/memory/tree_todo/schemas.py:1-80]()\n\n---\n\n## Adding New Tools\n\nTo add a new tool to the system:\n\n1. **Create tool class** extending `BaseTool` in `src/agent/tool/`\n2. **Define Pydantic fields** with descriptions and type hints\n3. **Implement `run()` method** with execution logic\n4. **Import in dispatcher** [action.py:7-8]()\n5. **Add dispatch case** in `call_tools_safely()` [action.py:15-20]()\n\nFor a step-by-step guide, see [Creating New Tools](#8.2).\n\n**Sources:** [src/agent/tool/base_tool.py:7-76](), [src/agent/action.py:7-20]()\n\n---\n\n# Page: BaseTool Interface\n\n# BaseTool Interface\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/functino_call_err.design.md](docs/functino_call_err.design.md)\n- [src/agent/action.py](src/agent/action.py)\n- [src/agent/memory.py](src/agent/memory.py)\n- [src/agent/tool/base_tool.py](src/agent/tool/base_tool.py)\n- [src/memory/tree_todo/schemas.py](src/memory/tree_todo/schemas.py)\n- [src/memory/tree_todo/todo_track.py](src/memory/tree_todo/todo_track.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThe `BaseTool` interface defines the abstract contract that all tools in the algo_agent system must implement. It provides a standardized way to define tools that can be invoked by the LLM agent, including automatic schema generation for function calling, parameter validation, and execution logic.\n\nThis document covers the base tool interface, its required methods, and how tools integrate with the agent orchestration system. For details on specific tool implementations, see [Python Code Execution Tool](#4.2) and [Recursive Task Planning Tool](#4.3). For information on how tools are invoked during agent execution, see [Action Coordination](#3.4).\n\n**Sources:** [src/agent/tool/base_tool.py:1-76]()\n\n---\n\n## Abstract Class Structure\n\nThe `BaseTool` class is implemented as a Pydantic `BaseModel`, providing automatic parameter validation and JSON schema generation. All tools must inherit from this base class and implement its abstract methods.\n\n### Core Definition\n\n```python\nclass BaseTool(BaseModel):\n    \"\"\"ææå·¥å·çåºç±»ï¼å®ä¹ç»ä¸æ¥å£\"\"\"\n    tool_call_purpose: str = Field(\n        ..., \n        description=\"å·¥å·è°ç¨çç®çï¼å³è°ç¨è¯¥å·¥å·çå·ä½åºæ¯æé®é¢ã\"\n    )\n```\n\nEvery tool instance requires a `tool_call_purpose` parameter that describes why the tool is being invoked for a particular request. This helps maintain context and improves logging/debugging.\n\n**Sources:** [src/agent/tool/base_tool.py:7-12]()\n\n---\n\n## Tool Identification Methods\n\n### Class Method: `tool_name()`\n\nGenerates a unique identifier for the tool based on its class name. The naming convention automatically converts the class name to snake_case and removes \"Tool\" or \"tool\" suffixes.\n\n| Method | Return Type | Purpose |\n|--------|-------------|---------|\n| `tool_name()` | `str` | Unique tool identifier used for routing and dispatch |\n\n**Implementation:**\n```python\n@classmethod\ndef tool_name(cls) -> str:\n    \"\"\"å·¥å·å¯ä¸æ è¯åï¼ç¨äºè·¯ç±å¹éï¼å¦ \"weatherquery\"ï¼\"\"\"\n    return inflection.underscore(\n        cls.__name__.replace(\"Tool\", \"\").replace(\"tool\", \"\")\n    )\n```\n\n**Example:** A class named `ExecutePythonCodeTool` automatically generates the tool name `\"execute_python_code\"`.\n\n**Sources:** [src/agent/tool/base_tool.py:13-18]()\n\n---\n\n### Class Method: `tool_description()`\n\nExtracts the tool's docstring to provide a natural language description of its purpose and capabilities. This description is used by the LLM to determine when to invoke the tool.\n\n```python\n@classmethod\ndef tool_description(cls) -> str:\n    \"\"\"å·¥å·æè¿°ï¼ä¾ Agent çè§£ç¨éï¼\"\"\"\n    return inspect.getdoc(cls) or \"æ å·¥å·æè¿°\"\n```\n\n**Sources:** [src/agent/tool/base_tool.py:20-23]()\n\n---\n\n## Schema Generation System\n\n### Diagram: Tool Schema Generation Flow\n\n```mermaid\ngraph TB\n    BaseTool[\"BaseTool<br/>(base_tool.py)\"]\n    \n    subgraph \"Schema Methods\"\n        GetParam[\"get_parameter_schema()<br/>Returns Pydantic JSON Schema\"]\n        GetTool[\"get_tool_schema()<br/>Returns OpenAI Function Format\"]\n        ToolName[\"tool_name()<br/>Generates identifier\"]\n        ToolDesc[\"tool_description()<br/>Extracts docstring\"]\n    end\n    \n    subgraph \"Generated Schema Structure\"\n        Schema[\"Tool Schema Object\"]\n        FuncDef[\"function definition\"]\n        Params[\"parameters (JSON Schema)\"]\n        Required[\"required fields\"]\n    end\n    \n    subgraph \"LLM Integration\"\n        AgentTools[\"Agent Tools List\"]\n        LLMContext[\"LLM Context<br/>(tools parameter)\"]\n        FuncCall[\"Function Call Response\"]\n    end\n    \n    BaseTool --> GetParam\n    BaseTool --> GetTool\n    BaseTool --> ToolName\n    BaseTool --> ToolDesc\n    \n    GetTool --> ToolName\n    GetTool --> ToolDesc\n    GetTool --> GetParam\n    \n    GetTool --> Schema\n    Schema --> FuncDef\n    Schema --> Params\n    Schema --> Required\n    \n    Schema --> AgentTools\n    AgentTools --> LLMContext\n    LLMContext --> FuncCall\n```\n\n**Sources:** [src/agent/tool/base_tool.py:26-71](), [src/agent/action.py:1-49]()\n\n---\n\n### Method: `get_parameter_schema()`\n\nLeverages Pydantic's built-in JSON schema generation to produce a JSON Schema representation of the tool's parameters. This schema defines the types, descriptions, and validation rules for each parameter.\n\n```python\n@classmethod\ndef get_parameter_schema(cls) -> dict:\n    \"\"\"è·ååæ° JSON Schemaï¼ä¾ Agent æé åæ°ï¼\"\"\"\n    return cls.model_json_schema()\n```\n\nThe generated schema includes:\n- Parameter names and types\n- Field descriptions from `Field()` definitions\n- Validation constraints (enums, min/max values, etc.)\n- Required vs. optional fields\n\n**Sources:** [src/agent/tool/base_tool.py:25-28]()\n\n---\n\n### Method: `get_tool_schema()`\n\nCombines the tool identification methods and parameter schema into a complete OpenAI-compatible function calling schema. This format allows the LLM to understand when and how to invoke the tool.\n\n**Schema Structure:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `type` | `\"function\"` | Indicates this is a function tool |\n| `function.name` | `str` | Tool identifier from `tool_name()` |\n| `function.description` | `str` | Tool description from `tool_description()` |\n| `function.parameters` | `dict` | JSON Schema from `get_parameter_schema()` |\n| `function.strict` | `bool` | Enforces strict parameter validation |\n\n**Implementation:**\n```python\n@classmethod\ndef get_tool_schema(cls) -> str:\n    tool_schema = {\n        \"type\": \"function\",\n        \"function\": {\n            \"type\": \"function\",\n            \"name\": cls.tool_name(),\n            \"description\": cls.tool_description(),\n            \"parameters\": cls.get_parameter_schema(),\n            \"strict\": True,\n        },\n    }\n    return tool_schema\n```\n\n**Sources:** [src/agent/tool/base_tool.py:30-71]()\n\n---\n\n## Execution Contract\n\n### Abstract Method: `run()`\n\nThe `run()` method is the core execution logic that every tool must implement. It contains the actual functionality that gets invoked when the agent calls the tool.\n\n```python\ndef run(self) -> str:\n    \"\"\"å·¥å·æ ¸å¿æ§è¡é»è¾ï¼å­ç±»å¿é¡»å®ç°ï¼\"\"\"\n    raise NotImplementedError(\"ææå·¥å·å¿é¡»å®ç° run æ¹æ³\")\n```\n\n**Contract Requirements:**\n- **Return Type:** Must return a `str` containing the execution result\n- **Implementation:** Subclasses must override this method\n- **Error Handling:** Should handle exceptions gracefully and return meaningful error messages\n- **Side Effects:** Can modify global state (e.g., workspace variables) but must document this\n\n**Sources:** [src/agent/tool/base_tool.py:73-75]()\n\n---\n\n## Tool Registration and Dispatch\n\n### Diagram: Tool Dispatch Flow\n\n```mermaid\nsequenceDiagram\n    participant LLM as \"LLM<br/>(qwen-plus)\"\n    participant Agent as \"Agent<br/>(deep_research.py)\"\n    participant Dispatcher as \"call_tools_safely<br/>(action.py)\"\n    participant ToolClass as \"Concrete Tool<br/>(e.g., ExecutePythonCodeTool)\"\n    participant Run as \"tool.run()\"\n    \n    LLM->>Agent: \"assistant_output with tool_calls\"\n    Agent->>Agent: \"Extract tool_call_name and arguments\"\n    Agent->>Dispatcher: \"call_tools_safely(tool_info)\"\n    \n    Note over Dispatcher: \"tool_info = {<br/>tool_call_name,<br/>tool_call_arguments,<br/>content}\"\n    \n    Dispatcher->>Dispatcher: \"Parse JSON arguments\"\n    Dispatcher->>Dispatcher: \"Match function_name to tool class\"\n    \n    alt \"function_name == ExecutePythonCodeTool.tool_name()\"\n        Dispatcher->>ToolClass: \"ExecutePythonCodeTool(**arguments)\"\n        Note over ToolClass: \"Pydantic validates parameters\"\n    else \"function_name == RecursivePlanTreeTodoTool.tool_name()\"\n        Dispatcher->>ToolClass: \"RecursivePlanTreeTodoTool(**arguments)\"\n    end\n    \n    Dispatcher->>Run: \"execute_python_code_tool.run()\"\n    Run-->>Dispatcher: \"result_string\"\n    Dispatcher->>Dispatcher: \"tool_info['content'] = result\"\n    Dispatcher-->>Agent: \"tool_info with result\"\n    \n    alt \"Exception during execution\"\n        Dispatcher->>Dispatcher: \"Catch exception, format traceback\"\n        Dispatcher-->>Agent: \"tool_info with error message\"\n    end\n```\n\n**Sources:** [src/agent/action.py:10-47]()\n\n---\n\n### Implementation in `call_tools_safely()`\n\nThe action coordinator uses a conditional dispatch pattern to route tool calls to the appropriate implementation:\n\n```python\n@traceable\ndef call_tools_safely(tool_info: dict):\n    def call_tools(tool_info: dict):\n        function_name = tool_info[\"tool_call_name\"]\n        arguments = json.loads(tool_info[\"tool_call_arguments\"])\n        \n        if function_name == ExecutePythonCodeTool.tool_name():\n            execute_python_code_tool = ExecutePythonCodeTool(**arguments)\n            tool_info[\"content\"] = execute_python_code_tool.run()\n        elif function_name == RecursivePlanTreeTodoTool.tool_name():\n            recursive_plan_tree_todo_tool = RecursivePlanTreeTodoTool(**arguments)\n            tool_info[\"content\"] = recursive_plan_tree_todo_tool.run()\n        \n        return tool_info\n```\n\n**Dispatch Process:**\n1. Extract `function_name` from `tool_info[\"tool_call_name\"]`\n2. Parse JSON `arguments` from `tool_info[\"tool_call_arguments\"]`\n3. Match `function_name` against `ToolClass.tool_name()` for each registered tool\n4. Instantiate the tool class with unpacked arguments (Pydantic validates parameters)\n5. Call `tool.run()` and capture the result\n6. Store result in `tool_info[\"content\"]`\n7. Return updated `tool_info` to agent\n\n**Sources:** [src/agent/action.py:10-21]()\n\n---\n\n### Error Handling Wrapper\n\nThe outer `call_tools_safely()` function wraps the dispatch logic with comprehensive exception handling:\n\n```python\ntry:\n    return call_tools(tool_info)\nexcept Exception as e:\n    error_msg = traceback.format_exc()\n    global_logger.error(f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\", exc_info=True)\n    tool_info[\"content\"] = f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\"\n    return tool_info\n```\n\nThis ensures that tool failures don't crash the agent and provide detailed error information for debugging.\n\n**Sources:** [src/agent/action.py:40-47]()\n\n---\n\n## Tool Definition Pattern\n\n### Standard Tool Implementation Template\n\n```python\nfrom src.agent.tool.base_tool import BaseTool\nfrom pydantic import Field\n\nclass MyCustomTool(BaseTool):\n    \"\"\"\n    Brief description of what this tool does.\n    \n    Used when the agent needs to perform specific actions.\n    \"\"\"\n    \n    # Define tool-specific parameters\n    parameter_one: str = Field(\n        ..., \n        description=\"Description of parameter_one\"\n    )\n    parameter_two: int = Field(\n        default=10,\n        description=\"Description of parameter_two\"\n    )\n    \n    def run(self) -> str:\n        \"\"\"\n        Execute the tool's core logic.\n        \n        Returns:\n            str: Result message or error description\n        \"\"\"\n        try:\n            # Tool implementation\n            result = self._perform_action()\n            return f\"Success: {result}\"\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n```\n\n**Sources:** [src/agent/tool/base_tool.py:1-76]()\n\n---\n\n## Integration with Pydantic V2\n\nThe `BaseTool` class leverages Pydantic V2 features for robust parameter validation and schema generation:\n\n### Key Features Used\n\n| Feature | Purpose | Implementation |\n|---------|---------|---------------|\n| `BaseModel` | Base class providing validation | `class BaseTool(BaseModel)` |\n| `Field()` | Parameter metadata and constraints | Used in `tool_call_purpose` and subclass fields |\n| `model_json_schema()` | Automatic JSON Schema generation | Called by `get_parameter_schema()` |\n| Parameter validation | Runtime type checking | Automatic during tool instantiation |\n| `model_copy()` | Deep copying instances | Used in tool state management |\n\n### Validation Benefits\n\nWhen the dispatcher instantiates a tool with `ToolClass(**arguments)`, Pydantic automatically:\n- Validates parameter types match schema definitions\n- Enforces required fields are present\n- Applies field constraints (enums, ranges, patterns)\n- Coerces compatible types (e.g., string to int)\n- Raises `ValidationError` for invalid inputs\n\n**Sources:** [src/agent/tool/base_tool.py:1-6](), [src/memory/tree_todo/schemas.py:1-3]()\n\n---\n\n## Concrete Tool Examples\n\n### Tool Schema Comparison Table\n\n| Tool Class | Generated Name | Primary Purpose | Key Parameters |\n|------------|----------------|-----------------|----------------|\n| `ExecutePythonCodeTool` | `execute_python_code` | Execute Python code snippets | `python_code_snippet` |\n| `RecursivePlanTreeTodoTool` | `recursive_plan_tree_todo` | Manage hierarchical task trees | `current_plan_tree` |\n\n**Sources:** [src/agent/action.py:7-8](), [src/agent/action.py:15-20]()\n\n---\n\n## Relationship to Agent Architecture\n\n### Diagram: BaseTool in System Context\n\n```mermaid\ngraph TB\n    subgraph \"Tool System Layer\"\n        BaseTool[\"BaseTool<br/>(base_tool.py:7-75)\"]\n        Schema[\"Tool Schema Methods\"]\n        Run[\"run() Abstract Method\"]\n    end\n    \n    subgraph \"Concrete Tools\"\n        PythonTool[\"ExecutePythonCodeTool<br/>(python_tool.py)\"]\n        TodoTool[\"RecursivePlanTreeTodoTool<br/>(todo_tool.py)\"]\n    end\n    \n    subgraph \"Agent Orchestration\"\n        ActionCoord[\"call_tools_safely<br/>(action.py:10-47)\"]\n        AgentLoop[\"user_query<br/>(deep_research.py)\"]\n    end\n    \n    subgraph \"LLM Integration\"\n        ToolsParam[\"tools parameter<br/>(list of tool schemas)\"]\n        FunctionCall[\"function_call / tool_calls<br/>in LLM response\"]\n    end\n    \n    BaseTool --> Schema\n    BaseTool --> Run\n    \n    BaseTool -.inherits.-> PythonTool\n    BaseTool -.inherits.-> TodoTool\n    \n    PythonTool --> ActionCoord\n    TodoTool --> ActionCoord\n    \n    Schema --> ToolsParam\n    ToolsParam --> AgentLoop\n    AgentLoop --> FunctionCall\n    FunctionCall --> ActionCoord\n    \n    ActionCoord -.invokes.-> Run\n```\n\n**Sources:** [src/agent/tool/base_tool.py:1-76](), [src/agent/action.py:1-49]()\n\n---\n\n## Tool Name Conventions\n\nThe `tool_name()` method uses a specific transformation algorithm:\n\n### Transformation Process\n\n1. **Input:** Python class name (e.g., `ExecutePythonCodeTool`)\n2. **Remove suffix:** Strip \"Tool\" or \"tool\" â `ExecutePythonCode`\n3. **Convert to snake_case:** Apply `inflection.underscore()` â `execute_python_code`\n4. **Output:** Tool identifier for function calling\n\n### Example Transformations\n\n| Class Name | Generated Tool Name |\n|------------|---------------------|\n| `ExecutePythonCodeTool` | `execute_python_code` |\n| `RecursivePlanTreeTodoTool` | `recursive_plan_tree_todo` |\n| `WeatherQueryTool` | `weather_query` |\n| `SearchTool` | `search` |\n\nThe library `inflection` handles the camelCase to snake_case conversion reliably.\n\n**Sources:** [src/agent/tool/base_tool.py:4](), [src/agent/tool/base_tool.py:13-18]()\n\n---\n\n## Adding New Tools to the System\n\nTo add a new tool to the agent system:\n\n### Step 1: Create Tool Class\n\nCreate a new file in `src/agent/tool/` that inherits from `BaseTool`:\n\n```python\nfrom src.agent.tool.base_tool import BaseTool\nfrom pydantic import Field\n\nclass MyNewTool(BaseTool):\n    \"\"\"Description of what this tool does.\"\"\"\n    \n    param1: str = Field(..., description=\"Parameter description\")\n    \n    def run(self) -> str:\n        # Implementation\n        return \"result\"\n```\n\n### Step 2: Register in Dispatcher\n\nAdd a conditional branch in `call_tools_safely()` at [src/agent/action.py:14-20]():\n\n```python\nelif function_name == MyNewTool.tool_name():\n    my_new_tool = MyNewTool(**arguments)\n    tool_info[\"content\"] = my_new_tool.run()\n```\n\n### Step 3: Import Tool\n\nAdd the import statement at the top of [src/agent/action.py]():\n\n```python\nfrom src.agent.tool.my_new_tool import MyNewTool\n```\n\nThe tool is now automatically available to the agent through the schema generation system.\n\n**Sources:** [src/agent/action.py:7-8](), [src/agent/action.py:14-20]()\n\n---\n\n## Design Rationale\n\n### Why Pydantic BaseModel?\n\nUsing Pydantic as the foundation provides several advantages:\n\n1. **Automatic Validation:** Parameters are validated at instantiation time\n2. **Schema Generation:** JSON Schema is auto-generated from type hints\n3. **Documentation:** Field descriptions become part of the schema\n4. **Type Safety:** IDE support for autocomplete and type checking\n5. **Serialization:** Built-in JSON serialization for logging and debugging\n\n### Why Class Methods for Metadata?\n\nTool identification methods (`tool_name()`, `tool_description()`, `get_tool_schema()`) are class methods because:\n\n1. They describe the tool class, not a specific instance\n2. They can be called without instantiation\n3. They're needed before parameters are available\n4. They enable tool discovery and schema generation at startup\n\n### Why String Return Type?\n\nThe `run()` method returns `str` rather than structured data because:\n\n1. **LLM Compatibility:** String results integrate seamlessly into conversation context\n2. **Flexibility:** Tools can format output for readability\n3. **Error Messages:** Failures can be described naturally\n4. **Consistency:** Uniform interface regardless of tool complexity\n\n**Sources:** [src/agent/tool/base_tool.py:1-76]()\n\n---\n\n## Summary\n\nThe `BaseTool` interface provides a standardized contract for all tools in the algo_agent system:\n\n- **Identification:** Automatic tool naming and description extraction\n- **Schema Generation:** OpenAI-compatible function calling schemas\n- **Parameter Validation:** Pydantic-based type checking and constraints\n- **Execution Contract:** Abstract `run()` method for tool logic\n- **Error Handling:** Wrapped in `call_tools_safely()` for robustness\n- **Extensibility:** Simple pattern for adding new tools\n\nThis architecture enables the LLM to discover, understand, and invoke tools dynamically while maintaining type safety and comprehensive error handling.\n\n**Sources:** [src/agent/tool/base_tool.py:1-76](), [src/agent/action.py:1-49]()\n\n---\n\n# Page: Python Code Execution Tool\n\n# Python Code Execution Tool\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [README.md](README.md)\n- [docs/error correction.design.md](docs/error correction.design.md)\n- [docs/log.md](docs/log.md)\n- [src/agent/tool/python_tool.py](src/agent/tool/python_tool.py)\n- [src/agent/tool/todo_tool.py](src/agent/tool/todo_tool.py)\n- [src/runtime/cwd.py](src/runtime/cwd.py)\n- [src/runtime/subprocess_python_executor.py](src/runtime/subprocess_python_executor.py)\n- [src/runtime/workspace.py](src/runtime/workspace.py)\n- [src/utils/__pycache__/__init__.cpython-312.pyc](src/utils/__pycache__/__init__.cpython-312.pyc)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThe `ExecutePythonCodeTool` is a core agent capability that enables the execution of arbitrary Python code snippets in an isolated, stateful environment. This tool serves as the agent's computational engine, allowing it to perform data processing, algorithm implementation, calculations, and file operations during its reasoning process.\n\n**Scope of this document:**\n- Tool interface, parameters, and schema definition\n- Execution workflow from tool invocation to result delivery\n- State persistence mechanisms across multiple executions\n- Working directory management and environment setup\n- Error handling and timeout enforcement\n- Return format and LLM feedback\n\n**Related documentation:**\n- For the abstract tool interface: see [BaseTool Interface](#4.1)\n- For detailed execution strategies (subprocess, subthread, direct): see [Execution Runtime](#5)\n- For global variable management: see [Workspace State Management](#5.5)\n- For execution result schemas: see [ExecutionResult and Status Handling](#5.4)\n\n---\n\n## Tool Definition and Schema\n\nThe `ExecutePythonCodeTool` class extends `BaseTool` and defines two primary parameters that the LLM must provide when invoking the tool:\n\n| Parameter | Type | Required | Default | Description |\n|-----------|------|----------|---------|-------------|\n| `python_code_snippet` | `str` | Yes | - | Valid Python code to execute. Must not contain malicious operations. |\n| `timeout` | `int` | No | 30 | Maximum execution time in seconds before termination. |\n\n### Tool Description for LLM Context\n\nThe tool provides a detailed description that guides the LLM on proper usage:\n\n```\nå¿é¡»è°ç¨å¨æ¯ä¸è½®æ¨çä¸­ï¼ä½ä¸ºè®¡ç®å·¥å·ã\nå¨æç¶æçç¯å¢ä¸­æ§è¡Pythonä»£ç çæ®µï¼ç±»ä¼¼äºå¨Jupyter Notebookä¸­è¿è¡ååæ ¼ã\n\nåè½ï¼\n- ä½¿ç¨ Python 3.12 æ§è¡è®¡ç®ãæ°æ®å¤çåé»è¾æ§è¡\n- ç¶ææä¹åï¼å¨ä¸æ¬¡è°ç¨ä¸­å®ä¹çåéåå½æ°ä¼ä¸ºåç»­è°ç¨ä¿ç\n- éè¯¯åé¦ï¼å¦æä»£ç è¿è¡å¤±è´¥ï¼å°è¿åè¯¦ç»çåæº¯ä¿¡æ¯\n\nå³é®è§åï¼\n1. è¾åºå¯è§æ§ï¼å¿é¡»ä½¿ç¨ print(...) æè½æ¥çä»»ä½ç»æ\n2. å¯¼å¥æ¨¡åï¼ç±äºåºååéå¶ï¼æ¨¡åä¸ä¼æç»­å­å¨ï¼å¨æ¯ä¸ªä»£ç çæ®µä¸­å§ç»éæ°å¯¼å¥å¿è¦çæ¨¡å\n3. å®å¨æ§ï¼æ éå¾ªç¯æè¿è¡æ¶é´æé¿çä»£ç å°è¢«è¶æ¶æºå¶ç»æ­¢\n4. ä¾èµç®¡çï¼å¯ä½¿ç¨ subprocess.check_call([\"uv\", \"add\", package_name]) å®è£ä¾èµ\n```\n\nSources: [src/agent/tool/python_tool.py:13-40]()\n\n---\n\n## Execution Flow Architecture\n\n### High-Level Workflow\n\nThe following diagram illustrates how code execution flows from tool invocation through the execution runtime and workspace management back to the LLM:\n\n```mermaid\ngraph TB\n    LLM[\"LLM (qwen-plus)\"]\n    AgentLoop[\"Agent Decision Loop\"]\n    PythonTool[\"ExecutePythonCodeTool\"]\n    WorkspaceGet[\"workspace.get_arg_globals()\"]\n    Executor[\"subthread_python_executor<br/>run_structured_in_thread()\"]\n    WorkspaceAppend[\"workspace.append_out_globals()\"]\n    \n    LLM -->|\"tool_call with<br/>python_code_snippet<br/>timeout\"| AgentLoop\n    AgentLoop -->|\"dispatch to tool\"| PythonTool\n    PythonTool -->|\"1. Retrieve execution context\"| WorkspaceGet\n    WorkspaceGet -->|\"Returns filtered globals dict\"| PythonTool\n    PythonTool -->|\"2. Execute code with context\"| Executor\n    Executor -->|\"Returns ExecutionResult<br/>(status, stdout, globals)\"| PythonTool\n    PythonTool -->|\"3. Persist output globals\"| WorkspaceAppend\n    PythonTool -->|\"4. Return ret_tool2llm\"| AgentLoop\n    AgentLoop -->|\"tool result message\"| LLM\n    \n    subgraph \"State Management\"\n        WorkspaceGet\n        WorkspaceAppend\n        GlobalsList[\"arg_globals_list<br/>out_globals_list\"]\n        WorkspaceGet -.->|\"reads from\"| GlobalsList\n        WorkspaceAppend -.->|\"writes to\"| GlobalsList\n    end\n```\n\nSources: [src/agent/tool/python_tool.py:41-50](), [src/runtime/workspace.py:81-98]()\n\n### Code Execution Method\n\nThe `run()` method orchestrates the entire execution process:\n\n```python\ndef run(self) -> str:\n    # 1. Retrieve the current execution context (global variables from previous executions)\n    execution_context: Optional[Dict[str, Any]] = workspace.get_arg_globals()\n    \n    # 2. Log the code snippet for debugging\n    global_logger.info(f\"æ§è¡Pythonä»£ç çæ®µï¼{pprint.pformat(self.python_code_snippet)}\")\n    \n    # 3. Execute code in subthread executor with timeout\n    exec_result: subthread_python_executor.ExecutionResult = subthread_python_executor.run_structured_in_thread(\n        command=self.python_code_snippet, \n        _globals=execution_context,\n        timeout=self.timeout\n    )\n    \n    # 4. Persist modified globals for next execution\n    workspace.append_out_globals(exec_result.arg_globals)\n    \n    # 5. Return formatted result to LLM\n    return exec_result.ret_tool2llm\n```\n\nSources: [src/agent/tool/python_tool.py:41-50]()\n\n---\n\n## State Persistence Mechanism\n\nOne of the tool's most important features is **stateful execution** - variables and functions defined in one execution persist to subsequent executions, mimicking a Jupyter notebook environment.\n\n### Workspace Global Variables\n\nThe workspace module maintains two global lists that track execution state:\n\n| List | Purpose | Content |\n|------|---------|---------|\n| `arg_globals_list` | Input globals for each execution | Filtered globals dict passed to executor |\n| `out_globals_list` | Output globals after each execution | Modified globals returned from executor |\n\n### State Flow Diagram\n\n```mermaid\nsequenceDiagram\n    participant Tool as ExecutePythonCodeTool\n    participant WSGet as workspace.get_arg_globals()\n    participant WSFilter as filter_and_deepcopy_globals()\n    participant Executor as Execution Runtime\n    participant WSAppend as workspace.append_out_globals()\n    \n    Note over Tool: Execution 1: x = 10\n    Tool->>WSGet: Retrieve context\n    WSGet->>WSGet: arg_globals_list empty,<br/>initialize_workspace()\n    WSGet->>WSFilter: Filter builtins & modules\n    WSFilter-->>Tool: {'__name__': '__main__'}\n    Tool->>Executor: exec(\"x = 10\", globals)\n    Executor-->>Tool: ExecutionResult with<br/>arg_globals={'x': 10}\n    Tool->>WSAppend: Persist output\n    WSAppend->>WSFilter: Filter & deepcopy\n    WSFilter->>WSAppend: Add to out_globals_list\n    \n    Note over Tool: Execution 2: print(x + 5)\n    Tool->>WSGet: Retrieve context\n    WSGet->>WSGet: Read from out_globals_list[-1]\n    WSGet->>WSFilter: Filter & deepcopy\n    WSFilter-->>Tool: {'x': 10, '__name__': '__main__'}\n    Tool->>Executor: exec(\"print(x + 5)\", globals)\n    Executor-->>Tool: ExecutionResult with stdout=\"15\"\n    Tool->>WSAppend: Persist output\n```\n\nSources: [src/runtime/workspace.py:81-98](), [src/runtime/workspace.py:38-78]()\n\n### Global Variable Filtering\n\nTo ensure cross-process serialization, the workspace filters globals before persistence:\n\n**Filtering Rules:**\n1. **Exclude `__builtins__`** - Standard Python builtins are not serialized\n2. **Exclude module objects** - Module references cannot be pickled reliably\n3. **Verify pickle serialization** - Only objects that can be serialized via `pickle.dumps()` are retained\n4. **Deep copy values** - All retained values are deep-copied to prevent reference issues\n\n```python\ndef filter_and_deepcopy_globals(original_globals: Dict[str, Any]) -> Dict[str, Any]:\n    filtered_dict = {}\n    for key, value in original_globals.items():\n        # Exclude __builtins__\n        if key == '__builtins__':\n            continue\n        # Exclude module types\n        import sys\n        if isinstance(value, type(sys)):\n            continue\n        # Verify picklability\n        if _is_serializable(value) is False:\n            continue\n        # Deep copy valid values\n        filtered_dict[key] = copy.deepcopy(value)\n    return filtered_dict\n```\n\nSources: [src/runtime/workspace.py:38-78]()\n\n---\n\n## Execution Runtime Integration\n\nWhile the `ExecutePythonCodeTool` defaults to using `subthread_python_executor`, it can be configured to use different execution strategies. The tool abstracts the execution backend, focusing on the interface contract rather than implementation details.\n\n### Executor Selection\n\nThe tool currently uses the **subthread executor** for a balance of isolation and performance:\n\n```python\nexec_result: subthread_python_executor.ExecutionResult = subthread_python_executor.run_structured_in_thread(\n    command=self.python_code_snippet, \n    _globals=execution_context,\n    timeout=self.timeout\n)\n```\n\n**Rationale for subthread executor:**\n- **Moderate isolation**: Runs in separate thread, preventing blocking of main agent loop\n- **Shared memory**: Can access and modify globals dictionary without IPC overhead\n- **Timeout support**: Can enforce execution time limits\n- **Stdout capture**: Redirects output to buffer for collection\n\nFor detailed information on all execution strategies (subprocess, subthread, direct), see [Execution Runtime](#5).\n\nSources: [src/agent/tool/python_tool.py:44-48]()\n\n---\n\n## Timeout and Error Handling\n\n### Timeout Enforcement\n\nThe tool accepts a `timeout` parameter (default: 30 seconds) that limits execution time:\n\n```python\ntimeout: int = Field(\n    30, \n    description=\"æ§è¡ä»£ç çæå¤§æ¶é´ï¼ç§ï¼ãå¦æä»£ç è¿è¡æ¶é´è¶è¿æ­¤å¼ï¼å°è¢«ç»æ­¢å¹¶è¿åéè¯¯æ¶æ¯ã\"\n)\n```\n\nWhen a timeout occurs:\n1. The executor terminates the running code\n2. An `ExecutionResult` is created with `exit_status=ExecutionStatus.TIMEOUT`\n3. The LLM receives a formatted error message indicating the timeout\n\n### Error Categories\n\nThe execution system captures four types of execution outcomes:\n\n| Status | Enum Value | Cause | Example |\n|--------|-----------|-------|---------|\n| Success | `ExecutionStatus.SUCCESS` | Code executed without exceptions | `print(\"hello\")` |\n| Failure | `ExecutionStatus.FAILURE` | Python exception raised | `1 / 0` (ZeroDivisionError) |\n| Timeout | `ExecutionStatus.TIMEOUT` | Execution exceeded timeout limit | `while True: pass` |\n| Crashed | `ExecutionStatus.CRASHED` | Process/thread crashed unexpectedly | Segmentation fault |\n\n### Exception Information\n\nWhen execution fails, the `ExecutionResult` captures comprehensive error details:\n\n```python\nExecutionResult(\n    exit_status=ExecutionStatus.FAILURE,\n    exception_repr=repr(e),              # \"ZeroDivisionError('division by zero')\"\n    exception_type=type(e).__name__,     # \"ZeroDivisionError\"\n    exception_value=str(e),              # \"division by zero\"\n    exception_traceback=traceback.format_exc()  # Full stack trace\n)\n```\n\nThis detailed feedback allows the LLM to diagnose and correct errors in subsequent tool calls.\n\nSources: [src/runtime/subprocess_python_executor.py:52-69](), [src/runtime/schemas.py]()\n\n---\n\n## Working Directory Management\n\nEach execution occurs in a dedicated working directory to isolate file operations and prevent conflicts between concurrent executions.\n\n### Directory Context Setup\n\nThe `cwd` module provides working directory management:\n\n```python\n# In subprocess executor\ndef _worker_with_pipe(...):\n    # Set working directory for this execution\n    cwd.create_cwd('./wsm/2/g7-2')\n    # ... execute code ...\n```\n\n**Directory structure:**\n```\nalgo_agent/\nâââ wsm/          # Workspace root\nâ   âââ 1/        # Execution group 1\nâ   â   âââ g4-1/ # Specific execution workspace\nâ   âââ 2/        # Execution group 2\nâ   â   âââ g7-2/ # Specific execution workspace\nâ   âââ 3/        # Execution group 3\n```\n\n### Context Manager Pattern\n\nFor temporary directory changes, the `ChangeDirectory` context manager ensures proper cleanup:\n\n```python\nwith ChangeDirectory('./wsm/3/g8-1'):\n    # Execute operations in this directory\n    # File I/O, data processing, etc.\n    pass\n# Automatically restored to original directory\n```\n\nThis pattern is useful when:\n- Code needs to read/write files in a specific location\n- Multiple executions must be isolated from each other\n- Testing different working directory scenarios\n\nSources: [src/runtime/cwd.py:9-28](), [src/runtime/cwd.py:31-54](), [src/runtime/subprocess_python_executor.py:27-28]()\n\n---\n\n## Return Format and LLM Feedback\n\n### ExecutionResult Schema\n\nThe executor returns a structured `ExecutionResult` object containing all execution details:\n\n```mermaid\nclassDiagram\n    class ExecutionResult {\n        +str arg_command\n        +dict arg_globals\n        +int arg_timeout\n        +ExecutionStatus exit_status\n        +int exit_code\n        +str ret_stdout\n        +str ret_tool2llm\n        +str exception_repr\n        +str exception_type\n        +str exception_value\n        +str exception_traceback\n    }\n    \n    class ExecutionStatus {\n        <<enumeration>>\n        SUCCESS\n        FAILURE\n        TIMEOUT\n        CRASHED\n    }\n    \n    ExecutionResult --> ExecutionStatus\n```\n\n### LLM-Formatted Response\n\nThe `ret_tool2llm` field provides a formatted message tailored for LLM consumption:\n\n**Success Example:**\n```\næ§è¡æåï¼\næ åè¾åºï¼\nHello, World!\nResult: 42\n```\n\n**Failure Example:**\n```\næ§è¡å¤±è´¥ï¼\néè¯¯ç±»åï¼ZeroDivisionError\néè¯¯ä¿¡æ¯ï¼division by zero\nå¼å¸¸å æ ï¼\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nZeroDivisionError: division by zero\n```\n\n**Timeout Example:**\n```\næ§è¡è¶æ¶ï¼\nè¶æ¶è®¾ç½®ï¼30ç§\nä»£ç å¯è½åå«æ éå¾ªç¯æèæ¶è¿é¿çæä½ã\n```\n\nThis formatted feedback enables the LLM to:\n1. Understand whether execution succeeded\n2. Read captured output via `print()` statements\n3. Diagnose errors from exception details\n4. Adjust code and retry with corrections\n\nSources: [src/runtime/schemas.py](), [src/agent/tool/python_tool.py:50]()\n\n---\n\n## Execution Flow Example\n\n### Natural Language to Code Entity Mapping\n\n```mermaid\ngraph LR\n    subgraph \"Natural Language Space\"\n        UserIntent[\"User: 'Calculate<br/>sum of 1 to 100'\"]\n        LLMDecision[\"LLM decides to<br/>use Python tool\"]\n    end\n    \n    subgraph \"Tool Layer\"\n        ToolCall[\"tool_call with<br/>python_code_snippet:<br/>'sum(range(1, 101))'\"]\n        ExecutePython[\"ExecutePythonCodeTool.run()\"]\n    end\n    \n    subgraph \"Execution Layer\"\n        GetGlobals[\"workspace.get_arg_globals()\"]\n        RunThread[\"run_structured_in_thread()\"]\n        AppendGlobals[\"workspace.append_out_globals()\"]\n    end\n    \n    subgraph \"Result Space\"\n        ExecResult[\"ExecutionResult<br/>exit_status=SUCCESS<br/>ret_stdout='5050'\"]\n        ToolResult[\"tool result message<br/>to LLM\"]\n        LLMResponse[\"LLM: 'The sum<br/>is 5050'\"]\n    end\n    \n    UserIntent --> LLMDecision\n    LLMDecision --> ToolCall\n    ToolCall --> ExecutePython\n    ExecutePython --> GetGlobals\n    GetGlobals --> RunThread\n    RunThread --> ExecResult\n    ExecResult --> AppendGlobals\n    AppendGlobals --> ToolResult\n    ToolResult --> LLMResponse\n```\n\nSources: [src/agent/tool/python_tool.py:41-50]()\n\n### Multi-Execution State Example\n\nThe following example demonstrates state persistence across multiple tool invocations:\n\n**Execution 1: Define data structure**\n```python\n# LLM calls tool with:\npython_code_snippet = \"\"\"\nimport json\ndata = {'users': [{'id': 1, 'name': 'Alice'}, {'id': 2, 'name': 'Bob'}]}\nprint(\"Data initialized\")\n\"\"\"\n# Result: \"Data initialized\"\n# State: data variable persisted\n```\n\n**Execution 2: Process data**\n```python\n# LLM calls tool with:\npython_code_snippet = \"\"\"\nimport json  # Must re-import (modules don't persist)\nuser_names = [u['name'] for u in data['users']]\nprint(\"User names:\", user_names)\n\"\"\"\n# Result: \"User names: ['Alice', 'Bob']\"\n# State: data and user_names both persisted\n```\n\n**Execution 3: Compute statistics**\n```python\n# LLM calls tool with:\npython_code_snippet = \"\"\"\ncount = len(user_names)\nprint(f\"Total users: {count}\")\n\"\"\"\n# Result: \"Total users: 2\"\n# State: All variables (data, user_names, count) persisted\n```\n\nThis stateful execution model allows the agent to:\n- Break complex operations into logical steps\n- Inspect intermediate results between operations\n- Build up complex data structures incrementally\n- Reuse computed values across multiple reasoning steps\n\nSources: [src/agent/tool/python_tool.py:14-40](), [src/runtime/workspace.py:81-98]()\n\n---\n\n## Module Import Considerations\n\nDue to serialization constraints in the state persistence mechanism, **imported modules do not persist across executions**. The tool description explicitly guides the LLM on this behavior:\n\n**Rule:** Always re-import necessary modules in each code snippet.\n\n**Correct Pattern:**\n```python\n# Execution 1\nimport numpy as np\narr = np.array([1, 2, 3])\nprint(arr)\n\n# Execution 2\nimport numpy as np  # Re-import required\nresult = np.sum(arr)  # arr persists, np does not\nprint(result)\n```\n\n**Incorrect Pattern:**\n```python\n# Execution 1\nimport numpy as np\narr = np.array([1, 2, 3])\n\n# Execution 2\nresult = np.sum(arr)  # Error: 'np' is not defined\n```\n\n**Why modules don't persist:**\n- Module objects fail the `_is_serializable()` check in `filter_and_deepcopy_globals()`\n- They are excluded from `out_globals_list`\n- This prevents pickle serialization errors and ensures clean cross-process boundaries\n\nSources: [src/agent/tool/python_tool.py:24-26](), [src/runtime/workspace.py:68-73]()\n\n---\n\n## Dependency Installation\n\nThe tool supports dynamic dependency installation during execution:\n\n```python\npython_code_snippet = \"\"\"\nimport subprocess\nsubprocess.check_call([\"uv\", \"add\", \"pandas\"])\nimport pandas as pd\ndf = pd.DataFrame({'a': [1, 2, 3]})\nprint(df)\n\"\"\"\n```\n\nThis capability enables the agent to:\n1. Detect missing dependencies from `ImportError` exceptions\n2. Install required packages using the `uv` package manager\n3. Retry execution with the newly installed dependency\n4. Continue with data processing or computation\n\n**Note:** Installation executes in the same subprocess/thread as the code, so installed packages are immediately available.\n\nSources: [src/agent/tool/python_tool.py:27]()\n\n---\n\n## Integration with Agent Loop\n\nThe Python execution tool integrates seamlessly with the agent's decision loop:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Agent as Deep Research Agent\n    participant LLM\n    participant Action as Action Coordinator\n    participant PyTool as ExecutePythonCodeTool\n    participant Executor\n    \n    User->>Agent: \"Process this dataset\"\n    Agent->>LLM: Generate response with tools\n    LLM-->>Agent: tool_call: ExecutePythonCodeTool\n    Agent->>Action: call_tools_safely()\n    Action->>PyTool: Instantiate with parameters\n    PyTool->>PyTool: workspace.get_arg_globals()\n    PyTool->>Executor: run_structured_in_thread()\n    Executor-->>PyTool: ExecutionResult\n    PyTool->>PyTool: workspace.append_out_globals()\n    PyTool-->>Action: ret_tool2llm\n    Action-->>Agent: Tool result\n    Agent->>LLM: Continue with tool result\n    LLM-->>Agent: Next action or final answer\n    Agent-->>User: Response\n```\n\n**Key integration points:**\n1. **Tool discovery**: Agent includes tool schema in LLM context\n2. **Parameter validation**: Pydantic validates `python_code_snippet` and `timeout`\n3. **Safe execution**: `call_tools_safely()` wraps execution with exception handling\n4. **Result appending**: Tool result added to message history for LLM context\n5. **Iterative refinement**: LLM can call tool multiple times to refine solutions\n\nFor details on the action coordination mechanism, see [Action Coordination](#3.4).\n\nSources: [src/agent/tool/python_tool.py:13-50]()\n\n---\n\n## Summary\n\nThe `ExecutePythonCodeTool` provides the agent with powerful computational capabilities through:\n\n| Feature | Implementation | Benefit |\n|---------|----------------|---------|\n| **Stateful Execution** | `workspace.get_arg_globals()` / `append_out_globals()` | Variables persist like Jupyter notebooks |\n| **Isolation** | Subthread executor with timeout | Safe execution without blocking agent |\n| **Error Feedback** | `ExecutionResult` with traceback | LLM can diagnose and fix errors |\n| **Output Capture** | Redirected stdout/stderr | LLM sees print() output |\n| **Timeout Protection** | Enforced time limits | Prevents infinite loops |\n| **Working Directory** | `cwd.create_cwd()` | Isolated file operations |\n| **Dependency Management** | `uv add` support | Dynamic package installation |\n\nThis tool serves as the primary mechanism for the agent to perform data analysis, algorithm implementation, file processing, and computational tasks during its research and problem-solving activities.\n\nSources: [src/agent/tool/python_tool.py:13-50](), [src/runtime/workspace.py](), [src/runtime/subprocess_python_executor.py](), [src/runtime/cwd.py]()\n\n---\n\n# Page: Recursive Task Planning Tool\n\n# Recursive Task Planning Tool\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/agent/tool/base_tool.py](src/agent/tool/base_tool.py)\n- [src/agent/tool/python_tool.py](src/agent/tool/python_tool.py)\n- [src/agent/tool/todo_tool.py](src/agent/tool/todo_tool.py)\n- [src/memory/tree_todo/schemas.py](src/memory/tree_todo/schemas.py)\n- [src/memory/tree_todo/todo_track.py](src/memory/tree_todo/todo_track.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThe Recursive Task Planning Tool (`RecursivePlanTreeTodoTool`) enables the agent to maintain hierarchical task structures with nested subtasks, status tracking, and automated change detection. This tool allows the LLM to decompose complex research or problem-solving goals into manageable task trees, track progress across multiple reasoning steps, and analyze what has changed between planning iterations.\n\nThis document covers the tool's implementation, data schemas, version tracking mechanism, and integration with the agent loop. For information about the base tool interface, see [BaseTool Interface](#4.1). For code execution capabilities, see [Python Code Execution Tool](#4.2).\n\n**Sources:** [src/agent/tool/todo_tool.py:1-37](), [src/memory/tree_todo/schemas.py:1-81]()\n\n---\n\n## Tool Interface\n\nThe `RecursivePlanTreeTodoTool` class extends `BaseTool` and provides a structured interface for managing recursive task trees.\n\n### Class Definition\n\n```\nRecursivePlanTreeTodoTool(BaseTool)\nâââ tool_call_purpose: str          # Required: purpose of this tool invocation\nâââ recursive_plan_tree: RecursivePlanTree  # The task tree to process\n```\n\nThe tool's `run()` method delegates to `todo_track.run()`, which:\n1. Stores the current plan tree as a new version\n2. Compares with the previous version to detect changes\n3. Renders a Markdown visualization of the tree\n4. Calculates status statistics\n\nThe tool's docstring specifies that it **must be called** before other tools as part of the agent's thinking and recording process, then other tools can be invoked for execution.\n\n**Sources:** [src/agent/tool/todo_tool.py:10-36]()\n\n### Tool Schema Generation\n\nLike all tools, `RecursivePlanTreeTodoTool` automatically generates a JSON schema for LLM consumption through the inherited `get_tool_schema()` method. The schema includes:\n\n- `tool_call_purpose`: Description of why the tool is being invoked\n- `recursive_plan_tree`: A nested structure with `RecursivePlanTree` definition including all node properties\n\nThe schema uses JSON Schema `$defs` to define recursive references for `RecursivePlanTreeNode`, allowing unlimited nesting depth.\n\n**Sources:** [src/agent/tool/base_tool.py:30-71](), [logs/global.log:214-308]()\n\n---\n\n## Data Model\n\nThe task planning system uses three core Pydantic models to represent hierarchical task structures.\n\n### Task Status Enumeration\n\nThe `TaskStatus` enum defines six possible states for any task node:\n\n| Status | Value | Symbol | Description |\n|--------|-------|--------|-------------|\n| PENDING | `\"pending\"` | â³ | Task is waiting to be executed |\n| PROCESSING | `\"processing\"` | â¡ï¸ | Task is currently being executed |\n| COMPLETED | `\"completed\"` | â | Task finished successfully |\n| FAILED | `\"failed\"` | â | Task execution failed |\n| RETRY | `\"retry\"` | â»ï¸ | Task needs to be retried |\n| SKIPPED | `\"skipped\"` | â | Task was intentionally skipped |\n\nEach status includes display properties:\n- `display_symbol`: Visual emoji indicator for Markdown rendering\n- `display_desc`: Human-readable description in Chinese\n\n**Sources:** [src/memory/tree_todo/schemas.py:7-41]()\n\n### RecursivePlanTreeNode\n\nThe core task unit that can contain child tasks recursively:\n\n```python\nRecursivePlanTreeNode:\n    task_id: str                          # Auto-generated UUID (e.g., \"TASK-123e4567\")\n    task_name: str                        # Unique descriptive name (required)\n    description: str = \"\"                 # Detailed explanation (optional)\n    status: TaskStatus = PENDING          # Current execution state\n    output: str = \"\"                      # Results when completed/failed\n    dependencies: List[str] | None        # List of task_name strings this depends on\n    research_directions: List[str] | None # Research paths for complex tasks\n    children: List[RecursivePlanTreeNode] | None  # Nested subtasks\n```\n\n**Key behaviors:**\n- `task_id` is auto-generated using UUID if not provided\n- `task_name` must be globally unique as it's referenced by `dependencies` lists\n- `children` is set to `None` if empty (validated by `empty_children_to_none`)\n- Self-referential structure resolved via `model_rebuild()`\n\n**Sources:** [src/memory/tree_todo/schemas.py:44-64]()\n\n### RecursivePlanTree\n\nThe top-level container representing the entire planning structure:\n\n```python\nRecursivePlanTree:\n    core_goal: str                        # Overall objective (required)\n    tree_nodes: List[RecursivePlanTreeNode]  # Root-level tasks\n    next_action: Dict[str, Any] = {}      # Suggested next steps (optional)\n    references: List[str] | None          # Resource links/citations (optional)\n```\n\nThe `tree_nodes` list contains all root-level tasks, each of which may have nested children forming a hierarchical structure.\n\n**Sources:** [src/memory/tree_todo/schemas.py:68-80]()\n\n### Data Model Diagram\n\n```mermaid\nclassDiagram\n    class TaskStatus {\n        <<enumeration>>\n        PENDING\n        PROCESSING\n        COMPLETED\n        FAILED\n        RETRY\n        SKIPPED\n        +display_symbol() str\n        +display_desc() str\n    }\n    \n    class RecursivePlanTreeNode {\n        +task_id: str\n        +task_name: str\n        +description: str\n        +status: TaskStatus\n        +output: str\n        +dependencies: List[str]\n        +research_directions: List[str]\n        +children: List[RecursivePlanTreeNode]\n    }\n    \n    class RecursivePlanTree {\n        +core_goal: str\n        +tree_nodes: List[RecursivePlanTreeNode]\n        +next_action: Dict\n        +references: List[str]\n    }\n    \n    class RecursivePlanTreeTodoTool {\n        +tool_call_purpose: str\n        +recursive_plan_tree: RecursivePlanTree\n        +run() str\n    }\n    \n    RecursivePlanTreeNode --> TaskStatus\n    RecursivePlanTreeNode --> RecursivePlanTreeNode : children\n    RecursivePlanTree --> RecursivePlanTreeNode : tree_nodes\n    RecursivePlanTreeTodoTool --> RecursivePlanTree\n```\n\n**Sources:** [src/memory/tree_todo/schemas.py:1-81](), [src/agent/tool/todo_tool.py:10-26]()\n\n---\n\n## Version Tracking System\n\nThe tool maintains a history of plan tree versions to enable change detection and incremental updates.\n\n### Global State Variables\n\nThe tracking system maintains three global lists in `todo_track.py`:\n\n```python\narg_todo_list: List[RecursivePlanTree]  # Input history (all submitted versions)\nout_todo_list: List[RecursivePlanTree]  # Output history (processed versions)\ntrack_diff_result_list: List[str]       # Change summaries\n```\n\nEach list is initialized with an empty placeholder tree with `core_goal=\"ç©ºè®¡åæ ç­å¾åå§å\"`.\n\n**Sources:** [src/memory/tree_todo/todo_track.py:6-18]()\n\n### Version Comparison Workflow\n\n```mermaid\nsequenceDiagram\n    participant LLM as \"LLM (Agent)\"\n    participant Tool as \"RecursivePlanTreeTodoTool\"\n    participant Track as \"todo_track.run()\"\n    participant Analyze as \"_analyze_changes()\"\n    participant Render as \"_render_plan_tree_markdown()\"\n    \n    LLM->>Tool: \"Call with recursive_plan_tree\"\n    Tool->>Track: \"run(current_plan_tree)\"\n    \n    Track->>Track: \"Retrieve last_plan from arg_todo_list[-1]\"\n    Track->>Track: \"Append current_plan_tree to arg_todo_list\"\n    \n    alt \"First invocation (no history)\"\n        Track->>Track: \"changes_summary = 'â é¦æ¬¡åå»ºè®¡åæ '\"\n    else \"Subsequent invocations\"\n        Track->>Analyze: \"_analyze_changes(last_plan, current_plan)\"\n        Analyze->>Analyze: \"collect_all_task_ids() for both trees\"\n        Analyze->>Analyze: \"Detect new, deleted, status-changed tasks\"\n        Analyze->>Analyze: \"Detect hierarchy changes (parent shifts)\"\n        Analyze-->>Track: \"Formatted change summary\"\n    end\n    \n    Track->>Render: \"_render_plan_tree_markdown(tree_nodes, 0)\"\n    Render->>Render: \"Recursively format with indentation\"\n    Render-->>Track: \"Markdown string\"\n    \n    Track->>Track: \"_calculate_status_statistics()\"\n    Track-->>Tool: \"Dict with changes_summary, markdown, stats\"\n    Tool-->>LLM: \"Formatted string output\"\n```\n\n**Sources:** [src/memory/tree_todo/todo_track.py:21-49]()\n\n### Change Detection Logic\n\nThe `_analyze_changes()` function identifies four types of modifications:\n\n1. **New Tasks** (ð): Tasks present in current tree but not in last version\n   - Collected by comparing `task_id` sets\n   - Reported with task names\n\n2. **Deleted Tasks** (ðï¸): Tasks in last version but removed from current\n   - Typically indicates scope changes or task completion consolidation\n\n3. **Status Changes** (ð): Tasks with different `status` values between versions\n   - Displays transitions like \"pending â completed\"\n   - Shows old and new status descriptions\n\n4. **Hierarchy Adjustments** (ð): Tasks moved to different parent nodes\n   - Detected by comparing parent task names via `_find_parent_task()`\n   - Indicates task reorganization\n\n**Sources:** [src/memory/tree_todo/todo_track.py:62-121]()\n\n### Task Lookup Functions\n\n```python\n_get_task_by_id(nodes, task_id) -> RecursivePlanTreeNode | None\n    # Recursively searches tree for node with matching task_id\n    # Depth-first traversal through children\n\n_find_parent_task(nodes, target_task_id) -> RecursivePlanTreeNode | None\n    # Recursively finds parent node containing target_task_id in children\n    # Returns None if task is at root level\n```\n\nThese utility functions enable efficient tree traversal for change detection.\n\n**Sources:** [src/memory/tree_todo/todo_track.py:51-135]()\n\n---\n\n## Markdown Rendering\n\nThe tool generates human-readable Markdown representations of the task tree for LLM consumption and debugging.\n\n### Rendering Algorithm\n\n```mermaid\ngraph TD\n    Start[\"_render_plan_tree_markdown(nodes, indent_level)\"] --> Loop[\"For each node in nodes\"]\n    Loop --> GetStatus[\"Get node.status.display_symbol\"]\n    GetStatus --> BuildLine[\"Build task line:<br/>indent + '- ' + symbol + task_name + task_id\"]\n    \n    BuildLine --> CheckStatus{\"node.status in<br/>[FAILED, RETRY, SKIPPED]?\"}\n    CheckStatus -->|Yes| AddStatus[\"Append: '| ç¶æï¼' + status_desc\"]\n    CheckStatus -->|No| CheckDesc\n    AddStatus --> CheckDesc\n    \n    CheckDesc{\"node.description<br/>not empty?\"}\n    CheckDesc -->|Yes| AddDesc[\"Append: '  > è¯´æï¼' + description\"]\n    CheckDesc -->|No| CheckOutput\n    AddDesc --> CheckOutput\n    \n    CheckOutput{\"node.output<br/>not empty?\"}\n    CheckOutput -->|Yes| AddOutput[\"Append: '  > ç»æï¼' + output\"]\n    CheckOutput -->|No| CheckChildren\n    AddOutput --> CheckChildren\n    \n    CheckChildren{\"node.children<br/>exists?\"}\n    CheckChildren -->|Yes| Recurse[\"Recursively call with<br/>children, indent_level + 1\"]\n    CheckChildren -->|No| AppendLine\n    Recurse --> AppendLine[\"Append formatted line\"]\n    \n    AppendLine --> Loop\n    Loop --> Return[\"Return joined lines\"]\n```\n\n**Sources:** [src/memory/tree_todo/todo_track.py:137-171]()\n\n### Example Markdown Output\n\nFor a task tree with nested structure, the output format is:\n\n```markdown\n- [â³] **è¯»åå¹¶è§£æ schema.json æä»¶**ï¼IDï¼TASK-abc123ï¼\n  > è¯´æï¼å è½½ schema.json æä»¶åå®¹ï¼è§£æå¶ JSON ç»æ\n  - [â] **è¯å«å³é®å®ä½**ï¼IDï¼TASK-def456ï¼\n    > ç»æï¼å·²è¯å« Task, Carrier, Resource ç­æ ¸å¿å®ä½\n  - [â¡ï¸] **åæå®ä½é´å³ç³»**ï¼IDï¼TASK-ghi789ï¼ | ç¶æï¼æ­£å¨æ§è¡\n- [â³] **è¯»åå¹¶æ£æ¥åºæ¥ååºæ°æ®æä»¶**ï¼IDï¼TASK-jkl012ï¼\n```\n\nEach task displays:\n- Indentation level (2 spaces per level)\n- Status symbol\n- Bold task name\n- Task ID in parentheses\n- Optional description prefixed with `> è¯´æï¼`\n- Optional output prefixed with `> ç»æï¼`\n- Nested children with increased indentation\n\n**Sources:** [src/memory/tree_todo/todo_track.py:137-171]()\n\n---\n\n## Status Statistics\n\nThe tool calculates aggregate statistics across all tasks in the tree.\n\n### Statistics Structure\n\nThe `_calculate_status_statistics()` function returns a dictionary with:\n\n```python\n{\n    \"pending\": int,      # Count of PENDING tasks\n    \"processing\": int,   # Count of PROCESSING tasks\n    \"completed\": int,    # Count of COMPLETED tasks\n    \"failed\": int,       # Count of FAILED tasks\n    \"retry\": int,        # Count of RETRY tasks\n    \"skipped\": int,      # Count of SKIPPED tasks\n    \"__total\": int,      # Total task count\n    \"__completion_rate\": float,  # completed / total (rounded to 2 decimals)\n    \"__pending_rate\": float      # pending / total (rounded to 2 decimals)\n}\n```\n\n### Calculation Process\n\n1. Initialize all status counts to 0\n2. Recursively traverse `tree_nodes` and all nested `children`\n3. Increment counter for each node's `status.value`\n4. Calculate derived metrics (total, completion rate, pending rate)\n\n**Sources:** [src/memory/tree_todo/todo_track.py:173-201]()\n\n---\n\n## Integration with Agent Loop\n\nThe recursive task planning tool plays a critical role in the agent's reasoning process.\n\n### Agent Workflow with Todo Tool\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Agent as \"user_query()\"\n    participant LLM as \"LLM Service\"\n    participant TodoTool as \"RecursivePlanTreeTodoTool\"\n    participant PyTool as \"ExecutePythonCodeTool\"\n    \n    User->>Agent: \"Initial query with complex goal\"\n    Agent->>Agent: \"init_messages_with_system_prompt()\"\n    \n    Note over Agent,LLM: \"System prompt instructs:<br/>ç»´æ¤ä¸é¢åæè®¡åæ \"\n    \n    Agent->>LLM: \"Generate with tool schemas\"\n    LLM->>LLM: \"Decompose goal into task tree\"\n    LLM-->>Agent: \"tool_calls: [recursive_plan_tree_todo]\"\n    \n    Agent->>TodoTool: \"Call with initial tree structure\"\n    TodoTool->>TodoTool: \"Store in arg_todo_list[0]\"\n    TodoTool->>TodoTool: \"Render markdown + stats\"\n    TodoTool-->>Agent: \"â é¦æ¬¡åå»ºè®¡åæ <br/>Markdown visualization\"\n    \n    Agent->>Agent: \"Append tool result to messages\"\n    Agent->>LLM: \"Continue with updated context\"\n    LLM-->>Agent: \"tool_calls: [execute_python_code]\"\n    \n    Agent->>PyTool: \"Execute task code\"\n    PyTool-->>Agent: \"Execution results\"\n    \n    Agent->>LLM: \"Continue with execution results\"\n    LLM->>LLM: \"Update task statuses based on results\"\n    LLM-->>Agent: \"tool_calls: [recursive_plan_tree_todo]\"\n    \n    Agent->>TodoTool: \"Call with updated tree\"\n    TodoTool->>TodoTool: \"Compare with arg_todo_list[-1]\"\n    TodoTool->>TodoTool: \"Detect: ð ç¶æåæ´<br/>pending â completed\"\n    TodoTool-->>Agent: \"Change summary + updated markdown\"\n    \n    Agent->>Agent: \"Loop until LLM returns final answer\"\n```\n\n**Sources:** [logs/utils.log:1-58](), [logs/global.log:1-308]()\n\n### System Prompt Integration\n\nThe agent's system prompt explicitly mentions maintaining a task tree:\n\n```\nä½ çä¸»å¾ªç¯æ¯ç»´æ¤ä¸é¢åæè®¡åæ ï¼\n1. ç½åå¼å¾è®¡ç®åæ¢ç´¢çæ¹åï¼å½¢æåæè®¡åæ çèç¹ã\n2. æ ¹æ®åæè®¡åæ çèç¹ï¼å³å®æ¯å¦éè¦è°ç¨å·¥å·æ¥è·åä¿¡æ¯ã\n3. å¨å¿è¦æ¶ï¼è°ç¨å·¥å·å¹¶å¤çè¿åçä¿¡æ¯ï¼ä»¥ä¾¿æä¾åç¡®çç­æ¡ã\n4. æ ¹æ®è·åçä¿¡æ¯ï¼æ´æ°åæè®¡åæ ï¼ç»§ç»­è¿è¡åæï¼ç´å°å¾åºæç»ç»è®ºã\n```\n\nThis instructs the LLM to use the todo tool as a thinking and planning mechanism throughout the reasoning process.\n\n**Sources:** [logs/global.log:115-126]()\n\n---\n\n## Usage Example from Logs\n\nThe following demonstrates actual tool usage from the emergency response planning scenario.\n\n### Initial Tree Creation\n\nThe LLM called `recursive_plan_tree_todo` with a comprehensive research plan:\n\n```json\n{\n  \"tool_call_purpose\": \"å»ºç«åå§åæè®¡åæ ï¼æç¡®ç ç©¶ç®æ åä»»å¡åè§£æ¹å\",\n  \"recursive_plan_tree\": {\n    \"core_goal\": \"åºäºæä¾çåºæ¥ææ´æ°æ®ï¼æ·±å¥çè§£æ°æ®ç»æå¹¶è®¾è®¡å¯æµè¯çå¤æºè½ä½ååè°åº¦ç®æ³ã\",\n    \"tree_nodes\": [\n      {\n        \"task_id\": \"T1\",\n        \"task_name\": \"è¯»åå¹¶è§£æ schema.json æä»¶\",\n        \"description\": \"å è½½ schema.json æä»¶åå®¹ï¼è§£æå¶ JSON ç»æ...\",\n        \"status\": \"pending\",\n        \"research_directions\": [\n          \"è¯å«å³é®å®ä½ï¼Task, Carrier, Resource, Location...\",\n          \"åæå®ä½é´å³ç³»ï¼ä»»å¡ä¸è½½ä½çåéå³ç³»...\"\n        ],\n        \"dependencies\": null,\n        \"children\": null,\n        \"output\": \"\",\n        \"references\": [\"./schema.json\"]\n      },\n      {\n        \"task_id\": \"T2\",\n        \"task_name\": \"è¯»åå¹¶æ£æ¥åºæ¥ååºæ°æ®æä»¶\",\n        \"description\": \"æ¹éè¯»å emergency_response_data_01.json è³ 05.json...\",\n        \"status\": \"pending\",\n        \"dependencies\": [\"è¯»åå¹¶è§£æ schema.json æä»¶\"],\n        ...\n      },\n      ...\n    ]\n  }\n}\n```\n\nThe tree contained 12 tasks (T1-T12) organized hierarchically with dependencies and research directions. Tasks ranged from data structure analysis to algorithm implementation and performance evaluation.\n\n**Sources:** [logs/utils.log:57-58]()\n\n### Task Dependencies\n\nNotice that `T2` specifies:\n```json\n\"dependencies\": [\"è¯»åå¹¶è§£æ schema.json æä»¶\"]\n```\n\nThis references the `task_name` of `T1`, establishing an execution order constraint. The LLM uses these dependencies to sequence tool calls appropriately.\n\n**Sources:** [logs/utils.log:57-58]()\n\n### Multiple Tool Calls Pattern\n\nAfter establishing the plan tree, the LLM made multiple `execute_python_code` tool calls in a single response to execute tasks in parallel:\n\n```python\ntool_calls=[\n    ChatCompletionMessageFunctionToolCall(\n        id='call_3085f1f75d534390a7c2b7',\n        function=Function(\n            name='execute_python_code',\n            arguments='{\"tool_call_purpose\": \"è¯»åå¹¶è§£æ schema.json æä»¶\",...}'\n        )\n    ),\n    ChatCompletionMessageFunctionToolCall(\n        id='call_ab025bb2bb3e430aaefd92',\n        function=Function(\n            name='execute_python_code',\n            arguments='{\"tool_call_purpose\": \"è¯»åå¹¶è§£æ emergency_response_data_01.json...\"}'\n        )\n    ),\n    ...\n]\n```\n\nThis demonstrates the tool's role in coordinating complex multi-step research processes.\n\n**Sources:** [logs/utils.log:127-129]()\n\n---\n\n## Implementation Notes\n\n### Empty Return Value\n\nThe current implementation of `RecursivePlanTreeTodoTool.run()` returns an empty string:\n\n```python\ndef run(self, ) -> Dict[str, str]:\n    result = todo_track.run(self.recursive_plan_tree)\n    s  = (f\"åæ´æ»ç»ï¼\\n{result['changes_summary']}\")\n    s += (f\"Markdownæ¸åï¼\\n{result['markdown_todo_list']}\\n\")\n    s += (f\"status_statistics: {pprint.pformat(result['status_statistics'])}\")\n    s = \"\"  # Overwritten to empty string\n    return s\n```\n\nThe function constructs a formatted string with change summary, markdown, and statistics, but then overwrites it with an empty string before returning. This appears to be intentional to avoid cluttering the conversation history, with the actual state management happening through the global `arg_todo_list` storage.\n\n**Sources:** [src/agent/tool/todo_tool.py:28-36]()\n\n### Model Serialization\n\nThe Pydantic v2 migration is evident in the code:\n- `model_copy(deep=True)` instead of `copy()`\n- `model_dump()` instead of `dict()`\n- `model_rebuild()` for self-referential models\n\nThese changes maintain compatibility with Pydantic v2's stricter validation and improved performance.\n\n**Sources:** [src/memory/tree_todo/todo_track.py:32](), [src/agent/tool/todo_tool.py:70]()\n\n### Task ID Generation\n\nTask IDs are automatically generated using UUID4 if not provided:\n\n```python\ntask_id: str = Field(\n    default_factory=lambda: f\"TASK-{str(uuid.uuid4())}\", \n    description=\"ä»»å¡å¯ä¸ID...\"\n)\n```\n\nHowever, the LLM can also provide explicit IDs (like \"T1\", \"T2\") for easier reference in subsequent calls.\n\n**Sources:** [src/memory/tree_todo/schemas.py:46]()\n\n---\n\n## Summary\n\nThe Recursive Task Planning Tool provides:\n\n| Capability | Implementation |\n|------------|----------------|\n| **Hierarchical Planning** | Recursive `RecursivePlanTreeNode` with unlimited nesting |\n| **Status Tracking** | Six-state `TaskStatus` enum with visual symbols |\n| **Version History** | Global `arg_todo_list` storing all submitted versions |\n| **Change Detection** | Automated comparison of task IDs, statuses, and hierarchy |\n| **Visualization** | Markdown rendering with indentation and status symbols |\n| **Statistics** | Aggregate counts and completion rates |\n| **LLM Integration** | JSON schema generation for function calling |\n\nThis tool enables the agent to maintain complex research plans across multiple reasoning iterations, providing structure and memory to the otherwise stateless LLM conversation.\n\n**Sources:** [src/agent/tool/todo_tool.py:1-133](), [src/memory/tree_todo/schemas.py:1-81](), [src/memory/tree_todo/todo_track.py:1-201]()\n\n---\n\n# Page: Execution Runtime\n\n# Execution Runtime\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [docs/crashed.design.md](docs/crashed.design.md)\n- [src/runtime/before_thread/plt_back_chinese.py](src/runtime/before_thread/plt_back_chinese.py)\n- [src/runtime/cwd.py](src/runtime/cwd.py)\n- [src/runtime/python_executor.py](src/runtime/python_executor.py)\n- [src/runtime/schemas.py](src/runtime/schemas.py)\n- [src/runtime/subprocess_python_executor.py](src/runtime/subprocess_python_executor.py)\n- [src/runtime/subthread_python_executor.py](src/runtime/subthread_python_executor.py)\n- [tests/playground/crashed.py](tests/playground/crashed.py)\n- [tests/playground/gen/g9/beijing_scenic_spot_schema_with_play_hours.json](tests/playground/gen/g9/beijing_scenic_spot_schema_with_play_hours.json)\n- [tests/playground/gen/g9/beijing_scenic_spot_validated_data_with_play_hours.json](tests/playground/gen/g9/beijing_scenic_spot_validated_data_with_play_hours.json)\n- [tests/playground/gen/g9/g9.py](tests/playground/gen/g9/g9.py)\n- [tests/playground/subprocess_output.py](tests/playground/subprocess_output.py)\n- [tests/unit/runtime/test_exec_runner.py](tests/unit/runtime/test_exec_runner.py)\n- [tests/unit/runtime/test_python_executor.py](tests/unit/runtime/test_python_executor.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThe Execution Runtime system provides safe, isolated execution of arbitrary Python code snippets with comprehensive error handling and state management. This page provides an overview of the multi-strategy execution architecture that enables the agent to run user-provided code with varying levels of isolation and performance characteristics.\n\nFor detailed information about specific execution strategies, see:\n- [Subprocess Execution](#5.1) - Process-isolated execution with pipe-based IPC\n- [Subthread Execution](#5.2) - Thread-based execution with buffer capture\n- [Direct Execution](#5.3) - In-process execution for maximum performance\n- [ExecutionResult Schema](#5.4) - Structured result format and status codes\n- [Workspace State Management](#5.5) - Global variable persistence across executions\n- [Working Directory Management](#5.6) - Environment setup and output redirection\n\n## Multi-Strategy Execution Architecture\n\nThe system implements three distinct execution strategies, each optimized for different use cases. The choice of strategy balances isolation strength, resource overhead, and execution performance.\n\n| Strategy | Isolation Level | Process Model | IPC Mechanism | Timeout Support | Crash Detection | Primary Use Case |\n|----------|----------------|---------------|---------------|-----------------|-----------------|------------------|\n| **Subprocess** | Full process isolation | `multiprocessing.Process` | Pipe-based bidirectional | Yes (terminate) | Yes (exitcode) | Production, untrusted code |\n| **Subthread** | Shared memory space | `threading.Thread` | In-memory buffer | Yes (non-blocking) | Limited | Matplotlib, GUI libraries |\n| **Direct** | No isolation | Same process | Queue | Yes (terminate worker) | No | Testing, trusted code |\n\nSources: [src/runtime/subprocess_python_executor.py](), [src/runtime/subthread_python_executor.py](), [src/runtime/python_executor.py]()\n\n## System Architecture\n\nThe following diagram illustrates how the three execution strategies interact with the core runtime components:\n\n```mermaid\ngraph TB\n    subgraph \"Execution Request\"\n        INPUT[\"Python Code String<br/>+ _globals dict<br/>+ timeout int\"]\n    end\n    \n    subgraph \"Strategy Dispatcher\"\n        TOOL[\"ExecutePythonCodeTool\"]\n        SELECTOR[\"Strategy Selection Logic\"]\n    end\n    \n    subgraph \"Execution Strategies\"\n        SUBPROCESS[\"run_structured_in_subprocess<br/>subprocess_python_executor.py\"]\n        SUBTHREAD[\"run_structured_in_thread<br/>subthread_python_executor.py\"]\n        DIRECT[\"run_structured<br/>python_executor.py\"]\n    end\n    \n    subgraph \"Execution Workers\"\n        SUBPROC_WORKER[\"_worker_with_pipe<br/>Separate Process\"]\n        THREAD_WORKER[\"_worker_with_buffer<br/>Separate Thread\"]\n        DIRECT_WORKER[\"worker_with_globals_capture<br/>Same Process\"]\n    end\n    \n    subgraph \"Runtime Infrastructure\"\n        PIPE[\"multiprocessing.Pipe<br/>_PipeType.STDOUT/RESULT\"]\n        BUFFER[\"list[str] buffer<br/>shared memory\"]\n        QUEUE[\"multiprocessing.Queue\"]\n        \n        CWD[\"cwd.create_cwd<br/>cwd.ChangeDirectory\"]\n        STDOUT[\"sys.stdout/stderr<br/>redirection\"]\n    end\n    \n    subgraph \"Result Processing\"\n        SCHEMA[\"ExecutionResult<br/>schemas.py\"]\n        STATUS[\"ExecutionStatus<br/>SUCCESS/FAILURE/TIMEOUT/CRASHED\"]\n        WORKSPACE[\"workspace.filter_and_deepcopy_globals\"]\n    end\n    \n    INPUT --> TOOL\n    TOOL --> SELECTOR\n    \n    SELECTOR -->|\"process isolation\"| SUBPROCESS\n    SELECTOR -->|\"GUI libraries\"| SUBTHREAD\n    SELECTOR -->|\"testing\"| DIRECT\n    \n    SUBPROCESS --> SUBPROC_WORKER\n    SUBTHREAD --> THREAD_WORKER\n    DIRECT --> DIRECT_WORKER\n    \n    SUBPROC_WORKER -.uses.-> PIPE\n    SUBPROC_WORKER -.uses.-> CWD\n    SUBPROC_WORKER -.redirects.-> STDOUT\n    \n    THREAD_WORKER -.uses.-> BUFFER\n    THREAD_WORKER -.uses.-> CWD\n    THREAD_WORKER -.redirects.-> STDOUT\n    \n    DIRECT_WORKER -.uses.-> QUEUE\n    DIRECT_WORKER -.redirects.-> STDOUT\n    \n    SUBPROC_WORKER --> SCHEMA\n    THREAD_WORKER --> SCHEMA\n    DIRECT_WORKER --> SCHEMA\n    \n    SCHEMA --> STATUS\n    SCHEMA --> WORKSPACE\n```\n\n**Diagram: Execution Runtime Architecture** - Shows the relationship between execution strategies, worker processes/threads, and the shared result schema.\n\nSources: [src/runtime/subprocess_python_executor.py:76-163](), [src/runtime/subthread_python_executor.py:86-128](), [src/runtime/python_executor.py:92-130]()\n\n## Execution Flow\n\nAll three strategies follow a common execution pattern with strategy-specific implementations:\n\n```mermaid\nsequenceDiagram\n    participant Tool as ExecutePythonCodeTool\n    participant Executor as Execution Strategy\n    participant Worker as Worker Process/Thread\n    participant Runtime as exec() Runtime\n    participant Result as ExecutionResult\n    \n    Tool->>Executor: run_structured_in_X(command, _globals, timeout)\n    \n    Note over Executor: Setup IPC channel<br/>(Pipe/Buffer/Queue)\n    \n    Executor->>Worker: Start worker process/thread\n    \n    Note over Worker: Redirect stdout/stderr<br/>Change working directory\n    \n    Worker->>Runtime: exec(command, _globals, _locals)\n    \n    alt Code Executes Successfully\n        Runtime-->>Worker: Execution completes\n        Worker->>Result: Create SUCCESS result\n    else Exception Raised\n        Runtime-->>Worker: Exception caught\n        Worker->>Result: Create FAILURE result<br/>+ traceback\n    else Process/Thread Crashes\n        Runtime--xWorker: Unexpected termination\n        Note over Executor: No result received\n        Executor->>Result: Create CRASHED result\n    end\n    \n    par Timeout Monitoring\n        Executor->>Executor: join(timeout)\n        alt Timeout Exceeded\n            Executor->>Worker: terminate/kill\n            Executor->>Result: Create TIMEOUT result\n        end\n    end\n    \n    Worker->>Executor: Send result via IPC\n    Executor->>Executor: Collect stdout buffer\n    Executor->>Executor: Set exit_code\n    Executor->>Executor: Generate ret_tool2llm\n    Executor-->>Tool: Return ExecutionResult\n```\n\n**Diagram: Common Execution Flow** - Sequence diagram showing the standardized execution lifecycle across all strategies.\n\nSources: [src/runtime/subprocess_python_executor.py:76-163](), [src/runtime/subthread_python_executor.py:86-128]()\n\n## ExecutionResult Schema\n\nAll execution strategies return a unified `ExecutionResult` object that encapsulates the execution outcome:\n\n### Core Structure\n\nThe `ExecutionResult` model defined in [src/runtime/schemas.py:56-71]() contains:\n\n| Field Category | Field Name | Type | Description |\n|---------------|------------|------|-------------|\n| **Input Arguments** | `arg_command` | `str` | The executed Python code |\n| | `arg_timeout` | `int` | Configured timeout in seconds |\n| | `arg_globals` | `Dict[str, Any]` | Filtered global variables (post-execution) |\n| **Execution Status** | `exit_status` | `ExecutionStatus` | Enum: SUCCESS/FAILURE/TIMEOUT/CRASHED |\n| | `exit_code` | `Optional[int]` | Process exit code (subprocess only) |\n| **Error Information** | `exception_repr` | `Optional[str]` | Python repr() of exception |\n| | `exception_type` | `Optional[str]` | Exception class name |\n| | `exception_value` | `Optional[str]` | Exception message |\n| | `exception_traceback` | `Optional[str]` | Full traceback string |\n| **Output Capture** | `ret_stdout` | `str` | Combined stdout/stderr output |\n| | `ret_tool2llm` | `Optional[str]` | Formatted message for LLM consumption |\n\nSources: [src/runtime/schemas.py:56-71]()\n\n### ExecutionStatus Enum\n\nThe `ExecutionStatus` enum [src/runtime/schemas.py:10-47]() defines four possible outcomes:\n\n```python\nclass ExecutionStatus(str, Enum):\n    SUCCESS = \"success\"   # Code executed without exceptions\n    FAILURE = \"failure\"   # Python exception raised during execution\n    TIMEOUT = \"timeout\"   # Execution exceeded timeout limit\n    CRASHED = \"crashed\"   # Process terminated unexpectedly (SegFault, OOM)\n```\n\nEach status has an associated LLM-friendly message template generated by `ExecutionStatus.get_return_llm()` [src/runtime/schemas.py:18-47]() that formats the result for the agent to understand and act upon.\n\nSources: [src/runtime/schemas.py:10-47]()\n\n## Subprocess Execution Strategy\n\nThe **subprocess executor** provides the strongest isolation by executing code in a separate OS process using `multiprocessing.Process`.\n\n### Key Characteristics\n\n- **Full Process Isolation**: Child process has its own memory space, preventing memory corruption in the parent\n- **Pipe-Based IPC**: Bidirectional communication using `multiprocessing.Pipe()` for stdout streaming and result transmission\n- **Crash Detection**: Can detect segmentation faults, memory errors, and other process crashes via exit codes\n- **Timeout Enforcement**: Uses `Process.terminate()` to forcefully kill runaway processes\n\n### Implementation Entry Point\n\n```python\n@traceable\ndef run_structured_in_subprocess(\n    command: str,\n    _globals: dict[str, Any] | None = None,\n    _locals: Optional[Dict] = None,\n    timeout: Optional[int] = None,\n) -> ExecutionResult:\n```\n\nLocation: [src/runtime/subprocess_python_executor.py:76-163]()\n\nFor detailed implementation analysis, see [Subprocess Execution](#5.1).\n\nSources: [src/runtime/subprocess_python_executor.py:19-163]()\n\n## Subthread Execution Strategy\n\nThe **subthread executor** runs code in a separate thread within the same process, sharing the memory space with the parent.\n\n### Key Characteristics\n\n- **Shared Memory Space**: All threads share the same Python interpreter and global state\n- **Buffer-Based Output**: Captures stdout/stderr using in-memory list buffer\n- **GUI Library Support**: Required for libraries like `matplotlib` that have GUI backend restrictions\n- **Limited Crash Protection**: Cannot fully isolate from crashes like SegFaults\n\n### Implementation Entry Point\n\n```python\n@traceable\ndef run_structured_in_thread(\n    command: str,\n    _globals: dict[str, Any] | None = None,\n    _locals: Optional[Dict] = None,\n    timeout: Optional[int] = None,\n) -> ExecutionResult:\n```\n\nLocation: [src/runtime/subthread_python_executor.py:86-128]()\n\n### Special Initialization\n\nBefore any matplotlib code execution, the system imports [src/runtime/before_thread/plt_back_chinese.py:1-13]() which:\n1. Sets matplotlib backend to `Agg` (non-GUI) via `matplotlib.use(\"Agg\")`\n2. Configures Chinese font support with `SimHei`\n\nThis prevents GUI-related warnings and errors when running plotting code in threads.\n\nSources: [src/runtime/subthread_python_executor.py:13-128](), [src/runtime/before_thread/plt_back_chinese.py:1-13]()\n\n## Direct Execution Strategy\n\nThe **direct executor** runs code in the same process with minimal isolation, used primarily for testing and trusted code.\n\n### Key Characteristics\n\n- **In-Process Execution**: No separate process or thread created\n- **Queue-Based Communication**: Uses `multiprocessing.Queue` for result passing\n- **Fastest Execution**: No process/thread startup overhead\n- **No Crash Protection**: Cannot recover from process crashes\n\n### Implementation Entry Point\n\n```python\n@traceable\ndef run_structured(\n    command: str, \n    _globals: dict[str, Any] | None = None, \n    _locals: Optional[Dict] = None, \n    timeout: Optional[int] = None\n) -> ExecutionResult:\n```\n\nLocation: [src/runtime/python_executor.py:92-130]()\n\nFor detailed implementation, see [Direct Execution](#5.3).\n\nSources: [src/runtime/python_executor.py:28-130]()\n\n## Working Directory and Output Management\n\n### Directory Context Management\n\nThe `cwd` module provides context managers for safely changing working directories and redirecting output streams:\n\n#### ChangeDirectory Context Manager\n\n```python\nclass ChangeDirectory:\n    \"\"\"Automatically restores original directory on exit\"\"\"\n    def __init__(self, target_dir):\n        self.target_dir = target_dir\n        self.original_dir = None\n```\n\nLocation: [src/runtime/cwd.py:31-54]()\n\nUsage pattern:\n```python\nwith cwd.ChangeDirectory('./wsm/4/g9-1'):\n    # Code executes in this directory\n    exec(command, _globals, _locals)\n# Directory automatically restored\n```\n\n#### Output Redirection Context Manager\n\n```python\nclass Change_STDOUT_STDERR:\n    \"\"\"Captures stdout/stderr to custom writer\"\"\"\n    def __init__(self, new_stdout, new_stderr=None):\n        self.original_stdout = sys.stdout\n        self.original_stderr = sys.stderr\n```\n\nLocation: [src/runtime/cwd.py:56-77]()\n\nFor more details, see [Working Directory and Environment](#5.6).\n\nSources: [src/runtime/cwd.py:9-92]()\n\n## Workspace State Management\n\nThe execution runtime integrates with the workspace management system to persist variables across multiple code executions.\n\n### Global Variable Filtering\n\nThe `ExecutionResult` model automatically filters and validates global variables using a field validator [src/runtime/schemas.py:77-83]():\n\n```python\n@field_validator('arg_globals')\n@classmethod        \ndef field_validate_globals(cls, value: Dict[str, Any]) -> Dict[str, Any]:\n    if value is None:\n        return {}\n    return workspace.filter_and_deepcopy_globals(value)\n```\n\nThis ensures that only serializable, non-builtin objects are preserved between executions.\n\nFor complete details on workspace management, see [Workspace State Management](#5.5).\n\nSources: [src/runtime/schemas.py:77-83]()\n\n## Strategy Selection Guidelines\n\nThe following table provides guidance on when to use each execution strategy:\n\n| Use Case | Recommended Strategy | Reasoning |\n|----------|---------------------|-----------|\n| Untrusted user code | Subprocess | Full isolation prevents memory corruption |\n| Long-running computations | Subprocess | Timeout enforcement via process termination |\n| Code that might crash | Subprocess | Exit code detection for crash analysis |\n| Matplotlib/plotting | Subthread | GUI backend compatibility |\n| Data visualization | Subthread | Shared memory for efficient data access |\n| Quick calculations | Direct | Minimal overhead for simple operations |\n| Unit testing | Direct | Fast iteration, controlled environment |\n| I/O heavy operations | Subprocess | Prevents blocking main process |\n\n## Implementation Details by Strategy\n\n### Subprocess: Pipe-Based Communication\n\nThe subprocess strategy uses a two-channel protocol defined by `_PipeType` enum [src/runtime/subprocess_python_executor.py:13-16]():\n\n```python\nclass _PipeType(str, Enum):\n    STDOUT = \"stdout\"   # Real-time output streaming\n    RESULT = \"result\"   # Final ExecutionResult object\n```\n\nA dedicated reader thread [src/runtime/subprocess_python_executor.py:88-112]() continuously reads from the pipe and dispatches messages to appropriate buffers.\n\n### Subthread: Buffer Writer\n\nThe subthread strategy uses a custom writer class [src/runtime/subthread_python_executor.py:23-32]() that appends output to a shared list:\n\n```python\nclass _BufferWriter:\n    def __init__(self, buffer: list[str]):\n        self.buffer = buffer\n    \n    def write(self, msg: str):\n        if msg:\n            self.buffer.append(msg)\n```\n\n### Traceback Filtering\n\nThe subthread executor implements specialized traceback filtering [src/runtime/subthread_python_executor.py:47-72]() to extract only the relevant `<string>` (exec'd code) portions, removing infrastructure noise.\n\nSources: [src/runtime/subprocess_python_executor.py:13-112](), [src/runtime/subthread_python_executor.py:23-72]()\n\n## Exit Code Semantics\n\nProcess exit codes follow standard conventions [src/runtime/schemas.py:50-54]():\n\n| Exit Code Range | Meaning | Detection |\n|----------------|---------|-----------|\n| `0` | Normal exit | SUCCESS or FAILURE (exception caught) |\n| `> 0` | Error exit | FAILURE or CRASHED |\n| `< 0` (e.g. `-15`) | Signal termination | TIMEOUT (SIGTERM) or CRASHED (SIGSEGV=-11) |\n| `-11` / `139` | Segmentation fault | CRASHED |\n| `-9` | SIGKILL | TIMEOUT (force kill) |\n\nThe subprocess executor captures exit codes via `Process.exitcode` [src/runtime/subprocess_python_executor.py:160]().\n\nSources: [src/runtime/schemas.py:50-54](), [src/runtime/subprocess_python_executor.py:160]()\n\n## Error Handling and Recovery\n\nAll strategies implement consistent error handling:\n\n1. **Exception Capture**: Python exceptions are caught and stored in `ExecutionResult` with full traceback\n2. **Timeout Detection**: Separate monitoring detects execution exceeding configured timeout\n3. **Crash Detection**: Subprocess strategy detects unexpected process termination\n4. **Output Preservation**: All stdout/stderr captured before termination\n\nThe `ExecutionStatus.get_return_llm()` method [src/runtime/schemas.py:18-47]() formats these errors into actionable messages for the LLM to diagnose and fix code issues.\n\nSources: [src/runtime/schemas.py:18-47](), [src/runtime/subprocess_python_executor.py:120-163]()\n\n## Testing and Validation\n\nThe system includes comprehensive test suites:\n\n- **Unit Tests**: [tests/unit/runtime/test_python_executor.py]() - Tests basic executor functionality\n- **Integration Tests**: [tests/playground/subprocess_output.py]() - Validates timeout, crashes, exceptions\n- **Crash Scenarios**: [tests/playground/crashed.py]() - Tests recursion limits, OOM, SegFault detection\n\nExample test output from [tests/playground/subprocess_output.py:296-705]() demonstrates:\n- Timeout detection with partial output\n- Exception capture with line numbers\n- Segmentation fault detection (exit code 139)\n- Recursion error handling\n- OOM detection\n\nSources: [tests/unit/runtime/test_python_executor.py](), [tests/playground/subprocess_output.py:162-292](), [tests/playground/crashed.py]()\n\n---\n\n# Page: Subprocess Execution\n\n# Subprocess Execution\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [docs/crashed.design.md](docs/crashed.design.md)\n- [src/runtime/cwd.py](src/runtime/cwd.py)\n- [src/runtime/schemas.py](src/runtime/schemas.py)\n- [src/runtime/subprocess_python_executor.py](src/runtime/subprocess_python_executor.py)\n- [tests/playground/crashed.py](tests/playground/crashed.py)\n- [tests/playground/subprocess_output.py](tests/playground/subprocess_output.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page covers the **subprocess execution strategy**, which provides the strongest isolation level for Python code execution in the algo_agent system. Subprocess execution uses separate operating system processes with pipe-based inter-process communication (IPC) to execute user code safely.\n\nThe subprocess executor handles:\n- Full process isolation with separate memory spaces\n- Real-time stdout/stderr capture via pipes\n- Timeout enforcement with graceful termination\n- Crash detection for segmentation faults, stack overflows, and memory exhaustion\n- Working directory management for isolated execution environments\n- Exit code analysis for diagnostic information\n\nFor other execution strategies, see [Subthread Execution](#5.2) and [Direct Execution](#5.3). For the execution result format, see [ExecutionResult and Status Handling](#5.4). For workspace state management, see [Workspace State Management](#5.5).\n\n---\n\n## Architecture Overview\n\nThe subprocess execution system consists of three coordinated components: the parent process that manages execution, the child subprocess that runs user code, and a reader thread that captures output in real-time.\n\n```mermaid\ngraph TB\n    Parent[\"Parent Process<br/>run_structured_in_subprocess\"]\n    Child[\"Child Process<br/>_worker_with_pipe\"]\n    Reader[\"Reader Thread<br/>_reader\"]\n    \n    Pipe[\"Multiprocessing Pipe<br/>parent_conn â child_conn\"]\n    \n    Buffers[\"Output Buffers<br/>subprocess_stdout_buffer<br/>subprocess_result_container\"]\n    \n    Result[\"ExecutionResult<br/>exit_status + exit_code<br/>ret_stdout + arg_globals\"]\n    \n    Parent -->|\"multiprocessing.Process(target=_worker_with_pipe)\"| Child\n    Parent -->|\"threading.Thread(target=_reader)\"| Reader\n    \n    Child -->|\"send(STDOUT, msg)\"| Pipe\n    Child -->|\"send(RESULT, ExecutionResult)\"| Pipe\n    \n    Pipe -->|\"recv()\"| Reader\n    Reader -->|\"append\"| Buffers\n    \n    Parent -->|\"p.join(timeout)\"| Child\n    Parent -->|\"t.join()\"| Reader\n    Parent -->|\"assemble final result\"| Result\n    \n    Buffers -->|\"data source\"| Result\n```\n\n**Diagram: Subprocess Execution Architecture**\n\nThe parent process creates a bidirectional pipe, spawns a child process to execute code, and launches a reader thread to capture output. The child sends two types of messages: real-time stdout and a final ExecutionResult. The parent waits for the child to complete or timeout, then assembles the final result.\n\n**Sources:** [src/runtime/subprocess_python_executor.py:76-163]()\n\n---\n\n## Process Isolation Model\n\nSubprocess execution provides complete isolation by running user code in a separate operating system process. This isolates:\n\n| Isolation Aspect | Implementation | Benefit |\n|------------------|----------------|---------|\n| **Memory Space** | Separate process address space | Prevents memory corruption of parent process |\n| **CPU Resources** | OS-level process scheduling | Can be terminated without affecting parent |\n| **File Handles** | Inherited but independent after fork | File operations don't interfere with parent |\n| **Working Directory** | Changed in child only via `cwd.create_cwd()` | Isolates file I/O operations |\n| **Exception Handling** | Child catches and serializes exceptions | Parent remains stable even on child crash |\n| **Signals** | Child can receive termination signals | Enables force-kill for runaway processes |\n\nThe isolation is achieved through the `multiprocessing.Process` class, which uses platform-appropriate process creation mechanisms (fork on Unix, spawn on Windows).\n\n**Sources:** [src/runtime/subprocess_python_executor.py:106-117](), [docs/crashed.design.md:339-400]()\n\n---\n\n## Communication Protocol\n\nThe subprocess communicates with the parent through a pipe using a tagged message protocol. Messages are tagged with `_PipeType` to distinguish between different data types.\n\n```mermaid\ngraph LR\n    subgraph \"Child Process\"\n        Exec[\"exec(command, _globals, _locals)\"]\n        Writer[\"_PipeWriter<br/>sys.stdout = _PipeWriter\"]\n        SendStdout[\"send(STDOUT, msg)\"]\n        SendResult[\"send(RESULT, ExecutionResult)\"]\n    end\n    \n    subgraph \"Pipe\"\n        Channel[\"Multiprocessing Pipe<br/>bidirectional channel\"]\n    end\n    \n    subgraph \"Parent Thread (_reader)\"\n        Recv[\"recv() blocks until message\"]\n        Dispatch[\"if tag == STDOUT:<br/>    buffer.append(data)<br/>elif tag == RESULT:<br/>    container.append(data)\"]\n    end\n    \n    Exec -->|\"print() calls write()\"| Writer\n    Writer -->|\"PipeType.STDOUT\"| SendStdout\n    Exec -->|\"on completion/exception\"| SendResult\n    \n    SendStdout --> Channel\n    SendResult --> Channel\n    Channel --> Recv\n    Recv --> Dispatch\n```\n\n**Diagram: Pipe-Based Communication Protocol**\n\n### Message Types\n\nThe `_PipeType` enum defines two message types:\n\n```python\n@unique\nclass _PipeType(str, Enum):\n    STDOUT = \"stdout\"  # Real-time output from print() statements\n    RESULT = \"result\"  # Final ExecutionResult object\n```\n\n**Sources:** [src/runtime/subprocess_python_executor.py:13-16]()\n\n### PipeWriter Implementation\n\nThe child process redirects `sys.stdout` and `sys.stderr` to a custom `_PipeWriter` class that sends output through the pipe:\n\n```python\nclass _PipeWriter:\n    def __init__(self, child_conn: PipeConnection):\n        self.child_conn = child_conn\n\n    def write(self, msg: str):\n        if msg:\n            # Tag as STDOUT for real-time output\n            self.child_conn.send((_PipeType.STDOUT, msg))\n\n    def flush(self):\n        pass\n```\n\nThis enables real-time streaming of output back to the parent process without buffering delays.\n\n**Sources:** [src/runtime/subprocess_python_executor.py:30-45]()\n\n---\n\n## Worker Process Implementation\n\nThe `_worker_with_pipe` function executes in the child process and handles code execution, exception capture, and result transmission.\n\n### Execution Flow\n\n```mermaid\nsequenceDiagram\n    participant Main as Child Main\n    participant CWD as cwd.create_cwd\n    participant Redir as stdout/stderr redirect\n    participant Exec as exec()\n    participant Result as ExecutionResult\n    participant Pipe as child_conn\n    \n    Main->>CWD: create_cwd('./wsm/2/g7-2')\n    CWD-->>Main: working directory set\n    \n    Main->>Redir: sys.stdout = _PipeWriter(child_conn)\n    Main->>Redir: sys.stderr = sys.stdout\n    \n    Main->>Exec: exec(command, _globals, _locals)\n    \n    alt Execution succeeds\n        Exec-->>Main: returns normally\n        Main->>Result: ExecutionResult(exit_status=SUCCESS)\n    else Exception occurs\n        Exec-->>Main: raises exception\n        Main->>Result: ExecutionResult(exit_status=FAILURE)<br/>+ exception details\n    end\n    \n    Main->>Pipe: send((RESULT, res))\n    Main->>Pipe: close()\n```\n\n**Diagram: Worker Process Execution Sequence**\n\n### Success Case\n\nWhen code executes successfully, the worker constructs a success result:\n\n```python\nres = ExecutionResult(\n    arg_command=command,\n    arg_globals=_globals or {},\n    arg_timeout=timeout,\n    exit_status=ExecutionStatus.SUCCESS,\n)\n```\n\n**Sources:** [src/runtime/subprocess_python_executor.py:49-57]()\n\n### Failure Case\n\nWhen an exception occurs, the worker captures comprehensive error information:\n\n```python\nres = ExecutionResult(\n    arg_command=command,\n    arg_globals=_globals or {},\n    arg_timeout=timeout,\n    exit_status=ExecutionStatus.FAILURE,\n    exception_repr=repr(e),\n    exception_type=type(e).__name__,\n    exception_value=str(e),\n    exception_traceback=traceback.format_exc(),\n)\n```\n\nThis includes the exception representation, type, value, and full traceback for debugging.\n\n**Sources:** [src/runtime/subprocess_python_executor.py:58-69]()\n\n### Result Transmission\n\nThe final result is always sent through the pipe, even on exceptions:\n\n```python\nfinally:\n    # Tag as RESULT for final object\n    child_conn.send((_PipeType.RESULT, res))\n    child_conn.close()\n```\n\nThe pipe close operation signals EOF to the reader thread.\n\n**Sources:** [src/runtime/subprocess_python_executor.py:70-73]()\n\n---\n\n## Parent Process Coordination\n\nThe `run_structured_in_subprocess` function orchestrates subprocess execution from the parent process. It manages process lifecycle, output capture, timeout detection, and result assembly.\n\n### Setup Phase\n\n```python\n_locals = _globals  # Must be consistent for exec() namespace behavior\nparent_conn, child_conn = multiprocessing.Pipe()\nsubprocess_stdout_buffer: list[str] = []\nsubprocess_result_container: list[ExecutionResult] = []\n```\n\nThe parent creates:\n- A bidirectional pipe for communication\n- A buffer to accumulate stdout messages\n- A container to hold the final ExecutionResult\n\n**Sources:** [src/runtime/subprocess_python_executor.py:83-86]()\n\n### Process and Thread Launch\n\n```python\np = multiprocessing.Process(\n    target=_worker_with_pipe, args=(command, _globals, _locals, timeout, child_conn)\n)\np.start()\n\n# Must close parent's copy of child_conn for EOF detection\nchild_conn.close()\n\nt = threading.Thread(\n    target=_reader,\n    args=(parent_conn, subprocess_stdout_buffer, subprocess_result_container),\n)\nt.start()\n```\n\nKey details:\n- The child process receives `child_conn` for sending messages\n- The parent must close its copy of `child_conn` so the reader can detect EOF\n- The reader thread runs in the parent process and blocks on `recv()`\n\n**Sources:** [src/runtime/subprocess_python_executor.py:114-126]()\n\n### Reader Thread Implementation\n\nThe `_reader` function continuously reads messages from the pipe until EOF:\n\n```python\ndef _reader(\n    parent_conn: PipeConnection,\n    subprocess_stdout_buffer: list[str],\n    subprocess_result_container: list[ExecutionResult],\n) -> None:\n    \"\"\"Read messages from subprocess and dispatch by type\"\"\"\n    try:\n        while True:\n            # Blocking read until EOF\n            msg: tuple[_PipeType, str | ExecutionResult] = parent_conn.recv()\n            # Protocol dispatch\n            if isinstance(msg, tuple) and len(msg) == 2:\n                tag, data = msg\n                if tag == _PipeType.STDOUT:\n                    subprocess_stdout_buffer.append(data)\n                elif tag == _PipeType.RESULT:\n                    subprocess_result_container.append(data)\n    except (EOFError, OSError) as e:\n        # EOF indicates child closed pipe, normal termination\n        pass\n```\n\nThe reader accumulates messages in shared lists that the parent can access after joining.\n\n**Sources:** [src/runtime/subprocess_python_executor.py:88-112]()\n\n---\n\n## Timeout Enforcement\n\nTimeout detection uses `Process.join(timeout)` to wait for the child process with a time limit. Three scenarios are handled:\n\n### Timeout Detection Flow\n\n```mermaid\ngraph TB\n    Start[\"p.join(timeout)\"]\n    Check{\"p.is_alive()\"}\n    \n    Start --> Check\n    \n    Check -->|\"True (timeout)\"| Terminate[\"p.terminate()\"]\n    Terminate --> Join[\"p.join() (wait for termination)\"]\n    Join --> CloseConn[\"parent_conn.close()\"]\n    CloseConn --> JoinThread[\"t.join() (wait for reader)\"]\n    JoinThread --> BuildTimeout[\"Build ExecutionResult<br/>exit_status=TIMEOUT\"]\n    \n    Check -->|\"False (completed)\"| JoinThread2[\"t.join()\"]\n    JoinThread2 --> CheckResult{\"subprocess_result_container<br/>has result?\"}\n    \n    CheckResult -->|\"Yes\"| UseResult[\"Use child's result\"]\n    CheckResult -->|\"No\"| BuildCrashed[\"Build ExecutionResult<br/>exit_status=CRASHED\"]\n```\n\n**Diagram: Timeout and Crash Detection Flow**\n\n### Timeout Case\n\n```python\np.join(timeout)  \nif p.is_alive():\n    global_logger.info(\"---------- 1. è¶æ¶æåµï¼ç±ç¶è¿ç¨æå»º ExecutionResult\")\n    # Ensure process truly terminates\n    p.terminate()  \n    # Close connection to stop reader\n    p.join()  \n    parent_conn.close()\n    t.join()\n    \n    final_res = ExecutionResult(\n        arg_command=command,\n        arg_timeout=timeout,\n        arg_globals=_globals or {},\n        exit_status=ExecutionStatus.TIMEOUT,\n    )\n```\n\nThe parent:\n1. Calls `terminate()` to send SIGTERM to the child\n2. Waits for actual termination with `join()`\n3. Closes the pipe to unblock the reader thread\n4. Constructs a TIMEOUT result\n\n**Sources:** [src/runtime/subprocess_python_executor.py:128-144]()\n\n### Normal Completion\n\n```python\nelse:\n    global_logger.info(\"---------- 2. æ­£å¸¸æå¼å¸¸éåºï¼ä»å­è¿ç¨è·å ExecutionResult\")\n    t.join()\n\n    if subprocess_result_container:\n        global_logger.info(\"---------- 2.1 å­è¿ç¨æ­£å¸¸éåºï¼ execæ­£å¸¸ãexecå¼å¸¸\")\n        final_res: ExecutionResult = subprocess_result_container[0]\n    else:\n        global_logger.info(\"---------- 2.2 å­è¿ç¨å´©æºéåºï¼å¦ SegFault\")\n        final_res = ExecutionResult(\n            arg_command=command,\n            arg_timeout=timeout,\n            arg_globals=_globals or {},\n            exit_status=ExecutionStatus.CRASHED,\n        )\n```\n\nIf the process exits within the timeout:\n- **Case 2.1**: The result container has the child's ExecutionResult (SUCCESS or FAILURE)\n- **Case 2.2**: The result container is empty, indicating a crash before result transmission\n\n**Sources:** [src/runtime/subprocess_python_executor.py:145-159]()\n\n---\n\n## Crash Detection\n\nThe subprocess executor can detect and handle various crash scenarios by examining whether the child sent a result before terminating.\n\n### Crash Scenarios\n\n| Crash Type | Trigger | Exit Code | Result Container | Detection |\n|------------|---------|-----------|------------------|-----------|\n| **SegFault** | `os._exit(139)` | 139 (SIGSEGV) | Empty | No result sent |\n| **Stack Overflow** | Infinite recursion | 1 | Empty* | PicklingError prevents result |\n| **Memory Exhaustion** | Large allocations | 1 | Empty* | PicklingError prevents result |\n| **Normal Exception** | `raise ZeroDivisionError` | 0 | Contains FAILURE | Exception caught and sent |\n| **Timeout** | `time.sleep(long_time)` | -15 (SIGTERM) | Empty | Parent enforces timeout |\n\n*Note: Stack overflow and OOM may capture exceptions but fail to pickle/send the result.\n\n### Exit Code Interpretation\n\nThe parent retrieves the exit code after the process terminates:\n\n```python\nfinal_res.exit_code = p.exitcode  # Fill exitcode field\n```\n\nExit code meanings:\n- `exitcode = 0`: Normal termination (SUCCESS or caught exception)\n- `exitcode > 0`: Error exit (e.g., 1 for unhandled exception)\n- `exitcode < 0`: Killed by signal (e.g., -15 for SIGTERM, -11 for SIGSEGV)\n\n**Sources:** [src/runtime/subprocess_python_executor.py:160](), [src/runtime/schemas.py:50-54]()\n\n### Example: SegFault Detection\n\nWhen a segmentation fault occurs:\n\n```python\n# Code that triggers SegFault\nimport os\nos._exit(139)  # Simulates SIGSEGV (exit code 139)\n```\n\nThe subprocess exits immediately without sending a result. The parent detects this:\n\n1. `p.is_alive()` returns `False` (process terminated)\n2. `subprocess_result_container` is empty (no result sent)\n3. `p.exitcode` is 139 (indicates segmentation fault)\n4. Parent constructs `ExecutionStatus.CRASHED` result\n\n**Sources:** [tests/playground/subprocess_output.py:215-234]()\n\n### Example: Recursion Error\n\nInfinite recursion triggers a `RecursionError`:\n\n```python\ndef recursive_crash(depth=0):\n    if depth % 100 == 0:\n        print(f\"Current recursion depth: {depth}\")\n    recursive_crash(depth + 1)\n\nrecursive_crash()\n```\n\nThe child process catches the exception and builds a FAILURE result, but pickling fails because the function `recursive_crash` cannot be serialized. The result container remains empty, and the parent detects a CRASHED status.\n\n**Sources:** [tests/playground/subprocess_output.py:235-259](), [tests/playground/crashed.py:4-47]()\n\n---\n\n## Working Directory Management\n\nThe child process sets its working directory before executing user code using the `cwd` module.\n\n### Directory Setup\n\n```python\ndef _worker_with_pipe(\n    command: str,\n    _globals: dict[str, Any] | None,\n    _locals: Optional[Dict],\n    timeout: Optional[int],\n    child_conn: PipeConnection,\n) -> None:\n    \"\"\"Execute a command in a subprocess with output captured via pipes.\"\"\"\n    cwd.create_cwd('./wsm/2/g7-2')  # Set working directory for this child\n    \n    # ... rest of execution\n```\n\n**Sources:** [src/runtime/subprocess_python_executor.py:19-28]()\n\n### Directory Creation\n\nThe `create_cwd` function ensures the directory exists and changes to it:\n\n```python\ndef create_cwd(cwd=None):\n    \"\"\"\n    Wrapper function for subprocess to change working directory before executing task.\n    \"\"\"\n    cwd = create_folder.get_or_create_subfolder(fix_relate_from_project=cwd)\n    \n    global_logger.info(f\"Child PID: {os.getpid()} changing to directory: {cwd}\")\n    if not cwd:\n        return False\n    try:\n        os.chdir(cwd)\n        global_logger.info(f\"Child PID: {os.getpid()} changed to: {os.getcwd()}\")\n        return True\n    except OSError as e:\n        global_logger.error(f\"Child PID: {os.getpid()} failed to change directory: {e}\")\n        return False\n```\n\nThis isolates file I/O operations to a specific workspace directory, preventing interference between concurrent executions.\n\n**Sources:** [src/runtime/cwd.py:9-28]()\n\n---\n\n## Execution Result Assembly\n\nAfter the child process completes and the reader thread finishes, the parent assembles the final ExecutionResult by combining data from multiple sources.\n\n### Result Construction\n\n```python\nfinal_res.exit_code = p.exitcode  # Fill exitcode field\nfinal_res.ret_stdout = \"\".join(subprocess_stdout_buffer)  # Fill stdout field\nfinal_res.ret_tool2llm = ExecutionStatus.get_return_llm(final_res.exit_status, final_res)\nreturn final_res\n```\n\nThe final result contains:\n\n| Field | Source | Purpose |\n|-------|--------|---------|\n| `arg_command` | Child or parent | Original code executed |\n| `arg_timeout` | Child or parent | Timeout limit used |\n| `arg_globals` | Child or parent | Global variables (filtered) |\n| `exit_status` | Child or parent | SUCCESS/FAILURE/TIMEOUT/CRASHED |\n| `exit_code` | Parent (`p.exitcode`) | OS-level exit code |\n| `ret_stdout` | Reader buffer | Accumulated output |\n| `ret_tool2llm` | Generated | Formatted message for LLM |\n| `exception_*` | Child (if FAILURE) | Exception details |\n\n**Sources:** [src/runtime/subprocess_python_executor.py:160-163]()\n\n### LLM-Formatted Output\n\nThe `get_return_llm` static method formats the result for the LLM based on execution status:\n\n```python\n@classmethod\ndef get_return_llm(cls, status: \"ExecutionStatus\", result: \"ExecutionResult\") -> str:\n    \"\"\"Generate formatted description for LLM based on execution status\"\"\"\n    _desc_map = {\n        cls.SUCCESS: \"## Code execution successful, output complete, task finished\\n\"\n                    f\"### Terminal output:\\n\"\n                    f\"{result.ret_stdout}\",\n        cls.FAILURE: \"## Code execution failed, exception thrown, debug based on error\\n\"\n                    f\"### Terminal output:\\n\"\n                    f\"{result.ret_stdout}\"\n                    f\"### Original code:\\n\"\n                    f\"{source_code.add_line_numbers(result.arg_command)}\\n\"\n                    f\"### Error information:\\n\"\n                    f\"{result.exception_traceback}\",\n        cls.TIMEOUT: \"## Code execution timeout, forced exit, retry with adjusted timeout\\n\"\n                    f\"### Terminal output:\\n\"\n                    f\"{result.ret_stdout}\"\n                    f\"### Timeout limit: {result.arg_timeout} seconds\\n\",\n        cls.CRASHED: \"## Code execution crashed, process abnormal exit, debug based on error\\n\"\n                    f\"### Terminal output:\\n\"\n                    f\"{result.ret_stdout}\"\n                    f\"### Exit code: {result.exit_code}\\n\",\n    }\n    return _desc_map.get(status, f\"Unknown status: {status}\")\n```\n\nThis provides context-specific messages that help the LLM understand what went wrong and how to proceed.\n\n**Sources:** [src/runtime/schemas.py:18-47]()\n\n---\n\n## Usage Example\n\nA complete example of subprocess execution:\n\n```python\nfrom src.runtime.subprocess_python_executor import run_structured_in_subprocess\n\n# Define code to execute\ntest_code = \"\"\"\nimport time\nprint(\"Starting computation...\")\nresult = sum(range(1000000))\nprint(f\"Result: {result}\")\ntime.sleep(1)\nprint(\"Computation complete\")\n\"\"\"\n\n# Execute with 10-second timeout\nmy_globals = {\"initial_var\": 123}\nresult = run_structured_in_subprocess(\n    command=test_code,\n    _globals=my_globals,\n    _locals=None,\n    timeout=10\n)\n\n# Check result\nprint(f\"Status: {result.exit_status}\")\nprint(f\"Exit code: {result.exit_code}\")\nprint(f\"Output:\\n{result.ret_stdout}\")\nprint(f\"Globals: {result.arg_globals.keys()}\")\n```\n\nThe result will have:\n- `exit_status = ExecutionStatus.SUCCESS`\n- `exit_code = 0`\n- `ret_stdout` containing all print output\n- `arg_globals` containing `initial_var` and `result`\n\n**Sources:** [src/runtime/subprocess_python_executor.py:165-204]()\n\n---\n\n## Summary\n\nThe subprocess execution strategy provides robust isolation and comprehensive error handling:\n\n**Key Features:**\n- Full process isolation with separate memory spaces\n- Real-time output capture via pipe-based IPC\n- Reliable timeout enforcement with process termination\n- Automatic crash detection for segfaults, OOM, and stack overflows\n- Working directory isolation for file operations\n- Detailed exit code analysis for debugging\n\n**Trade-offs:**\n- Higher overhead than thread-based execution\n- Inter-process communication adds latency\n- Global variables must be picklable for transmission\n- Process creation time varies by platform (fork vs spawn)\n\n**When to Use:**\n- Code that may crash or consume excessive resources\n- Untrusted code requiring strong isolation\n- Long-running computations that need timeout guarantees\n- Code that manipulates the file system\n\nFor lighter-weight execution with shared memory, see [Subthread Execution](#5.2). For fastest execution without isolation, see [Direct Execution](#5.3).\n\n**Sources:** [src/runtime/subprocess_python_executor.py](), [src/runtime/schemas.py](), [docs/crashed.design.md]()\n\n---\n\n# Page: Subthread Execution\n\n# Subthread Execution\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/crashed.design.md](docs/crashed.design.md)\n- [src/runtime/before_thread/plt_back_chinese.py](src/runtime/before_thread/plt_back_chinese.py)\n- [src/runtime/schemas.py](src/runtime/schemas.py)\n- [src/runtime/subthread_python_executor.py](src/runtime/subthread_python_executor.py)\n- [tests/playground/crashed.py](tests/playground/crashed.py)\n- [tests/playground/gen/g9/beijing_scenic_spot_schema_with_play_hours.json](tests/playground/gen/g9/beijing_scenic_spot_schema_with_play_hours.json)\n- [tests/playground/gen/g9/beijing_scenic_spot_validated_data_with_play_hours.json](tests/playground/gen/g9/beijing_scenic_spot_validated_data_with_play_hours.json)\n- [tests/playground/gen/g9/g9.py](tests/playground/gen/g9/g9.py)\n- [tests/playground/subprocess_output.py](tests/playground/subprocess_output.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page documents the **subthread execution strategy**, a thread-based approach for running Python code snippets with controlled isolation and output capture. This executor runs code in a separate Python thread within the same process, providing a balance between execution speed and basic isolation.\n\nFor complete process isolation, see [Subprocess Execution](#5.1). For in-process execution without any isolation, see [Direct Execution](#5.3). For the data structures returned by all executors, see [ExecutionResult and Status Handling](#5.4).\n\n---\n\n## Architecture Overview\n\nThe subthread executor uses Python's `threading` module to execute code in a worker thread. Unlike subprocess execution, threads share the same memory space with the parent process, which improves performance but reduces isolation. Output is captured through a buffer-based writer that redirects `sys.stdout` and `sys.stderr`.\n\n### Key Characteristics\n\n| Feature | Implementation |\n|---------|---------------|\n| **Isolation Level** | Thread-based (shared memory space) |\n| **Output Capture** | Buffer-based via custom writer class |\n| **Timeout Support** | Yes, via `Thread.join(timeout)` |\n| **Force Termination** | No (threads cannot be forcefully killed) |\n| **Exit Code** | Not available (set to `None`) |\n| **Performance** | Faster than subprocess, slower than direct |\n\n---\n\n## Core Components\n\n### Entry Point Function\n\nThe main entry point is `run_structured_in_thread`, which orchestrates the entire execution flow:\n\n**Function Signature:**\n```python\ndef run_structured_in_thread(\n    command: str,\n    _globals: dict[str, Any] | None = None,\n    _locals: Optional[Dict] = None,\n    timeout: Optional[int] = None,\n) -> ExecutionResult\n```\n\n**Key Responsibilities:**\n1. Initialize stdout buffer and result container\n2. Create and start worker thread\n3. Wait for completion or timeout\n4. Construct final `ExecutionResult`\n5. Fill in `ret_stdout` and `ret_tool2llm` fields\n\nSources: [src/runtime/subthread_python_executor.py:86-128]()\n\n### Worker Thread Function\n\n```mermaid\ngraph TB\n    subgraph \"_worker_with_buffer\"\n        SETUP[\"Set up _BufferWriter<br/>Redirect stdout/stderr\"]\n        CWD[\"Enter working directory<br/>cwd.ChangeDirectory\"]\n        EXEC[\"Execute code<br/>exec(command, _globals, _locals)\"]\n        SUCCESS[\"Build SUCCESS<br/>ExecutionResult\"]\n        ERROR[\"Catch Exception<br/>Filter traceback\"]\n        FAILURE[\"Build FAILURE<br/>ExecutionResult\"]\n        APPEND[\"Append result to<br/>result_container\"]\n    end\n    \n    SETUP --> CWD\n    CWD --> EXEC\n    EXEC -->|No Exception| SUCCESS\n    EXEC -->|Exception Raised| ERROR\n    ERROR --> FAILURE\n    SUCCESS --> APPEND\n    FAILURE --> APPEND\n```\n\nThe `_worker_with_buffer` function runs in the spawned thread and handles:\n- Output redirection to buffer\n- Working directory changes\n- Code execution via `exec()`\n- Exception capture and traceback filtering\n- Result construction\n\nSources: [src/runtime/subthread_python_executor.py:13-84]()\n\n---\n\n## Output Capture Mechanism\n\n### BufferWriter Class\n\nThe `_BufferWriter` class implements a file-like interface to capture output:\n\n```mermaid\nclassDiagram\n    class _BufferWriter {\n        +list~str~ buffer\n        +__init__(buffer: list)\n        +write(msg: str)\n        +flush()\n    }\n    \n    class SysModule {\n        stdout\n        stderr\n    }\n    \n    _BufferWriter --> SysModule : \"replaces\"\n```\n\n**Implementation Details:**\n- Stores output in a `list[str]` passed by reference\n- Both `sys.stdout` and `sys.stderr` redirected to same instance\n- Thread-safe due to GIL protection\n- No need for explicit flushing\n\n**Why Buffer-Based Instead of Pipes:**\n\n| Aspect | Buffer-Based (Subthread) | Pipe-Based (Subprocess) |\n|--------|--------------------------|-------------------------|\n| Complexity | Simple list append | Requires reader thread, protocol handling |\n| Performance | Fast (in-memory) | Slower (IPC overhead) |\n| Reliability | No broken pipe issues | Must handle EOF, broken pipes |\n| Synchronization | Automatic (GIL) | Manual reader thread coordination |\n\nSources: [src/runtime/subthread_python_executor.py:23-32]()\n\n---\n\n## Execution Flow\n\n### Complete Sequence Diagram\n\n```mermaid\nsequenceDiagram\n    participant Caller\n    participant Main as \"run_structured_in_thread\"\n    participant Thread as \"Worker Thread\"\n    participant Buffer as \"stdout_buffer (list)\"\n    participant Container as \"result_container (list)\"\n    \n    Caller->>Main: command, _globals, timeout\n    Main->>Main: Create stdout_buffer = []\n    Main->>Main: Create result_container = []\n    Main->>Thread: Start thread with _worker_with_buffer\n    \n    Thread->>Thread: Initialize _BufferWriter(stdout_buffer)\n    Thread->>Thread: Redirect sys.stdout/stderr\n    Thread->>Thread: Enter cwd.ChangeDirectory\n    Thread->>Thread: Enter cwd.Change_STDOUT_STDERR\n    \n    alt Code Executes Successfully\n        Thread->>Thread: exec(command, _globals, _locals)\n        Thread->>Buffer: write(...) during execution\n        Thread->>Thread: Build ExecutionResult(SUCCESS)\n        Thread->>Container: Append result\n    else Exception Raised\n        Thread->>Thread: Catch exception\n        Thread->>Buffer: write(...) error output\n        Thread->>Thread: filter_exec_traceback()\n        Thread->>Thread: Build ExecutionResult(FAILURE)\n        Thread->>Container: Append result\n    end\n    \n    Main->>Main: t.join(timeout)\n    \n    alt Thread Completes in Time\n        Main->>Container: Get result_container[0]\n        Main->>Main: final_res = container[0]\n    else Thread Timeout\n        Main->>Main: Build ExecutionResult(TIMEOUT)\n        Main->>Main: Note: Thread still running\n    end\n    \n    Main->>Main: final_res.exit_code = None\n    Main->>Buffer: final_res.ret_stdout = \"\".join(buffer)\n    Main->>Main: final_res.ret_tool2llm = format_for_llm()\n    Main->>Caller: Return ExecutionResult\n```\n\nSources: [src/runtime/subthread_python_executor.py:86-128]()\n\n---\n\n## Timeout Handling\n\n### Mechanism\n\n```python\nt.join(timeout)\nif t.is_alive():\n    # Thread still running - timeout occurred\n    final_res = ExecutionResult(\n        arg_command=command,\n        arg_timeout=timeout,\n        arg_globals=_globals or {},\n        exit_status=ExecutionStatus.TIMEOUT,\n    )\n```\n\n### Critical Limitation\n\n**Threads Cannot Be Force-Killed in Python:**\n\nUnlike subprocesses which can be terminated with `process.terminate()` or killed with signals, Python threads cannot be forcefully stopped. When a timeout occurs:\n\n1. The main thread returns a `TIMEOUT` status\n2. The worker thread **continues running** in the background\n3. Any subsequent operations may be affected by the still-running thread\n\n**Implications:**\n- Infinite loops in user code will leave zombie threads\n- Resource leaks possible if timeout occurs\n- GIL contention from background threads\n- Shared memory may be corrupted by timed-out threads\n\n**Comparison with Subprocess:**\n\n| Executor | Timeout Action | Post-Timeout State |\n|----------|----------------|-------------------|\n| Subthread | Mark as timeout, return | Thread continues running |\n| Subprocess | `terminate()` then `kill()` | Process fully terminated |\n\nSources: [src/runtime/subthread_python_executor.py:102-111]()\n\n---\n\n## Exception Handling and Traceback Filtering\n\n### Why Traceback Filtering Is Needed\n\nWhen code executed via `exec()` raises an exception, the traceback includes:\n1. **External frames** - executor framework code (`_worker_with_buffer`, `exec()` call)\n2. **Internal frames** - user code executed within `<string>`\n\nUsers only need to see the frames from their own code, not the executor internals.\n\n### Filter Implementation\n\n```mermaid\ngraph TB\n    START[\"Get traceback.format_exc()\"]\n    SPLIT[\"Split into lines\"]\n    INIT[\"Initialize exec_lines = []<br/>in_exec_block = False\"]\n    \n    LOOP{\"For each line\"}\n    CHECK1{\"Contains 'Traceback'?\"}\n    CHECK2{\"Contains '<string>'?\"}\n    CHECK3{\"in_exec_block?\"}\n    CHECK4{\"Starts with 'File'<br/>and no '<string>'?\"}\n    \n    ADD[\"Add to exec_lines\"]\n    SETFLAG[\"in_exec_block = True<br/>Add to exec_lines\"]\n    RESETFLAG[\"in_exec_block = False\"]\n    JOIN[\"Join lines with newline\"]\n    \n    START --> SPLIT --> INIT --> LOOP\n    LOOP -->|Yes| CHECK1\n    CHECK1 -->|Yes| ADD\n    CHECK1 -->|No| CHECK2\n    CHECK2 -->|Yes| SETFLAG\n    CHECK2 -->|No| CHECK3\n    CHECK3 -->|Yes| CHECK4\n    CHECK3 -->|No| LOOP\n    CHECK4 -->|Yes| RESETFLAG\n    CHECK4 -->|No| ADD\n    RESETFLAG --> LOOP\n    ADD --> LOOP\n    SETFLAG --> LOOP\n    LOOP -->|Done| JOIN\n```\n\nThe `filter_exec_traceback` function extracts only the relevant stack frames:\n\n**Key Features:**\n- Preserves the \"Traceback\" header line\n- Includes all lines containing `<string>` (exec'd code marker)\n- Includes subsequent lines within the exec block\n- Stops when encountering new stack frames from executor code\n- Returns cleaned traceback string\n\n**Example Transformation:**\n\nBefore filtering:\n```\nTraceback (most recent call last):\n  File \"d:\\...\\subthread_python_executor.py\", line 37, in _worker_with_buffer\n    exec(command, _globals, _locals)\n  File \"<string>\", line 4, in <module>\nZeroDivisionError: division by zero\n```\n\nAfter filtering:\n```\nTraceback (most recent call last):\n  File \"<string>\", line 4, in <module>\nZeroDivisionError: division by zero\n```\n\nSources: [src/runtime/subthread_python_executor.py:47-72]()\n\n---\n\n## Working Directory and Environment Setup\n\n### Directory Management\n\n```mermaid\ngraph LR\n    A[\"Original CWD<br/>(project root)\"]\n    B[\"Enter Context<br/>cwd.ChangeDirectory\"]\n    C[\"Changed CWD<br/>./wsm/4/g9-1\"]\n    D[\"Execute Code<br/>in new directory\"]\n    E[\"Exit Context<br/>Restore CWD\"]\n    F[\"Back to Original\"]\n    \n    A --> B\n    B --> C\n    C --> D\n    D --> E\n    E --> F\n```\n\nThe executor changes the working directory to a workspace-specific path:\n```python\nwith cwd.ChangeDirectory('./wsm/4/g9-1'):\n    with cwd.Change_STDOUT_STDERR(_BufferWriter(stdout_buffer)):\n        exec(command, _globals, _locals)\n```\n\n**Purpose:**\n- Isolate file I/O operations between different executions\n- Provide consistent working directory for relative paths\n- Enable workspace-based file organization\n\n**Context Managers Used:**\n1. `cwd.ChangeDirectory` - Changes `os.getcwd()` temporarily\n2. `cwd.Change_STDOUT_STDERR` - Redirects output streams\n\nSources: [src/runtime/subthread_python_executor.py:35-37]()\n\n---\n\n## Pre-Execution Setup for Matplotlib\n\n### The Threading Issue\n\nMatplotlib has two critical issues when used in non-main threads:\n\n1. **GUI Backend Warning**: Default backends (TkAgg, Qt5Agg) cannot create GUI windows from threads\n2. **Chinese Font Rendering**: Default fonts don't support Chinese characters, causing glyph warnings\n\n### Solution: plt_back_chinese Module\n\n```mermaid\ngraph TB\n    IMPORT[\"Import plt_back_chinese module\"]\n    SETBACK[\"matplotlib.use('Agg')<br/>Set non-GUI backend\"]\n    SETFONT[\"plt.rcParams['font.sans-serif'] = ['SimHei']<br/>Set Chinese font\"]\n    SETMINUS[\"plt.rcParams['axes.unicode_minus'] = False<br/>Fix minus sign\"]\n    \n    IMPORT --> SETBACK\n    SETBACK --> SETFONT\n    SETFONT --> SETMINUS\n    \n    style SETBACK fill:#f9f9f9\n    style SETFONT fill:#f9f9f9\n```\n\n**Module Purpose:**\nThis module must be imported **before any thread executes matplotlib code**. It:\n- Sets backend to `\"Agg\"` (non-interactive, file-only)\n- Configures Chinese font support (`SimHei`)\n- Fixes Unicode minus sign rendering\n\n**Import Location:**\n```python\nfrom src.runtime.before_thread import plt_back_chinese\n```\n\nThis import happens at the top of `subthread_python_executor.py`, ensuring configuration is applied before any threads spawn.\n\nSources: [src/runtime/before_thread/plt_back_chinese.py:1-13](), [src/runtime/subthread_python_executor.py:9]()\n\n---\n\n## ExecutionResult Construction\n\n### Status Determination Logic\n\n```mermaid\ngraph TB\n    START[\"Worker thread executes\"]\n    \n    TIMEOUT[\"Main thread: t.join(timeout)\"]\n    ALIVE{\"t.is_alive()?\"}\n    \n    CONTAINER{\"result_container<br/>has result?\"}\n    \n    SUCCESS[\"ExecutionStatus.SUCCESS<br/>(exec completed)\"]\n    FAILURE[\"ExecutionStatus.FAILURE<br/>(exception caught)\"]\n    TIMEOUT_STATUS[\"ExecutionStatus.TIMEOUT<br/>(parent constructs)\"]\n    CRASHED[\"ExecutionStatus.CRASHED<br/>(no result, but finished)\"]\n    \n    START --> TIMEOUT\n    TIMEOUT --> ALIVE\n    ALIVE -->|True| TIMEOUT_STATUS\n    ALIVE -->|False| CONTAINER\n    CONTAINER -->|Yes, SUCCESS type| SUCCESS\n    CONTAINER -->|Yes, FAILURE type| FAILURE\n    CONTAINER -->|Empty| CRASHED\n```\n\n### Field Population\n\n| Field | Populated By | When |\n|-------|--------------|------|\n| `arg_command` | Worker or Main | Always |\n| `arg_timeout` | Worker or Main | Always |\n| `arg_globals` | Worker or Main | Always (filtered) |\n| `exit_status` | Worker or Main | Always |\n| `exit_code` | Main thread | Always (`None` for threads) |\n| `exception_*` | Worker thread | Only on FAILURE |\n| `ret_stdout` | Main thread | After join (from buffer) |\n| `ret_tool2llm` | Main thread | After status determined |\n\n### Success Case\n\n```python\nres = ExecutionResult(\n    arg_command=command,\n    arg_globals=_globals or {},\n    arg_timeout=timeout,\n    exit_status=ExecutionStatus.SUCCESS,\n)\nresult_container.append(res)\n```\n\nSources: [src/runtime/subthread_python_executor.py:39-44]()\n\n### Failure Case\n\n```python\nres = ExecutionResult(\n    arg_command=command,\n    arg_globals=_globals or {},\n    arg_timeout=timeout,\n    exit_status=ExecutionStatus.FAILURE,\n    exception_repr=repr(e),\n    exception_type=type(e).__name__,\n    exception_value=str(e),\n    exception_traceback=filter_exec_traceback(),\n)\nresult_container.append(res)\n```\n\nSources: [src/runtime/subthread_python_executor.py:73-82]()\n\n### Timeout Case\n\n```python\nfinal_res = ExecutionResult(\n    arg_command=command,\n    arg_timeout=timeout,\n    arg_globals=_globals or {},\n    exit_status=ExecutionStatus.TIMEOUT,\n)\n```\n\nSources: [src/runtime/subthread_python_executor.py:106-111]()\n\n### Crashed Case\n\nWhen the thread finishes but `result_container` is empty (theoretically possible if the thread crashes before appending):\n\n```python\nfinal_res = ExecutionResult(\n    arg_command=command,\n    arg_timeout=timeout,\n    arg_globals=_globals or {},\n    exit_status=ExecutionStatus.CRASHED,\n)\n```\n\nSources: [src/runtime/subthread_python_executor.py:119-124]()\n\n---\n\n## Comparison with Other Executors\n\n### Architecture Comparison\n\n```mermaid\ngraph TB\n    subgraph \"Subprocess Execution\"\n        SP_MAIN[\"Main Process\"]\n        SP_CHILD[\"Child Process<br/>(separate memory)\"]\n        SP_PIPE[\"Pipe/IPC\"]\n        \n        SP_MAIN -->|spawn| SP_CHILD\n        SP_CHILD -->|send result| SP_PIPE\n        SP_PIPE -->|read| SP_MAIN\n    end\n    \n    subgraph \"Subthread Execution\"\n        ST_MAIN[\"Main Thread\"]\n        ST_WORKER[\"Worker Thread<br/>(shared memory)\"]\n        ST_BUFFER[\"Buffer List<br/>(shared reference)\"]\n        \n        ST_MAIN -->|start| ST_WORKER\n        ST_WORKER -->|append| ST_BUFFER\n        ST_MAIN -->|read| ST_BUFFER\n    end\n    \n    subgraph \"Direct Execution\"\n        D_MAIN[\"Main Thread<br/>(no isolation)\"]\n        D_QUEUE[\"Queue<br/>(optional)\"]\n        \n        D_MAIN -->|exec directly| D_QUEUE\n    end\n```\n\n### Feature Matrix\n\n| Feature | Subprocess | Subthread | Direct |\n|---------|-----------|-----------|--------|\n| **Process Isolation** | Full | None | None |\n| **Memory Space** | Separate | Shared | Shared |\n| **Force Termination** | Yes (SIGTERM/SIGKILL) | No | No |\n| **Output Capture** | Pipe-based | Buffer-based | Queue/Direct |\n| **Exit Code** | Yes (from OS) | No | No |\n| **Crash Detection** | Yes (exitcode != 0) | Limited | Limited |\n| **Startup Overhead** | High | Low | None |\n| **GIL Impact** | None (separate process) | Yes (shares GIL) | Yes (shares GIL) |\n| **Variable Pickling** | Required | Not required | Not required |\n| **Working Directory** | Can change safely | Can change safely | Affects main thread |\n\n### When to Use Each Executor\n\n**Use Subprocess When:**\n- Maximum isolation is required\n- Code may crash or corrupt memory\n- Need ability to force-kill on timeout\n- Running untrusted or experimental code\n\n**Use Subthread When:**\n- Need better performance than subprocess\n- Want some isolation from main thread\n- Timeout is likely but not critical\n- Code is relatively safe\n\n**Use Direct When:**\n- Maximum performance required\n- Full trust in code\n- No timeout needed\n- Running simple, fast operations\n\nSources: High-level diagrams, [src/runtime/subthread_python_executor.py:1-183]()\n\n---\n\n## Shared Memory Considerations\n\n### Global Variable Sharing\n\nUnlike subprocess execution, threads share the same global namespace:\n\n```mermaid\ngraph TB\n    subgraph \"Subprocess Execution\"\n        P1[\"Parent Process<br/>globals = {a: 1}\"]\n        P2[\"Child Process<br/>globals = {a: 1} (copy)\"]\n        P1 -->|fork/spawn| P2\n        P2 -.->|no sharing| P1\n    end\n    \n    subgraph \"Subthread Execution\"  \n        T1[\"Main Thread<br/>_globals dict\"]\n        T2[\"Worker Thread<br/>same _globals dict\"]\n        T1 -->|shared reference| T2\n        T2 -->|modifies same object| T1\n    end\n```\n\n**Implications:**\n\n1. **Variable Mutations Are Visible:**\n   ```python\n   _globals = {\"data\": [1, 2, 3]}\n   # Worker thread executes: data.append(4)\n   # Main thread sees: {\"data\": [1, 2, 3, 4]}\n   ```\n\n2. **Race Conditions Possible:**\n   - Multiple threads could modify shared objects\n   - GIL provides some protection but not complete safety\n   - Use locks for thread-safe operations\n\n3. **Module Import Side Effects:**\n   - Imports affect global module cache\n   - Changes to `sys.modules` visible to all threads\n\n### Workspace Globals Management\n\nThe `arg_globals` field in `ExecutionResult` undergoes filtering:\n\n```python\n@field_validator('arg_globals')\n@classmethod        \ndef field_validate_globals(cls, value: Dict[str, Any]) -> Dict[str, Any]:\n    if value is None:\n        return {}\n    return workspace.filter_and_deepcopy_globals(value)\n```\n\n**Filtering Process:**\n1. Exclude built-in objects (`__builtins__`)\n2. Exclude module references\n3. Deep copy remaining values to prevent mutation\n4. Check picklability (for consistency with subprocess)\n\nThis ensures the returned `arg_globals` can be safely persisted in the workspace state.\n\nSources: [src/runtime/schemas.py:77-83](), [src/runtime/subthread_python_executor.py:41]()\n\n---\n\n## Use Cases in the Codebase\n\n### Example: Beijing Scenic Spot Data Processing\n\nThe test files show subthread execution being used for data processing tasks:\n\n```python\nresult = run_structured_in_thread(test_code, {}, timeout=10)\n```\n\n**Typical Use Case Pattern:**\n\n1. **Data Schema Definition:**\n   - Pydantic models for validation\n   - CSV parsing and transformation\n   - JSON schema generation\n\n2. **Geographic Data Processing:**\n   - Coordinate parsing\n   - Statistical calculations\n   - Data validation\n\n3. **Visualization:**\n   - Matplotlib figure generation\n   - Chart saving to files\n   - Non-interactive plotting\n\n**Why Subthread Is Suitable:**\n- Fast enough for data processing\n- Matplotlib works with proper backend setup\n- Shared memory allows efficient data passing\n- No need for maximum isolation\n\nSources: [tests/playground/gen/g9/g9.py:1-153](), [src/runtime/subthread_python_executor.py:130-183]()\n\n---\n\n## Limitations and Risks\n\n### Cannot Force-Terminate Threads\n\n**The Core Problem:**\n```python\nt.join(timeout)\nif t.is_alive():\n    # Thread is STILL RUNNING - we can only mark it as timed out\n    # No way to kill it like subprocess.terminate()\n```\n\n**Consequences:**\n- Infinite loops create zombie threads\n- Resource leaks from uncompleted operations\n- Memory usage grows if many timeouts occur\n- GIL contention affects overall performance\n\n### Shared Memory Corruption Risk\n\n**Scenario:**\n1. Thread starts executing user code\n2. Timeout occurs, main thread returns\n3. Worker thread continues running in background\n4. Worker thread modifies shared globals\n5. Next execution uses corrupted globals\n\n**Mitigation:**\n- Deep copy globals before passing to executor\n- Validate globals after execution\n- Use subprocess executor for untrusted code\n\n### GIL Limitations\n\nThe Global Interpreter Lock (GIL) means:\n- Only one thread executes Python bytecode at a time\n- CPU-bound operations don't parallelize\n- I/O-bound operations still benefit (GIL released during I/O)\n\n**Performance Implications:**\n\n| Operation Type | Benefit from Threading |\n|---------------|----------------------|\n| Pure computation | No (GIL bottleneck) |\n| File I/O | Yes (GIL released) |\n| Network I/O | Yes (GIL released) |\n| Sleep/waiting | Yes (GIL released) |\n\n### No Exit Code Available\n\nUnlike processes, threads don't have exit codes:\n```python\nfinal_res.exit_code = None  # Threads don't have exit codes\n```\n\nThis means:\n- Can't distinguish different crash types via exit code\n- Must rely on exception information\n- No signal information (SIGSEGV, SIGABRT, etc.)\n\nSources: [src/runtime/subthread_python_executor.py:102-125](), [docs/crashed.design.md:236-260]()\n\n---\n\n## Summary\n\nThe subthread execution strategy provides a **middle ground** between full process isolation and direct in-process execution:\n\n**Advantages:**\n- Faster startup than subprocess\n- Buffer-based output capture (simpler than pipes)\n- Shared memory space (no pickling required)\n- Working directory isolation\n- Timeout support\n\n**Disadvantages:**\n- Cannot force-kill timed-out threads\n- Shared memory risks (corruption, race conditions)\n- GIL contention for CPU-bound tasks\n- No exit code information\n- Limited crash detection\n\n**Best Used For:**\n- Data processing tasks with moderate trust level\n- Operations needing matplotlib with proper backend setup\n- Scenarios where subprocess overhead is too high\n- Code that rarely times out\n\nFor maximum safety and isolation, prefer [Subprocess Execution](#5.1). For maximum performance with trusted code, consider [Direct Execution](#5.3).\n\nSources: [src/runtime/subthread_python_executor.py:1-183](), [src/runtime/schemas.py:1-111]()\n\n---\n\n# Page: Direct Execution\n\n# Direct Execution\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/crashed.design.md](docs/crashed.design.md)\n- [docs/python.design.md](docs/python.design.md)\n- [src/runtime/python_executor.py](src/runtime/python_executor.py)\n- [src/runtime/schemas.py](src/runtime/schemas.py)\n- [tests/playground/crashed.py](tests/playground/crashed.py)\n- [tests/playground/subprocess_output.py](tests/playground/subprocess_output.py)\n- [tests/unit/runtime/test_exec_runner.py](tests/unit/runtime/test_exec_runner.py)\n- [tests/unit/runtime/test_python_executor.py](tests/unit/runtime/test_python_executor.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page documents the **direct execution strategy** for running Python code within the algo_agent system. Direct execution runs code in-process (within the same Python interpreter) for maximum performance, eliminating the overhead of process creation and inter-process communication. This strategy is implemented in [src/runtime/python_executor.py]() and provides functions for executing Python code snippets with output capture and state management.\n\nFor isolated execution with stronger safety guarantees, see [Subprocess Execution](#5.1) and [Subthread Execution](#5.2). For information about execution results and status codes, see [ExecutionResult and Status Handling](#5.4).\n\n---\n\n## Overview\n\nDirect execution offers the **fastest execution mode** by running code in the same process as the calling code, avoiding the overhead of:\n- Process creation and teardown\n- Memory serialization/deserialization\n- Inter-process communication (pipes, shared memory)\n\nHowever, this speed comes with trade-offs in isolation and safety. The system provides both simple string-based execution (`run()`) and structured result execution (`run_structured()`) with timeout support via multiprocessing.\n\n### Key Characteristics\n\n| Characteristic | Direct Execution |\n|----------------|------------------|\n| **Performance** | Fastest (no process overhead) |\n| **Isolation** | None (shares memory space) |\n| **Safety** | Lowest (crashes affect parent process) |\n| **Timeout Support** | Via multiprocessing.Process |\n| **Global State** | Can be captured and returned |\n| **Use Case** | Trusted code, maximum speed required |\n\nSources: [src/runtime/python_executor.py:1-164]()\n\n---\n\n## Execution Flow Architecture\n\nThe following diagram shows how direct execution integrates with the multiprocessing system for timeout support:\n\n```mermaid\ngraph TB\n    CALLER[\"Caller<br/>(ExecutePythonCodeTool)\"]\n    \n    subgraph \"Direct Execution Entry Points\"\n        RUN[\"run()<br/>Returns: str\"]\n        RUN_STRUCT[\"run_structured()<br/>Returns: ExecutionResult\"]\n    end\n    \n    subgraph \"Execution Path Decision\"\n        TIMEOUT_CHECK{\"timeout<br/>is None?\"}\n    end\n    \n    subgraph \"Direct Mode (No Timeout)\"\n        DIRECT_WORKER[\"worker_with_globals_capture()<br/>Called directly in-process\"]\n        DIRECT_EXEC[\"exec(command, _globals, _locals)\"]\n        DIRECT_CAPTURE[\"StringIO captures stdout\"]\n    end\n    \n    subgraph \"Multiprocessing Mode (With Timeout)\"\n        QUEUE[\"multiprocessing.Queue\"]\n        PROCESS[\"multiprocessing.Process<br/>target=worker_with_globals_capture\"]\n        WORKER[\"worker_with_globals_capture()<br/>Runs in separate process\"]\n        WORKER_EXEC[\"exec(command, _globals, _locals)\"]\n        WORKER_CAPTURE[\"StringIO captures stdout\"]\n        RESULT_PUT[\"queue.put(ExecutionResult)\"]\n    end\n    \n    RESULT[\"ExecutionResult<br/>or str output\"]\n    \n    CALLER --> RUN\n    CALLER --> RUN_STRUCT\n    RUN --> TIMEOUT_CHECK\n    RUN_STRUCT --> TIMEOUT_CHECK\n    \n    TIMEOUT_CHECK -->|\"Yes<br/>(Direct)\"| DIRECT_WORKER\n    TIMEOUT_CHECK -->|\"No<br/>(Process)\"| QUEUE\n    \n    DIRECT_WORKER --> DIRECT_EXEC\n    DIRECT_EXEC --> DIRECT_CAPTURE\n    DIRECT_CAPTURE --> RESULT\n    \n    QUEUE --> PROCESS\n    PROCESS --> WORKER\n    WORKER --> WORKER_EXEC\n    WORKER_EXEC --> WORKER_CAPTURE\n    WORKER_CAPTURE --> RESULT_PUT\n    RESULT_PUT --> QUEUE\n    QUEUE -->|\"queue.get()\"| RESULT\n    \n    RESULT --> CALLER\n```\n\nSources: [src/runtime/python_executor.py:92-163]()\n\n---\n\n## Worker Functions\n\nThe direct execution system provides two worker functions that handle the actual code execution with different output formats.\n\n### worker_with_globals_capture()\n\nThis function is the **primary worker for structured execution**, capturing both output and global variable state. It returns an `ExecutionResult` object with comprehensive execution information.\n\n**Function Signature:**\n```python\ndef worker_with_globals_capture(\n    command: str,\n    _globals: Optional[Dict],\n    _locals: Optional[Dict],\n    queue: multiprocessing.Queue,\n    timeout: Optional[int],\n) -> None\n```\n\n**Execution Flow:**\n\n```mermaid\ngraph TD\n    START[\"worker_with_globals_capture() called\"]\n    REDIRECT[\"Redirect sys.stdout to StringIO\"]\n    \n    TRY_EXEC[\"Try: exec(command, _globals, _locals)\"]\n    SUCCESS_RESULT[\"Build ExecutionResult<br/>exit_status=SUCCESS<br/>stdout=mystdout.getvalue()\"]\n    \n    EXCEPT[\"Except: Catch exception\"]\n    FAILURE_RESULT[\"Build ExecutionResult<br/>exit_status=FAILURE<br/>exception details captured\"]\n    \n    FINALLY[\"Finally: Restore sys.stdout\"]\n    QUEUE_PUT[\"queue.put(exec_result)\"]\n    END[\"Return None\"]\n    \n    START --> REDIRECT\n    REDIRECT --> TRY_EXEC\n    TRY_EXEC -->|\"Success\"| SUCCESS_RESULT\n    TRY_EXEC -->|\"Exception\"| EXCEPT\n    EXCEPT --> FAILURE_RESULT\n    SUCCESS_RESULT --> FINALLY\n    FAILURE_RESULT --> FINALLY\n    FINALLY --> QUEUE_PUT\n    QUEUE_PUT --> END\n```\n\n**Key Implementation Details:**\n\n1. **Output Redirection**: [src/runtime/python_executor.py:42-43]()\n   ```python\n   old_stdout = sys.stdout\n   sys.stdout = mystdout = StringIO()\n   ```\n\n2. **Success Path**: [src/runtime/python_executor.py:45-54]()\n   - Executes code with `exec(command, exec_globals, exec_locals)`\n   - Builds `ExecutionResult` with `exit_status=ExecutionStatus.SUCCESS`\n   - Captures stdout content\n\n3. **Failure Path**: [src/runtime/python_executor.py:55-67]()\n   - Catches any exception during execution\n   - Captures exception type, value, and traceback\n   - Builds `ExecutionResult` with `exit_status=ExecutionStatus.FAILURE`\n\n4. **Result Communication**: [src/runtime/python_executor.py:69-70]()\n   - Uses multiprocessing.Queue to send result back to parent\n   - Works in both direct and process-isolated modes\n\nSources: [src/runtime/python_executor.py:28-70]()\n\n### worker()\n\nThis function is a **simpler worker for string-based execution**, returning only the captured output or error message as a string.\n\n**Function Signature:**\n```python\ndef worker(\n    command: str,\n    _globals: Optional[Dict],\n    _locals: Optional[Dict],\n    queue: multiprocessing.Queue,\n) -> None\n```\n\n**Differences from `worker_with_globals_capture()`:**\n\n| Feature | worker_with_globals_capture() | worker() |\n|---------|-------------------------------|----------|\n| Return Type | ExecutionResult (via queue) | str (via queue) |\n| Input Sanitization | None | Uses `sanitize_input()` |\n| Exception Handling | Structured with traceback | Returns formatted string |\n| Global State Capture | Yes | No |\n| Use Case | Structured execution pipeline | Legacy/simple execution |\n\n**Implementation**: [src/runtime/python_executor.py:72-89]()\n\nSources: [src/runtime/python_executor.py:72-89]()\n\n---\n\n## Execution Entry Points\n\n### run_structured()\n\nThe **primary entry point** for direct execution with structured results. Decorated with `@traceable` for observability.\n\n**Function Signature:**\n```python\n@traceable\ndef run_structured(\n    command: str, \n    _globals: dict[str, Any] | None = None, \n    _locals: Optional[Dict] = None, \n    timeout: Optional[int] = None\n) -> ExecutionResult\n```\n\n**Execution Logic:**\n\n```mermaid\ngraph TD\n    START[\"run_structured() called\"]\n    CREATE_QUEUE[\"Create multiprocessing.Queue()\"]\n    TIMEOUT_CHECK{\"timeout<br/>is not None?\"}\n    \n    subgraph \"With Timeout (Process Mode)\"\n        CREATE_PROC[\"Create Process<br/>target=worker_with_globals_capture\"]\n        START_PROC[\"p.start()\"]\n        JOIN_PROC[\"p.join(timeout)\"]\n        ALIVE_CHECK{\"p.is_alive()?\"}\n        TERMINATE[\"p.terminate()<br/>Build TIMEOUT ExecutionResult\"]\n        GET_RESULT[\"Get result from queue\"]\n    end\n    \n    subgraph \"Without Timeout (Direct Mode)\"\n        DIRECT_CALL[\"Call worker_with_globals_capture<br/>directly (no process)\"]\n        DIRECT_RESULT[\"Get result from queue\"]\n    end\n    \n    RETURN[\"Return ExecutionResult\"]\n    \n    START --> CREATE_QUEUE\n    CREATE_QUEUE --> TIMEOUT_CHECK\n    \n    TIMEOUT_CHECK -->|\"Yes\"| CREATE_PROC\n    CREATE_PROC --> START_PROC\n    START_PROC --> JOIN_PROC\n    JOIN_PROC --> ALIVE_CHECK\n    ALIVE_CHECK -->|\"Yes<br/>(Timeout)\"| TERMINATE\n    ALIVE_CHECK -->|\"No<br/>(Finished)\"| GET_RESULT\n    TERMINATE --> RETURN\n    GET_RESULT --> RETURN\n    \n    TIMEOUT_CHECK -->|\"No\"| DIRECT_CALL\n    DIRECT_CALL --> DIRECT_RESULT\n    DIRECT_RESULT --> RETURN\n```\n\n**Timeout Handling**: [src/runtime/python_executor.py:114-124]()\n\nWhen a timeout occurs:\n1. Process is still alive after `p.join(timeout)`\n2. `p.terminate()` is called to forcibly stop execution\n3. An `ExecutionResult` with `exit_status=ExecutionStatus.TIMEOUT` is constructed by the parent process\n4. No globals are captured (unsafe to retrieve from terminated process)\n\n**Direct Mode**: [src/runtime/python_executor.py:126-130]()\n\nWhen `timeout is None`:\n- `worker_with_globals_capture()` is called **directly in the current process**\n- No multiprocessing overhead\n- Maximum performance\n- Result still uses queue for API consistency\n\nSources: [src/runtime/python_executor.py:92-130]()\n\n### run()\n\nA **legacy/simpler entry point** that returns execution output as a plain string rather than a structured result.\n\n**Function Signature:**\n```python\ndef run(\n    command: str, \n    _globals: dict[str, Any] | None = None, \n    _locals: Optional[Dict] = None, \n    timeout: Optional[int] = None\n) -> str\n```\n\n**Key Differences from `run_structured()`:**\n\n1. Uses the simpler `worker()` function instead of `worker_with_globals_capture()`\n2. Returns string output or error message instead of `ExecutionResult`\n3. Timeout handling returns `\"Execution timed out\"` string\n4. Does not capture or return global variable state\n\n**Implementation**: [src/runtime/python_executor.py:132-163]()\n\nSources: [src/runtime/python_executor.py:132-163]()\n\n---\n\n## Queue-Based Communication\n\nThe direct execution system uses `multiprocessing.Queue` for communication between the caller and the worker, even in direct mode. This provides a **consistent API** regardless of execution mode.\n\n### Queue Communication Pattern\n\n```mermaid\nsequenceDiagram\n    participant Caller as Caller\n    participant Queue as multiprocessing.Queue\n    participant Worker as worker_with_globals_capture\n    \n    Caller->>Queue: Create queue = Queue()\n    \n    alt With Timeout (Process Mode)\n        Caller->>Worker: Start Process(target=worker, args=(queue,))\n        Worker->>Worker: Execute code\n        Worker->>Worker: Build ExecutionResult\n        Worker->>Queue: queue.put(result)\n        Caller->>Queue: result = queue.get()\n    else Without Timeout (Direct Mode)\n        Caller->>Worker: Call worker() directly with queue\n        Worker->>Worker: Execute code in-process\n        Worker->>Worker: Build ExecutionResult\n        Worker->>Queue: queue.put(result)\n        Caller->>Queue: result = queue.get()\n    end\n    \n    Queue-->>Caller: Return ExecutionResult\n```\n\n**Why Use Queue in Direct Mode?**\n\nEven when executing code directly (no timeout), the system uses a queue because:\n1. **API Consistency**: Both modes use the same calling convention\n2. **Clean Result Passing**: Queue provides a standard result container\n3. **Future Extensibility**: Easy to switch modes without API changes\n4. **Exception Isolation**: Queue naturally handles serializable results\n\nSources: [src/runtime/python_executor.py:104-130]()\n\n---\n\n## Integration with ExecutionResult Schema\n\nDirect execution integrates with the structured result system through the `ExecutionResult` schema.\n\n### Result Construction\n\n**Success Case**: [src/runtime/python_executor.py:47-54]()\n```python\nres = ExecutionResult(\n    arg_command=command,\n    arg_globals=_globals or {},\n    arg_timeout=timeout,\n    exit_status=ExecutionStatus.SUCCESS,\n    # ret_stdout filled later from StringIO\n)\n```\n\n**Failure Case**: [src/runtime/python_executor.py:56-67]()\n```python\nres = ExecutionResult(\n    arg_command=command,\n    arg_globals=_globals or {},\n    arg_timeout=timeout,\n    exit_status=ExecutionStatus.FAILURE,\n    exception_repr=repr(e),\n    exception_type=type(e).__name__,\n    exception_value=str(e),\n    exception_traceback=source_code.get_exception_traceback()\n)\n```\n\n**Timeout Case** (constructed by caller): [src/runtime/python_executor.py:117-124]()\n```python\nreturn ExecutionResult(\n    arg_command=command,\n    arg_timeout=timeout,\n    arg_globals=_globals or {},\n    exit_status=ExecutionStatus.TIMEOUT,\n    ret_stdout=\"\",\n)\n```\n\n### Status Handling Summary\n\n| Status | Created By | Globals Captured? | Exception Info? |\n|--------|-----------|-------------------|-----------------|\n| SUCCESS | Worker | Yes (filtered & deepcopied) | No |\n| FAILURE | Worker | Yes (pre-exception state) | Yes (full traceback) |\n| TIMEOUT | Caller | No (unsafe) | No |\n| CRASHED | Not applicable | Not applicable | Not applicable* |\n\n*CRASHED status is only possible in subprocess execution (covered in [5.1](#5.1))\n\nSources: [src/runtime/python_executor.py:47-67](), [src/runtime/python_executor.py:117-124](), [src/runtime/schemas.py:11-16]()\n\n---\n\n## Use Cases and Trade-offs\n\n### When to Use Direct Execution\n\n**Appropriate Use Cases:**\n1. **Trusted Code**: Executing code from trusted sources or pre-validated snippets\n2. **Performance Critical**: When execution speed is paramount and overhead must be minimized\n3. **State Sharing**: When you need direct access to the caller's namespace\n4. **Simple Operations**: Quick calculations or transformations without external dependencies\n5. **Testing**: Unit tests where isolation isn't necessary\n\n**Example from Tests**: [tests/unit/runtime/test_python_executor.py:78-86]()\n```python\nres = run_structured(\"\"\"import time\nc = 10\ntime.sleep(5)\"\"\", my_globals, my_locals, timeout=1)\n```\n\n### When NOT to Use Direct Execution\n\n**Avoid Direct Execution For:**\n1. **Untrusted Code**: User-provided code that hasn't been sandboxed\n2. **Long-Running Tasks**: Operations that might hang indefinitely\n3. **Memory-Intensive**: Code that could exhaust system memory\n4. **Crash-Prone Code**: Operations that could cause segfaults or crashes\n5. **Isolation Required**: When you need guaranteed separation from parent process\n\n**Use Subprocess Instead**: For these scenarios, use [Subprocess Execution](#5.1)\n\nSources: [src/runtime/python_executor.py:92-163](), [tests/unit/runtime/test_python_executor.py:1-104]()\n\n### Performance Comparison\n\n```mermaid\ngraph LR\n    subgraph \"Execution Overhead (Relative)\"\n        DIRECT[\"Direct Execution<br/>Baseline: 1x\"]\n        SUBTHREAD[\"Subthread Execution<br/>~2-3x overhead\"]\n        SUBPROCESS[\"Subprocess Execution<br/>~10-50x overhead\"]\n    end\n    \n    DIRECT -->|\"Thread creation,<br/>buffer setup\"| SUBTHREAD\n    SUBTHREAD -->|\"Process creation,<br/>pipe setup,<br/>serialization\"| SUBPROCESS\n```\n\n| Strategy | Startup Time | Memory Overhead | Isolation | Safety |\n|----------|--------------|-----------------|-----------|--------|\n| **Direct** | ~0ms | None | None | Lowest |\n| **Subthread** | ~1-5ms | Minimal | Partial | Medium |\n| **Subprocess** | ~10-100ms | High (full copy) | Complete | Highest |\n\nSources: [src/runtime/python_executor.py:1-164]()\n\n---\n\n## Limitations and Risks\n\n### Critical Limitations\n\n1. **No Crash Protection**\n   - If executed code crashes (segfault, stack overflow), the **entire parent process crashes**\n   - No mechanism to recover from catastrophic failures\n   - See crash examples in [tests/playground/crashed.py:1-124]()\n\n2. **No Isolation**\n   - Executed code shares the same memory space as the caller\n   - Can access and modify parent process variables\n   - Can import and interfere with system modules\n\n3. **Timeout Implementation**\n   - Timeout support requires multiprocessing.Process\n   - When timeout is used, it's **no longer truly \"direct\"** - it becomes subprocess execution\n   - True direct mode (`timeout=None`) has **no timeout protection**\n\n4. **Global State Pollution**\n   - Executed code can modify `sys.modules`, global variables, etc.\n   - Changes persist after execution completes\n   - No cleanup mechanism for side effects\n\n### Security Considerations\n\n**Never use direct execution for untrusted code:**\n\n```python\n# DANGEROUS - Direct execution of user input\nuser_code = request.get(\"code\")  # From HTTP request\nresult = run_structured(user_code, timeout=None)  # NO ISOLATION!\n```\n\n**Safe alternative - Use subprocess:**\n```python\n# SAFE - Subprocess isolation for untrusted code\nuser_code = request.get(\"code\")\nresult = run_structured_in_subprocess(user_code, timeout=10)  # Isolated\n```\n\n### Namespace Behavior\n\nThe interaction between `_globals` and `_locals` requires careful handling. From [docs/python.design.md:427-534](), Python's `exec()` has specific rules:\n\n- When both `_globals` and `_locals` are provided separately, module-level assignments go to `_locals`\n- To ensure functions are accessible in `_globals`, use: `_locals = _globals`\n- Direct execution in this codebase uses: [src/runtime/python_executor.py:39-40]()\n  ```python\n  exec_globals = _globals\n  exec_locals  = _locals\n  ```\n\nSources: [src/runtime/python_executor.py:28-70](), [tests/playground/crashed.py:1-124](), [docs/python.design.md:427-534](), [docs/crashed.design.md:1-404]()\n\n---\n\n## Comparison with Other Execution Strategies\n\n### Direct vs Subthread vs Subprocess\n\n```mermaid\ngraph TB\n    subgraph \"Isolation Levels\"\n        DIRECT[\"Direct Execution<br/>âââââââââââââ<br/>â¢ Same process<br/>â¢ Same memory<br/>â¢ No protection\"]\n        \n        SUBTHREAD[\"Subthread Execution<br/>âââââââââââââ<br/>â¢ Same process<br/>â¢ Shared memory<br/>â¢ Timeout via threading\"]\n        \n        SUBPROCESS[\"Subprocess Execution<br/>âââââââââââââ<br/>â¢ Separate process<br/>â¢ Isolated memory<br/>â¢ Full protection\"]\n    end\n    \n    RISK[\"Risk Level\"]\n    SPEED[\"Speed\"]\n    \n    DIRECT -->|\"Increasing\"| RISK\n    SUBTHREAD -->|\"Increasing\"| RISK\n    SUBPROCESS -->|\"Increasing\"| RISK\n    \n    SUBPROCESS -->|\"Increasing\"| SPEED\n    SUBTHREAD -->|\"Increasing\"| SPEED\n    DIRECT -->|\"Increasing\"| SPEED\n```\n\n| Feature | Direct | Subthread | Subprocess |\n|---------|--------|-----------|------------|\n| **Implementation** | `exec()` in-process | `Thread` + `exec()` | `Process` + pipe IPC |\n| **Crash Protection** | None | None | Full |\n| **Memory Isolation** | None | None | Full |\n| **Timeout Method** | Process (when needed) | Thread + flag | Process termination |\n| **Globals Capture** | Direct reference | Shared mutable state | Serialized via pipe |\n| **Startup Overhead** | None (~0ms) | Minimal (~1ms) | Significant (~50ms) |\n| **Best For** | Trusted, fast operations | I/O-bound tasks | Untrusted code |\n\nSources: [src/runtime/python_executor.py:1-164](), [tests/playground/subprocess_output.py:1-293]()\n\n---\n\n## Code Examples\n\n### Basic Direct Execution\n\nFrom [tests/unit/runtime/test_python_executor.py:10-21]():\n\n```python\nmy_globals = {\"a\": 123, \"b\": [1, 2, 3]}\nmy_locals = {\"a\": 123, \"b\": [1, 2, 3]}\n\nres = run(\"a+=100000\", my_globals, my_locals)  # Direct execution\nprint(res)  # Empty string (assignment has no output)\n\nres = run(\"print(a)\", my_globals, my_locals)\nprint(res)  # \"100123\" (modified value)\n```\n\n### Structured Execution with Globals Capture\n\nFrom [tests/unit/runtime/test_python_executor.py:89-97]():\n\n```python\nmy_globals = {\"a\": 123, \"b\": [1, 2, 3]}\nres = run_structured(\"\"\"import time\nc = 10\nimport scipy\n\"\"\", my_globals, None, timeout=20000)\n\nprint(res.exit_status)  # ExecutionStatus.SUCCESS\nprint(res.arg_globals)   # {'a': 123, 'b': [1, 2, 3], 'c': 10}\n```\n\n### Exception Handling\n\nFrom [tests/unit/runtime/test_python_executor.py:37-49]():\n\n```python\nmy_globals = {\"a\": 123, \"b\": [1, 2, 3]}\nres = run(\"print(c)\", my_globals, my_locals)  # NameError\nprint(res)  # Returns formatted error message\n```\n\nSources: [tests/unit/runtime/test_python_executor.py:10-104]()\n\n---\n\n## Summary\n\nDirect execution provides the **fastest code execution** strategy in the algo_agent system by running code in-process without isolation overhead. It is most appropriate for:\n\nâ Trusted code execution  \nâ Performance-critical operations  \nâ Simple calculations and transformations  \nâ Testing and development scenarios\n\nIt should **not be used** for:\n\nâ Untrusted user input  \nâ Long-running or hanging operations  \nâ Memory-intensive tasks  \nâ Code that might crash or cause instability\n\nFor production use with untrusted code, prefer [Subprocess Execution](#5.1) which provides complete isolation and crash protection.\n\nSources: [src/runtime/python_executor.py:1-164]()\n\n---\n\n# Page: ExecutionResult and Status Handling\n\n# ExecutionResult and Status Handling\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [docs/crashed.design.md](docs/crashed.design.md)\n- [src/runtime/cwd.py](src/runtime/cwd.py)\n- [src/runtime/schemas.py](src/runtime/schemas.py)\n- [src/runtime/subprocess_python_executor.py](src/runtime/subprocess_python_executor.py)\n- [tests/playground/crashed.py](tests/playground/crashed.py)\n- [tests/playground/subprocess_output.py](tests/playground/subprocess_output.py)\n\n</details>\n\n\n\nThis document covers the structured result format used to capture Python code execution outcomes and the status classification system. The `ExecutionResult` schema and `ExecutionStatus` enum provide a comprehensive way to represent successful executions, failures, timeouts, and process crashes, along with all relevant diagnostic information.\n\nFor information about how these results are generated by different execution strategies, see [Subprocess Execution](#5.1), [Subthread Execution](#5.2), and [Direct Execution](#5.3). For details on how workspace state is managed within execution results, see [Workspace State Management](#5.5).\n\n---\n\n## ExecutionStatus Enum\n\nThe `ExecutionStatus` enum defines four distinct states that categorize all possible execution outcomes. This enum is used throughout the execution runtime to classify results and determine appropriate error handling.\n\n| Status | Value | Meaning | When Assigned |\n|--------|-------|---------|---------------|\n| `SUCCESS` | `\"success\"` | Code executed without errors | Process exits normally (exit code 0) and exec() completes |\n| `FAILURE` | `\"failure\"` | Code raised an exception | Process exits normally but exec() raises a catchable exception |\n| `TIMEOUT` | `\"timeout\"` | Execution exceeded time limit | Parent process terminates subprocess after timeout period |\n| `CRASHED` | `\"crashed\"` | Process terminated abnormally | Process exits without sending result (SegFault, OOM, etc.) |\n\n**Sources:** [src/runtime/schemas.py:10-16]()\n\n```mermaid\ngraph TD\n    START[\"Code Execution Begins\"]\n    SUBPROCESS[\"Subprocess Starts\"]\n    EXEC[\"exec() Call\"]\n    \n    SUCCESS[\"ExecutionStatus.SUCCESS\"]\n    FAILURE[\"ExecutionStatus.FAILURE\"]\n    TIMEOUT[\"ExecutionStatus.TIMEOUT\"]\n    CRASHED[\"ExecutionStatus.CRASHED\"]\n    \n    START --> SUBPROCESS\n    SUBPROCESS --> TIMEOUT_CHECK{\"Timeout<br/>Exceeded?\"}\n    TIMEOUT_CHECK -->|Yes| TIMEOUT\n    TIMEOUT_CHECK -->|No| EXEC\n    \n    EXEC --> EXCEPTION{\"Exception<br/>Raised?\"}\n    EXCEPTION -->|No| RESULT_SENT{\"Result<br/>Sent?\"}\n    EXCEPTION -->|Yes| FAILURE\n    \n    RESULT_SENT -->|Yes| SUCCESS\n    RESULT_SENT -->|No| CRASHED\n    \n    SUCCESS --> END[\"Return Result\"]\n    FAILURE --> END\n    TIMEOUT --> END\n    CRASHED --> END\n```\n\n**Diagram:** ExecutionStatus State Machine\n\n**Sources:** [src/runtime/subprocess_python_executor.py:76-163](), [tests/playground/subprocess_output.py:68-156]()\n\n---\n\n## ExecutionResult Schema\n\nThe `ExecutionResult` is a Pydantic model that captures all information about a code execution attempt. It contains three categories of fields: input parameters, execution results, and parent-populated fields.\n\n### Field Categories\n\n**Input Parameters** (captured at execution start):\n- `arg_command`: The Python code string that was executed\n- `arg_timeout`: The timeout limit in seconds\n- `arg_globals`: The filtered and deep-copied global variables dictionary\n\n**Execution Results** (populated by subprocess or parent):\n- `exit_status`: The `ExecutionStatus` enum value\n- `exit_code`: Process exit code (0=normal, >0=error, <0=signal terminated)\n- `exception_repr`: String representation of the exception (`repr(e)`)\n- `exception_type`: Exception class name (e.g., `\"ZeroDivisionError\"`)\n- `exception_value`: Exception message (`str(e)`)\n- `exception_traceback`: Full traceback with stack frames\n\n**Parent-Populated Fields** (filled after subprocess completes):\n- `ret_stdout`: Combined stdout and stderr output\n- `ret_tool2llm`: Formatted message for LLM consumption\n\n**Sources:** [src/runtime/schemas.py:56-71]()\n\n### Field Population Table\n\n| Field | SUCCESS | FAILURE | TIMEOUT | CRASHED |\n|-------|---------|---------|---------|---------|\n| `arg_command` | â Set | â Set | â Set | â Set |\n| `arg_timeout` | â Set | â Set | â Set | â Set |\n| `arg_globals` | â Filtered | â Filtered | â Original | â Original |\n| `exit_status` | SUCCESS | FAILURE | TIMEOUT | CRASHED |\n| `exit_code` | 0 | 0 | -15 (SIGTERM) | Variable |\n| `exception_repr` | None | â Set | None | None |\n| `exception_type` | None | â Set | None | None |\n| `exception_value` | None | â Set | None | None |\n| `exception_traceback` | None | â Set | None | None |\n| `ret_stdout` | â Output | â Output | â Partial | â Partial |\n| `ret_tool2llm` | â Generated | â Generated | â Generated | â Generated |\n\n---\n\n## Status Determination Logic\n\nThe execution status is determined through a decision tree that evaluates process lifecycle events and outcomes. The logic differs slightly between subprocess execution (most comprehensive) and other execution modes.\n\n### Subprocess Execution Flow\n\n```mermaid\nsequenceDiagram\n    participant Parent as \"Parent Process\"\n    participant Subprocess as \"Child Process\"\n    participant Reader as \"Reader Thread\"\n    participant Buffer as \"stdout_buffer\"\n    participant Container as \"result_container\"\n    \n    Parent->>Subprocess: Start with timeout\n    Parent->>Reader: Start reader thread\n    Subprocess->>Subprocess: Redirect stdout/stderr\n    \n    alt Code executes successfully\n        Subprocess->>Buffer: Send stdout messages\n        Subprocess->>Container: Send ExecutionResult (SUCCESS)\n        Subprocess->>Subprocess: Exit with code 0\n        Parent->>Parent: join() returns (not alive)\n        Parent->>Parent: exit_status = SUCCESS\n    else Code raises exception\n        Subprocess->>Buffer: Send stdout messages\n        Subprocess->>Subprocess: Catch exception in try-except\n        Subprocess->>Container: Send ExecutionResult (FAILURE)\n        Subprocess->>Subprocess: Exit with code 0\n        Parent->>Parent: join() returns (not alive)\n        Parent->>Parent: exit_status = FAILURE\n    else Timeout exceeded\n        Parent->>Parent: join(timeout) - still alive\n        Parent->>Subprocess: terminate()\n        Parent->>Parent: Build ExecutionResult (TIMEOUT)\n        Parent->>Parent: exit_code = -15\n    else Process crashes\n        Subprocess->>Buffer: Send partial stdout\n        Subprocess->>Subprocess: SegFault/OOM - no result sent\n        Parent->>Parent: join() returns (not alive)\n        Parent->>Parent: result_container is empty\n        Parent->>Parent: Build ExecutionResult (CRASHED)\n        Parent->>Parent: exit_code = 139 or 1\n    end\n    \n    Parent->>Parent: Populate ret_stdout from buffer\n    Parent->>Parent: Generate ret_tool2llm\n    Parent->>Parent: Return ExecutionResult\n```\n\n**Diagram:** Status Determination in Subprocess Execution\n\n**Sources:** [src/runtime/subprocess_python_executor.py:128-163]()\n\n### Status Assignment Rules\n\n**SUCCESS Status** [src/runtime/subprocess_python_executor.py:49-57]():\n- Subprocess completes normally (process.is_alive() == False)\n- Result container has exactly one ExecutionResult\n- exec() completed without raising exceptions\n- Exit code is 0\n\n**FAILURE Status** [src/runtime/subprocess_python_executor.py:58-69]():\n- Subprocess completes normally (process.is_alive() == False)\n- Result container has exactly one ExecutionResult\n- exec() raised a Python exception that was caught\n- Exception details captured in try-except block\n- Exit code is 0 (process didn't crash, just code failed)\n\n**TIMEOUT Status** [src/runtime/subprocess_python_executor.py:130-144]():\n- Parent process determines subprocess is still alive after timeout\n- Parent calls process.terminate() to force termination\n- Parent constructs ExecutionResult with TIMEOUT status\n- Exit code is -15 (SIGTERM signal)\n- arg_globals contains original input (not filtered output)\n\n**CRASHED Status** [src/runtime/subprocess_python_executor.py:145-159]():\n- Subprocess terminates (process.is_alive() == False)\n- Result container is empty (no ExecutionResult received)\n- Indicates process-level failure (SegFault, signal termination, etc.)\n- Exit code varies: 139 (SIGSEGV), 1 (unhandled error), etc.\n- arg_globals contains original input (not filtered output)\n\n**Sources:** [src/runtime/subprocess_python_executor.py:76-163](), [tests/playground/subprocess_output.py:68-156]()\n\n---\n\n## Result Population Flow\n\nThe ExecutionResult is populated in stages depending on where execution succeeds or fails.\n\n```mermaid\ngraph TB\n    subgraph \"Subprocess Context\"\n        INIT[\"Initialize<br/>arg_command, arg_timeout, arg_globals\"]\n        EXEC[\"Execute with exec()\"]\n        SUCCESS_PATH[\"Success Path\"]\n        FAILURE_PATH[\"Exception Path\"]\n        \n        INIT --> EXEC\n        EXEC --> SUCCESS_PATH\n        EXEC --> FAILURE_PATH\n        \n        SUCCESS_PATH --> SUB_SUCCESS[\"Create ExecutionResult<br/>exit_status=SUCCESS<br/>arg_globals=filtered\"]\n        FAILURE_PATH --> SUB_FAILURE[\"Create ExecutionResult<br/>exit_status=FAILURE<br/>exception_*=populated\"]\n        \n        SUB_SUCCESS --> SEND1[\"Send via pipe\"]\n        SUB_FAILURE --> SEND2[\"Send via pipe\"]\n    end\n    \n    subgraph \"Parent Context\"\n        WAIT[\"Wait for subprocess\"]\n        CHECK{\"Process<br/>State?\"}\n        \n        WAIT --> CHECK\n        \n        CHECK -->|Timeout| PARENT_TIMEOUT[\"Create ExecutionResult<br/>exit_status=TIMEOUT\"]\n        CHECK -->|Completed| RECV_CHECK{\"Result<br/>Received?\"}\n        \n        RECV_CHECK -->|Yes| RECV_RESULT[\"Use subprocess result\"]\n        RECV_CHECK -->|No| PARENT_CRASHED[\"Create ExecutionResult<br/>exit_status=CRASHED\"]\n        \n        RECV_RESULT --> POPULATE\n        PARENT_TIMEOUT --> POPULATE\n        PARENT_CRASHED --> POPULATE\n        \n        POPULATE[\"Populate parent fields:<br/>- exit_code from process<br/>- ret_stdout from buffer<br/>- ret_tool2llm via get_return_llm\"]\n    end\n    \n    SEND1 --> WAIT\n    SEND2 --> WAIT\n```\n\n**Diagram:** ExecutionResult Population Stages\n\n**Sources:** [src/runtime/subprocess_python_executor.py:76-163]()\n\n### Global Variables Filtering\n\nThe `arg_globals` field undergoes automatic filtering and deep copying through a Pydantic field validator:\n\n```python\n@field_validator('arg_globals')\n@classmethod        \ndef field_validate_globals(cls, value: Dict[str, Any]) -> Dict[str, Any]:\n    if value is None:\n        return {}\n    return workspace.filter_and_deepcopy_globals(value)\n```\n\n**Sources:** [src/runtime/schemas.py:77-83]()\n\nThis filtering:\n- Removes `__builtins__` and module objects\n- Excludes non-picklable objects\n- Creates deep copies to prevent shared state issues\n- Only applies to SUCCESS and FAILURE statuses (when exec() completes)\n- For TIMEOUT and CRASHED, original globals are retained (execution didn't complete)\n\nFor complete details on workspace filtering, see [Workspace State Management](#5.5).\n\n---\n\n## LLM-Formatted Output Generation\n\nThe `get_return_llm` class method generates human-readable messages formatted specifically for LLM consumption. Each status has a distinct template optimized for debugging and iteration.\n\n### Template Structure\n\n```mermaid\ngraph LR\n    STATUS[\"ExecutionStatus\"]\n    METHOD[\"get_return_llm(status, result)\"]\n    \n    STATUS --> METHOD\n    \n    METHOD --> SUCCESS_MSG[\"SUCCESS:<br/>## Code executed successfully<br/>### Terminal output\"]\n    METHOD --> FAILURE_MSG[\"FAILURE:<br/>## Code failed<br/>### Terminal output<br/>### Original code with line numbers<br/>### Error traceback\"]\n    METHOD --> TIMEOUT_MSG[\"TIMEOUT:<br/>## Execution timeout<br/>### Terminal output<br/>### Timeout limit\"]\n    METHOD --> CRASHED_MSG[\"CRASHED:<br/>## Process crashed<br/>### Terminal output<br/>### Exit status code\"]\n```\n\n**Diagram:** LLM Message Generation by Status\n\n**Sources:** [src/runtime/schemas.py:18-47]()\n\n### Message Templates\n\n**SUCCESS Template** [src/runtime/schemas.py:24-27]():\n```\n## ä»£ç æ§è¡æåï¼è¾åºç»æå®æ´ï¼ä»»å¡å®æ\n### ç»ç«¯è¾åºï¼\n{result.ret_stdout}\n```\n\n**FAILURE Template** [src/runtime/schemas.py:28-35]():\n```\n## ä»£ç æ§è¡å¤±è´¥ï¼ä»£ç æåºå¼å¸¸ï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\n{result.ret_stdout}\n### åå§ä»£ç ï¼\n{source_code.add_line_numbers(result.arg_command)}\n### æ¥éä¿¡æ¯ï¼\n{result.exception_traceback}\n```\n\n**TIMEOUT Template** [src/runtime/schemas.py:36-40]():\n```\n## ä»£ç æ§è¡è¶æ¶ï¼å¼ºå¶éåºæ§è¡ï¼è°æ´è¶æ¶æ¶é´åéè¯\n### ç»ç«¯è¾åºï¼\n{result.ret_stdout}\n### è¶åºéå¶çæ¶é´ï¼{result.arg_timeout} ç§\n```\n\n**CRASHED Template** [src/runtime/schemas.py:41-45]():\n```\n## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\n{result.ret_stdout}\n### éåºç¶æç ï¼{result.exit_code}\n```\n\nThe FAILURE template uniquely includes line-numbered source code to help the LLM identify the exact location of errors.\n\n**Sources:** [src/runtime/schemas.py:18-47](), [src/runtime/source_code.py]()\n\n---\n\n## Exit Code Interpretation\n\nProcess exit codes provide additional diagnostic information, particularly for CRASHED status:\n\n| Exit Code Range | Meaning | Example Scenarios |\n|----------------|---------|-------------------|\n| 0 | Normal exit | Code executed successfully, or caught exception properly |\n| > 0 | Error exit | Code logic errors, command execution failures |\n| -1 to -255 | Signal termination | Exit code = -(signal number) |\n| -15 | SIGTERM | Timeout termination by parent process |\n| -9 | SIGKILL | Force kill (cannot be caught) |\n| -11 or 139 | SIGSEGV | Segmentation fault (139 = 128 + 11) |\n\n**Sources:** [src/runtime/schemas.py:50-54](), [tests/playground/subprocess_output.py:215-234]()\n\n---\n\n## Status Examples\n\n### SUCCESS Example\n\n**Code:**\n```python\nimport scipy\nc = 10\nprint(\"scipy imported\")\n```\n\n**Result:**\n```python\nExecutionResult(\n    arg_command='import time\\nc = 10\\nimport scipy\\nprint(\"scipy imported\")\\n',\n    arg_globals={'a': 123, 'b': [1, 2, 3], 'c': 10},\n    arg_timeout=20000,\n    exit_status=ExecutionStatus.SUCCESS,\n    exit_code=0,\n    exception_repr=None,\n    exception_type=None,\n    exception_value=None,\n    exception_traceback=None,\n    ret_stdout='scipy imported\\n',\n    ret_tool2llm='## ä»£ç æ§è¡æåï¼è¾åºç»æå®æ´ï¼ä»»å¡å®æ\\n### ç»ç«¯è¾åºï¼\\nscipy imported\\n'\n)\n```\n\n**Sources:** [tests/playground/subprocess_output.py:183-198](), [tests/playground/subprocess_output.py:362-372]()\n\n### FAILURE Example\n\n**Code:**\n```python\na = 123\nb = 0\nc = a/b\n```\n\n**Result:**\n```python\nExecutionResult(\n    arg_command='\\na = 123\\nb = 0\\nc = a/b\\n',\n    arg_globals={'a': 123, 'b': 0},\n    arg_timeout=20000,\n    exit_status=ExecutionStatus.FAILURE,\n    exit_code=0,\n    exception_repr=\"ZeroDivisionError('division by zero')\",\n    exception_type='ZeroDivisionError',\n    exception_value='division by zero',\n    exception_traceback='Traceback (most recent call last):\\n'\n                        '  File \"...subprocess_output.py\", line 42, in _worker_with_pipe\\n'\n                        '    exec(command, _globals, _locals)\\n'\n                        '  File \"<string>\", line 4, in <module>\\n'\n                        'ZeroDivisionError: division by zero\\n',\n    ret_stdout='',\n    ret_tool2llm='## ä»£ç æ§è¡å¤±è´¥ï¼ä»£ç æåºå¼å¸¸ï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\\n...'\n)\n```\n\n**Sources:** [tests/playground/subprocess_output.py:199-213](), [tests/playground/subprocess_output.py:396-427]()\n\n### TIMEOUT Example\n\n**Code:**\n```python\nimport time\nprint(\"Start sleeping...\", flush=True)\ntime.sleep(10)\nprint(\"Finished sleeping.\", flush=True)\n```\n\n**Result (timeout=3):**\n```python\nExecutionResult(\n    arg_command='\\nimport time\\nprint(\"Start sleeping...\", flush=True)\\ntime.sleep(10)\\n...',\n    arg_globals={'a': 123, 'b': [1, 2, 3]},  # Original, not filtered\n    arg_timeout=3,\n    exit_status=ExecutionStatus.TIMEOUT,\n    exit_code=-15,  # SIGTERM\n    exception_repr=None,\n    exception_type=None,\n    exception_value=None,\n    exception_traceback=None,\n    ret_stdout='Start sleeping...\\n',  # Partial output before termination\n    ret_tool2llm='## ä»£ç æ§è¡è¶æ¶ï¼å¼ºå¶éåºæ§è¡ï¼è°æ´è¶æ¶æ¶é´åéè¯\\n...'\n)\n```\n\n**Sources:** [tests/playground/subprocess_output.py:166-177](), [tests/playground/subprocess_output.py:320-337]()\n\n### CRASHED Example (SegFault)\n\n**Code:**\n```python\nimport os\nos._exit(139)  # Simulate SIGSEGV\n```\n\n**Result:**\n```python\nExecutionResult(\n    arg_command='\\nimport os\\ntry:\\n    os._exit(139)\\nexcept Exception as e:\\n...',\n    arg_globals={'a': 123, 'b': [1, 2, 3]},  # Original, not filtered\n    arg_timeout=3,\n    exit_status=ExecutionStatus.CRASHED,\n    exit_code=139,  # SIGSEGV = 128 + 11\n    exception_repr=None,  # No exception caught - process died\n    exception_type=None,\n    exception_value=None,\n    exception_traceback=None,\n    ret_stdout='',\n    ret_tool2llm='## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\\n...'\n)\n```\n\n**Sources:** [tests/playground/subprocess_output.py:215-234](), [tests/playground/subprocess_output.py:451-468]()\n\n### CRASHED Example (RecursionError with PickleError)\n\nWhen a RecursionError occurs and the resulting ExecutionResult cannot be pickled for IPC, the subprocess crashes during the send operation:\n\n**Code:**\n```python\ndef recursive_crash(depth=0):\n    if depth%100==0:\n        print(f\"å½åéå½æ·±åº¦ï¼{depth}\")\n    recursive_crash(depth + 1)\n\ntry:\n    recursive_crash()\nexcept RecursionError as e:\n    print(f\"\\nå´©æºåå ï¼{e}\")\n    raise e\n```\n\n**Result:**\n```python\nExecutionResult(\n    arg_command='...',\n    arg_globals={'a': 123, 'b': [1, 2, 3]},\n    arg_timeout=3,\n    exit_status=ExecutionStatus.CRASHED,\n    exit_code=1,  # Generic error exit\n    exception_repr=None,\n    exception_type=None,\n    exception_value=None,\n    exception_traceback=None,\n    ret_stdout='å½åéå½æ·±åº¦ï¼0\\n...\\n'\n                'å´©æºåå ï¼maximum recursion depth exceeded\\n'\n                'Process Process-5:\\n'\n                'Traceback (most recent call last):\\n'\n                '  ...\\n'\n                \"_pickle.PicklingError: Can't pickle <function recursive_crash at 0x...>\\n\",\n    ret_tool2llm='## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\\n...'\n)\n```\n\nThis scenario demonstrates a unique case where:\n1. The subprocess catches the RecursionError in the except block\n2. Creates an ExecutionResult with FAILURE status\n3. Attempts to send the result via pipe\n4. Pickling fails because `recursive_crash` function is in arg_globals\n5. Subprocess crashes with exit code 1 during the send operation\n6. Parent receives no result and classifies as CRASHED\n7. The pickle error appears in ret_stdout (subprocess stderr)\n\n**Sources:** [tests/playground/subprocess_output.py:235-259](), [tests/playground/subprocess_output.py:492-593]()\n\n---\n\n## Integration with Execution Strategies\n\nAll three execution strategies (subprocess, subthread, direct) return ExecutionResult objects, though their status determination capabilities differ:\n\n| Capability | Subprocess | Subthread | Direct |\n|------------|-----------|-----------|--------|\n| SUCCESS detection | â Full | â Full | â Full |\n| FAILURE detection | â Full | â Full | â Full |\n| TIMEOUT detection | â Full | â Limited | â Limited |\n| CRASHED detection | â Full | â None | â None |\n| Process isolation | â Complete | Partial | None |\n| Exit code capture | â Yes | â No | â No |\n\nOnly subprocess execution provides true CRASHED status detection because it:\n- Runs in a separate process with independent memory\n- Can be forcibly terminated without affecting parent\n- Provides exit codes for signal-based termination\n- Detects when no result is sent (process died before completing)\n\n**Sources:** [src/runtime/subprocess_python_executor.py:76-163](), [Diagram 3 from high-level architecture]()\n\n---\n\n# Page: Workspace State Management\n\n# Workspace State Management\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [docs/error correction.design.md](docs/error correction.design.md)\n- [docs/log.md](docs/log.md)\n- [src/agent/tool/python_tool.py](src/agent/tool/python_tool.py)\n- [src/agent/tool/todo_tool.py](src/agent/tool/todo_tool.py)\n- [src/runtime/workspace.py](src/runtime/workspace.py)\n- [src/utils/__pycache__/__init__.cpython-312.pyc](src/utils/__pycache__/__init__.cpython-312.pyc)\n\n</details>\n\n\n\nThis page documents the workspace state management system responsible for persisting Python global variables across multiple code executions. This enables stateful code execution similar to Jupyter notebooks, where variables defined in one execution remain available in subsequent executions.\n\nFor information about how code is actually executed in different isolation modes, see [Subprocess Execution](#5.1), [Subthread Execution](#5.2), and [Direct Execution](#5.3). For details on execution results and status codes, see [ExecutionResult and Status Handling](#5.4).\n\n---\n\n## Purpose and Scope\n\nThe workspace state management system solves a critical challenge: maintaining Python variable state across multiple isolated code executions. When the agent executes Python code snippets through the `ExecutePythonCodeTool`, each execution needs access to variables and objects created in previous executions, while ensuring only serializable, safe objects are persisted.\n\nThe system handles:\n- **State persistence**: Variables defined in execution N are available in execution N+1\n- **Serialization filtering**: Exclusion of non-picklable objects (modules, builtins, file handles, etc.)\n- **Deep copying**: Isolation of state to prevent unintended mutations\n- **Workspace initialization**: Setting up clean execution environments\n\nSources: [src/runtime/workspace.py:1-108]()\n\n---\n\n## Architecture Overview\n\nThe workspace system uses a **two-list architecture** where state flows from input â execution â output in a linear chain:\n\n```mermaid\ngraph TB\n    subgraph \"State Storage\"\n        ARG[\"arg_globals_list<br/>(list[dict])\"]\n        OUT[\"out_globals_list<br/>(list[dict])\"]\n    end\n    \n    subgraph \"State Processing\"\n        INIT[\"initialize_workspace()\"]\n        FILTER[\"filter_and_deepcopy_globals()\"]\n        GET[\"get_arg_globals()\"]\n        APPEND[\"append_out_globals()\"]\n    end\n    \n    subgraph \"Execution Flow\"\n        EXEC1[\"Execution 1\"]\n        EXEC2[\"Execution 2\"]\n        EXEC3[\"Execution 3\"]\n    end\n    \n    INIT -->|\"Creates initial<br/>workspace dict\"| FILTER\n    FILTER -->|\"Filters & copies\"| GET\n    GET -->|\"Provides input<br/>globals\"| EXEC1\n    \n    EXEC1 -->|\"Returns modified<br/>globals\"| APPEND\n    APPEND -->|\"Stores in<br/>out_globals_list\"| OUT\n    \n    OUT -->|\"Becomes input<br/>for next execution\"| GET\n    GET --> EXEC2\n    EXEC2 --> APPEND\n    \n    OUT --> GET\n    GET --> EXEC3\n    \n    ARG -.->|\"Tracks input<br/>state history\"| GET\n    OUT -.->|\"Tracks output<br/>state history\"| APPEND\n```\n\n**Key insight**: `arg_globals_list` stores the input state for each execution, while `out_globals_list` stores the output state. The output of execution N becomes the input of execution N+1 after filtering and deep copying.\n\nSources: [src/runtime/workspace.py:10-11](), [src/runtime/workspace.py:81-98]()\n\n---\n\n## Global State Lists\n\nThe workspace module maintains two module-level lists that form the backbone of state persistence:\n\n| Variable | Type | Purpose |\n|----------|------|---------|\n| `arg_globals_list` | `list[dict]` | Stores the **input** globals dictionary for each execution |\n| `out_globals_list` | `list[dict]` | Stores the **output** globals dictionary after each execution |\n\n**State Flow Pattern**:\n```\nExecution 1:  arg_globals_list[0] â execute â out_globals_list[0]\nExecution 2:  arg_globals_list[1] (= filtered out_globals_list[0]) â execute â out_globals_list[1]\nExecution 3:  arg_globals_list[2] (= filtered out_globals_list[1]) â execute â out_globals_list[2]\n```\n\nThis architecture provides:\n- **Version history**: Both lists grow with each execution, creating an audit trail\n- **Rollback capability**: Previous states remain accessible in the lists\n- **State isolation**: Deep copying prevents accidental mutations between executions\n\nSources: [src/runtime/workspace.py:10-11]()\n\n---\n\n## Workspace Initialization\n\nA fresh workspace dictionary is created using the `initialize_workspace()` function:\n\n```mermaid\ngraph LR\n    CREATE[\"__create_workspace()\"] --> EXEC[\"exec('', workspace)\"]\n    EXEC --> WORKSPACE[\"workspace dict<br/>(with builtins)\"]\n    WORKSPACE --> UPDATE[\"workspace.update()<br/>{'__name__': '__main__'}\"]\n    UPDATE --> RETURN[\"return workspace\"]\n```\n\n**Implementation Details**:\n- [src/runtime/workspace.py:14-17]() - `__create_workspace()` executes an empty string in a new dictionary, which populates it with Python builtins\n- [src/runtime/workspace.py:20-23]() - `initialize_workspace()` adds `__name__ = '__main__'` to make the workspace behave like a main module\n- The workspace dictionary mirrors the `globals()` dictionary you would find in a normal Python script\n\n**Why `exec(\"\")` is used**: Executing an empty string in a dictionary automatically populates it with the correct builtins, providing a properly initialized execution environment.\n\nSources: [src/runtime/workspace.py:14-23]()\n\n---\n\n## State Filtering and Serialization\n\nThe `filter_and_deepcopy_globals()` function is the most critical component, ensuring only safe, serializable objects persist between executions:\n\n```mermaid\ngraph TB\n    INPUT[\"Input: original_globals<br/>(dict)\"]\n    \n    INPUT --> LOOP[\"For each key, value<br/>in globals\"]\n    \n    LOOP --> CHECK1{\"key ==<br/>'__builtins__'?\"}\n    CHECK1 -->|Yes| SKIP1[\"Continue<br/>(skip this item)\"]\n    CHECK1 -->|No| CHECK2\n    \n    CHECK2{\"value is<br/>module type?\"}\n    CHECK2 -->|Yes| SKIP2[\"Continue<br/>(skip this item)\"]\n    CHECK2 -->|No| CHECK3\n    \n    CHECK3{\"_is_serializable()<br/>returns True?\"}\n    CHECK3 -->|No| SKIP3[\"Continue<br/>(skip this item)\"]\n    CHECK3 -->|Yes| DEEPCOPY\n    \n    DEEPCOPY[\"copy.deepcopy(value)\"] --> ADD[\"Add to<br/>filtered_dict\"]\n    \n    ADD --> LOOP\n    SKIP1 --> LOOP\n    SKIP2 --> LOOP\n    SKIP3 --> LOOP\n    \n    LOOP -->|Done| RETURN[\"Return filtered_dict\"]\n```\n\n### Serialization Checking\n\nThe internal `_is_serializable()` helper function [src/runtime/workspace.py:45-64]() tests if an object can be pickled:\n\n```python\ndef _is_serializable(value) -> bool:\n    import pickle\n    try:\n        pickle.dumps(value, protocol=pickle.HIGHEST_PROTOCOL)\n        return True\n    except (pickle.PicklingError, TypeError, AttributeError, \n            RecursionError, MemoryError):\n        return False\n```\n\n**Objects excluded from persistence**:\n- Built-in modules (`__builtins__`)\n- Imported modules (e.g., `import numpy` â `numpy` module object)\n- Non-picklable objects: file handles, thread locks, socket connections, lambda functions with closures\n- Objects causing `RecursionError` or `MemoryError` during serialization\n\n**Why deep copy?** Prevents mutations in one execution from affecting the stored state. Each execution gets an isolated copy of the previous state.\n\nSources: [src/runtime/workspace.py:38-78]()\n\n---\n\n## State Retrieval Process\n\nThe `get_arg_globals()` function [src/runtime/workspace.py:81-91]() provides the input globals for each execution:\n\n### First Execution\n```mermaid\ngraph LR\n    CHECK[\"arg_globals_list<br/>empty?\"]\n    CHECK -->|Yes| INIT[\"initialize_workspace()\"]\n    INIT --> FILTER[\"filter_and_deepcopy_globals()\"]\n    FILTER --> APPEND[\"arg_globals_list.append()\"]\n    APPEND --> RETURN[\"return filter_arg_globals\"]\n```\n\n### Subsequent Executions\n```mermaid\ngraph LR\n    CHECK[\"arg_globals_list<br/>not empty?\"]\n    CHECK -->|Yes| GET[\"arg_globals =<br/>out_globals_list[-1]\"]\n    GET --> FILTER[\"filter_and_deepcopy_globals()\"]\n    FILTER --> APPEND[\"arg_globals_list.append()\"]\n    APPEND --> RETURN[\"return filter_arg_globals\"]\n```\n\n**Key Logic**:\n- **Line 83-86**: If `arg_globals_list` is empty (first execution), initialize a fresh workspace\n- **Line 88-90**: Otherwise, take the **last output state** from `out_globals_list[-1]` as the new input\n- Both paths apply filtering and deep copying before appending to `arg_globals_list`\n\nThis creates a **state chain**: each execution's output becomes the next execution's input.\n\nSources: [src/runtime/workspace.py:81-91]()\n\n---\n\n## State Persistence Process\n\nAfter each code execution completes, `append_out_globals()` [src/runtime/workspace.py:94-97]() stores the output state:\n\n```mermaid\ngraph LR\n    EXEC[\"Code execution<br/>completes\"]\n    EXEC --> RESULT[\"ExecutionResult<br/>with arg_globals\"]\n    RESULT --> FILTER[\"filter_and_deepcopy_globals()\"]\n    FILTER --> APPEND[\"out_globals_list.append()\"]\n```\n\n**Implementation**:\n```python\ndef append_out_globals(out_globals: dict):\n    global out_globals_list\n    filter_out_globals = filter_and_deepcopy_globals(out_globals)\n    out_globals_list.append(filter_out_globals)\n```\n\n**Critical behavior**: The output globals are **filtered and deep copied** before storage, ensuring:\n- Non-serializable objects from the execution are excluded\n- The stored state is isolated from future modifications\n- State consistency is maintained\n\nSources: [src/runtime/workspace.py:94-97]()\n\n---\n\n## Serialization Rules and Exclusions\n\nThe following table summarizes what persists and what doesn't:\n\n| Object Type | Persists? | Reason |\n|-------------|-----------|--------|\n| Primitive values (`int`, `str`, `float`, `bool`) | â Yes | Picklable |\n| Lists, tuples, sets | â Yes | Picklable if contents are |\n| Dictionaries | â Yes | Picklable if contents are |\n| Custom class instances | â Yes | If class is picklable |\n| Functions (simple `def`) | â Yes | Picklable |\n| Pydantic models | â Yes | Picklable |\n| NumPy arrays | â Yes | Picklable |\n| Pandas DataFrames | â Yes | Picklable |\n| `__builtins__` | â No | Explicitly excluded |\n| Imported modules | â No | Not picklable |\n| Lambda functions | â Maybe | Depends on closure |\n| File handles | â No | Not picklable |\n| Thread objects | â No | Not picklable |\n| Socket connections | â No | Not picklable |\n\n**Common Pattern**: **Always re-import modules** at the start of each code snippet, as they won't persist. Example from the tool description [src/agent/tool/python_tool.py:25]():\n> \"è½ç¶åéä¼æç»­å­å¨ï¼ä½ç±äºåºååéå¶ï¼å¯¼å¥çæ¨¡åï¼å¦ `math`ã`json`ï¼å¯è½ä¸ä¼å¨æ¯æ¬¡è°ç¨ä¸­æç»­å­å¨ã**å¨æ¯ä¸ªä»£ç çæ®µä¸­å§ç»éæ°å¯¼å¥å¿è¦çæ¨¡å**ã\"\n\nSources: [src/runtime/workspace.py:38-78](), [src/agent/tool/python_tool.py:25-26]()\n\n---\n\n## Integration with Execution System\n\nThe workspace system integrates with the Python execution tool through two key calls:\n\n```mermaid\nsequenceDiagram\n    participant Tool as \"ExecutePythonCodeTool\"\n    participant WS as \"workspace module\"\n    participant Executor as \"subthread_python_executor\"\n    participant Lists as \"arg_globals_list<br/>out_globals_list\"\n    \n    Tool->>WS: \"get_arg_globals()\"\n    WS->>Lists: \"Check arg_globals_list\"\n    alt \"First execution\"\n        WS->>WS: \"initialize_workspace()\"\n    else \"Subsequent execution\"\n        Lists->>WS: \"out_globals_list[-1]\"\n    end\n    WS->>WS: \"filter_and_deepcopy_globals()\"\n    WS->>Lists: \"append to arg_globals_list\"\n    WS-->>Tool: \"return execution_context\"\n    \n    Tool->>Executor: \"run_structured_in_thread(<br/>code, _globals=execution_context)\"\n    Executor-->>Tool: \"ExecutionResult<br/>with arg_globals\"\n    \n    Tool->>WS: \"append_out_globals(<br/>exec_result.arg_globals)\"\n    WS->>WS: \"filter_and_deepcopy_globals()\"\n    WS->>Lists: \"append to out_globals_list\"\n```\n\n**Code Integration Points**:\n\n1. **Before execution** [src/agent/tool/python_tool.py:42]():\n   ```python\n   execution_context: Optional[Dict[str, Any]] = workspace.get_arg_globals()\n   ```\n   Retrieves the globals dictionary to pass to the executor.\n\n2. **After execution** [src/agent/tool/python_tool.py:49]():\n   ```python\n   workspace.append_out_globals(exec_result.arg_globals)\n   ```\n   Stores the modified globals dictionary for the next execution.\n\n3. **Executor integration** [src/agent/tool/python_tool.py:44-48]():\n   The execution context is passed to `run_structured_in_thread()` as the `_globals` parameter, which becomes the globals dictionary for the `exec()` call.\n\nSources: [src/agent/tool/python_tool.py:41-50]()\n\n---\n\n## Workspace Utility Functions\n\nAdditional helper functions provide introspection into workspace state:\n\n### `get_workspace_globals_dict()`\n[src/runtime/workspace.py:26-29]() - Extracts key-value pairs from a workspace dictionary:\n```python\ndef get_workspace_globals_dict(workspace: dict, include_special_vars: bool = False):\n    if include_special_vars:\n        return {k: v for k, v in workspace.items()}\n    return {k: v for k, v in workspace.items() if not k.startswith('__')}\n```\n\n### `get_workspace_globals_keys()`\n[src/runtime/workspace.py:32-35]() - Extracts only the keys from a workspace dictionary:\n```python\ndef get_workspace_globals_keys(workspace: dict, include_special_vars: bool = False):\n    if include_special_vars:\n        return [k for k in workspace.keys()]\n    return [k for k in workspace.keys() if not k.startswith('__')]\n```\n\n**Use case**: Debugging and inspecting what variables are currently in the workspace.\n\nSources: [src/runtime/workspace.py:26-35]()\n\n---\n\n## Complete State Flow Example\n\nHere's a concrete example of how state flows through three executions:\n\n```mermaid\ngraph TB\n    subgraph \"Execution 1: Define variable\"\n        E1_INPUT[\"Input globals: {}<br/>(empty, first execution)\"]\n        E1_CODE[\"Code: x = 10\"]\n        E1_OUTPUT[\"Output globals:<br/>{x: 10}\"]\n        E1_INPUT --> E1_CODE --> E1_OUTPUT\n    end\n    \n    subgraph \"Execution 2: Use and modify\"\n        E2_INPUT[\"Input globals:<br/>{x: 10}<br/>(from E1 output)\"]\n        E2_CODE[\"Code: y = x * 2\"]\n        E2_OUTPUT[\"Output globals:<br/>{x: 10, y: 20}\"]\n        E2_INPUT --> E2_CODE --> E2_OUTPUT\n    end\n    \n    subgraph \"Execution 3: Import filtered out\"\n        E3_INPUT[\"Input globals:<br/>{x: 10, y: 20}<br/>(from E2 output)\"]\n        E3_CODE[\"Code: import math<br/>z = math.sqrt(y)\"]\n        E3_OUTPUT[\"Output globals:<br/>{x: 10, y: 20, z: 4.47...}<br/>(math module excluded)\"]\n        E3_INPUT --> E3_CODE --> E3_OUTPUT\n    end\n    \n    E1_OUTPUT -.->|\"filter & deepcopy\"| E2_INPUT\n    E2_OUTPUT -.->|\"filter & deepcopy\"| E3_INPUT\n```\n\n**State Lists After Execution 3**:\n```\narg_globals_list = [\n    {},                          # E1 input\n    {x: 10},                     # E2 input\n    {x: 10, y: 20}               # E3 input\n]\n\nout_globals_list = [\n    {x: 10},                     # E1 output\n    {x: 10, y: 20},              # E2 output\n    {x: 10, y: 20, z: 4.47...}   # E3 output (math filtered out)\n]\n```\n\n**Note**: The `math` module imported in Execution 3 is **not** persisted to `out_globals_list` because it's filtered out by `filter_and_deepcopy_globals()`. If Execution 4 needs `math`, it must re-import it.\n\nSources: [src/runtime/workspace.py:38-97]()\n\n---\n\n## State Management Best Practices\n\nBased on the system design, follow these guidelines when using the workspace:\n\n1. **Always re-import modules**: Modules are filtered out and won't persist between executions.\n   \n2. **Use simple, picklable data structures**: Complex objects with non-picklable components (file handles, connections) will be silently excluded.\n\n3. **Be aware of state accumulation**: Variables persist indefinitely. Consider explicitly deleting variables when no longer needed:\n   ```python\n   del large_dataframe  # Free memory\n   ```\n\n4. **Check variable existence**: Since state persists, check if variables already exist before redefining:\n   ```python\n   if 'config' not in globals():\n       config = load_config()\n   ```\n\n5. **Debugging serialization issues**: If a variable isn't persisting, it likely failed the `_is_serializable()` check. Look for:\n   - Embedded file handles\n   - Thread locks or queues\n   - Lambda functions with complex closures\n   - Objects with `__getstate__` methods that fail\n\nSources: [src/agent/tool/python_tool.py:25-26](), [src/runtime/workspace.py:45-64]()\n\n---\n\n# Page: Working Directory and Environment\n\n# Working Directory and Environment\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [src/runtime/before_thread/plt_back_chinese.py](src/runtime/before_thread/plt_back_chinese.py)\n- [src/runtime/cwd.py](src/runtime/cwd.py)\n- [src/runtime/subprocess_python_executor.py](src/runtime/subprocess_python_executor.py)\n- [src/runtime/subthread_python_executor.py](src/runtime/subthread_python_executor.py)\n- [tests/playground/gen/g9/beijing_scenic_spot_schema_with_play_hours.json](tests/playground/gen/g9/beijing_scenic_spot_schema_with_play_hours.json)\n- [tests/playground/gen/g9/beijing_scenic_spot_validated_data_with_play_hours.json](tests/playground/gen/g9/beijing_scenic_spot_validated_data_with_play_hours.json)\n- [tests/playground/gen/g9/g9.py](tests/playground/gen/g9/g9.py)\n\n</details>\n\n\n\nThis page covers how the execution runtime manages working directories, redirects standard output streams, and configures the execution environment for isolated code execution. These mechanisms ensure that Python code snippets executed by the agent run in controlled, isolated environments with proper output capture.\n\nFor information about the overall execution strategies (subprocess, subthread, direct), see [Execution Runtime](#5). For details on workspace state management and global variable persistence, see [Workspace State Management](#5.5).\n\n---\n\n## Purpose and Scope\n\nThe working directory and environment management system provides:\n\n- **Working directory isolation**: Each execution can run in a dedicated directory to prevent file system conflicts\n- **Output stream capture**: stdout and stderr are redirected to capture execution output\n- **Environment configuration**: Execution-specific settings (e.g., matplotlib backend) are applied before code execution\n- **Cross-platform compatibility**: Utilities work consistently across subprocess and subthread execution modes\n\n---\n\n## Working Directory Management\n\n### Workspace Directory Structure\n\nThe system uses a workspace directory structure under `wsm/` (workspace management) to isolate execution environments. The typical structure is:\n\n```\nwsm/\nâââ 1/\nâ   âââ g4-1/      # Workspace for specific execution context\nâââ 2/\nâ   âââ g7-2/      # Another isolated workspace\nâââ 3/\nâ   âââ g8-1/\nâââ 4/\n    âââ g9-1/\n```\n\nThis structure is ignored by git via [.gitignore:213]() which excludes `/ws*/` patterns.\n\n**Sources:** [.gitignore:213](), [src/runtime/subprocess_python_executor.py:28]()\n\n### The create_cwd Function\n\nThe `create_cwd` function initializes a working directory for subprocess execution. It creates the directory if it doesn't exist and changes the process's current working directory.\n\n```mermaid\ngraph LR\n    A[\"Subprocess<br/>Launch\"] --> B[\"create_cwd\"]\n    B --> C[\"get_or_create_subfolder\"]\n    C --> D{\"Directory<br/>Exists?\"}\n    D -->|No| E[\"Create Directory\"]\n    D -->|Yes| F[\"os.chdir\"]\n    E --> F\n    F --> G[\"Log New CWD\"]\n    G --> H[\"Execute Code\"]\n```\n\n**Function Signature:**\n\n```python\ndef create_cwd(cwd=None) -> bool\n```\n\n**Implementation Details:**\n\n- **Location**: [src/runtime/cwd.py:9-28]()\n- **Parameters**: `cwd` - relative path from project root (e.g., `'./wsm/2/g7-2'`)\n- **Returns**: `True` if successful, `False` on error\n- **Logging**: Records PID, target directory, and success/failure status\n\nThe function is called in subprocess execution at [src/runtime/subprocess_python_executor.py:28]():\n\n```python\ncwd.create_cwd('./wsm/2/g7-2')\n```\n\n**Error Handling:**\n\n| Error Type | Handling |\n|------------|----------|\n| `OSError` | Logged with PID and error message, returns `False` |\n| `Exception` | Generic exception logged, returns `False` |\n\n**Sources:** [src/runtime/cwd.py:9-28](), [src/runtime/subprocess_python_executor.py:27-28]()\n\n### The ChangeDirectory Context Manager\n\nThe `ChangeDirectory` context manager provides automatic directory restoration after execution, ensuring the working directory is reset even if exceptions occur.\n\n```mermaid\ngraph TB\n    subgraph \"ChangeDirectory Context Manager\"\n        A[\"__enter__\"] --> B[\"Save Current Directory<br/>self.original_dir\"]\n        B --> C[\"Create Target Directory<br/>get_or_create_subfolder\"]\n        C --> D[\"os.chdir(target_dir)\"]\n        D --> E[\"Log Switch\"]\n        E --> F[\"Execute User Code\"]\n        F --> G[\"__exit__\"]\n        G --> H[\"os.chdir(original_dir)\"]\n        H --> I[\"Log Restoration\"]\n    end\n    \n    J[\"Exception?\"] -->|Yes| G\n    J -->|No| G\n    G --> K[\"Propagate Exception\"]\n```\n\n**Class Definition:**\n\n```python\nclass ChangeDirectory:\n    def __init__(self, target_dir)\n    def __enter__(self)\n    def __exit__(self, exc_type, exc_val, exc_tb)\n```\n\n**Implementation**: [src/runtime/cwd.py:31-54]()\n\n**Usage in Subthread Executor:**\n\nThe context manager is used in subthread execution at [src/runtime/subthread_python_executor.py:35-37]():\n\n```python\nwith cwd.ChangeDirectory('./wsm/4/g9-1'):\n    with cwd.Change_STDOUT_STDERR(_BufferWriter(stdout_buffer)):\n        exec(command, _globals, _locals)\n```\n\n**Usage in Test Files:**\n\nExample from [tests/playground/gen/g9/g9.py:1-3]():\n\n```python\nfrom src.runtime import cwd\ncwd.create_cwd('./tests/playground/gen/g9')\n```\n\n**Sources:** [src/runtime/cwd.py:31-54](), [src/runtime/subthread_python_executor.py:35-37](), [tests/playground/gen/g9/g9.py:1-3]()\n\n---\n\n## Output Stream Redirection\n\n### The Change_STDOUT_STDERR Context Manager\n\nThe `Change_STDOUT_STDERR` context manager redirects `sys.stdout` and `sys.stderr` to custom writers, enabling output capture during code execution.\n\n```mermaid\ngraph LR\n    A[\"Original<br/>sys.stdout\"] --> B[\"Change_STDOUT_STDERR<br/>__enter__\"]\n    B --> C[\"sys.stdout = custom_writer\"]\n    B --> D[\"sys.stderr = custom_writer\"]\n    C --> E[\"exec(command)\"]\n    D --> E\n    E --> F[\"Output Captured<br/>in Buffer/Pipe\"]\n    F --> G[\"__exit__\"]\n    G --> H[\"Restore Original<br/>sys.stdout\"]\n    G --> I[\"Restore Original<br/>sys.stderr\"]\n```\n\n**Class Definition:**\n\n```python\nclass Change_STDOUT_STDERR:\n    def __init__(self, new_stdout, new_stderr=None)\n    def __enter__(self)\n    def __exit__(self, exc_type, exc_val, exc_tb)\n```\n\n**Implementation**: [src/runtime/cwd.py:56-77]()\n\n**Key Features:**\n\n- Saves original stdout/stderr in `__enter__`\n- Redirects both streams to custom writer (default: stderr uses same as stdout if not specified)\n- Restores original streams in `__exit__` regardless of exceptions\n- Returns `False` from `__exit__` to propagate exceptions\n\n**Sources:** [src/runtime/cwd.py:56-77]()\n\n### Subprocess Output Capture\n\nIn subprocess execution, a custom `_PipeWriter` class redirects output to the parent process via multiprocessing pipes.\n\n```mermaid\ngraph TB\n    subgraph \"Subprocess\"\n        A[\"sys.stdout = _PipeWriter\"] --> B[\"write(msg)\"]\n        B --> C[\"child_conn.send<br/>(_PipeType.STDOUT, msg)\"]\n    end\n    \n    subgraph \"Parent Process\"\n        D[\"_reader Thread\"] --> E[\"parent_conn.recv()\"]\n        E --> F{\"Message Type\"}\n        F -->|STDOUT| G[\"Append to<br/>subprocess_stdout_buffer\"]\n        F -->|RESULT| H[\"Append to<br/>subprocess_result_container\"]\n    end\n    \n    C --> E\n```\n\n**_PipeWriter Implementation**: [src/runtime/subprocess_python_executor.py:30-45]()\n\n```python\nclass _PipeWriter:\n    def __init__(self, child_conn: PipeConnection):\n        self.child_conn = child_conn\n    \n    def write(self, msg: str):\n        if msg:\n            self.child_conn.send((_PipeType.STDOUT, msg))\n    \n    def flush(self):\n        pass\n```\n\nThe writer is activated at [src/runtime/subprocess_python_executor.py:47-48]():\n\n```python\nsys.stdout = _PipeWriter(child_conn)\nsys.stderr = sys.stdout\n```\n\n**Output Collection:**\n\nThe parent process reads messages in a dedicated thread [src/runtime/subprocess_python_executor.py:88-112]() and stores them in `subprocess_stdout_buffer`. After execution completes, the buffer is joined into the final result [src/runtime/subprocess_python_executor.py:161]():\n\n```python\nfinal_res.ret_stdout = \"\".join(subprocess_stdout_buffer)\n```\n\n**Sources:** [src/runtime/subprocess_python_executor.py:30-48](), [src/runtime/subprocess_python_executor.py:88-112](), [src/runtime/subprocess_python_executor.py:161]()\n\n### Subthread Output Capture\n\nIn subthread execution, a `_BufferWriter` class appends output to a shared list.\n\n```mermaid\ngraph LR\n    A[\"sys.stdout = _BufferWriter\"] --> B[\"write(msg)\"]\n    B --> C[\"stdout_buffer.append(msg)\"]\n    C --> D[\"Shared Memory<br/>List\"]\n    D --> E[\"Main Thread<br/>Reads Buffer\"]\n    E --> F[\"Join to String\"]\n```\n\n**_BufferWriter Implementation**: [src/runtime/subthread_python_executor.py:23-32]()\n\n```python\nclass _BufferWriter:\n    def __init__(self, buffer: list[str]):\n        self.buffer = buffer\n    \n    def write(self, msg: str):\n        if msg:\n            self.buffer.append(msg)\n    \n    def flush(self):\n        pass\n```\n\nThe writer is used within nested context managers [src/runtime/subthread_python_executor.py:35-37]():\n\n```python\nwith cwd.ChangeDirectory('./wsm/4/g9-1'):\n    with cwd.Change_STDOUT_STDERR(_BufferWriter(stdout_buffer)):\n        exec(command, _globals, _locals)\n```\n\nAfter execution, the buffer is joined [src/runtime/subthread_python_executor.py:126]():\n\n```python\nfinal_res.ret_stdout = \"\".join(stdout_buffer)\n```\n\n**Sources:** [src/runtime/subthread_python_executor.py:23-32](), [src/runtime/subthread_python_executor.py:35-37](), [src/runtime/subthread_python_executor.py:126]()\n\n---\n\n## Environment Setup\n\n### Matplotlib Backend Configuration\n\nWhen executing code in subthreads, matplotlib requires specific configuration to avoid GUI-related errors. The `plt_back_chinese` module performs this initialization.\n\n```mermaid\ngraph TB\n    A[\"Subthread Execution\"] --> B[\"Import before_thread.plt_back_chinese\"]\n    B --> C[\"matplotlib.use('Agg')<br/>Non-interactive Backend\"]\n    C --> D[\"Configure Chinese Fonts<br/>SimHei\"]\n    D --> E[\"Disable Unicode Minus\"]\n    E --> F[\"Execute User Code<br/>with Matplotlib\"]\n```\n\n**Module Location**: [src/runtime/before_thread/plt_back_chinese.py]()\n\n**Implementation**: [src/runtime/before_thread/plt_back_chinese.py:4-12]()\n\n```python\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n# Set non-GUI backend (critical)\nmatplotlib.use(\"Agg\")  \n\n# Configure Chinese font support\nplt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\nplt.rcParams[\"axes.unicode_minus\"] = False\n```\n\n**Purpose:**\n\n| Configuration | Reason |\n|---------------|--------|\n| `matplotlib.use(\"Agg\")` | Prevents GUI initialization errors in subthreads |\n| `font.sans-serif = [\"SimHei\"]` | Enables Chinese character rendering |\n| `axes.unicode_minus = False` | Prevents Unicode minus sign display issues |\n\nThe module is imported at [src/runtime/subthread_python_executor.py:9]() before any user code executes matplotlib commands.\n\n**Logging:**\n\nThe module logs initialization steps at [src/runtime/before_thread/plt_back_chinese.py:3]() and [src/runtime/before_thread/plt_back_chinese.py:13]() to track configuration completion.\n\n**Sources:** [src/runtime/before_thread/plt_back_chinese.py:1-13](), [src/runtime/subthread_python_executor.py:9]()\n\n### Execution Mode-Specific Setup\n\nDifferent execution modes require different environment setups:\n\n```mermaid\ngraph TB\n    A[\"Execution Request\"] --> B{\"Execution Mode\"}\n    \n    B -->|Subprocess| C[\"create_cwd<br/>New Process Space\"]\n    C --> D[\"_PipeWriter<br/>IPC Output\"]\n    D --> E[\"Isolated Environment\"]\n    \n    B -->|Subthread| F[\"ChangeDirectory<br/>Context Manager\"]\n    F --> G[\"Change_STDOUT_STDERR<br/>Buffer Output\"]\n    G --> H[\"plt_back_chinese<br/>Matplotlib Setup\"]\n    H --> I[\"Shared Memory Space\"]\n    \n    B -->|Direct| J[\"No Directory Change\"]\n    J --> K[\"Queue-based Output\"]\n    K --> L[\"Same Process Space\"]\n```\n\n**Subprocess Setup Sequence:**\n\n1. Launch subprocess with `multiprocessing.Process` [src/runtime/subprocess_python_executor.py:114-116]()\n2. Call `create_cwd` to set working directory [src/runtime/subprocess_python_executor.py:28]()\n3. Redirect stdout/stderr to `_PipeWriter` [src/runtime/subprocess_python_executor.py:47-48]()\n4. Execute code with `exec(command, _globals, _locals)` [src/runtime/subprocess_python_executor.py:50]()\n\n**Subthread Setup Sequence:**\n\n1. Launch thread with `threading.Thread` [src/runtime/subthread_python_executor.py:97-100]()\n2. Import matplotlib configuration [src/runtime/subthread_python_executor.py:9]()\n3. Enter `ChangeDirectory` context [src/runtime/subthread_python_executor.py:35]()\n4. Enter `Change_STDOUT_STDERR` context [src/runtime/subthread_python_executor.py:36]()\n5. Execute code with `exec(command, _globals, _locals)` [src/runtime/subthread_python_executor.py:37]()\n\n**Sources:** [src/runtime/subprocess_python_executor.py:27-50](), [src/runtime/subprocess_python_executor.py:114-116](), [src/runtime/subthread_python_executor.py:9](), [src/runtime/subthread_python_executor.py:35-37](), [src/runtime/subthread_python_executor.py:97-100]()\n\n---\n\n## Integration with Executors\n\n### Directory and Environment Flow\n\nThe following diagram shows how directory and environment management integrates with the execution flow:\n\n```mermaid\nsequenceDiagram\n    participant Tool as ExecutePythonCodeTool\n    participant Exec as run_structured_in_*\n    participant Env as Environment Setup\n    participant FS as File System\n    participant Code as exec(command)\n    \n    Tool->>Exec: Execute code snippet\n    \n    alt Subprocess Mode\n        Exec->>Env: create_cwd('./wsm/2/g7-2')\n        Env->>FS: Create/verify directory\n        Env->>FS: os.chdir(directory)\n        Exec->>Env: Create _PipeWriter\n        Env->>Code: sys.stdout = _PipeWriter\n    else Subthread Mode\n        Exec->>Env: Import plt_back_chinese\n        Env->>Env: matplotlib.use('Agg')\n        Exec->>Env: ChangeDirectory.__enter__()\n        Env->>FS: os.chdir(target_dir)\n        Exec->>Env: Change_STDOUT_STDERR.__enter__()\n        Env->>Code: sys.stdout = _BufferWriter\n    end\n    \n    Exec->>Code: exec(command, globals, locals)\n    Code-->>Env: Output generated\n    Env-->>Exec: Captured output\n    \n    alt Subprocess Mode\n        Exec->>Env: Close pipes\n    else Subthread Mode\n        Exec->>Env: Change_STDOUT_STDERR.__exit__()\n        Env->>Env: Restore sys.stdout/stderr\n        Exec->>Env: ChangeDirectory.__exit__()\n        Env->>FS: os.chdir(original_dir)\n    end\n    \n    Exec-->>Tool: ExecutionResult with output\n```\n\n**Sources:** [src/runtime/subprocess_python_executor.py:76-163](), [src/runtime/subthread_python_executor.py:87-128]()\n\n### Output Capture Comparison\n\n| Aspect | Subprocess | Subthread |\n|--------|-----------|-----------|\n| **Directory Setup** | `create_cwd()` function | `ChangeDirectory` context manager |\n| **Stdout Redirect** | `_PipeWriter` class | `_BufferWriter` class |\n| **Communication** | Pipe-based IPC | Shared memory list |\n| **Restoration** | Not needed (isolated process) | Automatic via `__exit__` |\n| **Matplotlib Setup** | Not needed (separate process) | `plt_back_chinese` module required |\n\n**Sources:** [src/runtime/subprocess_python_executor.py:30-48](), [src/runtime/subthread_python_executor.py:23-37]()\n\n---\n\n## Example Usage Patterns\n\n### Pattern 1: Subprocess Execution with Directory Isolation\n\nFrom [src/runtime/subprocess_python_executor.py:165-204]():\n\n```python\ntest_code = \"\"\"\nimport os\nprint(os.getcwd())\n\"\"\"\nresult = run_structured_in_subprocess(test_code, {}, timeout=600)\n```\n\nThe working directory will be `./wsm/2/g7-2` as set in [src/runtime/subprocess_python_executor.py:28]().\n\n### Pattern 2: Subthread Execution with Context Managers\n\nFrom [src/runtime/subthread_python_executor.py:171-183]():\n\n```python\nplt_code = \"\"\"\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(4, 3))\nplt.title(\"æµè¯ä¸­ææ¾ç¤º\")\nplt.plot([1, 2, 3], [1, 4, 9])\nplt.savefig(\"test_output.png\")\nplt.close()\nprint(\"å­çº¿ç¨ç»å¾å®æå¹¶æåä¿å­ test_output.png\")\n\"\"\"\nresult = run_structured_in_thread(plt_code, {}, timeout=10)\n```\n\nThis uses matplotlib with proper backend setup from `plt_back_chinese` and directory isolation via `ChangeDirectory`.\n\n### Pattern 3: Direct Directory Setup in Application Code\n\nFrom [tests/playground/gen/g9/g9.py:1-3]():\n\n```python\nfrom src.runtime import cwd\nimport matplotlib.pyplot as plt\ncwd.create_cwd('./tests/playground/gen/g9')\n```\n\nThis pattern is used in test scripts that need consistent working directories for file I/O operations.\n\n**Sources:** [src/runtime/subprocess_python_executor.py:165-204](), [src/runtime/subthread_python_executor.py:171-183](), [tests/playground/gen/g9/g9.py:1-3]()\n\n---\n\n## Summary\n\nThe working directory and environment management system provides:\n\n1. **Directory Isolation**: Via `create_cwd()` for subprocesses and `ChangeDirectory` for subthreads\n2. **Output Capture**: Via `_PipeWriter` for subprocesses and `_BufferWriter` for subthreads\n3. **Stream Management**: Via `Change_STDOUT_STDERR` context manager for automatic restoration\n4. **Environment Configuration**: Via `plt_back_chinese` for matplotlib in subthreads\n5. **Error Recovery**: Context managers ensure cleanup even on exceptions\n\nThese utilities work together to create isolated, reproducible execution environments that capture all output while maintaining system stability.\n\n**Sources:** [src/runtime/cwd.py](), [src/runtime/subprocess_python_executor.py](), [src/runtime/subthread_python_executor.py](), [src/runtime/before_thread/plt_back_chinese.py]()\n\n---\n\n# Page: Observability and Logging\n\n# Observability and Logging\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/utils/__pycache__/log_decorator.cpython-312.pyc](src/utils/__pycache__/log_decorator.cpython-312.pyc)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document describes the comprehensive logging and observability infrastructure in the algo_agent system. The logging system provides automatic instrumentation of function calls, execution timing, parameter tracking, exception handling, and multi-level logging capabilities. This infrastructure enables debugging, performance analysis, error detection, and system behavior reconstruction.\n\nFor information about the agent execution flow that produces these logs, see [Agent Orchestration](#3). For details about execution runtime error handling, see [Execution Runtime](#5).\n\n---\n\n## System Overview\n\nThe logging system is built around a decorator-based architecture that automatically instruments functions throughout the codebase. The system captures function calls, parameters, return values, execution times, and exceptions with minimal code intrusion. Multiple log files segregate information by purpose, and all logs use UTF-8 encoding with automatic directory management.\n\n**Key Capabilities:**\n- Automatic function call instrumentation via decorators\n- Multi-level logging hierarchy (all â trace â print)\n- Execution timing and performance tracking\n- Full stack trace capture on exceptions\n- Parameter and return value formatting\n- Rotating file handlers with automatic backup management\n\nSources: [src/utils/log_decorator.py:1-322]()\n\n---\n\n## Logging Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Application Layer\"\n        AGENT[\"user_query()<br/>deep_research.py\"]\n        TOOLS[\"Tool Functions<br/>python_tool.py<br/>todo_tool.py\"]\n        RUNTIME[\"Execution Functions<br/>subprocess_python_executor.py\"]\n        LLM[\"LLM Functions<br/>llm.py\"]\n    end\n    \n    subgraph \"Decorator Layer\"\n        TRACEABLE[\"@traceable<br/>Detailed function tracing\"]\n        LOGGER[\"global_logger<br/>User-facing logs\"]\n    end\n    \n    subgraph \"Logger Configuration\"\n        SETUP[\"setup_logger()<br/>- Creates logger instances<br/>- Configures handlers<br/>- Sets formatters\"]\n        LOGFUNC[\"log_function()<br/>Decorator factory<br/>- Captures args/kwargs<br/>- Times execution<br/>- Handles exceptions\"]\n    end\n    \n    subgraph \"Handler Layer\"\n        CONSOLE[\"ConsoleHandler<br/>stdout output\"]\n        FILE[\"RotatingFileHandler<br/>10MB max, 5 backups\"]\n    end\n    \n    subgraph \"Log Files\"\n        ALL[\"logs/all.log<br/>All system logs\"]\n        TRACE[\"logs/trace.log<br/>Detailed traces\"]\n        PRINT[\"logs/print.log<br/>User operations\"]\n    end\n    \n    AGENT --> TRACEABLE\n    AGENT --> LOGGER\n    TOOLS --> TRACEABLE\n    RUNTIME --> TRACEABLE\n    LLM --> TRACEABLE\n    \n    TRACEABLE --> LOGFUNC\n    LOGGER --> SETUP\n    LOGFUNC --> SETUP\n    \n    SETUP --> CONSOLE\n    SETUP --> FILE\n    \n    FILE --> ALL\n    FILE --> TRACE\n    FILE --> PRINT\n    \n    CONSOLE -.-> Terminal\n    \n    style LOGFUNC fill:#f9f9f9\n    style SETUP fill:#f9f9f9\n    style ALL fill:#e8f4f8\n    style TRACE fill:#e8f4f8\n    style PRINT fill:#e8f4f8\n```\n\n**Architecture Components:**\n\n| Component | File | Line Range | Purpose |\n|-----------|------|------------|---------|\n| `setup_logger()` | log_decorator.py | 19-63 | Creates configured logger instances |\n| `log_function()` | log_decorator.py | 153-261 | Decorator factory for function instrumentation |\n| `format_value()` | log_decorator.py | 68-86 | Formats complex objects for logging |\n| `all_logger` | log_decorator.py | 292-294 | Root logger instance |\n| `traceable` | log_decorator.py | 296-302 | Decorator for detailed function tracing |\n| `global_logger` | log_decorator.py | 304-306 | Logger for user-facing operations |\n\nSources: [src/utils/log_decorator.py:1-322]()\n\n---\n\n## Logger Configuration and Instances\n\n### Logger Setup Function\n\nThe `setup_logger()` function creates and configures logger instances with both console and file handlers:\n\n```mermaid\ngraph LR\n    INPUT[\"setup_logger()<br/>parameters\"]\n    \n    subgraph \"Configuration\"\n        CHECK[\"Check if logger exists\"]\n        CREATE[\"Create Logger instance\"]\n        LEVEL[\"Set log level\"]\n        FMT[\"Create Formatter\"]\n    end\n    \n    subgraph \"Handlers\"\n        CONSOLE[\"StreamHandler<br/>Console output\"]\n        FILE[\"RotatingFileHandler<br/>10MB, 5 backups\"]\n    end\n    \n    subgraph \"Setup\"\n        MKDIR[\"Create log directory<br/>if not exists\"]\n        ATTACH[\"Attach handlers\"]\n    end\n    \n    OUTPUT[\"Configured Logger\"]\n    \n    INPUT --> CHECK\n    CHECK -->|New| CREATE\n    CHECK -->|Exists| OUTPUT\n    CREATE --> LEVEL\n    LEVEL --> FMT\n    FMT --> CONSOLE\n    FMT --> FILE\n    FILE --> MKDIR\n    CONSOLE --> ATTACH\n    MKDIR --> ATTACH\n    ATTACH --> OUTPUT\n```\n\n**Key Features:**\n\n| Feature | Implementation | Location |\n|---------|---------------|----------|\n| Automatic directory creation | `os.makedirs(log_dir, exist_ok=True)` | [log_decorator.py:24]() |\n| Rotating file handler | 10MB max, 5 backup files | [log_decorator.py:54-59]() |\n| UTF-8 encoding | All file handlers use UTF-8 | [log_decorator.py:58]() |\n| Logger reuse | Checks `logging.root.manager.loggerDict` | [log_decorator.py:29-30]() |\n| Format | `[%(asctime)s]  %(message)s` | [log_decorator.py:42]() |\n\nSources: [src/utils/log_decorator.py:19-63]()\n\n### Pre-configured Logger Instances\n\nThree logger instances are pre-configured for different purposes:\n\n```mermaid\ngraph TB\n    subgraph \"Logger Hierarchy\"\n        ROOT[\"all_logger<br/>logger_name: 'root.all'<br/>file: logs/all.log\"]\n        TRACE[\"traceable_logger<br/>logger_name: 'root.all.trace'<br/>file: logs/trace.log\"]\n        GLOBAL[\"global_logger<br/>logger_name: 'root.all.print'<br/>file: logs/print.log\"]\n    end\n    \n    subgraph \"Usage Patterns\"\n        DECORATORS[\"@traceable<br/>Function instrumentation\"]\n        MANUAL[\"global_logger.info()<br/>Manual logging calls\"]\n    end\n    \n    subgraph \"Configuration\"\n        ALLLEVEL[\"level: DEBUG\"]\n        TRACELEVEL[\"level: DEBUG\"]\n        GLOBALLEVEL[\"level: DEBUG\"]\n    end\n    \n    ROOT --> TRACE\n    ROOT --> GLOBAL\n    \n    TRACE --> DECORATORS\n    GLOBAL --> MANUAL\n    \n    DECORATORS --> TRACELEVEL\n    MANUAL --> GLOBALLEVEL\n```\n\n**Logger Configurations:**\n\n| Logger | Variable | Logger Name | Log File | Purpose |\n|--------|----------|-------------|----------|---------|\n| All Logger | `all_logger` | `root.all` | `logs/all.log` | Comprehensive system-wide logs |\n| Traceable | `traceable` | `root.all.trace` | `logs/trace.log` | Detailed function execution traces |\n| Global | `global_logger` | `root.all.print` | `logs/print.log` | User-facing operation logs |\n\n**Usage Examples:**\n\n```python\n# Using the traceable decorator\n@traceable\ndef my_function(param1, param2):\n    return result\n\n# Using global_logger for manual logging\nglobal_logger.info(f\"User input: {user_query}\")\n```\n\nSources: [src/utils/log_decorator.py:288-306](), [src/agent/deep_research.py:4]()\n\n---\n\n## The log_function Decorator\n\nThe `log_function` decorator provides automatic instrumentation of functions with comprehensive logging capabilities:\n\n```mermaid\nsequenceDiagram\n    participant Caller\n    participant Decorator as @log_function\n    participant Logger\n    participant Function\n    \n    Caller->>Decorator: Call decorated function\n    Decorator->>Decorator: Extract function metadata<br/>(name, module, class, line)\n    Decorator->>Decorator: Format args/kwargs\n    Decorator->>Logger: Log [è°ç¨å¼å§]<br/>params, timestamp\n    \n    alt record_stack=True\n        Decorator->>Logger: Log [è°ç¨æ ]<br/>filtered stack trace\n    end\n    \n    Decorator->>Function: Execute function(*args, **kwargs)\n    \n    alt Success\n        Function-->>Decorator: Return result\n        Decorator->>Decorator: Calculate elapsed time\n        Decorator->>Decorator: Format result\n        Decorator->>Logger: Log [è°ç¨æå]<br/>time, result\n        Decorator-->>Caller: Return result\n    else Exception\n        Function-->>Decorator: Raise exception\n        Decorator->>Decorator: Capture traceback\n        Decorator->>Logger: Log [è°ç¨å¤±è´¥]<br/>exception details, stack\n        Decorator->>Decorator: Re-raise exception\n        Decorator-->>Caller: Exception propagates\n    end\n```\n\n**Decorator Parameters:**\n\n| Parameter | Type | Default | Purpose |\n|-----------|------|---------|---------|\n| `logger_name` | str | Required | Name for logger instance |\n| `log_file` | Optional[str] | None | Path to log file |\n| `level` | int | `logging.DEBUG` | Logging level |\n| `exclude_args` | Optional[list] | None | Argument names to exclude (e.g., passwords) |\n| `record_stack` | bool | True | Whether to record call stack |\n| `default_return_value` | Any | None | Fallback return value on exception |\n\n**Logged Information:**\n\n1. **Call Start** (`[è°ç¨å¼å§]`):\n   - Stack path: `module.class.function`\n   - Start timestamp\n   - Positional arguments (formatted)\n   - Keyword arguments (formatted)\n\n2. **Call Stack** (`[è°ç¨æ ]`):\n   - Filtered to project files only\n   - Format: `filename:lineno function_name`\n   - Excludes decorator wrapper frames\n\n3. **Call Success** (`[è°ç¨æå]`):\n   - Stack path\n   - Execution time in milliseconds\n   - Formatted return value\n\n4. **Call Failure** (`[è°ç¨å¤±è´¥]`):\n   - Stack path\n   - Execution time\n   - Exception type and message\n   - Full traceback\n   - Exception location\n\nSources: [src/utils/log_decorator.py:153-261]()\n\n### Value Formatting\n\nThe `format_value()` function handles complex object serialization:\n\n```mermaid\ngraph TB\n    INPUT[\"Value to format\"]\n    \n    PFORMAT[\"Try pprint.pformat()\"]\n    \n    HASDICT{\"Has __dict__<br/>attribute?\"}\n    COLLECTION{\"Is set<br/>or tuple?\"}\n    FALLBACK[\"Convert to string<br/>Truncate if > 500 chars\"]\n    \n    OUTPUT[\"Formatted string\"]\n    \n    INPUT --> PFORMAT\n    PFORMAT -->|Success| OUTPUT\n    PFORMAT -->|TypeError/ValueError| HASDICT\n    \n    HASDICT -->|Yes| OBJDICT[\"Format as:<br/>[ClassName] {...}\"]\n    HASDICT -->|No| COLLECTION\n    \n    COLLECTION -->|Yes| COLLSTR[\"Format as:<br/>type([items...])\"]\n    COLLECTION -->|No| FALLBACK\n    \n    OBJDICT --> OUTPUT\n    COLLSTR --> OUTPUT\n    FALLBACK --> OUTPUT\n```\n\n**Formatting Rules:**\n\n| Value Type | Format Strategy | Truncation |\n|------------|----------------|------------|\n| Standard types | `pprint.pformat()` | None |\n| Objects with `__dict__` | `[ClassName] {first 10 items}...` | 10 items |\n| Sets/Tuples | `type([first 20 items])...` | 20 items |\n| Other types | `str(value)` | 500 chars |\n\nSources: [src/utils/log_decorator.py:68-86]()\n\n---\n\n## Log Files and Content Structure\n\n### Log File Organization\n\n```mermaid\ngraph TB\n    subgraph \"logs/ Directory\"\n        subgraph \"Timestamp Subdirectory\"\n            ALL[\"all.log<br/>All logs from all loggers\"]\n            TRACE[\"trace.log<br/>@traceable decorated functions\"]\n            PRINT[\"print.log<br/>global_logger manual logs\"]\n        end\n        \n        BACKUP1[\"all.log.1<br/>Backup (oldest)\"]\n        BACKUP2[\"all.log.2\"]\n        BACKUP3[\"all.log.3\"]\n        BACKUP4[\"all.log.4\"]\n        BACKUP5[\"all.log.5<br/>Backup (newest)\"]\n    end\n    \n    subgraph \"Content Flow\"\n        DECORATED[\"Decorated functions<br/>@traceable\"]\n        MANUAL[\"Manual logging<br/>global_logger.info()\"]\n    end\n    \n    DECORATED --> TRACE\n    DECORATED --> ALL\n    MANUAL --> PRINT\n    MANUAL --> ALL\n    \n    ALL -.->|Rotation<br/>10MB threshold| BACKUP1\n```\n\n**Log File Characteristics:**\n\n| Log File | Purpose | Typical Contents | Size Limit |\n|----------|---------|------------------|------------|\n| `all.log` | Comprehensive logs | All logged events system-wide | 10MB |\n| `trace.log` | Function traces | Decorated function calls with timing | 10MB |\n| `print.log` | User operations | User queries, tool outputs, final answers | 10MB |\n| `*.log.N` | Backups | Rotated log files (N=1 is oldest) | 10MB each |\n\nSources: [src/utils/log_decorator.py:288-306](), [logs/utils.log:1-296](), [logs/global.log:1-907]()\n\n### Log Entry Format\n\nEach log entry follows a structured format:\n\n```\n[Timestamp] [Event Type] Details\n```\n\n**Example Log Entries:**\n\n```\n[2025-11-25 17:51:32,252]  ãè°ç¨å¼å§ã æ è·¯å¾ï¼ __main__.None.user_query | \n    å¼å§æ¶é´ï¼ 2025-11-25 17:51:32.252333 | \n    ä½ç½®åæ°ï¼ ('user input text...',) | \n    å³é®å­åæ°ï¼ {}\n\n[2025-11-25 17:51:32,383]  ãè°ç¨æ ã \n          D:\\path\\to\\file.py:202 wrapper\n          d:\\path\\to\\other.py:110 <module>\n\n[2025-11-25 17:52:14,089]  ãè°ç¨æåã æ è·¯å¾ï¼ llm.None.generate_assistant_output_append | \n    èæ¶ï¼ 41680.934ms | \n    è¿åå¼ï¼ ChatCompletionMessage(...)\n\n[2025-11-26 03:39:11,797]  å·¥å·è¾åºä¿¡æ¯ï¼ ## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\nProcess Process-1:\nTraceback (most recent call last):\n  File \"subprocess_python_executor.py\", line 44, in _worker_with_pipe\n    exec(command, _globals, _locals)\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265\n```\n\nSources: [logs/global.log:1-907](), [logs/utils.log:1-296]()\n\n---\n\n## Usage Patterns in the Codebase\n\n### Agent Layer Usage\n\nThe agent uses both `@traceable` decorators and manual `global_logger` calls:\n\n```mermaid\ngraph TB\n    subgraph \"deep_research.py\"\n        USER[\"user_query()<br/>Decorated with @traceable\"]\n        \n        USERINPUT[\"global_logger.info()<br/>'ç¨æ·è¾å¥ï¼'\"]\n        TOOLOUT[\"global_logger.info()<br/>'å·¥å·è¾åºä¿¡æ¯ï¼'\"]\n        FINALANS[\"global_logger.info()<br/>'æç»ç­æ¡ï¼'\"]\n        \n        USER --> USERINPUT\n        USER --> TOOLOUT\n        USER --> FINALANS\n    end\n    \n    subgraph \"Log Output\"\n        TRACE_START[\"[è°ç¨å¼å§] user_query<br/>with parameters\"]\n        TRACE_STACK[\"[è°ç¨æ ] filtered stack\"]\n        USER_MSG[\"User input message\"]\n        TOOL_MSG[\"Tool execution results\"]\n        FINAL_MSG[\"Final answer\"]\n        TRACE_END[\"[è°ç¨æå] user_query<br/>with timing\"]\n    end\n    \n    USERINPUT --> USER_MSG\n    TOOLOUT --> TOOL_MSG\n    FINALANS --> FINAL_MSG\n    \n    USER -.->|@traceable| TRACE_START\n    TRACE_START --> TRACE_STACK\n    TRACE_STACK --> TRACE_END\n```\n\n**Key Logging Points:**\n\n| Location | Line | Log Type | Message |\n|----------|------|----------|---------|\n| User input | [deep_research.py:17]() | `global_logger.info()` | `ç¨æ·è¾å¥ï¼ {user_input}` |\n| Tool output | [deep_research.py:44,62]() | `global_logger.info()` | `å·¥å· [function/tool] call è¾åºä¿¡æ¯ï¼` |\n| Final answer | [deep_research.py:73]() | `global_logger.info()` | `æç»ç­æ¡ï¼ {answer}` |\n| LLM output | [deep_research.py:68-72]() | `global_logger.info()` | `ç¬¬{N}è½®å¤§æ¨¡åè¾åºä¿¡æ¯ï¼` |\n\nSources: [src/agent/deep_research.py:1-129](), [logs/utils.log:1-296]()\n\n### Function Instrumentation Pattern\n\nFunctions decorated with `@traceable` automatically generate detailed logs:\n\n```mermaid\nsequenceDiagram\n    participant Code as Application Code\n    participant Dec as @traceable Decorator\n    participant Log as trace.log\n    participant Time as Timer\n    \n    Code->>Dec: Call function(args)\n    Dec->>Time: Start timer\n    Dec->>Log: [è°ç¨å¼å§] with args\n    Dec->>Log: [è°ç¨æ ] if enabled\n    \n    alt Successful execution\n        Dec->>Code: Execute function\n        Code-->>Dec: Return value\n        Dec->>Time: Stop timer\n        Dec->>Log: [è°ç¨æå]<br/>elapsed time, return value\n        Dec-->>Code: Return value\n    else Exception occurs\n        Dec->>Code: Execute function\n        Code-->>Dec: Exception raised\n        Dec->>Time: Stop timer\n        Dec->>Log: [è°ç¨å¤±è´¥]<br/>exception type, message, traceback\n        Dec-->>Code: Re-raise exception\n    end\n```\n\n**Example Instrumented Functions:**\n\n| Function | File | Decorator | Purpose |\n|----------|------|-----------|---------|\n| `user_query()` | deep_research.py | `@traceable` | Agent query processing |\n| `init_messages_with_system_prompt()` | memory.py | `@traceable` | Message initialization |\n| `get_tools_schema()` | tool/schema.py | `@traceable` | Tool schema generation |\n| `generate_assistant_output_append()` | llm.py | `@traceable` | LLM interaction |\n| `call_tools_safely()` | action.py | `@traceable` | Tool execution |\n\nSources: [logs/global.log:1-907]()\n\n---\n\n## Debugging and Analysis Capabilities\n\n### Error Detection and Diagnosis\n\nThe logging system captures comprehensive error information for debugging:\n\n```mermaid\ngraph TB\n    subgraph \"Error Types Captured\"\n        UNICODE[\"UnicodeDecodeError<br/>File encoding issues\"]\n        PICKLE[\"PickleError<br/>Serialization failures\"]\n        TIMEOUT[\"Timeout errors<br/>Long-running code\"]\n        CRASH[\"Process crashes<br/>SegFault, etc.\"]\n        GENERIC[\"General exceptions<br/>With full traceback\"]\n    end\n    \n    subgraph \"Log Information\"\n        LOC[\"Exception location<br/>file:line\"]\n        TYPE[\"Exception type\"]\n        MSG[\"Exception message\"]\n        TRACE[\"Full traceback\"]\n        CONTEXT[\"Function parameters\"]\n        TIMING[\"Execution time before failure\"]\n    end\n    \n    subgraph \"Analysis\"\n        REPRODUCE[\"Failure reproduction<br/>Parameters logged\"]\n        PATTERN[\"Pattern detection<br/>Similar errors\"]\n        HOTSPOT[\"Hotspot identification<br/>Frequent failures\"]\n    end\n    \n    UNICODE --> LOC\n    PICKLE --> TYPE\n    TIMEOUT --> MSG\n    CRASH --> TRACE\n    GENERIC --> CONTEXT\n    \n    LOC --> REPRODUCE\n    TYPE --> PATTERN\n    MSG --> PATTERN\n    TRACE --> REPRODUCE\n    CONTEXT --> REPRODUCE\n    TIMING --> HOTSPOT\n```\n\n**Example Error Log Entry:**\n\n```\n[2025-11-26 03:39:11,797]  å·¥å·è¾åºä¿¡æ¯ï¼ ## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåº\n### ç»ç«¯è¾åºï¼\nProcess Process-1:\nTraceback (most recent call last):\n  File \"subprocess_python_executor.py\", line 44, in _worker_with_pipe\n    exec(command, _globals, _locals)\n  File \"<string>\", line 4, in <module>\n  File \"json\\__init__.py\", line 293, in load\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265: illegal multibyte sequence\n```\n\nSources: [logs/utils.log:132-180]()\n\n### Performance Analysis\n\nThe logging system provides execution timing for performance analysis:\n\n```mermaid\ngraph LR\n    subgraph \"Logged Metrics\"\n        START[\"Start timestamp<br/>ç²¾ç¡®å°å¾®ç§\"]\n        END[\"End timestamp\"]\n        ELAPSED[\"Elapsed time<br/>æ¯«ç§ç²¾åº¦\"]\n    end\n    \n    subgraph \"Analysis Types\"\n        BOTTLENECK[\"Bottleneck identification<br/>Slowest functions\"]\n        TREND[\"Performance trends<br/>Over time\"]\n        COMPARE[\"Cross-function comparison<br/>Relative performance\"]\n    end\n    \n    START --> ELAPSED\n    END --> ELAPSED\n    \n    ELAPSED --> BOTTLENECK\n    ELAPSED --> TREND\n    ELAPSED --> COMPARE\n```\n\n**Performance Log Examples:**\n\n| Function | Execution Time | Location |\n|----------|---------------|----------|\n| `init_messages_with_system_prompt()` | 2.031ms | [global.log:115]() |\n| `get_tools_schema()` | 8.008ms | [global.log:189]() |\n| `generate_chat_completion()` | 41635.670ms | [global.log:891]() |\n| `generate_assistant_output_append()` | 41680.934ms | [global.log:893]() |\n\nSources: [logs/global.log:1-907]()\n\n### Call Stack Analysis\n\nThe decorator captures filtered call stacks for debugging execution flow:\n\n```mermaid\ngraph TB\n    subgraph \"Stack Filtering\"\n        RAW[\"inspect.stack()\"]\n        FILTER[\"Filter to project files\"]\n        FORMAT[\"Format as file:line function\"]\n        EXCLUDE[\"Exclude wrapper frames\"]\n    end\n    \n    subgraph \"Stack Information\"\n        FILE[\"File path\"]\n        LINE[\"Line number\"]\n        FUNC[\"Function name\"]\n    end\n    \n    subgraph \"Use Cases\"\n        FLOW[\"Execution flow tracing\"]\n        CALLER[\"Caller identification\"]\n        DEBUG[\"Step-by-step debugging\"]\n    end\n    \n    RAW --> FILTER\n    FILTER --> EXCLUDE\n    EXCLUDE --> FORMAT\n    \n    FORMAT --> FILE\n    FORMAT --> LINE\n    FORMAT --> FUNC\n    \n    FILE --> FLOW\n    LINE --> CALLER\n    FUNC --> DEBUG\n```\n\n**Example Stack Trace:**\n\n```\n[è°ç¨æ ]\n    D:\\zyt\\git_ln\\algo_agent\\src\\utils\\log_decorator.py:202 wrapper\n    d:\\zyt\\git_ln\\algo_agent\\src\\agent\\deep_research.py:26 user_query\n    D:\\zyt\\git_ln\\algo_agent\\src\\utils\\log_decorator.py:217 wrapper\n    d:\\zyt\\git_ln\\algo_agent\\src\\agent\\deep_research.py:110 <module>\n```\n\n**Stack Filtering Logic:**\n- Only includes files within the project directory\n- Uses `os.path.normcase()` for cross-platform compatibility\n- Excludes the decorator's `wrapper` function from the trace\n- Formats as: `{filename}:{lineno} {function}`\n\nSources: [src/utils/log_decorator.py:205-217](), [logs/global.log:54-57,110-114]()\n\n---\n\n## Configuration and Customization\n\n### Log Directory Management\n\nThe system automatically manages log directories with timestamp-based organization:\n\n```mermaid\ngraph TB\n    INIT[\"create_folder.get_or_create_subfolder()\"]\n    \n    CHECK{\"Directory<br/>exists?\"}\n    CREATE[\"os.makedirs()<br/>exist_ok=True\"]\n    \n    TIMESTAMP[\"Timestamp-based<br/>subdirectory\"]\n    \n    LOGDIR[\"sub_folder_for_logs\"]\n    \n    FILES[\"Log files created<br/>in subdirectory\"]\n    \n    INIT --> TIMESTAMP\n    TIMESTAMP --> CHECK\n    CHECK -->|No| CREATE\n    CHECK -->|Yes| LOGDIR\n    CREATE --> LOGDIR\n    LOGDIR --> FILES\n```\n\n**Directory Configuration:**\n\n| Variable | Value | Line |\n|----------|-------|------|\n| `sub_folder_for_logs` | `create_folder.get_or_create_subfolder(gen_time_path_from_project=\"logs\")` | [log_decorator.py:288]() |\n| `all_logger_file_name` | `os.path.join(sub_folder_for_logs, \"all.log\")` | [log_decorator.py:292]() |\n| `traceable_logger_file_name` | `os.path.join(sub_folder_for_logs, \"trace.log\")` | [log_decorator.py:296]() |\n| `global_logger_file_name` | `os.path.join(sub_folder_for_logs, \"print.log\")` | [log_decorator.py:304]() |\n\nSources: [src/utils/log_decorator.py:288-306]()\n\n### Creating Custom Loggers\n\nTo create a custom logger with specific configuration:\n\n**Example Custom Logger Creation:**\n\n```python\n# Create a logger for a specific module\ncustom_logger = setup_logger(\n    logger_name=\"module.custom\",\n    log_file=\"logs/custom.log\",\n    level=logging.INFO\n)\n\n# Use in code\ncustom_logger.info(\"Custom log message\")\n\n# Or create a decorator for automatic instrumentation\ncustom_decorator = lambda func: log_function(\n    logger_name=\"module.custom_trace\",\n    log_file=\"logs/custom_trace.log\",\n    level=logging.DEBUG,\n    exclude_args=[\"password\", \"secret\"],\n    record_stack=True\n)(func)\n\n@custom_decorator\ndef my_function(param1, password):\n    return result\n```\n\n**Decorator Parameters:**\n\n| Parameter | Purpose | Example Value |\n|-----------|---------|---------------|\n| `logger_name` | Unique logger identifier | `\"module.subsystem\"` |\n| `log_file` | Path to log file | `\"logs/subsystem.log\"` |\n| `level` | Minimum log level | `logging.DEBUG` |\n| `exclude_args` | Args to exclude from logs | `[\"password\", \"token\"]` |\n| `record_stack` | Enable stack tracing | `True` or `False` |\n| `default_return_value` | Fallback on exception | `[]`, `None`, etc. |\n\nSources: [src/utils/log_decorator.py:153-261,296-302]()\n\n---\n\n## Advanced Features\n\n### Exception Handling with Fallback Values\n\nThe decorator can provide fallback return values when exceptions occur (currently disabled in the code):\n\n```mermaid\ngraph TB\n    FUNC[\"Execute function\"]\n    \n    EXCEPTION{\"Exception<br/>raised?\"}\n    \n    LOG[\"Log exception details:<br/>- Type<br/>- Message<br/>- Traceback<br/>- Execution time\"]\n    \n    FALLBACK[\"Get default return value:<br/>1. Manual: default_return_value<br/>2. Auto: get_default_return_value()\"]\n    \n    RAISE[\"Re-raise exception<br/>(current behavior)\"]\n    \n    RETURN[\"Return fallback value<br/>(commented out)\"]\n    \n    FUNC --> EXCEPTION\n    EXCEPTION -->|Yes| LOG\n    EXCEPTION -->|No| SUCCESS[\"Return actual result\"]\n    \n    LOG --> RAISE\n    LOG -.->|If enabled| FALLBACK\n    FALLBACK -.-> RETURN\n```\n\n**Fallback Value Inference:**\n\n| Return Type Annotation | Default Value |\n|-----------------------|---------------|\n| `int` | `0` |\n| `float` | `0.0` |\n| `str` | `\"\"` |\n| `bool` | `False` |\n| `list` or `List[T]` | `[]` |\n| `dict` or `Dict[K,V]` | `{}` |\n| `tuple` or `Tuple[...]` | `()` |\n| `set` or `Set[T]` | `set()` |\n| Custom class | `ClassName()` (if possible) |\n| No annotation | `None` |\n\nThe `get_default_return_value()` function uses type hints to infer appropriate fallback values, but the current implementation re-raises exceptions instead of returning fallbacks.\n\nSources: [src/utils/log_decorator.py:91-148,233-257]()\n\n### Parameter Formatting and Truncation\n\nThe system intelligently formats different parameter types:\n\n```mermaid\ngraph TB\n    VALUE[\"Input value\"]\n    \n    PPRINT{\"pprint.pformat()<br/>succeeds?\"}\n    \n    HASDICT{\"Has __dict__<br/>attribute?\"}\n    \n    COLLECTION{\"Is set/tuple?\"}\n    \n    SUCCESS[\"Use pformat output\"]\n    DICT[\"Format as [ClassName]<br/>{first 10 dict items}...\"]\n    COLL[\"Format as type()<br/>[first 20 items]...\"]\n    STR[\"str(value)[:500]...\"]\n    \n    VALUE --> PPRINT\n    PPRINT -->|Yes| SUCCESS\n    PPRINT -->|No| HASDICT\n    HASDICT -->|Yes| DICT\n    HASDICT -->|No| COLLECTION\n    COLLECTION -->|Yes| COLL\n    COLLECTION -->|No| STR\n```\n\n**Truncation Rules:**\n\n| Object Type | Display Strategy | Truncation Limit |\n|-------------|-----------------|------------------|\n| Standard types | Pretty-printed | None |\n| Objects | `[ClassName] {first_10_items}...` | 10 items |\n| Large sets/tuples | `type([first_20_items])...` | 20 items |\n| Long strings | `str_value[:500]...` | 500 characters |\n\nSources: [src/utils/log_decorator.py:68-86]()\n\n---\n\n## Integration with System Components\n\n### Agent Integration\n\n```mermaid\ngraph LR\n    subgraph \"Agent Functions\"\n        UQ[\"user_query()\"]\n        IM[\"init_messages<br/>_with_system_prompt()\"]\n        GTS[\"get_tools_schema()\"]\n        GAO[\"generate_assistant<br/>_output_append()\"]\n        CT[\"call_tools_safely()\"]\n    end\n    \n    subgraph \"Logging\"\n        TR[\"@traceable<br/>decorator\"]\n        GL[\"global_logger\"]\n    end\n    \n    subgraph \"Log Files\"\n        TL[\"trace.log\"]\n        PL[\"print.log\"]\n        AL[\"all.log\"]\n    end\n    \n    UQ --> TR\n    IM --> TR\n    GTS --> TR\n    GAO --> TR\n    CT --> TR\n    \n    UQ --> GL\n    \n    TR --> TL\n    TR --> AL\n    GL --> PL\n    GL --> AL\n```\n\n**Logged Agent Operations:**\n\n| Operation | Log Type | Information Captured |\n|-----------|----------|---------------------|\n| User query received | `global_logger.info()` | Full user input text |\n| Message initialization | `@traceable` | System prompt, user message |\n| Tool schema generation | `@traceable` | Tool list, generated schemas |\n| LLM interaction | `@traceable` | Messages, tools, response timing |\n| Tool execution | `@traceable` + `global_logger` | Tool name, arguments, results |\n| Final answer | `global_logger.info()` | Model's final response |\n\nSources: [src/agent/deep_research.py:15-73](), [logs/utils.log:1-296]()\n\n### Runtime Integration\n\nExecution runtime components use logging for debugging code execution:\n\n```mermaid\ngraph TB\n    subgraph \"Subprocess Execution\"\n        WORKER[\"_worker_with_pipe()\"]\n        EXEC[\"exec(command)\"]\n        ERROR[\"Exception handling\"]\n    end\n    \n    subgraph \"Logged Events\"\n        PID[\"Process PID\"]\n        WORKDIR[\"Working directory change\"]\n        EXCEPTION[\"Subprocess exceptions\"]\n        TIMEOUT[\"Timeout detection\"]\n        CRASH[\"Process crash\"]\n    end\n    \n    subgraph \"Log Output\"\n        UTILS[\"utils.log\"]\n        GLOBAL[\"global.log\"]\n    end\n    \n    WORKER --> PID\n    WORKER --> WORKDIR\n    EXEC --> EXCEPTION\n    EXEC --> TIMEOUT\n    EXEC --> CRASH\n    \n    PID --> UTILS\n    WORKDIR --> UTILS\n    EXCEPTION --> UTILS\n    TIMEOUT --> UTILS\n    CRASH --> UTILS\n    \n    UTILS --> GLOBAL\n```\n\n**Runtime Log Entries:**\n\n| Event | Format | Example |\n|-------|--------|---------|\n| Process creation | `å­è¿ç¨ PID: {pid}` | [utils.log:130]() |\n| Directory change | `å­è¿ç¨ PID: {pid} å·²å°å·¥ä½ç®å½æ´æ¹ä¸º: {path}` | [utils.log:131]() |\n| Exception exit | `---------- 2.1.2 å­è¿ç¨å¼å¸¸ç»æ` | [utils.log:132]() |\n| Pipe error | `Pipe reader error: type={type}, value={value}` | [utils.log:133]() |\n| Crash exit | `---------- 2.2 å­è¿ç¨å´©æºéåº` | [utils.log:136]() |\n| Tool output | `å·¥å·è¾åºä¿¡æ¯ï¼ ## ä»£ç æ§è¡å´©æº` | [utils.log:136-180]() |\n\nSources: [logs/utils.log:130-288]()\n\n---\n\n## Best Practices and Recommendations\n\n### When to Use Each Logger\n\n| Scenario | Logger | Rationale |\n|----------|--------|-----------|\n| Function entry/exit/timing | `@traceable` | Automatic instrumentation, detailed traces |\n| User-facing operations | `global_logger.info()` | Clean, high-level operation logs |\n| Debugging complex flows | `@traceable` with `record_stack=True` | Full call stack for diagnosis |\n| Sensitive data | Custom logger with `exclude_args` | Exclude passwords, tokens, secrets |\n| Performance monitoring | `@traceable` | Automatic timing capture |\n\n### Log Analysis Workflow\n\n```mermaid\ngraph TB\n    START[\"Issue or question\"]\n    \n    IDENTIFY[\"Identify relevant log file:<br/>- trace.log for function flows<br/>- print.log for user operations<br/>- all.log for comprehensive view\"]\n    \n    SEARCH[\"Search for key terms:<br/>- Function names<br/>- Error messages<br/>- Timestamps\"]\n    \n    TRACE[\"Follow call stack:<br/>ãè°ç¨æ ãentries\"]\n    \n    TIMING[\"Analyze timing:<br/>ãè°ç¨æåãentries\"]\n    \n    ERROR[\"Examine errors:<br/>ãè°ç¨å¤±è´¥ãentries\"]\n    \n    REPRODUCE[\"Reproduce issue:<br/>Use logged parameters\"]\n    \n    START --> IDENTIFY\n    IDENTIFY --> SEARCH\n    SEARCH --> TRACE\n    SEARCH --> TIMING\n    SEARCH --> ERROR\n    ERROR --> REPRODUCE\n    TIMING --> IDENTIFY\n```\n\n### Performance Considerations\n\n| Aspect | Impact | Mitigation |\n|--------|--------|-----------|\n| File I/O overhead | Minimal due to buffering | Use rotating handlers to limit file size |\n| Formatting overhead | Low for simple types | Complex objects are truncated automatically |\n| Stack trace collection | Moderate overhead | Disable with `record_stack=False` if not needed |\n| Log file growth | Can become large over time | Automatic rotation (10MB max, 5 backups) |\n| Parameter serialization | Can slow down for large objects | Truncation limits (500 chars, 10 items, etc.) |\n\nSources: [src/utils/log_decorator.py:19-322]()\n\n---\n\n# Page: Logging System Architecture\n\n# Logging System Architecture\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/utils/__pycache__/log_decorator.cpython-312.pyc](src/utils/__pycache__/log_decorator.cpython-312.pyc)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document describes the logging system architecture in the algo_agent codebase, focusing on the core logging infrastructure, decorator-based instrumentation, and logger hierarchy. The logging system provides comprehensive observability across all components through automated function tracing, parameter capture, execution timing, and error tracking.\n\nFor information about the actual log files and how to use them for debugging, see [Log Files and Analysis](#6.2). For performance monitoring and bottleneck identification, see [Tracing and Performance Monitoring](#6.3).\n\n**Sources:** [src/utils/log_decorator.py:1-322]()\n\n---\n\n## Architecture Overview\n\nThe logging system is built around three core components: a flexible logger setup function, a powerful decorator for automatic instrumentation, and a hierarchy of specialized logger instances.\n\n### System Components Diagram\n\n```mermaid\ngraph TB\n    subgraph \"Logger Setup Layer\"\n        setup_logger[\"setup_logger()<br/>Configuration Function\"]\n        formatter[\"LogFormatter<br/>[asctime] message\"]\n        console[\"ConsoleHandler\"]\n        file[\"RotatingFileHandler<br/>10MB max, 5 backups\"]\n    end\n    \n    subgraph \"Decorator Layer\"\n        log_function[\"log_function()<br/>Core Decorator Factory\"]\n        traceable_dec[\"traceable<br/>Lambda Decorator\"]\n        wrapper[\"wrapper()<br/>Instrumented Function\"]\n    end\n    \n    subgraph \"Logger Instance Hierarchy\"\n        all_logger[\"all_logger<br/>root.all<br/>logs/all.log\"]\n        traceable_logger[\"traceable<br/>root.all.trace<br/>logs/trace.log\"]\n        global_logger[\"global_logger<br/>root.all.print<br/>logs/print.log\"]\n    end\n    \n    subgraph \"Application Code\"\n        agent[\"deep_research.py<br/>Uses global_logger\"]\n        tools[\"Tool Functions<br/>Uses traceable decorator\"]\n    end\n    \n    setup_logger --> formatter\n    setup_logger --> console\n    setup_logger --> file\n    \n    log_function --> wrapper\n    traceable_dec --> log_function\n    \n    setup_logger --> all_logger\n    setup_logger --> traceable_logger\n    setup_logger --> global_logger\n    \n    traceable_dec --> traceable_logger\n    \n    agent --> global_logger\n    tools --> traceable_dec\n    \n    all_logger -.parent.-> traceable_logger\n    all_logger -.parent.-> global_logger\n```\n\n**Sources:** [src/utils/log_decorator.py:19-63](), [src/utils/log_decorator.py:288-306]()\n\n---\n\n## Logger Setup and Configuration\n\n### The `setup_logger` Function\n\nThe `setup_logger` function creates and configures logger instances with both console and file handlers. It implements automatic log directory creation and rotating file handlers to prevent disk space exhaustion.\n\n**Function Signature:**\n\n```python\ndef setup_logger(\n    logger_name: str,\n    log_file: Optional[str] = None,\n    level: int = logging.DEBUG\n) -> logging.Logger\n```\n\n**Key Features:**\n\n| Feature | Implementation | Purpose |\n|---------|---------------|---------|\n| **Automatic Directory Creation** | `os.makedirs(log_dir, exist_ok=True)` | Creates log directories if missing |\n| **Singleton Pattern** | Checks `logging.root.manager.loggerDict` | Reuses existing loggers |\n| **Rotating File Handler** | `maxBytes=10*1024*1024`, `backupCount=5` | Prevents disk exhaustion |\n| **UTF-8 Encoding** | `encoding=\"utf-8\"` | Handles international characters |\n| **Log Propagation** | `logger.propagate = True` | Enables hierarchical logging |\n\n**Sources:** [src/utils/log_decorator.py:19-63]()\n\n### Logger Format Configuration\n\nThe system uses a simplified format for readability:\n\n```\n[%(asctime)s]  %(message)s\n```\n\nThis format was chosen after iterating through more verbose formats. The simplified version focuses on timestamp and message, with contextual information added by the decorator rather than the formatter.\n\n**Sources:** [src/utils/log_decorator.py:36-46]()\n\n### Log Directory Management\n\nLog files are stored in timestamped subdirectories to organize execution sessions:\n\n```python\nsub_folder_for_logs = create_folder.get_or_create_subfolder(\n    gen_time_path_from_project=\"logs\"\n)\n```\n\nThis creates a structure like `logs/2024-01-15_14-30-45/` for each run, preventing log file collisions across multiple executions.\n\n**Sources:** [src/utils/log_decorator.py:288-289]()\n\n---\n\n## The `log_function` Decorator\n\nThe `log_function` decorator is the core instrumentation mechanism that automatically logs function calls, parameters, execution time, results, and errors.\n\n### Decorator Architecture Diagram\n\n```mermaid\nsequenceDiagram\n    participant App as \"Application Code\"\n    participant Wrapper as \"wrapper()<br/>Decorator Logic\"\n    participant Logger as \"Logger Instance\"\n    participant OrigFunc as \"Original Function\"\n    participant Utils as \"Utility Functions\"\n    \n    App->>Wrapper: Call decorated function(args, kwargs)\n    \n    Wrapper->>Wrapper: Extract function metadata<br/>(name, module, class, line)\n    Wrapper->>Utils: format_value(args)\n    Wrapper->>Utils: format_value(kwargs)\n    Utils-->>Wrapper: Formatted parameters\n    \n    Wrapper->>Logger: Log \"ãè°ç¨å¼å§ã\"<br/>(stack path, time, params)\n    \n    alt record_stack=True\n        Wrapper->>Wrapper: inspect.stack()\n        Wrapper->>Logger: Log \"ãè°ç¨æ ã\"<br/>(filtered stack trace)\n    end\n    \n    Wrapper->>OrigFunc: Execute func(args, kwargs)\n    \n    alt Success\n        OrigFunc-->>Wrapper: result\n        Wrapper->>Utils: format_value(result)\n        Utils-->>Wrapper: Formatted result\n        Wrapper->>Logger: Log \"ãè°ç¨æåã\"<br/>(elapsed time, result)\n        Wrapper-->>App: Return result\n    else Exception\n        OrigFunc-->>Wrapper: Exception\n        Wrapper->>Wrapper: Extract traceback\n        Wrapper->>Logger: Log \"ãè°ç¨å¤±è´¥ã\"<br/>(elapsed time, exception, traceback)\n        Wrapper-->>App: Re-raise exception\n    end\n```\n\n**Sources:** [src/utils/log_decorator.py:153-261]()\n\n### Decorator Parameters\n\n| Parameter | Type | Default | Purpose |\n|-----------|------|---------|---------|\n| `logger_name` | `str` | Required | Logger instance name |\n| `log_file` | `Optional[str]` | `None` | File path for logs |\n| `level` | `int` | `logging.DEBUG` | Minimum log level |\n| `exclude_args` | `Optional[list]` | `None` | Parameter names to exclude from logs |\n| `record_stack` | `bool` | `True` | Whether to record call stack |\n| `default_return_value` | `Any` | `None` | Fallback value on exception (currently disabled) |\n\n**Sources:** [src/utils/log_decorator.py:153-161]()\n\n### Function Metadata Extraction\n\nThe decorator extracts comprehensive metadata about each function call:\n\n```python\nfunc_name = func.__name__\nmodule_name = inspect.getmodule(func).__name__\nlineno = inspect.getsourcelines(func)[1]\nfile_path = inspect.getfile(func)\n```\n\nFor methods, it also extracts the class name:\n\n```python\nif args:\n    first_arg = args[0]\n    if hasattr(first_arg, func_name) and not inspect.isfunction(first_arg):\n        class_name = first_arg.__class__.__name__\n    elif inspect.isclass(first_arg) and func_name in dir(first_arg):\n        class_name = first_arg.__name__\n```\n\nThis creates a full stack path like `module_name.class_name.func_name` for precise tracking.\n\n**Sources:** [src/utils/log_decorator.py:167-182]()\n\n### Parameter Formatting\n\nThe `format_value` function handles complex Python objects gracefully:\n\n**Formatting Strategy:**\n\n```mermaid\ngraph TD\n    input[\"Input Value\"]\n    \n    pformat[\"Try pprint.pformat()\"]\n    has_dict[\"Has __dict__?\"]\n    is_collection[\"Is set/tuple?\"]\n    fallback[\"str(value)\"]\n    \n    input --> pformat\n    pformat -->|Success| output1[\"Pretty-printed string\"]\n    pformat -->|TypeError/ValueError| has_dict\n    \n    has_dict -->|Yes| dict_format[\"Format first 10 attributes<br/>as JSON\"]\n    has_dict -->|No| is_collection\n    \n    is_collection -->|Yes| coll_format[\"Format first 20 items\"]\n    is_collection -->|No| fallback\n    \n    fallback --> truncate[\"Truncate to 500 chars<br/>if longer\"]\n    \n    dict_format --> output2[\"Limited object repr\"]\n    coll_format --> output3[\"Limited collection\"]\n    truncate --> output4[\"Truncated string\"]\n```\n\n**Sources:** [src/utils/log_decorator.py:68-86]()\n\n### Call Stack Recording\n\nWhen `record_stack=True`, the decorator captures and filters the call stack:\n\n```python\ndef is_in_project(file_path):\n    project_root = os.path.abspath(os.getcwd())\n    stack_abs_path = os.path.abspath(file_path)\n    return os.path.normcase(stack_abs_path).startswith(\n        os.path.normcase(project_root)\n    )\n\nstack_str = \"\\n\".join([\n    f\"          {frame.filename}:{frame.lineno} {frame.function}\" \n    for frame in stack_info \n    if is_in_project(frame.filename) and not frame.function == \"wrapper\"\n])\n```\n\nThis filters out standard library frames and focuses on project-specific code, making stack traces more relevant.\n\n**Sources:** [src/utils/log_decorator.py:205-217]()\n\n### Error Handling and Tracing\n\nOn exception, the decorator captures comprehensive error information:\n\n```python\nexception_type = type(e).__name__\nexception_msg = str(e)\nexc_lineno = inspect.trace()[-1][2] if inspect.trace() else lineno\ntraceback_str = traceback.format_exc()\n\nlogger.error(\n    f\"ãè°ç¨å¤±è´¥ã æ è·¯å¾ï¼ {stack_full_path} | èæ¶ï¼ {elapsed_time:.3f}ms \"\n    f\"| å¼å¸¸ä½ç½®ï¼ {module_name}.{class_name}.{func_name}:{exc_lineno} \"\n    f\"| å¼å¸¸ç±»åï¼ {exception_type} | å¼å¸¸ä¿¡æ¯ï¼ {exception_msg} \"\n    f\"| å æ ä¿¡æ¯ï¼ {traceback_str}\",\n    exc_info=True\n)\n\nraise  # Re-raise to maintain normal exception flow\n```\n\nThe decorator re-raises exceptions after logging, ensuring the application behaves normally while capturing diagnostic information.\n\n**Sources:** [src/utils/log_decorator.py:233-257]()\n\n---\n\n## Logger Instance Hierarchy\n\nThe system defines three specialized logger instances with a hierarchical naming scheme that enables log propagation.\n\n### Logger Hierarchy Diagram\n\n```mermaid\ngraph TB\n    subgraph \"Logger Hierarchy\"\n        root[\"root logger<br/>(Python built-in)\"]\n        \n        all[\"all_logger<br/>Logger Name: root.all<br/>File: logs/{timestamp}/all.log<br/>Level: DEBUG\"]\n        \n        trace[\"traceable decorator<br/>Logger Name: root.all.trace<br/>File: logs/{timestamp}/trace.log<br/>Level: DEBUG<br/>Excludes: password, token, secret\"]\n        \n        print[\"global_logger<br/>Logger Name: root.all.print<br/>File: logs/{timestamp}/print.log<br/>Level: DEBUG\"]\n    end\n    \n    subgraph \"Usage Patterns\"\n        manual[\"Manual Logging<br/>global_logger.info()\"]\n        decorated[\"Decorated Functions<br/>@traceable\"]\n    end\n    \n    root -.propagation.-> all\n    all -.propagation.-> trace\n    all -.propagation.-> print\n    \n    manual --> print\n    decorated --> trace\n    \n    trace -.also writes to.-> all\n    print -.also writes to.-> all\n```\n\n**Sources:** [src/utils/log_decorator.py:292-306]()\n\n### `all_logger` - Comprehensive Logging\n\nThe root logger for the entire system:\n\n```python\nall_logger_file_name = os.path.join(sub_folder_for_logs, \"all.log\")\nall_logger = setup_logger(\n    logger_name=\"root.all\",\n    log_file=all_logger_file_name,\n    level=logging.DEBUG\n)\n```\n\n**Purpose:** Receives all log messages from child loggers through propagation, providing a complete system log.\n\n**Sources:** [src/utils/log_decorator.py:292-294]()\n\n### `traceable` - Detailed Function Tracing\n\nA lambda-wrapped decorator for comprehensive function instrumentation:\n\n```python\ntraceable_logger_file_name = os.path.join(sub_folder_for_logs, \"trace.log\")\ntraceable = lambda func: log_function(\n    logger_name=\"root.all.trace\",\n    log_file=traceable_logger_file_name,\n    exclude_args=[\"password\", \"token\", \"secret\"],\n    level=logging.DEBUG\n)(func)\n```\n\n**Key Features:**\n- Records full call stacks\n- Excludes sensitive parameters (`password`, `token`, `secret`)\n- Captures timing and exceptions\n- Used for tools and executors\n\n**Sources:** [src/utils/log_decorator.py:296-302]()\n\n### `global_logger` - User Operation Logging\n\nThe logger used for manual application-level logging:\n\n```python\nglobal_logger_file_name = os.path.join(sub_folder_for_logs, \"print.log\")\nglobal_logger = setup_logger(\n    logger_name=\"root.all.print\",\n    log_file=global_logger_file_name,\n    level=logging.DEBUG\n)\n```\n\n**Purpose:** Used in the agent for logging user queries, tool outputs, and final answers. Does not use the decorator; requires manual `logger.info()` calls.\n\n**Sources:** [src/utils/log_decorator.py:304-306]()\n\n---\n\n## Integration with Application Code\n\n### Agent Integration\n\nThe deep research agent uses `global_logger` for milestone logging:\n\n```python\nfrom src.utils import global_logger, traceable\n\ndef user_query(user_input):\n    user_hint = \"ç¨æ·è¾å¥ï¼\"\n    global_logger.info(f\"{user_hint} ï¼ {user_input}\\n\\n\")\n    \n    # ... LLM interaction ...\n    \n    global_logger.info(f\"å·¥å· tool call è¾åºä¿¡æ¯ï¼ {tool_output}\\n\")\n    global_logger.info(\"-\" * 60)\n    \n    global_logger.info(f\"æç»ç­æ¡ï¼ {assistant_output.content}\")\n```\n\n**Pattern:** The agent uses `global_logger` for human-readable milestone events (user input, tool outputs, final answers) rather than fine-grained function tracing.\n\n**Sources:** [src/agent/deep_research.py:4](), [src/agent/deep_research.py:17](), [src/agent/deep_research.py:28-29](), [src/agent/deep_research.py:44-45](), [src/agent/deep_research.py:62-63](), [src/agent/deep_research.py:68-73]()\n\n### Tool and Executor Decoration\n\nWhile not shown in the provided files, tools and executors use the `@traceable` decorator for automatic instrumentation:\n\n```python\n@traceable\ndef tool_function(param1, param2):\n    # Tool implementation\n    return result\n```\n\nThis pattern automatically logs:\n- Function entry with parameters\n- Execution time\n- Return values or exceptions\n- Full call stack on errors\n\n**Sources:** [src/utils/log_decorator.py:297-302]()\n\n---\n\n## Advanced Features\n\n### Automatic Default Return Values\n\nThe system includes (currently disabled) functionality to generate safe default return values on exceptions:\n\n```python\ndef get_default_return_value(func: Callable) -> Any:\n    \"\"\"\n    æ ¹æ®å½æ°è¿åå¼æ³¨è§£ï¼çæå¯¹åºçé»è®¤å¼ï¼ååºç¨ï¼\n    æ¯æï¼ åºç¡ç±»åãæ³åï¼List/Dict/Tupleï¼ãèªå®ä¹ç±»ãNone\n    \"\"\"\n    type_hints = get_type_hints(func)\n    return_type = type_hints.get(\"return\", None)\n    \n    # Handle basic types\n    if return_type == int: return 0\n    elif return_type == str: return \"\"\n    elif return_type == list: return []\n    # ... etc\n```\n\nThis feature analyzes function type hints to return appropriate default values instead of raising exceptions, enabling graceful degradation. However, it's currently disabled in favor of raising exceptions to maintain normal error flow.\n\n**Sources:** [src/utils/log_decorator.py:91-148](), [src/utils/log_decorator.py:249-257]()\n\n### Parameter Exclusion\n\nThe decorator supports excluding sensitive parameters from logs:\n\n```python\nexclude_args=[\"password\", \"token\", \"secret\"]\n```\n\nThis prevents credential leakage in log files while maintaining comprehensive logging for other parameters.\n\n**Sources:** [src/utils/log_decorator.py:157](), [src/utils/log_decorator.py:300]()\n\n### Performance Timing\n\nEvery logged function call includes microsecond-precision timing:\n\n```python\nstart_time = time.perf_counter()\n# ... function execution ...\nelapsed_time = (time.perf_counter() - start_time) * 1000  # Convert to ms\n```\n\nThis enables performance analysis across the entire system. See [Tracing and Performance Monitoring](#6.3) for analysis techniques.\n\n**Sources:** [src/utils/log_decorator.py:183](), [src/utils/log_decorator.py:224]()\n\n---\n\n## Usage Examples\n\n### Example: Manual Logging\n\n```python\nfrom src.utils import global_logger\n\ndef process_query(query):\n    global_logger.info(f\"Processing query: {query}\")\n    # ... processing ...\n    global_logger.info(f\"Query complete\")\n```\n\n### Example: Automatic Function Tracing\n\n```python\nfrom src.utils import traceable\n\n@traceable\ndef test_function(a: int, b: str, c: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Test function with automatic tracing\"\"\"\n    return {\"result\": a + int(b) + c[\"key\"]}\n```\n\n**Log Output:**\n```\n[2024-01-15 14:30:45.123]  ãè°ç¨å¼å§ã æ è·¯å¾ï¼ __main__.None.test_function | å¼å§æ¶é´ï¼ 2024-01-15 14:30:45.123456 | ä½ç½®åæ°ï¼ (1, '2', {'key': 3}) | å³é®å­åæ°ï¼ {}\n[2024-01-15 14:30:45.125]  ãè°ç¨æ ã \n          test.py:10 <module>\n[2024-01-15 14:30:45.126]  ãè°ç¨æåã æ è·¯å¾ï¼ __main__.None.test_function | èæ¶ï¼ 3.456ms | è¿åå¼ï¼ {'result': 6}\n```\n\n**Sources:** [src/utils/log_decorator.py:310-319]()\n\n---\n\n## Logger Configuration Summary\n\n| Logger | Name | File | Used For | Decorated |\n|--------|------|------|----------|-----------|\n| `all_logger` | `root.all` | `all.log` | Receives all logs via propagation | N/A |\n| `traceable` | `root.all.trace` | `trace.log` | Function-level tracing with stacks | Decorator |\n| `global_logger` | `root.all.print` | `print.log` | Agent milestone events | Manual calls |\n\nAll loggers write to timestamped subdirectories under `logs/` and use rotating file handlers (10MB max, 5 backups) with UTF-8 encoding.\n\n**Sources:** [src/utils/log_decorator.py:288-306]()\n\n---\n\n# Page: Log Files and Analysis\n\n# Log Files and Analysis\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/utils/__pycache__/log_decorator.cpython-312.pyc](src/utils/__pycache__/log_decorator.cpython-312.pyc)\n\n</details>\n\n\n\nThis document describes the logging infrastructure in the algo_agent system, including the structure and content of various log files, how to interpret them, and techniques for analyzing logs for debugging and performance monitoring.\n\nFor information about the logging system architecture and decorators, see [Logging System Architecture](#6.1). For performance profiling with the `@traceable` decorator, see [Tracing and Performance Monitoring](#6.3).\n\n---\n\n## Overview\n\nThe algo_agent system maintains multiple specialized log files that capture different aspects of system execution. These logs provide comprehensive visibility into user operations, system-level function calls, code execution, and errors. All log files are stored in the `logs/` directory with rotating file handlers to manage disk space.\n\n**Key Log Files:**\n\n| Log File | Purpose | Content Type | Primary Use Case |\n|----------|---------|--------------|------------------|\n| `utils.log` | User operations | User queries, tool outputs, LLM responses | User interaction tracking |\n| `global.log` | System traces | Function calls, parameters, timing, stack traces | Debugging and performance analysis |\n| `trace.log` | Detailed execution | Decorated function traces with full context | Deep debugging and tracing |\n| `all.log` | Comprehensive logs | All log entries from all sources | Complete system audit |\n\nSources: [logs/utils.log:1-296](), [logs/global.log:1-1500]()\n\n---\n\n## Log File Structure and Format\n\n### Common Format Elements\n\nAll log files share a consistent timestamp format and structure:\n\n```\n[YYYY-MM-DD HH:MM:SS,mmm] <message content>\n```\n\n**Example:**\n```\n[2025-11-25 17:51:32,379]  ç¨æ·è¾å¥ï¼ ï¼ \nä½ æå¨çå·¥ä½è·¯å¾ä¸é¢ï¼å¯ä»¥è¯»åä¸ä¸æä»¶...\n```\n\nThe timestamp includes millisecond precision (`,379`) for accurate timing correlation across multiple log entries.\n\nSources: [logs/utils.log:1-5](), [logs/global.log:1-10]()\n\n---\n\n## utils.log - User Operations Log\n\n### Purpose and Content\n\n`utils.log` captures high-level user interactions and tool execution results. It provides a chronological view of:\n\n- User input queries\n- Tool execution outputs (Python code execution, task planning)\n- LLM responses (both intermediate and final)\n- Process-level errors and crashes\n\n### Log Entry Types\n\n#### User Input Entries\n\n```\n[2025-11-25 17:51:32,379]  ç¨æ·è¾å¥ï¼ ï¼ \n<user query content>\n```\n\nThese entries mark the start of a new user interaction cycle.\n\n#### Tool Output Entries\n\n```\n[2025-11-25 17:52:14,152]  å·¥å·è¾åºä¿¡æ¯ï¼ \n\n[2025-11-25 17:52:14,153]  ------------------------------------------------------------\n```\n\nTool outputs are followed by separator lines and include execution results, errors, or status messages.\n\n#### LLM Response Entries\n\n```\n[2025-11-26 03:39:09,868]  \nç¬¬2è½®å¤§æ¨¡åè¾åºä¿¡æ¯ï¼ChatCompletionMessage(content='', refusal=None, ...)\n```\n\nLLM responses show which iteration of the decision loop is running and include structured message objects with tool calls.\n\n#### Process Execution Logs\n\n```\n[2025-11-26 03:39:11,530]  å­è¿ç¨ PID: 11336 è¦å°å·¥ä½ç®å½æ´æ¹ä¸º: D:\\zyt\\git_ln\\algo_agent\\wsm\\1\\g4-1\n[2025-11-26 03:39:11,531]  å­è¿ç¨ PID: 11336 å·²å°å·¥ä½ç®å½æ´æ¹ä¸º: D:\\zyt\\git_ln\\algo_agent\\wsm\\1\\g4-1\n```\n\nThese trace subprocess creation and working directory changes for isolated code execution.\n\n#### Error Messages\n\n```\n[2025-11-26 03:39:11,797]  å·¥å·è¾åºä¿¡æ¯ï¼ ## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\nProcess Process-1:\nTraceback (most recent call last):\n  File \"D:\\zyt\\git_ln\\algo_agent\\src\\runtime\\subprocess_python_executor.py\", line 44, in _worker_with_pipe\n    exec(command, _globals, _locals)\n  File \"<string>\", line 4, in <module>\n  File \"C:\\Users\\...\\json\\__init__.py\", line 293, in load\n    return loads(fp.read(),\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265: illegal multibyte sequence\n```\n\nError entries include full stack traces, exception types, and contextual information for debugging.\n\nSources: [logs/utils.log:1-296]()\n\n---\n\n## global.log - System-Level Traces\n\n### Purpose and Content\n\n`global.log` provides detailed function-level tracing with automatic parameter capture, timing, and return value logging. This file is generated by the `@log_function` decorator and captures the complete execution flow.\n\n### Function Call Structure\n\nEach function call is logged with three markers:\n\n#### 1. Call Start Marker\n\n```\n[2025-11-25 17:51:32,252]  ãè°ç¨å¼å§ã æ è·¯å¾ï¼ __main__.None.user_query | å¼å§æ¶é´ï¼ 2025-11-25 17:51:32.252333 | ä½ç½®åæ°ï¼ (...) | å³é®å­åæ°ï¼ {}\n```\n\n**Components:**\n- **æ è·¯å¾ (Stack Path):** Module.Class.Function format (e.g., `__main__.None.user_query`)\n- **å¼å§æ¶é´ (Start Time):** Precise timestamp with microseconds\n- **ä½ç½®åæ° (Positional Arguments):** Full dump of all positional arguments\n- **å³é®å­åæ° (Keyword Arguments):** Dictionary of keyword arguments\n\n#### 2. Call Stack Trace\n\n```\n[2025-11-25 17:51:32,379]  ãè°ç¨æ ã \n          D:\\zyt\\git_ln\\algo_agent\\src\\utils\\log_decorator.py:202 wrapper\n          d:\\zyt\\git_ln\\algo_agent\\src\\agent\\deep_research.py:110 <module>\n```\n\nShows the complete call stack with file paths and line numbers, enabling precise location tracking.\n\n#### 3. Call Completion Marker\n\n**Success:**\n```\n[2025-11-25 17:51:32,385]  ãè°ç¨æåã æ è·¯å¾ï¼ memory.None.init_messages_with_system_prompt | èæ¶ï¼ 2.031ms | è¿åå¼ï¼ [...]\n```\n\n**Failure:**\n```\n[2025-11-26 03:39:14,380]  ãè°ç¨å¤±è´¥ã æ è·¯å¾ï¼ ... | èæ¶ï¼ 12.345ms | å¼å¸¸ä½ç½®ï¼ ... | å¼å¸¸ç±»åï¼ ... | å¼å¸¸ä¿¡æ¯ï¼ ... | å æ ä¿¡æ¯ï¼ ...\n```\n\n**Timing Information:**\n- **èæ¶ (Elapsed Time):** Execution duration in milliseconds with microsecond precision\n- **è¿åå¼ (Return Value):** Full dump of return value (for success)\n- **å¼å¸¸ä¿¡æ¯ (Exception Info):** Complete exception details (for failure)\n\nSources: [logs/global.log:1-1000]()\n\n---\n\n## Log Analysis Techniques\n\n### Tracing User Request Flow\n\nTo trace a complete user request from input to output:\n\n```mermaid\ngraph TD\n    Start[\"User Input in utils.log\"]\n    AgentStart[\"Agent function call in global.log<br/>ãè°ç¨å¼å§ã __main__.None.user_query\"]\n    MemoryInit[\"Memory initialization<br/>ãè°ç¨å¼å§ã memory.None.init_messages_with_system_prompt\"]\n    LLMCall[\"LLM call<br/>ãè°ç¨å¼å§ã llm.None.generate_assistant_output_append\"]\n    ToolCall[\"Tool execution<br/>ãè°ç¨å¼å§ã action.None.call_tools_safely\"]\n    CodeExec[\"Subprocess execution in utils.log<br/>å­è¿ç¨ PID: XXXXX\"]\n    Result[\"Tool output in utils.log\"]\n    AgentEnd[\"Agent completion<br/>ãè°ç¨æåã __main__.None.user_query\"]\n    \n    Start --> AgentStart\n    AgentStart --> MemoryInit\n    MemoryInit --> LLMCall\n    LLMCall --> ToolCall\n    ToolCall --> CodeExec\n    CodeExec --> Result\n    Result --> AgentEnd\n    \n    style Start fill:#f9f9f9\n    style AgentEnd fill:#f9f9f9\n```\n\n**Search Pattern:**\n1. Find user input timestamp in `utils.log`: `[YYYY-MM-DD HH:MM:SS,mmm]  ç¨æ·è¾å¥`\n2. Search `global.log` for same timestamp range and `user_query` function\n3. Follow the call stack through child function calls\n4. Track tool execution results back in `utils.log`\n\nSources: [logs/utils.log:1-100](), [logs/global.log:1-500]()\n\n### Identifying Performance Bottlenecks\n\nThe `èæ¶` (elapsed time) field in `global.log` enables performance analysis:\n\n```\n[2025-11-25 17:52:14,072]  ãè°ç¨æåã æ è·¯å¾ï¼ llm.None.generate_chat_completion | èæ¶ï¼ 41635.670ms | è¿åå¼ï¼ ChatCompletion(...)\n```\n\n**Interpretation:**\n- Times above 10,000ms (10 seconds) indicate LLM API calls\n- Times above 1,000ms (1 second) may indicate I/O operations or subprocess execution\n- Times below 100ms indicate in-process operations\n\n**Analysis Workflow:**\n\n```mermaid\ngraph LR\n    Extract[\"Extract timing data<br/>grep 'èæ¶'\"]\n    Parse[\"Parse milliseconds<br/>awk/sed extraction\"]\n    Sort[\"Sort by duration<br/>sort -n\"]\n    Identify[\"Identify outliers<br/>> 5000ms\"]\n    Correlate[\"Correlate with<br/>function names\"]\n    \n    Extract --> Parse\n    Parse --> Sort\n    Sort --> Identify\n    Identify --> Correlate\n```\n\nSources: [logs/global.log:891-892]()\n\n### Debugging Code Execution Errors\n\nWhen Python code execution fails, logs contain detailed error information:\n\n#### Error Pattern in utils.log\n\n```\n[2025-11-26 03:39:11,797]  å·¥å·è¾åºä¿¡æ¯ï¼ ## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\nProcess Process-1:\nTraceback (most recent call last):\n  File \"D:\\zyt\\git_ln\\algo_agent\\src\\runtime\\subprocess_python_executor.py\", line 44, in _worker_with_pipe\n    exec(command, _globals, _locals)\n  File \"<string>\", line 4, in <module>\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265: illegal multibyte sequence\n### éåºç¶æç ï¼1\n```\n\n**Debugging Steps:**\n\n1. **Locate the Error:**\n   - Search for \"ä»£ç æ§è¡å´©æº\" or \"è¿ç¨å¼å¸¸éåº\" in `utils.log`\n   - Note the timestamp and PID\n\n2. **Identify Root Cause:**\n   - Read the exception type (e.g., `UnicodeDecodeError`)\n   - Examine the stack trace to find the failing line\n   - Check file and line number references\n\n3. **Check Related Context:**\n   - Search `global.log` for the same timestamp\n   - Find the tool call that triggered execution\n   - Review input parameters in the \"ä½ç½®åæ°\" section\n\n4. **Common Error Types:**\n\n| Error Type | Common Cause | Log Indicator |\n|------------|--------------|---------------|\n| `UnicodeDecodeError` | File encoding mismatch (UTF-8 vs GBK) | `'gbk' codec can't decode` |\n| `PickleError` | Non-serializable objects in globals | `cannot pickle 'TextIOWrapper'` |\n| `TimeoutError` | Code execution exceeded timeout | Status indicator in execution result |\n| `UnboundLocalError` | Variable access before assignment | `cannot access local variable` |\n\nSources: [logs/utils.log:136-287]()\n\n### Tracking Task Planning Evolution\n\nThe `RecursivePlanTreeTodoTool` creates hierarchical task plans logged in detail:\n\n```mermaid\ngraph TD\n    T1[\"T1: Read schema.json<br/>Status: pending\"]\n    T2[\"T2: Read data files<br/>Depends: T1\"]\n    T3[\"T3: Build data models<br/>Depends: T1\"]\n    T4[\"T4: Implement loader<br/>Depends: T2, T3\"]\n    T5[\"T5: Analyze distribution<br/>Depends: T4\"]\n    \n    T1 --> T2\n    T1 --> T3\n    T2 --> T4\n    T3 --> T4\n    T4 --> T5\n    \n    style T1 fill:#f9f9f9\n```\n\n**Log Location:**\n- Task creation: Search for `\"recursive_plan_tree_todo\"` in `global.log`\n- Task structure: Look for `\"tree_nodes\"` field with full JSON structure\n- Task dependencies: Examine `\"dependencies\"` arrays\n\nSources: [logs/global.log:909-1574]()\n\n---\n\n## Error Patterns and Resolution\n\n### Common Error Scenarios\n\n#### 1. Encoding Errors (UnicodeDecodeError)\n\n**Log Pattern:**\n```\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265: illegal multibyte sequence\n```\n\n**Root Cause:** Windows default encoding (GBK) conflicts with UTF-8 encoded JSON files.\n\n**Resolution:** Add `encoding='utf-8'` parameter to all file operations:\n```python\nwith open('file.json', 'r', encoding='utf-8') as f:\n    data = json.load(f)\n```\n\n**Log Location:** [logs/utils.log:132-147]()\n\n#### 2. Serialization Errors (PickleError)\n\n**Log Pattern:**\n```\nTypeError: cannot pickle 'TextIOWrapper' instances\n```\n\n**Root Cause:** File handles or other non-serializable objects in workspace globals.\n\n**Resolution:** Close file handles before function returns, or use context managers.\n\n**Log Location:** [logs/utils.log:148-179]()\n\n#### 3. Subprocess Communication Errors\n\n**Log Pattern:**\n```\n[2025-11-26 03:39:11,624]  Pipe reader error: type=EOFError, value=, kv=EOFError()\n[2025-11-26 03:39:11,770]  ---------- 2. æ­£å¸¸æå¼å¸¸éåºï¼ä»å­è¿ç¨è·å ExecutionResult\n[2025-11-26 03:39:11,777]  ---------- 2.2 å­è¿ç¨å´©æºéåºï¼å¦ SegFault\n```\n\n**Root Cause:** Subprocess crashed before sending execution result.\n\n**Debugging:** Check the stack trace above the EOFError for the actual crash reason.\n\n**Log Location:** [logs/utils.log:132-180]()\n\nSources: [logs/utils.log:130-290]()\n\n---\n\n## Log File Rotation and Management\n\n### File Size Management\n\nThe system uses `RotatingFileHandler` for automatic log rotation:\n\n- **Max Size:** Each log file can grow to a configured maximum size (default: typically 10MB)\n- **Backup Count:** Multiple backup files are maintained (e.g., `utils.log.1`, `utils.log.2`)\n- **Rotation Trigger:** When max size is reached, current log is renamed to `.1`, previous `.1` becomes `.2`, etc.\n\n### Log File Locations\n\n```\nlogs/\nâââ utils.log           # Current user operations log\nâââ global.log          # Current system traces\nâââ trace.log           # Current detailed execution traces\nâââ all.log             # Current comprehensive log\nâââ utils.log.1         # Previous rotation\nâââ utils.log.2         # Older rotation\nâââ ...\n```\n\n**Configuration:** Defined in the `setup_logger` function, which sets up handlers with rotation parameters.\n\nSources: Based on Diagram 4 from high-level architecture\n\n---\n\n## Advanced Analysis Scenarios\n\n### Correlating Logs Across Files\n\nTo fully understand a failure, correlate entries across multiple log files:\n\n```mermaid\ngraph LR\n    UtilsLog[\"utils.log<br/>Timestamp T1<br/>User query starts\"]\n    GlobalLog1[\"global.log<br/>Timestamp T1<br/>Function calls begin\"]\n    GlobalLog2[\"global.log<br/>Timestamp T2<br/>Tool execution\"]\n    UtilsLog2[\"utils.log<br/>Timestamp T2<br/>Subprocess error\"]\n    GlobalLog3[\"global.log<br/>Timestamp T3<br/>Error handling\"]\n    \n    UtilsLog -->|\"Search timestamp T1\"| GlobalLog1\n    GlobalLog1 -->|\"Follow call chain\"| GlobalLog2\n    GlobalLog2 -->|\"Check tool output\"| UtilsLog2\n    UtilsLog2 -->|\"Return to error handler\"| GlobalLog3\n```\n\n**Process:**\n1. Identify the problematic timestamp in `utils.log`\n2. Search `global.log` for entries within Â±1 second\n3. Build a timeline of function calls and events\n4. Cross-reference stack traces with error messages\n5. Identify the first point of failure in the call chain\n\n### Performance Profiling\n\nExtract timing data for statistical analysis:\n\n**Command-line example:**\n```bash\n# Extract all timing data\ngrep \"èæ¶\" logs/global.log | awk -F'èæ¶ï¼' '{print $2}' | awk -F'ms' '{print $1}' > timings.txt\n\n# Calculate statistics\ncat timings.txt | sort -n | awk '\n    BEGIN { sum=0; count=0; }\n    { values[count++]=$1; sum+=$1; }\n    END {\n        print \"Count:\", count\n        print \"Mean:\", sum/count, \"ms\"\n        print \"Median:\", values[int(count/2)], \"ms\"\n        print \"95th percentile:\", values[int(count*0.95)], \"ms\"\n        print \"Max:\", values[count-1], \"ms\"\n    }\n'\n```\n\nThis produces performance distribution metrics for identifying outliers and bottlenecks.\n\n### Call Graph Reconstruction\n\nReconstruct the call graph from stack traces:\n\n```mermaid\ngraph TD\n    MainUserQuery[\"__main__.user_query<br/>Line: deep_research.py:110\"]\n    MemoryInit[\"memory.init_messages_with_system_prompt<br/>Line: deep_research.py:19\"]\n    ToolSchema[\"tool.schema.get_tools_schema<br/>Line: deep_research.py:20\"]\n    LLMGenerate[\"llm.generate_assistant_output_append<br/>Line: deep_research.py:26\"]\n    LLMExtract[\"llm.extract_assistant_output_from_chat<br/>Line: llm.py:33\"]\n    LLMChat[\"llm.generate_chat_completion<br/>Line: llm.py:25\"]\n    ActionCall[\"action.call_tools_safely<br/>Line: deep_research.py:43\"]\n    ActionTools[\"action.call_tools<br/>Line: action.py:42\"]\n    \n    MainUserQuery --> MemoryInit\n    MainUserQuery --> ToolSchema\n    MainUserQuery --> LLMGenerate\n    LLMGenerate --> LLMExtract\n    LLMExtract --> LLMChat\n    MainUserQuery --> ActionCall\n    ActionCall --> ActionTools\n    \n    style MainUserQuery fill:#f9f9f9\n```\n\n**Data Source:** Stack traces in the \"ãè°ç¨æ ã\" sections of `global.log`.\n\nSources: [logs/global.log:54-115](), [logs/global.log:496-694]()\n\n---\n\n## Best Practices for Log Analysis\n\n### 1. Use Timestamps for Correlation\n\nAlways correlate log entries using precise timestamps. The millisecond precision allows exact matching across files.\n\n### 2. Follow the Call Chain\n\nStart from high-level user operations in `utils.log`, then drill down into `global.log` for detailed function traces.\n\n### 3. Understand Log Hierarchy\n\n- `utils.log`: User-facing events\n- `global.log`: System-level function traces\n- `trace.log`: Detailed instrumented traces\n- `all.log`: Complete audit trail\n\n### 4. Look for Pattern Markers\n\nKey markers to search for:\n- `ç¨æ·è¾å¥` - User query start\n- `ãè°ç¨å¼å§ã` - Function entry\n- `ãè°ç¨æåã` - Successful completion\n- `ãè°ç¨å¤±è´¥ã` - Function failure\n- `ä»£ç æ§è¡å´©æº` - Code execution crash\n- `Pipe reader error` - Subprocess communication failure\n\n### 5. Check Both Success and Failure Paths\n\nEven when debugging failures, review successful executions to understand the expected flow.\n\n### 6. Monitor Execution Times\n\nTrack `èæ¶` values to identify:\n- LLM API calls (typically 10-60 seconds)\n- Code execution (varies, typically < 5 seconds)\n- In-process operations (typically < 100ms)\n\nUnusual timing values often indicate problems even when errors aren't explicitly logged.\n\nSources: [logs/utils.log:1-296](), [logs/global.log:1-2000]()\n\n---\n\n# Page: Tracing and Performance Monitoring\n\n# Tracing and Performance Monitoring\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/utils/__pycache__/log_decorator.cpython-312.pyc](src/utils/__pycache__/log_decorator.cpython-312.pyc)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page documents the tracing and performance monitoring capabilities in the algo_agent system. It covers how to use the `@traceable` decorator to instrument functions, how to read and analyze execution traces in log files, and techniques for identifying performance bottlenecks.\n\nFor information about the underlying logging infrastructure and decorator implementation, see [Logging System Architecture](#6.1). For details on the structure and content of specific log files, see [Log Files and Analysis](#6.2).\n\n---\n\n## Overview\n\nThe algo_agent system provides comprehensive tracing through the `@log_function` decorator, which automatically captures:\n\n- **Function entry and exit** with timestamps\n- **Execution time** in milliseconds with microsecond precision\n- **Call stack paths** showing the complete invocation hierarchy\n- **Arguments and return values** with intelligent serialization\n- **Exception traces** with full stack information\n\nAll traced functions write to [logs/global.log]() and [logs/trace.log](), enabling post-execution performance analysis and debugging.\n\n---\n\n## Trace Log Format\n\n### Standard Trace Entry Structure\n\nEach function invocation generates three log entries:\n\n```\n[timestamp] ãè°ç¨å¼å§ã æ è·¯å¾ï¼ <stack_path> | å¼å§æ¶é´ï¼ <datetime> | ä½ç½®åæ°ï¼ <args> | å³é®å­åæ°ï¼ <kwargs>\n[timestamp] ãè°ç¨æ ã \n          <file_path:line> <function>\n          <file_path:line> <caller_function>\n          ...\n[timestamp] ãè°ç¨æåã æ è·¯å¾ï¼ <stack_path> | èæ¶ï¼ <time>ms | è¿åå¼ï¼ <result>\n```\n\n**Example from logs/global.log:**\n\n```\n[2025-11-25 17:51:32,252] ãè°ç¨å¼å§ã æ è·¯å¾ï¼ __main__.None.user_query | å¼å§æ¶é´ï¼ 2025-11-25 17:51:32.252333 | ä½ç½®åæ°ï¼ (...)\n[2025-11-25 17:51:32,379] ãè°ç¨æ ã \n          D:\\zyt\\git_ln\\algo_agent\\src\\utils\\log_decorator.py:202 wrapper\n          d:\\zyt\\git_ln\\algo_agent\\src\\agent\\deep_research.py:110 <module>\n[2025-11-25 17:52:14,089] ãè°ç¨æåã æ è·¯å¾ï¼ llm.None.generate_assistant_output_append | èæ¶ï¼ 41680.934ms | è¿åå¼ï¼ <ChatCompletionMessage>\n```\n\n**Sources:** [logs/global.log:1-900]()\n\n### Stack Path Format\n\nThe stack path follows the pattern: `<module>.<class>.<function>`\n\n- `__main__.None.user_query` - top-level function in main module\n- `llm.None.generate_chat_completion` - module-level function in llm\n- `action.None.call_tools_safely` - function in action module\n\n**Sources:** [logs/global.log:1-900]()\n\n---\n\n## Timing Measurement Architecture\n\n### How Execution Time is Captured\n\n```mermaid\ngraph TB\n    FunctionEntry[\"Function Entry<br/>@log_function decorated\"]\n    StartTimer[\"Record start_time<br/>time.perf_counter()\"]\n    StartLog[\"Log ãè°ç¨å¼å§ã<br/>with timestamp and args\"]\n    \n    Execute[\"Execute Function Body\"]\n    \n    Success[\"Success Path\"]\n    Exception[\"Exception Path\"]\n    \n    EndTimer[\"Calculate elapsed_time<br/>end - start (ms)\"]\n    SuccessLog[\"Log ãè°ç¨æåã<br/>with elapsed_time and result\"]\n    \n    ExceptionLog[\"Log ãè°ç¨å¤±è´¥ã<br/>with elapsed_time and traceback\"]\n    \n    Return[\"Return Result\"]\n    Raise[\"Re-raise Exception\"]\n    \n    FunctionEntry --> StartTimer\n    StartTimer --> StartLog\n    StartLog --> Execute\n    \n    Execute --> Success\n    Execute --> Exception\n    \n    Success --> EndTimer\n    Exception --> EndTimer\n    \n    EndTimer --> SuccessLog\n    EndTimer --> ExceptionLog\n    \n    SuccessLog --> Return\n    ExceptionLog --> Raise\n```\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n### Precision and Accuracy\n\nThe system uses `time.perf_counter()` for timing measurements, providing:\n- **Microsecond precision** (displayed as fractional milliseconds)\n- **Monotonic clock** (unaffected by system clock adjustments)\n- **Sub-millisecond accuracy** in timing differences\n\n**Example timing outputs:**\n- `2.031ms` - Fast operation\n- `41680.934ms` - Long-running LLM call\n- `4.624ms` - Quick utility function\n\n**Sources:** [logs/global.log:115-900]()\n\n---\n\n## Analyzing Performance from Logs\n\n### Identifying Slow Operations\n\n#### Workflow for Performance Analysis\n\n```mermaid\ngraph LR\n    ReadLog[\"Read logs/global.log\"]\n    ExtractTiming[\"Extract ãè°ç¨æåã entries<br/>with èæ¶ field\"]\n    SortByTime[\"Sort by execution time\"]\n    IdentifyBottleneck[\"Identify functions > threshold\"]\n    \n    AnalyzeStack[\"Analyze ãè°ç¨æ ã<br/>for context\"]\n    CheckFrequency[\"Check call frequency<br/>in log\"]\n    \n    Optimize[\"Target optimization\"]\n    \n    ReadLog --> ExtractTiming\n    ExtractTiming --> SortByTime\n    SortByTime --> IdentifyBottleneck\n    \n    IdentifyBottleneck --> AnalyzeStack\n    IdentifyBottleneck --> CheckFrequency\n    \n    AnalyzeStack --> Optimize\n    CheckFrequency --> Optimize\n```\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n#### Example Analysis from Logs\n\n**Slowest Operations (from logs/global.log):**\n\n| Function | Execution Time | Call Count | Total Time | Bottleneck Level |\n|----------|----------------|------------|------------|------------------|\n| `generate_chat_completion` | 41635.670ms | 1 | 41635ms | **Critical** |\n| `generate_assistant_output_append` | 41680.934ms | 1 | 41680ms | **Critical** |\n| `user_query` | ~42000ms | 1 | 42000ms | **Critical** |\n| `call_tools_safely` | 34.693ms | 1 | 34ms | Moderate |\n| `get_tools_schema` | 8.008ms | 1 | 8ms | Fast |\n| `init_messages_with_system_prompt` | 2.031ms | 1 | 2ms | Fast |\n\n**Key Insight:** The LLM generation operations dominate execution time (~99% of total), with `generate_chat_completion` being the primary bottleneck.\n\n**Sources:** [logs/global.log:891-900]()\n\n---\n\n## Call Stack Analysis\n\n### Understanding Call Hierarchies\n\nThe `ãè°ç¨æ ã` entries show the complete invocation path from entry point to current function:\n\n```\nãè°ç¨æ ã \n          D:\\zyt\\git_ln\\algo_agent\\src\\utils\\log_decorator.py:202 wrapper\n          d:\\zyt\\git_ln\\algo_agent\\src\\agent\\llm.py:33 generate_assistant_output_append\n          D:\\zyt\\git_ln\\algo_agent\\src\\utils\\log_decorator.py:217 wrapper\n          d:\\zyt\\git_ln\\algo_agent\\src\\agent\\deep_research.py:26 user_query\n          D:\\zyt\\git_ln\\algo_agent\\src\\utils\\log_decorator.py:217 wrapper\n          d:\\zyt\\git_ln\\algo_agent\\src\\agent\\deep_research.py:110 <module>\n```\n\n**Reading the Stack (bottom-to-top):**\n1. Entry point: [src/agent/deep_research.py:110]() `<module>`\n2. Calls through wrapper: [src/utils/log_decorator.py:217]()\n3. Into: [src/agent/deep_research.py:26]() `user_query`\n4. Calls through wrapper again\n5. Into: [src/agent/llm.py:33]() `generate_assistant_output_append`\n\n**Sources:** [logs/global.log:496-500](), [logs/global.log:688-694]()\n\n### Call Graph Reconstruction\n\n```mermaid\ngraph TB\n    MainModule[\"src/agent/deep_research.py<br/>main entry point\"]\n    UserQuery[\"user_query()<br/>41680ms total\"]\n    \n    InitMessages[\"init_messages_with_system_prompt()<br/>2.031ms\"]\n    GetTools[\"get_tools_schema()<br/>8.008ms\"]\n    GenAssist[\"generate_assistant_output_append()<br/>41680.934ms\"]\n    \n    ExtractOutput[\"extract_assistant_output_from_chat()<br/>41658.357ms\"]\n    GenChat[\"generate_chat_completion()<br/>41635.670ms\"]\n    \n    HasTool[\"has_tool_call()<br/>4.624ms\"]\n    CallTools[\"call_tools_safely()<br/>34.693ms\"]\n    CallToolsInner[\"call_tools()<br/>9.949ms\"]\n    \n    MainModule --> UserQuery\n    UserQuery --> InitMessages\n    UserQuery --> GetTools\n    UserQuery --> GenAssist\n    \n    GenAssist --> ExtractOutput\n    ExtractOutput --> GenChat\n    \n    UserQuery --> HasTool\n    UserQuery --> CallTools\n    CallTools --> CallToolsInner\n```\n\n**Sources:** [logs/global.log:1-1574]()\n\n---\n\n## Argument and Return Value Tracing\n\n### Serialization Strategy\n\nThe `log_function` decorator uses intelligent serialization for arguments and return values:\n\n**Supported Types:**\n- Primitive types: `str`, `int`, `float`, `bool`\n- Collections: `list`, `dict`, `tuple`, `set`\n- Pydantic models: Serialized via `.dict()` or JSON\n- Custom objects: Attempts `__dict__` inspection or pretty-print\n\n**Truncation Rules:**\n- Large strings: Show first 1000 characters + `\"...\"`\n- Long lists: Show first 10 items + `\"...\"`\n- Deep structures: Limited recursion depth\n\n**Example - Complex Return Value:**\n```\nè¿åå¼ï¼ ChatCompletionMessage(content='', refusal=None, role='assistant', ...)\n```\n\n**Sources:** [logs/global.log:891-892]()\n\n### Filtering Sensitive Data\n\nThe decorator automatically filters sensitive fields from logs:\n- `password`\n- `token`\n- `secret`\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n---\n\n## Exception Tracing\n\n### Exception Capture Format\n\nWhen a traced function raises an exception:\n\n```\nãè°ç¨å¤±è´¥ã æ è·¯å¾ï¼ <path> | èæ¶ï¼ <time>ms | å¼å¸¸ä½ç½®ï¼ <location> | å¼å¸¸ç±»åï¼ <type> | å¼å¸¸ä¿¡æ¯ï¼ <message> | å æ ä¿¡æ¯ï¼ <traceback>\n```\n\n**Example from logs/utils.log:**\n```\n## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\nProcess Process-1:\nTraceback (most recent call last):\n  File \"D:\\zyt\\git_ln\\algo_agent\\src\\runtime\\subprocess_python_executor.py\", line 44, in _worker_with_pipe\n    exec(command, _globals, _locals)\n  File \"<string>\", line 4, in <module>\n  File \"C:\\Users\\zooos\\AppData\\Roaming\\uv\\python\\cpython-3.12.11-windows-x86_64-none\\Lib\\json\\__init__.py\", line 293, in load\n    return loads(fp.read(),\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265: illegal multibyte sequence\n```\n\n**Sources:** [logs/utils.log:132-146]()\n\n### Exception Analysis Workflow\n\n```mermaid\ngraph TB\n    ExceptionRaised[\"Exception Raised<br/>in Traced Function\"]\n    CaptureTime[\"Calculate elapsed_time<br/>up to exception point\"]\n    CaptureType[\"Capture exception type<br/>and message\"]\n    \n    GetTraceback[\"Extract full traceback<br/>traceback.format_exc()\"]\n    GetLocation[\"Identify exception line<br/>file:line\"]\n    \n    LogFailure[\"Log ãè°ç¨å¤±è´¥ã entry<br/>with all details\"]\n    Reraise[\"Re-raise exception<br/>(for upstream handling)\"]\n    \n    ExceptionRaised --> CaptureTime\n    ExceptionRaised --> CaptureType\n    ExceptionRaised --> GetTraceback\n    ExceptionRaised --> GetLocation\n    \n    CaptureTime --> LogFailure\n    CaptureType --> LogFailure\n    GetTraceback --> LogFailure\n    GetLocation --> LogFailure\n    \n    LogFailure --> Reraise\n```\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n---\n\n## Performance Metrics Extraction\n\n### Key Metrics Available\n\nFrom trace logs, you can extract:\n\n1. **Per-Function Metrics**\n   - Average execution time\n   - Min/max execution time\n   - Call frequency\n   - Failure rate\n\n2. **System-Wide Metrics**\n   - Total request processing time\n   - Time distribution by component\n   - Bottleneck identification\n   - Concurrency level (via process/thread IDs)\n\n### Example Metrics Calculation\n\n**From logs/global.log analysis:**\n\n```python\n# Pseudocode for metrics extraction\nmetrics = {\n    \"user_query\": {\n        \"total_time\": 42000,  # ms\n        \"calls\": 1,\n        \"avg_time\": 42000\n    },\n    \"generate_chat_completion\": {\n        \"total_time\": 41635.670,\n        \"calls\": 1,\n        \"percentage_of_total\": 99.1  # % of user_query time\n    },\n    \"get_tools_schema\": {\n        \"total_time\": 8.008,\n        \"calls\": 1,\n        \"percentage_of_total\": 0.02\n    }\n}\n```\n\n**Sources:** [logs/global.log:1-1574]()\n\n---\n\n## Practical Usage Patterns\n\n### Pattern 1: Identifying LLM Call Overhead\n\n**Goal:** Measure time spent in LLM API calls vs. local processing\n\n**Steps:**\n1. Search logs for `generate_chat_completion` entries\n2. Extract execution times for all LLM calls\n3. Compare to parent function (`generate_assistant_output_append`) time\n4. Identify overhead from serialization/network\n\n**Finding:** In the example trace, LLM call takes 41635ms of 41680ms total (99.9% efficiency).\n\n**Sources:** [logs/global.log:891-892]()\n\n### Pattern 2: Finding Redundant Tool Calls\n\n**Goal:** Detect repeated tool invocations with identical parameters\n\n**Steps:**\n1. Extract all `call_tools_safely` entries\n2. Compare `tool_call_arguments` fields\n3. Identify duplicate calls within same query\n4. Calculate wasted time from duplicates\n\n**Sources:** [logs/global.log:908-1574]()\n\n### Pattern 3: Detecting Memory Serialization Bottlenecks\n\n**Goal:** Find slow serialization in workspace globals\n\n**Steps:**\n1. Look for `PickleError` or `UnicodeDecodeError` in logs\n2. Check elapsed time for functions that filter/serialize globals\n3. Identify problematic object types\n\n**Example Issue Found:**\n```\nTypeError: cannot pickle 'TextIOWrapper' instances\n```\n**Root Cause:** File handles left in global scope during code execution\n\n**Sources:** [logs/utils.log:132-231]()\n\n---\n\n## Tracing Configuration\n\n### Decorator Application\n\nTo enable tracing on a function, apply the `@log_function` decorator:\n\n```python\nfrom src.utils.log_decorator import log_function\n\n@log_function(\n    exclude_args=['password', 'secret_key'],  # Filter sensitive args\n    record_stack=True,  # Include call stack\n    default_return_value=None  # Fallback if exception during logging\n)\ndef my_function(arg1, arg2):\n    # Function implementation\n    pass\n```\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n### Logger Selection\n\nThe system uses multiple loggers for different purposes:\n\n| Logger | Log File | Use Case |\n|--------|----------|----------|\n| `global_logger` | logs/global.log | System-level function traces |\n| `traceable_logger` | logs/trace.log | Detailed execution traces with stack info |\n| `all_logger` | logs/all.log | Comprehensive logs (all sources) |\n\nFunctions decorated with `@log_function` write to `global_logger` by default.\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n---\n\n## Bottleneck Identification Guide\n\n### Common Bottleneck Patterns\n\n#### 1. External API Calls (Most Common)\n\n**Signature in Logs:**\n- Function names containing `generate`, `call`, `completion`\n- Execution times in seconds (1000+ ms)\n- Low call count but high individual time\n\n**Example:**\n```\nãè°ç¨æåã æ è·¯å¾ï¼ llm.None.generate_chat_completion | èæ¶ï¼ 41635.670ms\n```\n\n**Action:** Consider caching, async execution, or response streaming.\n\n**Sources:** [logs/global.log:891]()\n\n#### 2. Serialization Overhead\n\n**Signature in Logs:**\n- Functions in `workspace.py` or `schemas.py`\n- `PickleError` or deep copy exceptions\n- Multiple consecutive calls with similar times\n\n**Action:** Optimize data structures, avoid unnecessary deep copies.\n\n**Sources:** [logs/utils.log:150-178]()\n\n#### 3. Repeated Computation\n\n**Signature in Logs:**\n- Same function called multiple times with identical args\n- Each call has similar execution time\n- Total time = sum of individual calls\n\n**Action:** Implement memoization or caching layer.\n\n---\n\n## Integration with Other Monitoring Tools\n\n### Log Aggregation\n\nThe trace logs can be ingested into standard log aggregation tools:\n\n**Compatible Formats:**\n- **ELK Stack:** Parse structured fields from log lines\n- **Splunk:** Index on timestamp and stack path\n- **Prometheus:** Extract timing metrics via log scraping\n- **Grafana Loki:** Query by function name and execution time\n\n**Key Fields for Extraction:**\n- Timestamp: `[2025-11-25 17:51:32,252]`\n- Stack path: `llm.None.generate_chat_completion`\n- Execution time: `41635.670ms`\n- Status: `ãè°ç¨æåã` or `ãè°ç¨å¤±è´¥ã`\n\n**Sources:** [logs/global.log:1-1574]()\n\n### Trace Visualization\n\n```mermaid\ngraph LR\n    LogFile[\"logs/global.log<br/>Raw trace data\"]\n    Parser[\"Log Parser<br/>Extract timing data\"]\n    \n    TimeSeriesDB[\"Time Series DB<br/>Store metrics\"]\n    Dashboard[\"Grafana Dashboard<br/>Visualize performance\"]\n    \n    FlameGraph[\"Flame Graph<br/>Call hierarchy\"]\n    \n    LogFile --> Parser\n    Parser --> TimeSeriesDB\n    Parser --> FlameGraph\n    \n    TimeSeriesDB --> Dashboard\n```\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n---\n\n## Best Practices\n\n### DO:\n- â Use `@log_function` on all public API functions\n- â Use `@log_function` on functions that perform I/O or external calls\n- â Set `exclude_args` for sensitive parameters\n- â Monitor log file sizes and implement rotation\n- â Aggregate timing metrics for trend analysis\n- â Use execution times to guide optimization efforts\n\n### DON'T:\n- â Trace hot path functions called thousands of times (logging overhead)\n- â Log large data structures without truncation (disk space)\n- â Use tracing in production without log rotation (disk full)\n- â Rely on tracing for real-time monitoring (use metrics instead)\n- â Forget to filter sensitive data from traces (security risk)\n\n### Performance Impact\n\n**Logging Overhead:**\n- Function entry/exit: ~0.1-0.5ms per call\n- Stack trace capture: ~1-2ms per call\n- Argument serialization: Variable (0.1-10ms depending on size)\n\n**Recommendation:** For functions called > 100 times per second, consider sampling (trace 1 in N calls).\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n---\n\n## Troubleshooting Guide\n\n### Issue: Missing Trace Entries\n\n**Symptom:** Expected function not appearing in logs\n\n**Possible Causes:**\n1. Function not decorated with `@log_function`\n2. Logger not properly initialized\n3. Log level too high (set to WARNING or ERROR)\n4. Exception during logging itself\n\n**Solution:** Verify decorator application and check logger configuration.\n\n---\n\n### Issue: Inaccurate Timing\n\n**Symptom:** Execution time doesn't match wall clock time\n\n**Possible Causes:**\n1. System clock adjustment during execution\n2. Nested decorator timing (measure includes decorator overhead)\n3. Concurrent execution (parallel threads/processes)\n\n**Solution:** Use `perf_counter()` for relative timing, avoid comparing across processes.\n\n---\n\n### Issue: Log File Too Large\n\n**Symptom:** logs/global.log growing to GB size\n\n**Possible Causes:**\n1. No log rotation configured\n2. Logging large return values\n3. High-frequency function tracing\n\n**Solution:** Enable `RotatingFileHandler` with size limits, increase truncation limits.\n\n**Sources:** High-level system diagram 4 (Observability and Logging Architecture)\n\n---\n\n# Page: Use Cases and Examples\n\n# Use Cases and Examples\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/agent/deep_research.py](src/agent/deep_research.py)\n- [src/utils/log_decorator.py](src/utils/log_decorator.py)\n\n</details>\n\n\n\nThis page demonstrates real-world usage patterns of the `algo_agent` system through concrete examples extracted from actual execution logs and code samples. It illustrates how the agent orchestrates tools, handles complex multi-step tasks, and processes data-intensive workloads.\n\nFor implementation details on tool execution, see [Python Code Execution Tool](#4.2). For task planning structures, see [Recursive Task Planning Tool](#4.3). For troubleshooting common issues encountered in these examples, see [Common Execution Errors](#10.1).\n\n---\n\n## Overview of Use Case Categories\n\nThe system has been deployed across several application domains:\n\n| Domain | Primary Tools Used | Typical Workflow |\n|--------|-------------------|------------------|\n| Emergency Response Planning | `RecursivePlanTreeTodoTool`, `ExecutePythonCodeTool` | Schema analysis â Data loading â Algorithm design â Performance testing |\n| Geographic Data Processing | `ExecutePythonCodeTool` | Data reading â Filtering â Graph construction â Visualization |\n| Multi-Day Travel Planning | `ExecutePythonCodeTool` | Route optimization â Time constraints â Metro/subway integration |\n| Pydantic Data Modeling | `ExecutePythonCodeTool` | Schema validation â Type checking â Data transformation |\n\n---\n\n## End-to-End Example: Emergency Response Planning\n\n### Use Case Description\n\nThe agent is tasked with analyzing emergency response data to design and test multi-agent scheduling algorithms. This involves reading JSON schemas, loading multiple data files, building Python data models, and implementing various scheduling strategies.\n\n**Input Data:**\n- `schema.json` - Complete data structure schema\n- `emergency_response_data_01.json` through `emergency_response_data_05.json` - Five scenario datasets\n\n**User Query (from logs):**\n\n```\nä½ çç®æ æ¯åºäºä»¥ä¸æ°æ®ç¹ç¹åä½¿ç¨å»ºè®®ï¼å¶å®ä¸ä¸ªè¯¦ç»çç ç©¶è®¡åï¼å¸®å©ä½ æ·±å¥äºè§£æ°æ®ç»æï¼\nå¹¶åæå¦ä½å©ç¨è¿äºæ°æ®è§£å³åºæ¥ç©èµè¿è¾è°åº¦ä¸­çé®é¢ã\nè¯·ååºå·ä½çä»»å¡æ­¥éª¤ï¼åæ¬ä½ä¸éäºä»¥ä¸æ¹é¢ï¼\n1. æ°æ®ç»æåæï¼å¦ä½çè§£åè§£æ schema.json ä¸­å®ä¹çåä¸ªå®ä½åå¶å³ç³»ã\n2. æ°æ®çæé»è¾ï¼å¦ä½çè§£éæºæ°æ®çæçç­ç¥åæ¹æ³ã\n3. åºæ¯æ¨¡æï¼å¦ä½å©ç¨çæçæ°æ®æ¨¡æä¸åçåºæ¥ææ´åºæ¯ã\n4. ç®æ³æµè¯ï¼å¦ä½è®¾è®¡å¤ç§ä¸åçç®æ³å®éªæ¥æµè¯å¤æºè½ä½ååè°åº¦ç®æ³çæ§è½åææã\n```\n\n### Agent Execution Flow\n\n```mermaid\ngraph TB\n    UserQuery[\"User Query:<br/>Emergency Response Planning\"]\n    \n    InitPlan[\"LLM generates initial plan<br/>RecursivePlanTreeTodoTool\"]\n    \n    Tasks[\"Task Decomposition<br/>T1-T12 created\"]\n    \n    T1[\"T1: Read schema.json<br/>ExecutePythonCodeTool\"]\n    T2[\"T2: Read data files 01-05<br/>ExecutePythonCodeTool\"]\n    T3[\"T3: Build data model classes<br/>(Pydantic/dataclass)\"]\n    \n    Error[\"UnicodeDecodeError<br/>gbk vs utf-8\"]\n    Retry[\"LLM analyzes error<br/>Suggests encoding fix\"]\n    \n    T4[\"T4: Implement data loader<br/>Batch loading + validation\"]\n    T5[\"T5: Analyze data distribution<br/>Statistics + visualization\"]\n    \n    T6T7[\"T6: Simulate single scenario<br/>T7: Design scheduler framework\"]\n    T8T9[\"T8: Greedy algorithm<br/>T9: Weighted scoring model\"]\n    T10T11[\"T10: Path replanning<br/>T11: Evaluate metrics\"]\n    T12[\"T12: Run comparison experiments<br/>Generate report\"]\n    \n    UserQuery --> InitPlan\n    InitPlan --> Tasks\n    Tasks --> T1\n    Tasks --> T2\n    Tasks --> T3\n    T1 --> Error\n    T2 --> Error\n    Error --> Retry\n    Retry --> T1\n    T2 --> T4\n    T3 --> T4\n    T4 --> T5\n    T4 --> T6T7\n    T6T7 --> T8T9\n    T8T9 --> T10T11\n    T10T11 --> T12\n    \n    style Error fill:#ffcccc\n    style Retry fill:#ffffcc\n```\n\n**Sources:** [logs/utils.log:1-296](), [logs/global.log:1-1792](), [src/agent/deep_research.py:15-129]()\n\n### Task Planning Structure\n\nThe agent decomposes the problem into 12 hierarchical tasks:\n\n```mermaid\ngraph TD\n    Core[\"Core Goal:<br/>Design multi-agent scheduling algorithm\"]\n    \n    T1[\"T1: Read schema.json\"]\n    T2[\"T2: Read data files 01-05<br/>depends: T1\"]\n    T3[\"T3: Build data models<br/>depends: T1\"]\n    T4[\"T4: Data loader<br/>depends: T2, T3\"]\n    T5[\"T5: Analyze distribution<br/>depends: T4\"]\n    T6[\"T6: Simulate scenario<br/>depends: T4\"]\n    T7[\"T7: Scheduler framework<br/>depends: T3\"]\n    T8[\"T8: Greedy algorithm<br/>depends: T7\"]\n    T9[\"T9: Weighted scoring<br/>depends: T8\"]\n    T10[\"T10: Path replanning<br/>depends: T9\"]\n    T11[\"T11: Evaluate metrics<br/>depends: T8, T9\"]\n    T12[\"T12: Comparison experiments<br/>depends: T11\"]\n    \n    Core --> T1\n    Core --> T2\n    Core --> T3\n    T1 --> T2\n    T1 --> T3\n    T2 --> T4\n    T3 --> T4\n    T4 --> T5\n    T4 --> T6\n    T3 --> T7\n    T7 --> T8\n    T8 --> T9\n    T9 --> T10\n    T8 --> T11\n    T9 --> T11\n    T11 --> T12\n```\n\n**Sources:** [logs/utils.log:56-62](), [logs/global.log:908-1791]()\n\n### Key Implementation Details\n\n#### Task T1: Schema Reading with Error Handling\n\n**Initial attempt (failed):**\n\n```python\nimport json\nwith open('schema.json', 'r') as file:\n    schema = json.load(file)\n    print(json.dumps(schema, indent=2))\n```\n\n**Error encountered:**\n```\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265: illegal multibyte sequence\n```\n\n**LLM analysis and correction:**\n```python\nimport json\ntry:\n    with open('schema.json', 'r', encoding='utf-8') as f:\n        schema = json.load(f)\n    print(json.dumps(schema, indent=2, ensure_ascii=False))\nexcept Exception as e:\n    print(f\"Error reading schema.json: {e}\")\n```\n\nThe agent identifies the root cause: Windows default `gbk` encoding vs UTF-8 file encoding, and automatically suggests the fix.\n\n**Sources:** [logs/utils.log:130-290](), [logs/global.log:132-179]()\n\n#### Task T3: Data Model Construction\n\n**Recommended approach (from task description):**\n\n- Use `dataclass` or `pydantic` for strong typing\n- Core entities: `Task`, `Carrier`, `Resource`, `Location`, `Path`, `RiskPoint`\n- Relationships: task-to-carrier assignment, path-to-risk-point association\n\n#### Task T8-T9: Algorithm Implementation\n\n**T8 - Greedy Strategy:**\n- Select nearest available carrier for each task\n- Use Haversine formula for distance calculation\n- Track carrier status (idle/busy)\n\n**T9 - Weighted Scoring:**\n- Multi-factor evaluation: distance, priority, ETA, risk coefficient\n- Normalize dimensions\n- Dynamic weight adjustment\n\n**Sources:** [logs/global.log:1041-1070]()\n\n---\n\n## Multi-Day Tourism Route Planning Example\n\n### Problem Statement\n\nFrom [src/agent/deep_research.py:93-118](), the system solves a complex optimization problem:\n\n**Core Constraints:**\n- Maximize scenic spot quality (5A > 4A > 3A) and quantity\n- Minimize commute time\n- Daily constraint: â¤8 hours (daytime only, playtime + commute)\n- Each spot visit time â¥ recommended duration\n- Night transit allowed (doesn't count toward 8-hour limit)\n\n**Transportation Rules:**\n- Metro priority: 80 km/h\n- Walking fallback: 3.6 km/h\n- Night metro doesn't count toward daily limit\n\n**Input Data:**\n- `metro-draw-schema.json`, `metro-draw-data-80%.json` - Metro network\n- `beijing_scenic_spot_schema_with_play_hours.json`, `beijing_scenic_spot_validated_data_with_play_hours.json` - Scenic spots\n- Starting point: `[116.39088849999999, 39.92767]`\n\n### Expected Output\n\n```mermaid\ngraph LR\n    Input[\"Input Data<br/>Metro + Scenic Spots\"]\n    Parse[\"Parse JSON schemas<br/>Validate data\"]\n    \n    Algo1[\"Enumerate 1-10 day plans\"]\n    Algo2[\"Optimize for each day:<br/>- Maximize 5A/4A spots<br/>- Respect time limits\"]\n    \n    Path[\"Generate detailed paths:<br/>- Day/night transit<br/>- Distance/time tracking\"]\n    \n    Output[\"Output Files (UTF-8):<br/>- Route details<br/>- Time breakdown\"]\n    \n    Viz[\"Visualization:<br/>- Numbered charts<br/>- Day/night paths<br/>- Time annotations\"]\n    \n    Input --> Parse\n    Parse --> Algo1\n    Algo1 --> Algo2\n    Algo2 --> Path\n    Path --> Output\n    Path --> Viz\n```\n\n**Sources:** [src/agent/deep_research.py:76-129]()\n\n### Code Execution Pattern\n\nThe user query explicitly requires:\n\n```python\n# From the prompt structure:\nuser_query_prompt = \"\"\"\néæ±æ¯åºäºç»å®çJSONæ°æ®æä»¶...éè¿Pythonç¼ç¨å®ç°å¤æ¥ææ¸¸è·¯çº¿è§å\n\"\"\"\n\ntool_use_prompt = \"\"\"\n1. å¿é¡»ä½¿ç¨pythonå·¥å·è¿è¡ç®æ³ç¼ç è¾åºå¾å°è®¡ç®ç­æ¡ï¼ä¸è½ç´æ¥ç»åºç­æ¡ã\n2. å¨æ¯ä¸æ¬¡è¯»åæèè®¡ç®ç»æï¼å°æ°æ®ç»å¾å­ä¸æ¥èµ·ä¸ä¸ªå¸¦ç¼å·çææä¹çåå­ï¼\n3. å¹¶å°å½åå·¥ä½è·¯å¾åå¾çåå­æ¼æ¥èµ·æ¥è¾åºå¾ççç»å¯¹è·¯å¾è¾åºåºæ¥ã\n4. å¦æå­å¨æä»¶ä¹è¦ä½¿ç¨utf-8ç¼ç å­å¨ã\n\"\"\"\n```\n\n**Key requirements:**\n- Must use `ExecutePythonCodeTool` for implementation\n- Save visualizations with numbered, meaningful filenames\n- Output absolute paths for generated images\n- Use UTF-8 encoding for all file operations\n\n**Sources:** [src/agent/deep_research.py:120-128]()\n\n---\n\n## Code Entity Mapping: From User Intent to System Execution\n\n### Agent Orchestration Layer\n\n```mermaid\ngraph TB\n    subgraph \"User Layer\"\n        UQ[\"user_query(user_input)\"]\n    end\n    \n    subgraph \"Agent Core: deep_research.py\"\n        Init[\"memory.init_messages_with_system_prompt()\"]\n        Schema[\"tool.schema.get_tools_schema()\"]\n        LLMCall[\"llm.generate_assistant_output_append()\"]\n        CheckTool[\"llm.has_tool_call()\"]\n        Dispatch[\"action.call_tools_safely()\"]\n    end\n    \n    subgraph \"Tool Layer\"\n        PyTool[\"ExecutePythonCodeTool.run()\"]\n        TodoTool[\"RecursivePlanTreeTodoTool.run()\"]\n    end\n    \n    subgraph \"Execution Layer\"\n        SubProc[\"subprocess_python_executor.run_structured_in_subprocess()\"]\n        WorkDir[\"WORKING_DIR = wsm/1/g4-1\"]\n        ExecResult[\"ExecutionResult(status, stdout, stderr, globals)\"]\n    end\n    \n    UQ --> Init\n    UQ --> Schema\n    Init --> LLMCall\n    Schema --> LLMCall\n    LLMCall --> CheckTool\n    CheckTool -->|\"has tool_calls\"| Dispatch\n    Dispatch --> PyTool\n    Dispatch --> TodoTool\n    PyTool --> SubProc\n    SubProc --> WorkDir\n    SubProc --> ExecResult\n    ExecResult --> Dispatch\n    Dispatch --> LLMCall\n```\n\n**Sources:** [src/agent/deep_research.py:15-74](), [src/agent/action.py:1-60](), [src/tool/python_tool.py:1-100]()\n\n### Tool Execution Flow with Error Recovery\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Agent as deep_research.user_query\n    participant LLM as llm.generate_assistant_output_append\n    participant Action as action.call_tools_safely\n    participant PyTool as ExecutePythonCodeTool\n    participant SubProc as subprocess_python_executor\n    participant Logger as global_logger\n    \n    User->>Agent: Emergency response planning query\n    Agent->>LLM: Generate with tool schemas\n    LLM-->>Agent: tool_calls=[execute_python_code]\n    \n    Agent->>Action: call_tools_safely(tool_info)\n    Action->>PyTool: run(code_snippet)\n    PyTool->>SubProc: run_structured_in_subprocess()\n    \n    SubProc-->>PyTool: ExecutionResult(CRASHED)<br/>UnicodeDecodeError\n    PyTool-->>Action: Error message\n    Action->>Logger: Log execution failure\n    Action-->>Agent: Return error to messages\n    \n    Agent->>LLM: Generate with error context\n    LLM-->>Agent: Analysis + corrected code<br/>with encoding='utf-8'\n    \n    Agent->>Action: Retry with fixed code\n    Action->>PyTool: run(fixed_code)\n    PyTool->>SubProc: run_structured_in_subprocess()\n    SubProc-->>PyTool: ExecutionResult(SUCCESS)\n    PyTool-->>Action: Success + output\n    Action-->>Agent: Continue workflow\n```\n\n**Sources:** [src/agent/deep_research.py:32-65](), [src/agent/action.py:15-60](), [logs/global.log:132-290]()\n\n---\n\n## Common Patterns Across Use Cases\n\n### Pattern 1: Iterative Schema Understanding\n\n```mermaid\ngraph LR\n    Read[\"Read schema.json\"]\n    Parse[\"Parse JSON structure\"]\n    Extract[\"Extract entity definitions\"]\n    Model[\"Generate Pydantic models\"]\n    Validate[\"Validate against data files\"]\n    \n    Read --> Parse\n    Parse --> Extract\n    Extract --> Model\n    Model --> Validate\n    Validate -->|\"Schema mismatch\"| Extract\n```\n\n**Code pattern:**\n```python\n# Step 1: Read schema\nimport json\nwith open('schema.json', 'r', encoding='utf-8') as f:\n    schema = json.load(f)\n\n# Step 2: Extract entities\nentities = schema['definitions']  # or similar structure\n\n# Step 3: Build models (implied in task descriptions)\nfrom pydantic import BaseModel\nclass Task(BaseModel):\n    task_id: str\n    priority: int\n    # ... other fields from schema\n```\n\n**Sources:** [logs/global.log:1131-1423]()\n\n### Pattern 2: Batch Data Processing with Error Handling\n\n```python\nimport glob\nimport json\n\ndata_files = glob.glob('emergency_response_data_*.json')\ndatasets = []\n\nfor file_path in data_files:\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n            datasets.append(data)\n    except Exception as e:\n        print(f\"Error loading {file_path}: {e}\")\n\n# Aggregate statistics\nprint(f\"Loaded {len(datasets)} datasets\")\n```\n\n**Sources:** [logs/global.log:1160-1180]()\n\n### Pattern 3: Visualization with Path Output\n\n```python\nimport matplotlib.pyplot as plt\nimport os\n\n# Generate visualization\nplt.figure(figsize=(12, 8))\n# ... plotting code ...\n\n# Save with absolute path\ncurrent_dir = os.getcwd()\nfilename = \"01_data_distribution.png\"\noutput_path = os.path.join(current_dir, filename)\nplt.savefig(output_path, dpi=300, bbox_inches='tight')\n\nprint(f\"Visualization saved: {output_path}\")\n```\n\n**Sources:** [src/agent/deep_research.py:121-123]()\n\n---\n\n## Tool Call Patterns in Practice\n\n### RecursivePlanTreeTodoTool Usage\n\n**When the agent needs to:**\n- Establish initial research plan\n- Decompose complex problems\n- Track task dependencies\n- Maintain status across multiple steps\n\n**JSON structure from logs:**\n\n```json\n{\n  \"tool_call_purpose\": \"å»ºç«åå§åæè®¡åæ ï¼æç¡®ç ç©¶ç®æ åä»»å¡åè§£æ¹å\",\n  \"recursive_plan_tree\": {\n    \"core_goal\": \"åºäºæä¾çåºæ¥ææ´æ°æ®...è®¾è®¡å¯æµè¯çå¤æºè½ä½ååè°åº¦ç®æ³\",\n    \"tree_nodes\": [\n      {\n        \"task_id\": \"T1\",\n        \"task_name\": \"è¯»åå¹¶è§£æ schema.json æä»¶\",\n        \"status\": \"pending\",\n        \"dependencies\": null,\n        \"references\": [\"./schema.json\"]\n      },\n      {\n        \"task_id\": \"T2\",\n        \"task_name\": \"è¯»åå¹¶æ£æ¥åºæ¥ååºæ°æ®æä»¶\",\n        \"status\": \"pending\",\n        \"dependencies\": [\"è¯»åå¹¶è§£æ schema.json æä»¶\"],\n        \"references\": [\"./emergency_response_data_01.json\", \"...\"]\n      }\n    ]\n  }\n}\n```\n\n**Sources:** [logs/utils.log:56-62](), [logs/global.log:908-1125]()\n\n### ExecutePythonCodeTool Usage\n\n**When the agent needs to:**\n- Load and parse data files\n- Perform calculations\n- Generate visualizations\n- Test algorithms\n- Validate data structures\n\n**Example from error logs:**\n\n```json\n{\n  \"tool_call_purpose\": \"è¯»åå¹¶è§£æ schema.json æä»¶ï¼äºè§£æ°æ®ç»æå®ä¹\",\n  \"python_code_snippet\": \"import json\\n\\nwith open('schema.json', 'r') as file:\\n    schema = json.load(file)\\n    print(json.dumps(schema, indent=2))\"\n}\n```\n\n**Sources:** [logs/utils.log:127-129]()\n\n---\n\n## Performance Characteristics\n\n### Execution Times (from logs)\n\n| Operation | Time (ms) | Source |\n|-----------|-----------|--------|\n| LLM call (planning) | 41,636 | [logs/global.log:892]() |\n| Schema generation | 8 | [logs/global.log:189]() |\n| Tool dispatch | 35 | [logs/global.log:1574]() |\n| Memory initialization | 2 | [logs/global.log:115]() |\n\n### Working Directory Structure\n\n```\nwsm/\nâââ 1/\n    âââ g4-1/           # WORKING_DIR for subprocess execution\n        âââ schema.json\n        âââ emergency_response_data_01.json\n        âââ ...\n        âââ output/     # Generated visualizations and results\n```\n\n**Sources:** [logs/utils.log:130-131](), [src/runtime/subprocess_python_executor.py:44]()\n\n---\n\n## Summary: Key Takeaways\n\n1. **Multi-step reasoning**: The agent breaks down complex tasks into 10+ subtasks with dependencies\n2. **Error recovery**: Automatically analyzes failures (e.g., UnicodeDecodeError) and suggests fixes\n3. **Tool orchestration**: Seamlessly switches between planning (TodoTool) and execution (PythonTool)\n4. **State persistence**: Maintains global variables across multiple code executions\n5. **Comprehensive logging**: All operations tracked in [logs/global.log]() and [logs/utils.log]()\n\n**Sources:** [src/agent/deep_research.py:15-129](), [logs/utils.log:1-296](), [logs/global.log:1-1792]()\n\n---\n\n# Page: Emergency Response Planning Example\n\n# Emergency Response Planning Example\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/agent/tool/python_tool.py](src/agent/tool/python_tool.py)\n- [src/agent/tool/todo_tool.py](src/agent/tool/todo_tool.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page demonstrates a real-world use case from the system logs showing how the algo_agent processes a complex emergency response planning problem. The example illustrates the complete agent workflow: from receiving a user query about emergency data files, to building a hierarchical task plan, to executing Python code for data analysis and algorithm testing.\n\nThis example specifically showcases:\n- Multi-step research planning using the **RecursivePlanTreeTodoTool**\n- Handling of file I/O and data loading with **ExecutePythonCodeTool**\n- Error recovery from Unicode encoding and serialization issues\n- Task decomposition for multi-agent scheduling algorithm development\n\nFor details on the tools used, see [Python Code Execution Tool](#4.2) and [Recursive Task Planning Tool](#4.3). For troubleshooting the specific errors encountered, see [Common Execution Errors](#10.1).\n\n---\n\n## Problem Domain Overview\n\nThe user provided a set of emergency response scenario data files designed to test multi-agent coordination algorithms for disaster relief material transportation:\n\n### Data Files Structure\n\n| File | Description |\n|------|-------------|\n| `schema.json` | Complete data structure schema defining entities and relationships |\n| `emergency_response_data_01.json` to `emergency_response_data_05.json` | Five randomly generated emergency rescue scenarios |\n\n### Data Characteristics\n\nThe emergency response dataset exhibits the following properties:\n\n**Realism**\n- Coordinates generated within real geographic boundaries\n- Multiple disaster scenario types (earthquakes, floods, fires, etc.)\n- Realistic material and transport carrier attributes\n\n**Diversity**\n- Multiple task types with varying priority levels\n- Different transportation carrier types (trucks, drones, helicopters)\n- Randomly generated risk points and route obstacles\n\n**Completeness**\n- All core entities and their relationships included\n- Various state combinations covered (pending, in-progress, completed, failed)\n- Task feedback and rescue performance metrics\n\n**Extensibility**\n- Clear code structure for adding new fields or entity types\n- Adjustable generation strategies for specific scenarios\n\n### Intended Use Cases\n\n1. **Algorithm Testing**: Task assignment, path planning, ETA prediction algorithms\n2. **System Integration**: Emergency material transport scheduling system validation\n3. **Scenario Simulation**: Disaster rescue exercises and decision support system testing\n\nSources: [logs/utils.log:1-124](), [logs/global.log:1-180]()\n\n---\n\n## Agent Decision Process Flow\n\nThe following diagram shows how the agent processed the emergency response planning query through multiple decision points:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Agent as \"user_query function\"\n    participant Memory as \"init_messages_with_system_prompt\"\n    participant LLM as \"qwen-plus LLM\"\n    participant TodoTool as \"RecursivePlanTreeTodoTool\"\n    participant PythonTool as \"ExecutePythonCodeTool\"\n    \n    User->>Agent: \"å¶å®ç ç©¶è®¡ååæåºæ¥ææ´æ°æ®\"\n    Agent->>Memory: Initialize with system prompt + user query\n    Memory-->>Agent: messages list\n    \n    Agent->>LLM: \"generate_assistant_output_append()\"\n    Note over LLM: Analyze query requirements:<br/>- Data structure analysis<br/>- Algorithm testing<br/>- Scenario simulation\n    \n    LLM->>TodoTool: \"recursive_plan_tree_todo\"\n    Note over TodoTool: Create hierarchical task tree:<br/>T1-T12 tasks with dependencies\n    TodoTool-->>Agent: Task tree created\n    \n    Agent->>LLM: \"generate_assistant_output_append()\"\n    LLM->>PythonTool: \"execute_python_code\" x3\n    Note over PythonTool: Attempt to read:<br/>- schema.json<br/>- emergency_response_data_01.json<br/>- emergency_response_data_02.json\n    \n    PythonTool-->>Agent: UnicodeDecodeError (gbk codec)\n    Note over Agent: Error detected in all 3 executions\n    \n    Agent->>LLM: \"generate_assistant_output_append()\"\n    Note over LLM: Analyze errors:<br/>1. UTF-8 encoding issue<br/>2. File handle serialization issue\n    \n    LLM->>PythonTool: \"execute_python_code\" (corrected)\n    Note over PythonTool: Add encoding='utf-8'<br/>Close file properly\n    PythonTool-->>Agent: Success\n```\n\n**Key Decision Points:**\n\n1. **Initial Analysis** (Lines 309-495 in global.log): LLM receives user query and available tool schemas\n2. **Task Planning** (Line 57 in utils.log): Agent creates 12-task hierarchical plan via `RecursivePlanTreeTodoTool`\n3. **Execution Attempts** (Lines 127-289 in utils.log): Three parallel Python code executions attempted\n4. **Error Detection** (Line 289 in utils.log): All three executions crash with encoding errors\n5. **Error Analysis** (Line 289 in utils.log): LLM analyzes root causes and proposes fixes\n\nSources: [logs/utils.log:55-289](), [logs/global.log:309-495]()\n\n---\n\n## Hierarchical Task Planning\n\nThe agent decomposed the emergency response analysis into a 12-task recursive plan tree. Below is the structure and task dependencies:\n\n```mermaid\ngraph TB\n    T1[\"T1: è¯»åå¹¶è§£æ schema.json æä»¶\"]\n    T2[\"T2: è¯»åå¹¶æ£æ¥åºæ¥ååºæ°æ®æä»¶\"]\n    T3[\"T3: æå»ºæ°æ®æ¨¡åç±»\"]\n    T4[\"T4: å®ç°æ°æ®å è½½å¨\"]\n    T5[\"T5: åææ°æ®åå¸ç¹å¾\"]\n    T6[\"T6: æ¨¡æåä¸ææ´åºæ¯\"]\n    T7[\"T7: è®¾è®¡å¨æè°åº¦ç®æ³æ¡æ¶\"]\n    T8[\"T8: å®ç°åºç¡ææ´¾ç®æ³ï¼è´ªå¿ç­ç¥ï¼\"]\n    T9[\"T9: å®ç°é«çº§ææ´¾ç®æ³ï¼å æè¯åæ¨¡åï¼\"]\n    T10[\"T10: è®¾è®¡è·¯å¾éè§åæºå¶\"]\n    T11[\"T11: è¯ä¼°ç®æ³æ§è½ææ \"]\n    T12[\"T12: è¿è¡å¤ç»å¯¹æ¯å®éª\"]\n    \n    T1 --> T2\n    T1 --> T3\n    T2 --> T4\n    T3 --> T4\n    T4 --> T5\n    T4 --> T6\n    T3 --> T7\n    T7 --> T8\n    T8 --> T9\n    T9 --> T10\n    T8 --> T11\n    T9 --> T11\n    T11 --> T12\n    \n    style T1 fill:#f9f9f9\n    style T2 fill:#f9f9f9\n    style T3 fill:#f9f9f9\n    style T7 fill:#f9f9f9\n```\n\n### Task Breakdown Details\n\n**Phase 1: Data Understanding (T1-T4)**\n\n| Task ID | Task Name | Description | Dependencies | Status |\n|---------|-----------|-------------|--------------|--------|\n| T1 | è¯»åå¹¶è§£æ schema.json æä»¶ | Load schema.json, extract core entities (Task, Carrier, Resource, Location, Path, RiskPoint) | None | pending |\n| T2 | è¯»åå¹¶æ£æ¥åºæ¥ååºæ°æ®æä»¶ | Batch read data_01.json through data_05.json, validate against schema | T1 | pending |\n| T3 | æå»ºæ°æ®æ¨¡åç±» | Define Python classes using dataclass/pydantic based on schema | T1 | pending |\n| T4 | å®ç°æ°æ®å è½½å¨ | Write functions to load all JSON files into memory objects | T2, T3 | pending |\n\n**Phase 2: Data Analysis (T5-T6)**\n\n| Task ID | Task Name | Description | Research Directions |\n|---------|-----------|-------------|---------------------|\n| T5 | åææ°æ®åå¸ç¹å¾ | Statistical analysis of task priorities, disaster types, carrier distributions | Generate charts, calculate descriptive statistics |\n| T6 | æ¨¡æåä¸ææ´åºæ¯ | Select one data file, construct static rescue scenario, manually assign tasks | Define task-carrier matching rules, record unassigned tasks |\n\n**Phase 3: Algorithm Development (T7-T10)**\n\n| Task ID | Task Name | Key Approach |\n|---------|-----------|--------------|\n| T7 | è®¾è®¡å¨æè°åº¦ç®æ³æ¡æ¶ | Event-driven vs time-step simulation, state machine for carrier behavior |\n| T8 | å®ç°åºç¡ææ´¾ç®æ³ï¼è´ªå¿ç­ç¥ï¼ | Greedy assignment: select nearest available carrier, use Haversine distance formula |\n| T9 | å®ç°é«çº§ææ´¾ç®æ³ï¼å æè¯åæ¨¡åï¼ | Multi-factor scoring: distance, priority, ETA, risk coefficient with normalized weights |\n| T10 | è®¾è®¡è·¯å¾éè§åæºå¶ | Integrate A* or Dijkstra algorithm, dynamic graph weight updates for risk points |\n\n**Phase 4: Evaluation (T11-T12)**\n\n| Task ID | Task Name | Metrics |\n|---------|-----------|---------|\n| T11 | è¯ä¼°ç®æ³æ§è½ææ  | Task completion rate, average response delay, carrier utilization, high-priority task success rate |\n| T12 | è¿è¡å¤ç»å¯¹æ¯å®éª | Controlled experiments, cross-algorithm comparison with baseline, result visualization |\n\nThe `RecursivePlanTreeTodoTool` created this structure by instantiating a `RecursivePlanTree` object with `tree_nodes` containing nested `RecursivePlanTreeNode` instances. Each node includes:\n- `task_id`: Unique identifier (T1-T12)\n- `task_name`: Globally unique name referenced by dependencies\n- `status`: TaskStatus enum (pending/processing/completed/failed/retry/skipped)\n- `dependencies`: List of task_name values that must complete first\n- `research_directions`: Optional exploration directions for complex tasks\n\nSources: [logs/utils.log:57-58](), [src/agent/tool/todo_tool.py:1-133](), [src/memory/tree_todo/schemas.py]()\n\n---\n\n## Code Execution Flow and Error Handling\n\nThe agent attempted to execute Python code to read the data files, encountering multiple errors before succeeding. This demonstrates the system's error recovery capabilities.\n\n### Execution Attempt Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Tool Call from LLM\"\n        TC1[\"tool_calls[0]: execute_python_code<br/>purpose: è¯»å schema.json\"]\n        TC2[\"tool_calls[1]: execute_python_code<br/>purpose: è¯»å data_01.json\"]\n        TC3[\"tool_calls[2]: execute_python_code<br/>purpose: è¯»å data_02.json\"]\n    end\n    \n    subgraph \"ExecutePythonCodeTool\"\n        EPT[\"python_tool.py:run()\"]\n        WS[\"workspace.get_arg_globals()\"]\n    end\n    \n    subgraph \"Subprocess Execution\"\n        SE1[\"subprocess_python_executor<br/>run_structured_in_thread<br/>PID: 11336\"]\n        SE2[\"subprocess_python_executor<br/>run_structured_in_thread<br/>PID: 9308\"]\n        SE3[\"subprocess_python_executor<br/>run_structured_in_thread<br/>PID: 10528\"]\n    end\n    \n    subgraph \"Error Results\"\n        ER1[\"ExecutionResult<br/>status: CRASHED<br/>UnicodeDecodeError<br/>PickleError\"]\n        ER2[\"ExecutionResult<br/>status: CRASHED<br/>UnicodeDecodeError<br/>PickleError\"]\n        ER3[\"ExecutionResult<br/>status: CRASHED<br/>UnicodeDecodeError<br/>PickleError\"]\n    end\n    \n    TC1 --> EPT\n    TC2 --> EPT\n    TC3 --> EPT\n    \n    EPT --> WS\n    WS --> SE1\n    WS --> SE2\n    WS --> SE3\n    \n    SE1 --> ER1\n    SE2 --> ER2\n    SE3 --> ER3\n```\n\n### Error Cascade Analysis\n\nThe execution failures occurred in three stages, each revealing a different aspect of the error handling system:\n\n**Stage 1: Initial UnicodeDecodeError**\n\n```python\n# Original code (lines 127-146 in utils.log)\nimport json\n\nwith open('schema.json', 'r') as file:\n    schema = json.load(file)\n    print(json.dumps(schema, indent=2))\n```\n\n**Error Stack Trace:**\n```\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265: illegal multibyte sequence\n  File \"subprocess_python_executor.py\", line 44, in _worker_with_pipe\n    exec(command, _globals, _locals)\n  File \"<string>\", line 4, in <module>\n  File \"json/__init__.py\", line 293, in load\n    return loads(fp.read(), ...)\n```\n\n**Root Cause:** Windows default encoding is `gbk`, but JSON files were UTF-8 encoded with non-ASCII characters (Chinese text describing disaster scenarios).\n\n**Stage 2: Serialization Failure**\n\nAfter the initial error, the subprocess executor attempted to create an `ExecutionResult` object to report the failure, but encountered a second error:\n\n```\nTypeError: cannot pickle 'TextIOWrapper' instances\n  File \"subprocess_python_executor.py\", line 54, in _worker_with_pipe\n    res = ExecutionResult(...)\n  File \"schemas.py\", line 83, in field_validate_globals\n    return workspace.filter_and_deepcopy_globals(value)\n  File \"workspace.py\", line 55, in filter_and_deepcopy_globals\n    filtered_dict[key] = copy.deepcopy(value)\n```\n\n**Root Cause:** The file handle object (`file`) remained in the local namespace. When `ExecutionResult` tried to serialize `_globals` to return it to the parent process, the `copy.deepcopy()` operation in `workspace.filter_and_deepcopy_globals()` failed because file handles cannot be pickled.\n\n**Stage 3: Unbound Variable Error**\n\nThe exception handler in the subprocess worker attempted to send an error result but encountered:\n\n```\nUnboundLocalError: cannot access local variable 'res' where it is not associated with a value\n  File \"subprocess_python_executor.py\", line 66, in _worker_with_pipe\n    child_conn.send((_PipeType.RESULT, res))\n```\n\n**Root Cause:** The exception handler assumed `res` was defined, but the previous exception occurred during `res` construction, leaving the variable unbound.\n\nSources: [logs/utils.log:130-288](), [src/runtime/subprocess_python_executor.py:44-66](), [src/runtime/workspace.py:55]()\n\n### Agent Error Analysis Response\n\nAfter receiving three crashed execution results, the LLM analyzed the errors and proposed a corrected solution:\n\n```mermaid\ngraph LR\n    subgraph \"Error Detection\"\n        E1[\"3x ExecutionResult<br/>status: CRASHED\"]\n        E2[\"Extract error patterns:<br/>- UnicodeDecodeError<br/>- PickleError<br/>- UnboundLocalError\"]\n    end\n    \n    subgraph \"Root Cause Analysis\"\n        RC1[\"Issue 1:<br/>Missing encoding parameter<br/>Windows default: gbk<br/>Files encoded: utf-8\"]\n        RC2[\"Issue 2:<br/>File handle in globals<br/>TextIOWrapper not picklable<br/>Blocks serialization\"]\n    end\n    \n    subgraph \"Solution Strategy\"\n        S1[\"Add encoding='utf-8'<br/>to all open() calls\"]\n        S2[\"Ensure with block closes file<br/>Don't expose file handle<br/>Return only serializable data\"]\n    end\n    \n    E1 --> E2\n    E2 --> RC1\n    E2 --> RC2\n    RC1 --> S1\n    RC2 --> S2\n```\n\n**Corrected Code:**\n```python\nimport json\n\ntry:\n    with open('schema.json', 'r', encoding='utf-8') as f:\n        schema = json.load(f)\n    print(json.dumps(schema, indent=2, ensure_ascii=False))\nexcept Exception as e:\n    print(f\"Error reading schema.json: {e}\")\n```\n\n**Key Improvements:**\n1. Explicit `encoding='utf-8'` parameter\n2. File variable renamed to `f` (shorter scope)\n3. File closed before `print()` statement\n4. `ensure_ascii=False` for proper Unicode output\n5. Exception handling for graceful error reporting\n\nSources: [logs/utils.log:289-290](), [src/agent/tool/python_tool.py:41-50]()\n\n---\n\n## Code-Level Implementation Mapping\n\nThis section maps the high-level emergency response planning workflow to concrete code entities in the system.\n\n### Tool Invocation Pipeline\n\n```mermaid\ngraph TB\n    subgraph \"Agent Layer\"\n        UQ[\"user_query()<br/>deep_research.py:user_query\"]\n        GAOA[\"generate_assistant_output_append()<br/>llm.py:generate_assistant_output_append\"]\n        CTS[\"call_tools_safely()<br/>tool_coordinator.py:call_tools_safely\"]\n    end\n    \n    subgraph \"Tool Layer\"\n        RPT[\"RecursivePlanTreeTodoTool<br/>todo_tool.py:RecursivePlanTreeTodoTool\"]\n        EPT[\"ExecutePythonCodeTool<br/>python_tool.py:ExecutePythonCodeTool\"]\n        RPTRUN[\"run()<br/>todo_tool.py:28-36\"]\n        EPTRUN[\"run()<br/>python_tool.py:41-50\"]\n    end\n    \n    subgraph \"Execution Layer\"\n        RSTIT[\"run_structured_in_thread()<br/>subthread_python_executor.py\"]\n        GAG[\"get_arg_globals()<br/>workspace.py\"]\n        AOG[\"append_out_globals()<br/>workspace.py\"]\n    end\n    \n    subgraph \"Data Models\"\n        RPT_MODEL[\"RecursivePlanTree<br/>tree_todo/schemas.py\"]\n        RPTN_MODEL[\"RecursivePlanTreeNode<br/>tree_todo/schemas.py\"]\n        ER_MODEL[\"ExecutionResult<br/>runtime/schemas.py\"]\n    end\n    \n    UQ --> GAOA\n    GAOA --> CTS\n    CTS --> RPT\n    CTS --> EPT\n    \n    RPT --> RPTRUN\n    EPT --> EPTRUN\n    \n    RPTRUN --> RPT_MODEL\n    RPTRUN --> RPTN_MODEL\n    \n    EPTRUN --> GAG\n    EPTRUN --> RSTIT\n    RSTIT --> ER_MODEL\n    ER_MODEL --> AOG\n    AOG --> EPTRUN\n```\n\n### Key Function Execution Sequence\n\n| Step | Function | File | Purpose |\n|------|----------|------|---------|\n| 1 | `user_query()` | [src/agent/deep_research.py]() | Entry point for user query processing |\n| 2 | `init_messages_with_system_prompt()` | [src/agent/memory.py]() | Initialize conversation with system prompt |\n| 3 | `get_tools_schema()` | [src/agent/tool/schema.py]() | Generate JSON schemas for available tools |\n| 4 | `generate_assistant_output_append()` | [src/agent/llm.py]() | Request LLM response with tool calling |\n| 5 | `call_tools_safely()` | [src/agent/tool_coordinator.py]() | Dispatch tool calls from LLM |\n| 6 | `RecursivePlanTreeTodoTool.run()` | [src/agent/tool/todo_tool.py:28-36]() | Execute task tree management |\n| 7 | `ExecutePythonCodeTool.run()` | [src/agent/tool/python_tool.py:41-50]() | Execute Python code snippet |\n| 8 | `workspace.get_arg_globals()` | [src/runtime/workspace.py]() | Retrieve persistent global variables |\n| 9 | `run_structured_in_thread()` | [src/runtime/subthread_python_executor.py]() | Execute code in isolated thread |\n| 10 | `workspace.append_out_globals()` | [src/runtime/workspace.py]() | Persist updated global variables |\n\nSources: [src/agent/deep_research.py](), [src/agent/tool/python_tool.py:41-50](), [src/agent/tool/todo_tool.py:28-36](), [logs/global.log:1-695]()\n\n---\n\n## Data Schema Entities\n\nThe emergency response domain involves several interconnected entities. While the actual `schema.json` file was not successfully loaded in the logged session, the user description and task planning reveal the expected structure:\n\n### Core Entity Types\n\n```mermaid\nerDiagram\n    TASK ||--o{ CARRIER : \"assigned_to\"\n    TASK ||--|| RESOURCE : \"requires\"\n    TASK ||--o{ LOCATION : \"from/to\"\n    CARRIER ||--o{ PATH : \"follows\"\n    PATH ||--o{ RISK_POINT : \"contains\"\n    TASK ||--o{ TASK_FEEDBACK : \"has\"\n    \n    TASK {\n        string task_id PK\n        string task_type\n        string priority\n        string status\n        datetime created_at\n        string disaster_type\n    }\n    \n    CARRIER {\n        string carrier_id PK\n        string carrier_type\n        float capacity\n        string status\n        coordinate location\n    }\n    \n    RESOURCE {\n        string resource_id PK\n        string resource_type\n        float quantity\n        string unit\n    }\n    \n    LOCATION {\n        string location_id PK\n        coordinate lat_lng\n        string address\n        string location_type\n    }\n    \n    PATH {\n        string path_id PK\n        array waypoints\n        float distance\n        float estimated_time\n    }\n    \n    RISK_POINT {\n        string risk_id PK\n        coordinate location\n        string risk_type\n        float risk_level\n    }\n    \n    TASK_FEEDBACK {\n        string feedback_id PK\n        datetime timestamp\n        string status_update\n        float progress_percent\n    }\n```\n\n### RecursivePlanTreeNode Schema Mapping\n\nThe `RecursivePlanTreeNode` schema used by the agent mirrors the hierarchical structure needed for algorithm planning:\n\n```python\n# From src/memory/tree_todo/schemas.py\nclass RecursivePlanTreeNode(BaseModel):\n    task_id: str                              # \"T1\", \"T2\", etc.\n    task_name: str                            # Globally unique name\n    description: str = \"\"                     # Detailed execution requirements\n    status: TaskStatus = TaskStatus.PENDING   # pending/processing/completed/failed/retry/skipped\n    dependencies: Optional[List[str]] = None  # References to task_name values\n    children: Optional[List['RecursivePlanTreeNode']] = None  # Nested sub-tasks\n    research_directions: Optional[List[str]] = None\n    output: str = \"\"                          # Execution result\n```\n\nThis schema enables:\n- **Hierarchical decomposition**: Tasks can contain nested sub-tasks via `children`\n- **Dependency tracking**: `dependencies` list prevents premature execution\n- **Status management**: `TaskStatus` enum tracks progress through the workflow\n- **Persistent state**: `output` field stores execution results for subsequent tasks\n\nSources: [src/memory/tree_todo/schemas.py](), [logs/utils.log:57-58]()\n\n---\n\n## Lessons and Patterns\n\nThis emergency response planning example demonstrates several key patterns in the algo_agent system:\n\n### 1. Hierarchical Task Decomposition Pattern\n\n**Pattern:** Break complex problems into a tree of dependent sub-tasks\n\n**Implementation:**\n```python\n# From RecursivePlanTreeTodoTool\nRecursivePlanTree(\n    core_goal=\"åºäºæä¾çåºæ¥ææ´æ°æ®ï¼æ·±å¥çè§£æ°æ®ç»æå¹¶è®¾è®¡å¯æµè¯çå¤æºè½ä½ååè°åº¦ç®æ³\",\n    tree_nodes=[\n        RecursivePlanTreeNode(\n            task_name=\"è¯»åå¹¶è§£æ schema.json æä»¶\",\n            dependencies=None  # Root task\n        ),\n        RecursivePlanTreeNode(\n            task_name=\"æå»ºæ°æ®æ¨¡åç±»\",\n            dependencies=[\"è¯»åå¹¶è§£æ schema.json æä»¶\"]  # Depends on T1\n        )\n    ]\n)\n```\n\n**Benefits:**\n- Maintains clear execution order through dependency chains\n- Enables parallel execution of independent tasks (T2 and T3 both depend only on T1)\n- Provides checkpoints for error recovery\n\n### 2. Stateful Code Execution Pattern\n\n**Pattern:** Persist variables across multiple code executions using workspace globals\n\n**Implementation:**\n```python\n# From python_tool.py:41-50\ndef run(self) -> str:\n    execution_context: Optional[Dict[str, Any]] = workspace.get_arg_globals()\n    exec_result: ExecutionResult = subthread_python_executor.run_structured_in_thread(\n        command=self.python_code_snippet,\n        _globals=execution_context,  # Pass previous state\n        timeout=self.timeout\n    )\n    workspace.append_out_globals(exec_result.arg_globals)  # Persist new state\n    return exec_result.ret_tool2llm\n```\n\n**Benefits:**\n- Sequential code snippets can build on previous definitions\n- Enables Jupyter-notebook-like interactive development\n- Avoids redundant imports and setup in each execution\n\n### 3. Multi-Level Error Recovery Pattern\n\n**Pattern:** Cascade error information through multiple abstraction layers for LLM analysis\n\n**Error Flow:**\n1. **Execution Layer** (`subprocess_python_executor.py`): Captures raw exceptions\n2. **Result Schema** (`ExecutionResult`): Structures error information\n3. **Tool Layer** (`ExecutePythonCodeTool.run()`): Returns formatted error message\n4. **Agent Layer** (`user_query()`): Passes error to LLM for analysis\n5. **LLM Response**: Diagnoses root cause and proposes fix\n\n**Implementation in ExecutionResult:**\n```python\n# From runtime/schemas.py\nclass ExecutionResult(BaseModel):\n    status: ExecutionStatus  # SUCCESS/FAILURE/TIMEOUT/CRASHED\n    stdout: str              # Captured output\n    stderr: str              # Error messages\n    exception: str           # Formatted traceback\n```\n\n### 4. Encoding Safety Pattern\n\n**Anti-Pattern (Fails):**\n```python\nwith open('file.json', 'r') as f:  # Uses platform default encoding\n    data = json.load(f)\n```\n\n**Correct Pattern:**\n```python\nwith open('file.json', 'r', encoding='utf-8') as f:  # Explicit encoding\n    data = json.load(f)\n```\n\n**Why It Matters:**\n- Windows default encoding is `cp1252` or `gbk` depending on locale\n- JSON files typically use UTF-8 encoding\n- Non-ASCII characters (Chinese, emoji, etc.) cause `UnicodeDecodeError`\n\n### 5. Serialization Safety Pattern\n\n**Anti-Pattern (Fails):**\n```python\nfile = open('data.json', 'r', encoding='utf-8')\ndata = json.load(file)\n# 'file' object remains in namespace, blocks serialization\n```\n\n**Correct Pattern:**\n```python\nwith open('data.json', 'r', encoding='utf-8') as f:\n    data = json.load(f)\n# 'f' goes out of scope, only 'data' remains\n```\n\n**Why It Matters:**\n- `ExecutionResult` serializes `_globals` to transfer state between processes\n- File handles (`TextIOWrapper`) cannot be pickled\n- Context managers ensure resources are released before serialization\n\n### 6. Parallel Tool Invocation Pattern\n\nThe LLM can request multiple tool calls in a single response:\n\n```python\n# From logs showing tool_calls array\ntool_calls=[\n    ChatCompletionMessageFunctionToolCall(id='call_3085f1f75d534390a7c2b7', \n                                          function=Function(name='execute_python_code', ...)),\n    ChatCompletionMessageFunctionToolCall(id='call_ab025bb2bb3e430aaefd92',\n                                          function=Function(name='execute_python_code', ...)),\n    ChatCompletionMessageFunctionToolCall(id='call_92fd80e050c34c78a18ea9',\n                                          function=Function(name='execute_python_code', ...))\n]\n```\n\n**Benefits:**\n- Read multiple files concurrently\n- Reduces round trips to LLM\n- All results available for next reasoning step\n\n**Risks:**\n- If one execution fails, all may fail with same error\n- Harder to debug than sequential execution\n\nSources: [src/agent/tool/python_tool.py:41-50](), [src/runtime/workspace.py](), [logs/utils.log:127-290]()\n\n---\n\n## Performance Considerations\n\n### Subprocess Creation Overhead\n\nEach `ExecutePythonCodeTool` invocation spawns a new subprocess:\n\n```python\n# From subprocess_python_executor.py\np = Process(target=_worker_with_pipe, args=(parent_conn, child_conn, ...))\np.start()\n```\n\n**Metrics from logs:**\n- Process ID 11336: Started at 03:39:11.529, crashed at 03:39:11.770 (241ms)\n- Process ID 9308: Started at 03:39:12.906, crashed at 03:39:13.080 (174ms)\n- Process ID 10528: Started at 03:39:14.170, crashed at 03:39:14.370 (200ms)\n\n**Implications:**\n- ~200ms overhead per execution (including crash handling)\n- Three parallel executions took ~3 seconds total (serial bottleneck)\n- Successful execution would have been faster (no exception handling overhead)\n\n### LLM Latency\n\nFrom the logs, LLM response generation times:\n- First response (task tree creation): 42 seconds (17:51:32 â 17:52:14)\n- Second response (error analysis): 17 seconds (03:39:14 â 03:39:31)\n\n**Optimization Opportunities:**\n- Cache common task tree patterns for similar queries\n- Use streaming responses for faster user feedback\n- Implement speculative execution for predictable next steps\n\nSources: [logs/utils.log:127-289](), [logs/global.log:309-501]()\n\n---\n\n## Summary\n\nThe emergency response planning example demonstrates the algo_agent's capability to:\n\n1. **Decompose Complex Problems**: Created a 12-task hierarchical plan with clear dependencies\n2. **Execute Code Iteratively**: Attempted multiple Python executions with state persistence\n3. **Recover from Errors**: Analyzed encoding and serialization failures, proposed corrections\n4. **Bridge Language and Code**: Translated natural language requirements into concrete Python implementations\n5. **Maintain Context**: Tracked conversation history, tool results, and error states across multiple LLM calls\n\nThis use case exemplifies the core value proposition: an autonomous agent that can reason about data analysis workflows, write and debug code, and iteratively approach solutions to complex multi-step problems.\n\nFor similar examples with different problem domains, see [Data Processing and Visualization](#7.2) and [Geographic Data Processing](#7.3).\n\nSources: [logs/utils.log:1-296](), [logs/global.log:1-695](), [src/agent/tool/python_tool.py](), [src/agent/tool/todo_tool.py]()\n\n---\n\n# Page: Data Processing and Visualization\n\n# Data Processing and Visualization\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [src/runtime/before_thread/plt_back_chinese.py](src/runtime/before_thread/plt_back_chinese.py)\n- [src/runtime/subthread_python_executor.py](src/runtime/subthread_python_executor.py)\n- [tests/playground/gen/g7/01_valid7.py](tests/playground/gen/g7/01_valid7.py)\n- [tests/playground/gen/g7/02_plt.py](tests/playground/gen/g7/02_plt.py)\n- [tests/playground/gen/g7/03_valid.py](tests/playground/gen/g7/03_valid.py)\n- [tests/playground/gen/g7/04_plt.py](tests/playground/gen/g7/04_plt.py)\n- [tests/playground/gen/g7/05.dump_.py](tests/playground/gen/g7/05.dump_.py)\n- [tests/playground/gen/g7/06.dump_all.py](tests/playground/gen/g7/06.dump_all.py)\n- [tests/playground/gen/g7/metro-draw-data.json](tests/playground/gen/g7/metro-draw-data.json)\n- [tests/playground/gen/g7/metro-draw-schema.json](tests/playground/gen/g7/metro-draw-schema.json)\n- [tests/playground/gen/g7/subway.md](tests/playground/gen/g7/subway.md)\n- [tests/playground/gen/g9/beijing_scenic_spot_schema_with_play_hours.json](tests/playground/gen/g9/beijing_scenic_spot_schema_with_play_hours.json)\n- [tests/playground/gen/g9/beijing_scenic_spot_validated_data_with_play_hours.json](tests/playground/gen/g9/beijing_scenic_spot_validated_data_with_play_hours.json)\n- [tests/playground/gen/g9/g9.py](tests/playground/gen/g9/g9.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page demonstrates how the algo_agent system handles data processing and visualization tasks through concrete examples. It covers Pydantic-based data modeling, validation workflows, schema generation, data transformation pipelines, and matplotlib visualization with multi-language support. These examples showcase the system's capability to process structured data from various sources (CSV, JSON) and generate visual outputs.\n\nFor emergency response planning use cases, see [Emergency Response Planning Example](#7.1). For geographic data processing with street networks, see [Geographic Data Processing](#7.3).\n\n---\n\n## Pydantic Data Modeling\n\n### Scenic Spot Data Model\n\nThe system uses Pydantic for robust data validation and schema generation. The Beijing scenic spots example demonstrates enum validation, field validators, and CSV parsing.\n\n```mermaid\ngraph TB\n    CSV[\"åäº¬æ¯ç¹ç­çº§åå»ºè®®æ¸¸ç©æ¶é¿.csv<br/>Raw CSV Data\"]\n    READER[\"read_csv_to_pydantic()<br/>CSV Parser\"]\n    MODEL[\"ScenicSpot Model<br/>Pydantic BaseModel\"]\n    ENUMS[\"StarRatingEnum<br/>OpenStatusEnum<br/>Field Validators\"]\n    SCHEMA[\"beijing_scenic_spot_schema_with_play_hours.json\"]\n    DATA[\"beijing_scenic_spot_validated_data_with_play_hours.json\"]\n    \n    CSV --> READER\n    READER --> MODEL\n    MODEL --> ENUMS\n    ENUMS --> |\"Validates\"| MODEL\n    MODEL --> |\"model_json_schema()\"| SCHEMA\n    MODEL --> |\"model_dump()\"| DATA\n    \n    style MODEL fill:#f9f9f9\n    style ENUMS fill:#f9f9f9\n```\n\n**Diagram: Scenic Spot Data Processing Pipeline**\n\nThe `ScenicSpot` model includes multiple validation layers:\n\n| Field | Type | Validation | Purpose |\n|-------|------|------------|---------|\n| `name` | `str` | Required | Scenic spot name |\n| `star_rating` | `StarRatingEnum` | Enum validation | 3A/4A/5A rating |\n| `open_status` | `OpenStatusEnum` | Enum validation | open/close status |\n| `longitude` | `float` | Auto-conversion | Longitude coordinate |\n| `latitude` | `float` | Auto-conversion | Latitude coordinate |\n| `suggested_play_hours` | `float` | Custom validator | Must be > 0 |\n\nSources: [tests/playground/gen/g9/g9.py:19-73]()\n\n### Enum-Based Validation\n\nThe system defines strict enumerations for categorical data:\n\n```python\nclass StarRatingEnum(str, Enum):\n    THREE_A = \"3A\"\n    FOUR_A = \"4A\"\n    FIVE_A = \"5A\"\n\nclass OpenStatusEnum(str, Enum):\n    OPEN = \"open\"\n    CLOSE = \"close\"\n```\n\nField validators provide enhanced error messages when validation fails:\n\nSources: [tests/playground/gen/g9/g9.py:20-31](), [tests/playground/gen/g9/g9.py:42-72]()\n\n### CSV to Pydantic Conversion\n\nThe `read_csv_to_pydantic()` function demonstrates robust CSV parsing with error handling:\n\n```mermaid\ngraph LR\n    READ[\"Open CSV File\"]\n    PARSE[\"csv.DictReader\"]\n    LOOP[\"Iterate Rows\"]\n    EXTRACT[\"Extract Fields\"]\n    SPLIT[\"Split Coordinates\"]\n    VALIDATE[\"Pydantic Validation\"]\n    APPEND[\"Append to List\"]\n    ERROR[\"Log Error & Continue\"]\n    \n    READ --> PARSE\n    PARSE --> LOOP\n    LOOP --> EXTRACT\n    EXTRACT --> SPLIT\n    SPLIT --> VALIDATE\n    VALIDATE --> |\"Success\"| APPEND\n    VALIDATE --> |\"Exception\"| ERROR\n    ERROR --> LOOP\n    APPEND --> LOOP\n```\n\n**Diagram: CSV Parsing Workflow with Error Recovery**\n\nThe function handles malformed data gracefully by logging errors and continuing to the next row:\n\nSources: [tests/playground/gen/g9/g9.py:75-112]()\n\n---\n\n## Complex Schema Processing\n\n### Beijing Subway Data Model\n\nThe Beijing subway data demonstrates handling deeply nested JSON structures with multiple model layers.\n\n```mermaid\ngraph TB\n    subgraph \"Data Layers\"\n        DRW[\"DrwSubwayData<br/>Root Model\"]\n        LINE[\"DrwLine<br/>Line Model\"]\n        STATION[\"DrwStation<br/>Station Model\"]\n    end\n    \n    subgraph \"Station Fields\"\n        COORDS[\"sl: longitude,latitude\"]\n        TYPE[\"t: 1=normal, 2=transfer\"]\n        ROUTES[\"r: route_id|route_id\"]\n        MULTI[\"udpx, udsi, udli<br/>Semicolon-separated\"]\n    end\n    \n    DRW --> |\"l: List[DrwLine]\"| LINE\n    LINE --> |\"st: List[DrwStation]\"| STATION\n    STATION --> COORDS\n    STATION --> TYPE\n    STATION --> ROUTES\n    STATION --> MULTI\n```\n\n**Diagram: Subway Data Schema Hierarchy**\n\nThe schema includes multi-valued fields using delimiter-separated strings:\n\n| Field | Delimiter | Example | Description |\n|-------|-----------|---------|-------------|\n| `r` | `\\|` | `\"900000069871\\|110100023339\"` | Multiple route IDs |\n| `udsi` | `;` | `\"900000069872017;900000069871009\"` | Up/down station IDs |\n| `udli` | `;` | `\"900000069872;900000069871\"` | Up/down line IDs |\n\nSources: [tests/playground/gen/g7/subway.md:19-89](), [tests/playground/gen/g7/01_valid7.py:6-36]()\n\n### Field Validation with Coordinate Parsing\n\nThe `DrwStation` model includes a custom validator for coordinate fields:\n\n```python\n@field_validator('sl')\ndef parse_coordinate(cls, v: str) -> str:\n    if not v:\n        raise ValueError(\"ç»çº¬åº¦ä¸è½ä¸ºç©º\")\n    lon, lat = v.split(',')\n    try:\n        float(lon), float(lat)\n    except ValueError:\n        raise ValueError(f\"æ æçç»çº¬åº¦æ ¼å¼ï¼{v}\")\n    return v\n```\n\nThis ensures all coordinates are valid before the model is instantiated.\n\nSources: [tests/playground/gen/g7/01_valid7.py:6-24](), [tests/playground/gen/g7/03_valid.py:37-48]()\n\n---\n\n## Data Transformation Pipelines\n\n### Schema Generation and Simplification\n\nThe system supports transforming raw complex schemas into simplified models optimized for specific use cases.\n\n```mermaid\ngraph LR\n    RAW[\"BeijingMetroRaw<br/>Complex Source Data\"]\n    PARSE[\"read_raw_metro_json()\"]\n    TRANSFORM[\"convert_raw_to_draw_model()\"]\n    \n    subgraph \"Transformation Steps\"\n        EXTRACT[\"Extract all lines<br/>get_all_lines()\"]\n        DEDUPE[\"Deduplicate stations<br/>get_all_stations()\"]\n        COLOR[\"Generate colors<br/>generate_rainbow_colors()\"]\n        BUILD[\"Build DrawLine objects\"]\n    end\n    \n    DRAW[\"MetroDrawSchema<br/>Simplified Draw Model\"]\n    SCHEMA_OUT[\"metro-draw-schema.json\"]\n    DATA_OUT[\"metro-draw-data.json\"]\n    \n    RAW --> PARSE\n    PARSE --> TRANSFORM\n    TRANSFORM --> EXTRACT\n    EXTRACT --> DEDUPE\n    DEDUPE --> COLOR\n    COLOR --> BUILD\n    BUILD --> DRAW\n    DRAW --> |\"model_json_schema()\"| SCHEMA_OUT\n    DRAW --> |\"model_dump()\"| DATA_OUT\n```\n\n**Diagram: Data Transformation Pipeline from Raw to Draw-Optimized Model**\n\nThe transformation process:\n\n1. **Extraction**: Groups stations by line using `get_all_lines()`\n2. **Deduplication**: Identifies transfer stations appearing in multiple lines\n3. **Color Assignment**: Generates rainbow gradient colors for visual distinction\n4. **Model Building**: Creates `DrawStation` and `DrawLine` objects\n\nSources: [tests/playground/gen/g7/05.dump_.py:94-163](), [tests/playground/gen/g7/06.dump_all.py:94-163]()\n\n### Transfer Station Detection\n\nThe system automatically identifies transfer stations using two criteria:\n\n```python\nis_transfer = station_raw.is_transfer() or len(line_set) > 1\n```\n\nA station is marked as a transfer station if:\n- The raw data explicitly marks it (`t == '1'`)\n- It appears in multiple lines during deduplication\n\nSources: [tests/playground/gen/g7/05.dump_.py:137](), [tests/playground/gen/g7/06.dump_all.py:137]()\n\n---\n\n## Matplotlib Visualization\n\n### Thread-Safe Visualization Setup\n\nWhen executing visualization code in subthreads, the system requires proper matplotlib backend configuration.\n\n```mermaid\ngraph TB\n    BEFORE[\"before_thread/plt_back_chinese.py<br/>Pre-execution Setup\"]\n    \n    subgraph \"Initialization Steps\"\n        BACKEND[\"matplotlib.use('Agg')<br/>Non-GUI Backend\"]\n        FONT[\"Set Chinese Fonts<br/>SimHei/PingFang SC\"]\n        MINUS[\"Fix Unicode Minus<br/>unicode_minus=False\"]\n    end\n    \n    THREAD[\"subthread_python_executor.py<br/>Thread Execution\"]\n    IMPORT[\"Import matplotlib in thread\"]\n    SAFE[\"Thread-safe execution\"]\n    \n    BEFORE --> BACKEND\n    BACKEND --> FONT\n    FONT --> MINUS\n    MINUS --> THREAD\n    THREAD --> IMPORT\n    IMPORT --> SAFE\n```\n\n**Diagram: Matplotlib Thread-Safe Initialization Flow**\n\nThe setup must occur before any matplotlib operations in the subthread:\n\nSources: [src/runtime/before_thread/plt_back_chinese.py:1-13](), [src/runtime/subthread_python_executor.py:9]()\n\n### Chinese Font Configuration\n\nPlatform-specific font configuration ensures Chinese characters render correctly:\n\n| Platform | Font Family | Configuration |\n|----------|-------------|---------------|\n| Windows | `SimHei` | `plt.rcParams['font.sans-serif'] = ['SimHei']` |\n| macOS | `PingFang SC` | `plt.rcParams['font.sans-serif'] = ['PingFang SC']` |\n| Linux | `WenQuanYi Micro Hei` | `plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei']` |\n\nThe system also sets `axes.unicode_minus = False` to prevent minus sign rendering issues.\n\nSources: [tests/playground/gen/g7/04_plt.py:194-202]()\n\n### Subway Network Visualization\n\nThe `plot_subway_network()` function demonstrates complex multi-layer plotting:\n\n```mermaid\ngraph TB\n    INIT[\"Initialize Canvas<br/>plt.subplots()\"]\n    \n    subgraph \"Layer 1: Lines\"\n        MAP[\"Build route mapping<br/>get_route_station_mapping()\"]\n        COLORS[\"Assign colors<br/>plt.cm.tab10()\"]\n        PLOT_LINE[\"Plot line segments<br/>ax.plot()\"]\n    end\n    \n    subgraph \"Layer 2: Stations\"\n        NORMAL[\"Plot normal stations<br/>ax.scatter() white\"]\n        TRANSFER[\"Plot transfer stations<br/>ax.scatter() red\"]\n    end\n    \n    subgraph \"Layer 3: Annotations\"\n        LABEL[\"Annotate key stations<br/>ax.annotate()\"]\n    end\n    \n    BEAUTY[\"Add legends, grid, title\"]\n    SAVE[\"Save figure<br/>plt.savefig()\"]\n    \n    INIT --> MAP\n    MAP --> COLORS\n    COLORS --> PLOT_LINE\n    PLOT_LINE --> NORMAL\n    NORMAL --> TRANSFER\n    TRANSFER --> LABEL\n    LABEL --> BEAUTY\n    BEAUTY --> SAVE\n```\n\n**Diagram: Multi-Layer Subway Visualization Process**\n\nThe plotting strategy uses layering to ensure visual clarity:\n- Lines plotted first at lower z-order\n- Normal stations as white circles with gray borders\n- Transfer stations as larger red circles on top\n- Selective annotation of transfer stations to avoid clutter\n\nSources: [tests/playground/gen/g7/02_plt.py:115-223]()\n\n### Route-Based Line Plotting\n\nThe system groups stations by route ID for proper line connectivity:\n\n```python\ndef get_route_station_mapping(drw_data: DrwSubwayData) -> dict[str, list[DrwStation]]:\n    route_mapping = defaultdict(list)\n    for line in drw_data.l:\n        for station in line.st:\n            route_ids = station.r.split(\"|\")\n            for rid in route_ids:\n                if rid and station.sl:\n                    route_mapping[rid].append(station)\n    return route_mapping\n```\n\nThis ensures stations are connected in the correct order along each route segment.\n\nSources: [tests/playground/gen/g7/02_plt.py:99-113]()\n\n---\n\n## Complete Example Workflows\n\n### Scenic Spot Processing Workflow\n\n```mermaid\ngraph TD\n    START[\"CSV File Input\"]\n    READ[\"read_csv_to_pydantic()<br/>Parse & Validate\"]\n    \n    subgraph \"Validation Layer\"\n        ENUM[\"Enum Validation<br/>StarRatingEnum, OpenStatusEnum\"]\n        COORD[\"Coordinate Parsing<br/>longitude, latitude\"]\n        HOURS[\"Hours Validation<br/>suggested_play_hours > 0\"]\n    end\n    \n    MODEL[\"List[ScenicSpot]<br/>Validated Models\"]\n    \n    subgraph \"Output Generation\"\n        SCHEMA_GEN[\"Generate Schema<br/>model_json_schema()\"]\n        DATA_GEN[\"Generate Data<br/>model_dump()\"]\n        STATS[\"Calculate Statistics<br/>Count by star rating<br/>Average play hours\"]\n    end\n    \n    OUT_SCHEMA[\"beijing_scenic_spot_schema_with_play_hours.json\"]\n    OUT_DATA[\"beijing_scenic_spot_validated_data_with_play_hours.json\"]\n    CONSOLE[\"Console Statistics Output\"]\n    \n    START --> READ\n    READ --> ENUM\n    READ --> COORD\n    READ --> HOURS\n    ENUM --> MODEL\n    COORD --> MODEL\n    HOURS --> MODEL\n    MODEL --> SCHEMA_GEN\n    MODEL --> DATA_GEN\n    MODEL --> STATS\n    SCHEMA_GEN --> OUT_SCHEMA\n    DATA_GEN --> OUT_DATA\n    STATS --> CONSOLE\n```\n\n**Diagram: End-to-End Scenic Spot Data Processing Workflow**\n\nThe workflow generates both schema documentation and validated data files, along with descriptive statistics:\n\n```\nâ æåå¤ç 15 æ¡æ¯åºæ°æ®\nð æçº§åå¸:\n   - 3A: 9 æ¡\n   - 4A: 5 æ¡\n   - 5A: 1 æ¡\nð æ¸¸ç©æ¶é¿ç»è®¡:\n   - æ»å»ºè®®æ¸¸ç©æ¶é¿: 26.5 å°æ¶\n   - å¹³åå»ºè®®æ¸¸ç©æ¶é¿: 1.8 å°æ¶\n```\n\nSources: [tests/playground/gen/g9/g9.py:115-153]()\n\n### Subway Visualization Workflow\n\n```mermaid\ngraph TD\n    JSON[\"1100_drw_beijing.json<br/>Raw Subway Data\"]\n    VALIDATE[\"read_drw_subway_data()<br/>Pydantic Validation\"]\n    MODEL[\"DrwSubwayData<br/>Validated Model\"]\n    \n    subgraph \"Visualization Preparation\"\n        EXTRACT[\"Extract coordinates<br/>parse_lon_lat()\"]\n        ROUTE_MAP[\"Build route mapping<br/>get_route_station_mapping()\"]\n        CLASSIFY[\"Classify stations<br/>Transfer vs Normal\"]\n    end\n    \n    subgraph \"Matplotlib Rendering\"\n        CANVAS[\"Create canvas<br/>plt.subplots()\"]\n        LINES[\"Plot line segments<br/>ax.plot() with colors\"]\n        STATIONS[\"Plot station nodes<br/>ax.scatter()\"]\n        ANNOTATE[\"Add station labels<br/>ax.annotate()\"]\n        LEGEND[\"Add legend & grid<br/>ax.legend()\"]\n    end\n    \n    OUTPUT[\"subway_network.png<br/>High-DPI Image\"]\n    \n    JSON --> VALIDATE\n    VALIDATE --> MODEL\n    MODEL --> EXTRACT\n    MODEL --> ROUTE_MAP\n    MODEL --> CLASSIFY\n    EXTRACT --> CANVAS\n    ROUTE_MAP --> LINES\n    CLASSIFY --> STATIONS\n    STATIONS --> ANNOTATE\n    ANNOTATE --> LEGEND\n    LEGEND --> OUTPUT\n```\n\n**Diagram: Subway Network Visualization Workflow**\n\nThe visualization process separates data validation from rendering logic, ensuring the plotting code works with clean, validated data structures.\n\nSources: [tests/playground/gen/g7/02_plt.py:64-223]()\n\n---\n\n## Key Design Patterns\n\n### Model Composition Pattern\n\nThe system uses hierarchical Pydantic models to represent nested structures:\n\n```\nDrwSubwayData (root)\nâââ l: List[DrwLine]\n    âââ st: List[DrwStation]\n        âââ sl: str (coordinate)\n        âââ t: str (type)\n        âââ r: str (routes)\n```\n\nThis composition allows type-safe access at each level and automatic validation of the entire structure.\n\nSources: [tests/playground/gen/g7/01_valid7.py:27-36]()\n\n### Validation Chain Pattern\n\nField validators can reference other validators or call custom validation functions:\n\n1. **Type coercion**: Pydantic converts string to float automatically\n2. **Format validation**: Custom validator checks coordinate format\n3. **Range validation**: Ensures values are within acceptable bounds\n4. **Cross-field validation**: Validates relationships between fields\n\nSources: [tests/playground/gen/g9/g9.py:42-72]()\n\n### Separation of Concerns Pattern\n\nThe codebase separates:\n- **Data models**: Pydantic classes define structure and validation\n- **Transformation logic**: Functions convert between model types\n- **Visualization logic**: Matplotlib code operates on validated models\n- **I/O operations**: File reading/writing isolated from business logic\n\nThis separation enables testing each component independently and reusing models across different operations.\n\nSources: [tests/playground/gen/g7/06.dump_all.py:94-212]()\n\n---\n\n## Execution in Agent Context\n\nWhen the agent executes data processing code, it uses one of the execution strategies described in [Execution Runtime](#5). The matplotlib setup is particularly important for subthread execution:\n\n1. The `before_thread/plt_back_chinese.py` module is imported before any matplotlib code\n2. This sets the non-GUI backend (`Agg`) to prevent threading issues\n3. Chinese fonts are configured for proper character rendering\n4. Code executes safely in the isolated subthread environment\n\nSources: [src/runtime/subthread_python_executor.py:9](), [src/runtime/before_thread/plt_back_chinese.py:1-13]()\n\nThe workspace state management (see [Workspace State Management](#5.5)) ensures that data loaded in one execution persists for subsequent operations, allowing multi-step data processing workflows.\n\n---\n\n# Page: Geographic Data Processing\n\n# Geographic Data Processing\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [tests/playground/gen/g6/13_fil.py](tests/playground/gen/g6/13_fil.py)\n- [tests/playground/gen/g7/01_valid7.py](tests/playground/gen/g7/01_valid7.py)\n- [tests/playground/gen/g7/02_plt.py](tests/playground/gen/g7/02_plt.py)\n- [tests/playground/gen/g7/1100_drw_beijing.json](tests/playground/gen/g7/1100_drw_beijing.json)\n- [tests/playground/gen/g7/1100_info_beijing.json](tests/playground/gen/g7/1100_info_beijing.json)\n- [tests/playground/gen/g7/subway.md](tests/playground/gen/g7/subway.md)\n\n</details>\n\n\n\nThis page documents the geographic data processing capabilities in the algo_agent system, specifically focusing on Beijing subway/metro data and street network analysis. These examples demonstrate the agent's ability to work with structured geographic data, perform spatial filtering, construct network graphs, and generate visualizations.\n\nFor Python code execution mechanics, see [Python Code Execution Tool](#4.2). For data visualization examples, see [Data Processing and Visualization](#7.2).\n\n---\n\n## Overview\n\nThe geographic data processing examples showcase two primary use cases:\n\n1. **Beijing Subway Data Processing** - Parsing hierarchical JSON data representing subway lines and stations with geographic coordinates, validating with Pydantic models, and generating network visualizations\n2. **Beijing Street Network Filtering** - Filtering large-scale address/street datasets based on geographic proximity and connectivity using graph analysis\n\nThese examples demonstrate the agent's capability to handle real-world geographic datasets with multiple file formats (JSON, TSV), perform coordinate-based spatial operations, and apply graph algorithms for connectivity analysis.\n\n---\n\n## Beijing Subway Data Processing\n\n### Data Schema Overview\n\nThe Beijing subway data consists of two complementary JSON files with abbreviated field names:\n\n| File | Purpose | Root Fields |\n|------|---------|-------------|\n| `1100_drw_beijing.json` | Geographic/drawing data | `s` (subject), `i` (region ID), `l` (lines array) |\n| `1100_info_beijing.json` | Schedule/operational data | `i` (region ID), `l` (lines array) |\n\n**Key Field Mappings**:\n\n| Abbreviated | Full Name | Description |\n|-------------|-----------|-------------|\n| `st` | stations | Array of station objects |\n| `sl` | station location | Coordinates as \"longitude,latitude\" |\n| `n` | name | Station name |\n| `sid` | station ID | Unique station identifier |\n| `r` | route | Route IDs (pipe-separated) |\n| `t` | type | Station type (1=normal, 2=transfer) |\n| `ls` | line-station | Line-station association ID |\n| `lt` | leave time | Departure time |\n| `ft` | first/arrive time | Arrival time |\n\n**Sources**: [tests/playground/gen/g7/subway.md:19-48]()\n\n### Pydantic Data Models\n\nThe system uses Pydantic models to validate and parse the JSON data with type safety:\n\n```mermaid\nclassDiagram\n    class DrwSubwayData {\n        +str s\n        +str i\n        +List~DrwLine~ l\n    }\n    \n    class DrwLine {\n        +List~DrwStation~ st\n    }\n    \n    class DrwStation {\n        +Optional~str~ rs\n        +Optional~str~ sl\n        +str n\n        +str sid\n        +Optional~str~ r\n        +Optional~str~ t\n        +str si\n    }\n    \n    class InfoSubwayData {\n        +str i\n        +List~InfoLine~ l\n    }\n    \n    class InfoLine {\n        +str a\n        +List~InfoStation~ st\n    }\n    \n    class InfoStation {\n        +str ac\n        +List~InfoDirection~ d\n        +str si\n    }\n    \n    class InfoDirection {\n        +str ls\n        +Optional~str~ lt\n        +str n\n        +Optional~str~ ft\n    }\n    \n    DrwSubwayData --> DrwLine\n    DrwLine --> DrwStation\n    InfoSubwayData --> InfoLine\n    InfoLine --> InfoStation\n    InfoStation --> InfoDirection\n```\n\n**Model Definitions**:\n\n- **`DrwSubwayData`** [tests/playground/gen/g7/01_valid7.py:32-36]() - Root model for geographic drawing data\n- **`DrwLine`** [tests/playground/gen/g7/01_valid7.py:27-29]() - Represents a subway line with stations\n- **`DrwStation`** [tests/playground/gen/g7/01_valid7.py:6-24]() - Station with coordinates, routes, and metadata\n- **`InfoSubwayData`** [tests/playground/gen/g7/01_valid7.py:61-64]() - Root model for schedule/operational data\n- **`InfoLine`** [tests/playground/gen/g7/01_valid7.py:55-58]() - Line with operational station info\n- **`InfoStation`** [tests/playground/gen/g7/01_valid7.py:48-52]() - Station with directional schedule data\n- **`InfoDirection`** [tests/playground/gen/g7/01_valid7.py:40-45]() - Direction-specific timing information\n\n**Sources**: [tests/playground/gen/g7/01_valid7.py:1-135]()\n\n### Data Loading and Parsing\n\n**File Reading Functions**:\n\n```mermaid\ngraph LR\n    JSON[\"1100_drw_beijing.json<br/>1100_info_beijing.json\"] --> read_drw[\"read_drw_subway_data()\"]\n    JSON --> read_info[\"read_info_subway_data()\"]\n    \n    read_drw --> DrwSubwayData[\"DrwSubwayData<br/>Pydantic Model\"]\n    read_info --> InfoSubwayData[\"InfoSubwayData<br/>Pydantic Model\"]\n    \n    DrwSubwayData --> validation[\"Type Validation<br/>Field Checking\"]\n    InfoSubwayData --> validation\n    \n    validation --> error[\"ValidationError<br/>(if invalid)\"]\n    validation --> success[\"Parsed Model<br/>Instance\"]\n```\n\n**Key Functions**:\n\n- **`read_drw_subway_data(file_path)`** [tests/playground/gen/g7/01_valid7.py:72-89]() - Loads and validates geographic data\n  - Checks file existence with `Path.exists()`\n  - Opens file with UTF-8 encoding\n  - Instantiates `DrwSubwayData` model with automatic validation\n  - Raises `FileNotFoundError` if file missing\n\n- **`read_info_subway_data(file_path)`** [tests/playground/gen/g7/01_valid7.py:92-109]() - Loads and validates schedule data\n  - Same pattern as `read_drw_subway_data()`\n  - Returns `InfoSubwayData` instance\n\n**Sources**: [tests/playground/gen/g7/01_valid7.py:67-109]()\n\n### Geographic Coordinate Processing\n\n**Coordinate Parsing Flow**:\n\n```mermaid\ngraph TD\n    raw[\"Raw Coordinate String<br/>sl: '116.178945,39.925686'\"] --> parse[\"parse_lon_lat()\"]\n    \n    parse --> split[\"str.split(',')\"]\n    split --> convert[\"float(lon), float(lat)\"]\n    convert --> tuple[\"tuple[float, float]\"]\n    convert --> error[\"ValueError\"]\n    \n    tuple --> success[\"(116.178945, 39.925686)\"]\n    error --> none[\"None\"]\n```\n\n**`parse_lon_lat(sl_str)`** [tests/playground/gen/g7/02_plt.py:83-96]():\n- Extracts longitude and latitude from comma-separated string\n- Returns `tuple[float, float]` or `None` if parsing fails\n- Handles `None` input and malformed strings with exception handling\n\n**Route-Station Mapping**:\n\n**`get_route_station_mapping(drw_data)`** [tests/playground/gen/g7/02_plt.py:99-113]():\n- Constructs `dict[str, list[DrwStation]]` mapping route IDs to stations\n- Splits pipe-separated route IDs from `station.r` field\n- Filters stations without coordinates (`station.sl`)\n- Used for drawing connected line segments in visualization\n\n**Sources**: [tests/playground/gen/g7/02_plt.py:83-113]()\n\n### Subway Network Visualization\n\n**Visualization Architecture**:\n\n```mermaid\ngraph TB\n    data[\"DrwSubwayData\"] --> mapping[\"get_route_station_mapping()\"]\n    \n    mapping --> routes[\"route_mapping<br/>dict[route_id, stations]\"]\n    \n    routes --> plot_lines[\"Draw Line Segments<br/>ax.plot()\"]\n    data --> plot_stations[\"Draw Station Nodes<br/>ax.scatter()\"]\n    \n    plot_lines --> colors[\"Route Colors<br/>plt.cm.tab10\"]\n    plot_stations --> types[\"Station Types<br/>normal vs transfer\"]\n    \n    colors --> canvas[\"Matplotlib Figure\"]\n    types --> canvas\n    \n    canvas --> annotate[\"Annotate Transfer Stations<br/>ax.annotate()\"]\n    annotate --> save[\"plt.savefig()\"]\n    annotate --> display[\"plt.show()\"]\n```\n\n**`plot_subway_network(drw_data, figsize)`** [tests/playground/gen/g7/02_plt.py:115-223]():\n\n**Configuration**:\n- Sets Chinese font support: `plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]`\n- Initializes figure with specified `figsize` (default 12x10)\n\n**Rendering Steps**:\n\n1. **Line Segments** [tests/playground/gen/g7/02_plt.py:133-154]()\n   - Iterates through route mappings\n   - Filters stations with valid coordinates\n   - Assigns colors from `plt.cm.tab10` color map\n   - Plots connected line segments with `ax.plot(lons, lats, ...)`\n   - Uses `linewidth=2, alpha=0.7`\n\n2. **Station Nodes** [tests/playground/gen/g7/02_plt.py:157-191]()\n   - Separates normal stations (`t==\"1\"`) and transfer stations (`t==\"2\"`)\n   - **Normal stations**: white fill, gray border, size=50\n   - **Transfer stations**: red fill, dark red border, size=100 (more prominent)\n   - Renders with `ax.scatter()`\n\n3. **Annotations** [tests/playground/gen/g7/02_plt.py:194-203]()\n   - Labels first 10 transfer stations to avoid clutter\n   - Uses `ax.annotate()` with offset `(5, 5)` points\n   - Yellow background with rounded box style\n\n4. **Output** [tests/playground/gen/g7/02_plt.py:221-223]()\n   - Saves to `subway_network.png` at 300 DPI\n   - Displays interactively with `plt.show()`\n\n**Sources**: [tests/playground/gen/g7/02_plt.py:115-242]()\n\n---\n\n## Beijing Street Network Processing\n\n### Dataset Structure\n\nThe street network processing works with three TSV files containing OpenStreetMap-derived data:\n\n| File | Key Fields | Purpose |\n|------|-----------|---------|\n| `CN-addresses-beijing.tsv` | `postal_code_`, `city_`, `street_`, `x_min`, `y_min`, `x_max`, `y_max` | Address ranges with bounding boxes |\n| `CN-houses-beijing.tsv` | `postal_code`, `city`, `street`, `house_number`, `x`, `y` | Individual house locations |\n| `CN-streets-beijing.tsv` | `city`, `state`, `province`, `street_name`, `postal_code` | Street metadata |\n\n**Sources**: [tests/playground/gen/g6/13_fil.py:8-16]()\n\n### Data Filtering Pipeline\n\n**Overall Processing Flow**:\n\n```mermaid\ngraph TD\n    raw_files[\"Raw TSV Files\"] --> read[\"pd.read_csv()\"]\n    \n    read --> normalize[\"Column Name Normalization<br/>str.strip().replace()\"]\n    \n    normalize --> filter_streets[\"Filter Streets<br/>state=='åäº¬å¸'\"]\n    \n    filter_streets --> get_names[\"Extract Street Names<br/>df['street_name'].unique()\"]\n    \n    get_names --> filter_addr[\"Filter Addresses<br/>street_ in valid_streets\"]\n    \n    filter_addr --> graph[\"build_street_graph()<br/>NetworkX Graph\"]\n    \n    graph --> components[\"nx.connected_components()\"]\n    \n    components --> largest[\"max(components, key=len)\"]\n    \n    largest --> final_filter[\"Final Filter:<br/>- No missing fields<br/>- In largest component\"]\n    \n    final_filter --> output[\"Output Filtered TSVs\"]\n```\n\n**Key Processing Steps**:\n\n1. **Data Loading** [tests/playground/gen/g6/13_fil.py:13-21]()\n   - Reads TSV files with `pd.read_csv(sep=\"\\t\", encoding=\"utf-8-sig\")`\n   - Normalizes column names: strip whitespace, replace spaces with underscores\n\n2. **Geographic Filtering** [tests/playground/gen/g6/13_fil.py:24-38]()\n   - Filters streets for Beijing: `df_street[\"state\"] == \"åäº¬å¸\"`\n   - Creates valid street name set from both `df_street_bj` and `df_house`\n   - Filters addresses where `street_` appears in valid set\n\n3. **Graph Construction** [tests/playground/gen/g6/13_fil.py:41-97]()\n   - Builds connectivity graph (see below)\n\n4. **Component Analysis** [tests/playground/gen/g6/13_fil.py:100-106]()\n   - Finds largest connected component: `max(nx.connected_components(street_graph), key=len)`\n\n5. **Final Filtering** [tests/playground/gen/g6/13_fil.py:109-136]()\n   - Validates all required fields are non-null\n   - Filters to streets in largest connected component\n   - Ensures data consistency across all three files\n\n**Sources**: [tests/playground/gen/g6/13_fil.py:1-150]()\n\n### Street Connectivity Graph\n\n**Graph Construction Algorithm**:\n\n```mermaid\ngraph TD\n    init[\"Initialize NetworkX Graph<br/>G = nx.Graph()\"] --> nodes[\"Add All Street Nodes<br/>G.add_nodes_from()\"]\n    \n    nodes --> coord_extract[\"Extract Coordinates<br/>addresses: x_min, y_min<br/>houses: x, y\"]\n    \n    coord_extract --> loop_addr[\"Loop: addresses Ã addresses\"]\n    coord_extract --> loop_house[\"Loop: houses Ã houses\"]\n    coord_extract --> loop_cross[\"Loop: addresses Ã houses\"]\n    \n    loop_addr --> calc_dist[\"Calculate Distance<br/>sqrt((x1-x2)Â² + (y1-y2)Â²)\"]\n    loop_house --> calc_dist\n    loop_cross --> calc_dist\n    \n    calc_dist --> check[\"dist < threshold<br/>(0.001 degrees)\"]\n    \n    check --> edge[\"Add Edge<br/>G.add_edge(street1, street2)\"]\n    \n    edge --> graph_out[\"Connected Street Graph\"]\n```\n\n**`build_street_graph(df_addr, df_house)`** [tests/playground/gen/g6/13_fil.py:41-97]():\n\n**Parameters**:\n- `df_addr`: Addresses DataFrame with `street_`, `x_min`, `y_min` columns\n- `df_house`: Houses DataFrame with `street`, `x`, `y` columns\n\n**Algorithm**:\n\n1. **Initialization** [tests/playground/gen/g6/13_fil.py:47-56]()\n   - Creates empty `nx.Graph()`\n   - Extracts valid coordinate data from both DataFrames\n   - Adds all unique street names as nodes\n\n2. **Distance Threshold** [tests/playground/gen/g6/13_fil.py:60]()\n   - Uses `threshold = 0.001` degrees (~100 meters at Beijing's latitude)\n\n3. **Edge Construction** [tests/playground/gen/g6/13_fil.py:63-96]()\n   - Compares all pairs of street coordinates (O(nÂ²) complexity)\n   - Calculates Euclidean distance: `np.sqrt((x1-x2)**2 + (y1-y2)**2)`\n   - Adds edge if distance < threshold\n   - Handles three combinations:\n     - Address-to-address comparisons [lines 63-73]\n     - House-to-house comparisons [lines 76-86]\n     - Address-to-house comparisons [lines 88-96]\n\n**Return**: NetworkX `Graph` object with streets as nodes and proximity relationships as edges\n\n**Sources**: [tests/playground/gen/g6/13_fil.py:41-97]()\n\n### Field Completeness Validation\n\n**Required Fields by File**:\n\n```mermaid\ngraph LR\n    subgraph Addresses\n        addr_fields[\"postal_code_<br/>city_<br/>street_<br/>x_min, x_max<br/>y_min, y_max<br/>house_min, house_max<br/>house_odd, house_even\"]\n    end\n    \n    subgraph Houses\n        house_fields[\"postal_code<br/>city<br/>street<br/>house_number<br/>x, y<br/>country\"]\n    end\n    \n    subgraph Streets\n        street_fields[\"city<br/>country<br/>state<br/>province<br/>postal_code<br/>street_name\"]\n    end\n    \n    addr_fields --> check_addr[\"df[cols].notna().all(axis=1)\"]\n    house_fields --> check_house[\"df[cols].notna().all(axis=1)\"]\n    street_fields --> check_street[\"df[cols].notna().all(axis=1)\"]\n    \n    check_addr --> valid[\"Valid Records Only\"]\n    check_house --> valid\n    check_street --> valid\n```\n\n**Validation Implementation**:\n\n- **Addresses** [tests/playground/gen/g6/13_fil.py:111-118]()\n  - Required: `[\"postal_code_\", \"city_\", \"street_\", \"x_min\", \"x_max\", \"y_min\", \"y_max\", \"house_min\", \"house_max\", \"house_odd\", \"house_even\"]`\n  - Filter: `df_addr_bj[addr_required_cols].notna().all(axis=1)`\n\n- **Houses** [tests/playground/gen/g6/13_fil.py:122-129]()\n  - Required: `[\"postal_code\", \"city\", \"street\", \"house_number\", \"x\", \"y\", \"country\"]`\n  - Filter: `df_house_bj[house_required_cols].notna().all(axis=1)`\n\n- **Streets** [tests/playground/gen/g6/13_fil.py:132-136]()\n  - Required: `[\"city\", \"country\", \"state\", \"province\", \"postal_code\", \"street_name\"]`\n  - Filter: `df_street_bj[street_required_cols].notna().all(axis=1)`\n\nAll three filters also check membership in the largest connected component via:\n```python\ndf[\"street_name\"].isin(largest_cc)\n```\n\n**Sources**: [tests/playground/gen/g6/13_fil.py:109-136]()\n\n### Output Generation\n\n**File Writing**:\n\n```mermaid\ngraph LR\n    final_dfs[\"Filtered DataFrames\"] --> create_dir[\"output_dir.mkdir(exist_ok=True)\"]\n    \n    create_dir --> write_addr[\"df_addr_final.to_csv()<br/>CN-addresses.tsv\"]\n    create_dir --> write_house[\"df_house_final.to_csv()<br/>CN-houses.tsv\"]\n    create_dir --> write_street[\"df_street_final.to_csv()<br/>CN-streets.tsv\"]\n    \n    write_addr --> stats[\"Print Statistics\"]\n    write_house --> stats\n    write_street --> stats\n```\n\n**Output Path**: [tests/playground/gen/g6/13_fil.py:139-140]()\n```python\noutput_dir = data_dir / \"filtered_bj\"\noutput_dir.mkdir(exist_ok=True)\n```\n\n**Writing Parameters**: [tests/playground/gen/g6/13_fil.py:142-144]()\n- Separator: `sep=\"\\t\"`\n- No index: `index=False`\n- Encoding: `encoding=\"utf-8-sig\"` (BOM for Excel compatibility)\n\n**Statistics Output**: [tests/playground/gen/g6/13_fil.py:146-150]()\n- Filtered address count\n- Filtered house count\n- Filtered street count\n- Size of largest connected component\n\n**Sources**: [tests/playground/gen/g6/13_fil.py:138-150]()\n\n---\n\n## Common Patterns and Utilities\n\n### Data Loading Pattern\n\nBoth examples follow a consistent pattern for loading structured geographic data:\n\n1. **Path Validation**: Check file existence with `Path.exists()`\n2. **Encoding Specification**: Use explicit UTF-8 encoding\n3. **Error Handling**: Raise descriptive exceptions (`FileNotFoundError`)\n4. **Type Validation**: Pydantic models or pandas DataFrame operations\n\n### Coordinate Systems\n\n**Coordinate Formats**:\n\n| System | Format | Example | Use Case |\n|--------|--------|---------|----------|\n| Geographic (WGS84) | `\"longitude,latitude\"` string | `\"116.178945,39.925686\"` | Subway station locations |\n| Coordinate pair | `x`, `y` float columns | `x=116.332, y=39.723` | House/address positions |\n| Bounding box | `x_min`, `x_max`, `y_min`, `y_max` | Address ranges | Spatial filtering |\n\n### Distance Calculations\n\n**Euclidean Distance** [tests/playground/gen/g6/13_fil.py:68-70]():\n```python\ndist = np.sqrt((row1[\"x_min\"] - row2[\"x_min\"])**2 + \n               (row1[\"y_min\"] - row2[\"y_min\"])**2)\n```\n\n**Threshold Selection**:\n- 0.001 degrees â 100 meters at Beijing's latitude (~40Â°N)\n- Suitable for identifying adjacent streets\n- Trade-off between connectivity and false positives\n\n### Graph Analysis Tools\n\n**NetworkX Integration**:\n- **Graph Construction**: `G = nx.Graph()`\n- **Node Addition**: `G.add_nodes_from(street_list)`\n- **Edge Addition**: `G.add_edge(node1, node2)`\n- **Connected Components**: `nx.connected_components(G)` returns iterator of sets\n- **Largest Component**: `max(components, key=len)`\n\n**Sources**: [tests/playground/gen/g6/13_fil.py:47-106]()\n\n---\n\n## Usage Examples\n\n### Subway Data Processing\n\n**Complete workflow** [tests/playground/gen/g7/01_valid7.py:113-135]():\n\n```python\nimport os\ndir = os.path.dirname(__file__)\n\n# Load drawing data\ndrw_data = read_drw_subway_data(os.path.join(dir, \"1100_drw_beijing.json\"))\nprint(f\"ä¸»é¢ï¼{drw_data.s}ï¼åºåIDï¼{drw_data.i}\")\n\n# Access first station\nfirst_station = drw_data.l[0].st[0]\nprint(f\"ç¬¬ä¸ä¸ªç«ç¹åç§°ï¼{first_station.n}ï¼ç»çº¬åº¦ï¼{first_station.sl}\")\n\n# Load schedule data\ninfo_data = read_info_subway_data(os.path.join(dir, \"1100_info_beijing.json\"))\nprint(f\"åºåIDï¼{info_data.i}\")\n\n# Access schedule info\nfirst_direction = info_data.l[0].st[0].d[0]\nprint(f\"çº¿è·¯ç«ç¹IDï¼{first_direction.ls}ï¼åè½¦æ¶é´ï¼{first_direction.lt}\")\n```\n\n### Visualization\n\n**Generate network plot** [tests/playground/gen/g7/02_plt.py:226-242]():\n\n```python\nfrom pathlib import Path\n\n# Read data\npath = Path(__file__).parent\ndrw_file_path = path / \"1100_drw_beijing.json\"\ndrw_data = read_drw_subway_data(drw_file_path)\nprint(f\"æåè¯»åæ°æ®ï¼{drw_data.s}ï¼å±{len(drw_data.l)}æ¡çº¿è·¯\")\n\n# Generate visualization\nplot_subway_network(drw_data)\nprint(\"å°éç½ç»ç»å¶å®æï¼å·²ä¿å­ä¸º subway_network.png\")\n```\n\n### Street Network Filtering\n\n**Complete filtering process** [tests/playground/gen/g6/13_fil.py:6-150]():\n\nConfiguration and execution produces three filtered output files with statistics on:\n- Number of filtered addresses\n- Number of filtered houses  \n- Number of filtered streets\n- Size of largest connected street component\n\n**Sources**: [tests/playground/gen/g7/01_valid7.py:113-135](), [tests/playground/gen/g7/02_plt.py:226-242](), [tests/playground/gen/g6/13_fil.py:1-150]()\n\n---\n\n# Page: Development and Testing\n\n# Development and Testing\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.vscode/settings.json](.vscode/settings.json)\n- [docs/python.design.md](docs/python.design.md)\n- [docs/sql.md](docs/sql.md)\n- [pyproject.toml](pyproject.toml)\n- [src/agent/tool/base_tool.py](src/agent/tool/base_tool.py)\n- [src/memory/tree_todo/schemas.py](src/memory/tree_todo/schemas.py)\n- [src/memory/tree_todo/todo_track.py](src/memory/tree_todo/todo_track.py)\n- [uv.lock](uv.lock)\n\n</details>\n\n\n\nThis page provides an overview of development practices, testing strategies, and debugging approaches for contributors to the algo_agent codebase. It covers setting up the development environment, testing different system components, and leveraging the observability infrastructure for effective debugging.\n\nFor specific topics:\n- Testing execution modes and namespace behavior: see [Testing the Execution System](#8.1)\n- Implementing new tools: see [Creating New Tools](#8.2)\n- Setting up databases for examples: see [Database Setup for Examples](#8.3)\n\n---\n\n## Development Environment Setup\n\n### Project Configuration\n\nThe project uses Python 3.12+ with `uv` for dependency management. Core dependencies are defined in [pyproject.toml:7-20]().\n\n**Key dependencies for development:**\n\n| Dependency | Purpose |\n|------------|---------|\n| `openai>=2.7.2` | LLM integration (DashScope API compatible) |\n| `pydantic` | Data validation and schema generation |\n| `decorator>=5.2.1` | Function instrumentation (`@traceable`, `@log_function`) |\n| `deepdiff>=8.6.1` | Task tree change detection |\n| `matplotlib>=3.10.7` | Visualization in examples |\n| `pandas>=2.3.3` | Data processing |\n\n### VS Code Configuration\n\nThe project includes VS Code settings for Python development at [.vscode/settings.json:1-34](). Key configurations:\n\n- **PYTHONPATH setup**: Workspace folder automatically added to Python path\n- **Formatting**: Black formatter configured but auto-format disabled to prevent unwanted changes during refactoring\n- **Analysis mode**: Set to `openFilesOnly` to reduce resource usage during development\n\n```json\n{\n  \"python.analysis.diagnosticMode\": \"openFilesOnly\",\n  \"python.formatting.autoFormat\": false\n}\n```\n\nSources: [pyproject.toml:1-21](), [.vscode/settings.json:1-34]()\n\n---\n\n## Testing Philosophy\n\nThe codebase emphasizes **isolation testing** and **observability-driven debugging** rather than traditional unit test suites. This approach leverages:\n\n1. **Multiple execution strategies** with different isolation levels\n2. **Comprehensive logging** capturing every function call and state change\n3. **Structured execution results** with explicit status tracking\n4. **Version-controlled task state** for regression detection\n\n```mermaid\ngraph TB\n    DEV[\"Developer\"]\n    TEST[\"Test Code/Query\"]\n    EXEC[\"Execution System\"]\n    LOG[\"Log Analysis\"]\n    \n    DEV -->|\"Write\"| TEST\n    TEST -->|\"Execute in\"| EXEC\n    \n    subgraph \"Execution Modes\"\n        SUBPROCESS[\"run_structured_in_subprocess\"]\n        SUBTHREAD[\"run_structured_in_subthread\"]\n        DIRECT[\"run_structured_direct\"]\n    end\n    \n    EXEC --> SUBPROCESS\n    EXEC --> SUBTHREAD\n    EXEC --> DIRECT\n    \n    SUBPROCESS -->|\"ExecutionResult\"| LOG\n    SUBTHREAD -->|\"ExecutionResult\"| LOG\n    DIRECT -->|\"ExecutionResult\"| LOG\n    \n    LOG -->|\"Inspect traces\"| DEV\n    \n    subgraph \"Log Files\"\n        GLOBAL[\"logs/global.log\"]\n        TRACE[\"logs/trace.log\"]\n        UTILS[\"logs/utils.log\"]\n    end\n    \n    LOG --> GLOBAL\n    LOG --> TRACE\n    LOG --> UTILS\n```\n\n**Testing Strategy: Isolation Levels**\n\nSources: Architecture diagrams (Diagram 3: Multi-Strategy Execution System)\n\n---\n\n## Testing Individual Components\n\n### Tool System Testing\n\nWhen developing or testing tools, use the `BaseTool` interface contracts defined in [src/agent/tool/base_tool.py:7-76]():\n\n```mermaid\ngraph LR\n    TOOL[\"CustomTool extends BaseTool\"]\n    SCHEMA[\"get_tool_schema()\"]\n    RUN[\"run()\"]\n    VALIDATE[\"Pydantic validation\"]\n    \n    TOOL -->|\"Implements\"| SCHEMA\n    TOOL -->|\"Implements\"| RUN\n    TOOL -->|\"Auto-validates\"| VALIDATE\n    \n    TEST[\"Test Code\"]\n    TEST -->|\"1. Instantiate with params\"| TOOL\n    TEST -->|\"2. Call run()\"| TOOL\n    TEST -->|\"3. Assert output\"| RUN\n    \n    SCHEMA -->|\"Returns dict\"| AGENT[\"Agent can discover\"]\n```\n\n**Testing a tool implementation:**\n\n1. **Schema validation**: Ensure `get_tool_schema()` returns valid JSON Schema\n2. **Parameter validation**: Test with invalid parameters (Pydantic will raise `ValidationError`)\n3. **Execution logic**: Call `run()` and verify output matches expected format\n4. **Tool discovery**: Check that `tool_name()` returns expected identifier\n\nExample test pattern:\n```python\n# Test tool schema generation\ntool_schema = MyCustomTool.get_tool_schema()\nassert tool_schema[\"type\"] == \"function\"\nassert \"parameters\" in tool_schema[\"function\"]\n\n# Test execution with valid params\ntool = MyCustomTool(tool_call_purpose=\"test\", param1=\"value\")\nresult = tool.run()\nassert isinstance(result, str)\n\n# Test parameter validation\ntry:\n    invalid_tool = MyCustomTool(tool_call_purpose=\"test\")  # Missing required param\n    assert False, \"Should have raised ValidationError\"\nexcept ValidationError:\n    pass  # Expected\n```\n\nSources: [src/agent/tool/base_tool.py:7-76]()\n\n---\n\n## Testing Code Execution\n\n### Understanding Namespace Behavior\n\nThe execution system's complexity comes from Python's `exec()` namespace semantics. Key behaviors documented in [docs/python.design.md:1-645]():\n\n**Critical namespace rules:**\n\n| Scenario | `globals` | `locals` | Variable Assignment Destination |\n|----------|-----------|----------|-------------------------------|\n| `exec(code)` | Current `globals()` | Current `locals()` | Depends on scope |\n| `exec(code, {})` | Empty dict + `__builtins__` | Same as `globals` | `globals` dict |\n| `exec(code, {}, {})` | Empty dict + `__builtins__` | Empty dict | `locals` dict |\n\n**Testing namespace isolation:**\n\n```python\n# Test 1: Subprocess isolation\narg_globals = {\"__name__\": \"__main__\", \"x\": 10}\nresult = run_structured_in_subprocess(\n    code_snippet=\"y = x + 5\\nprint(y)\",\n    arg_globals=arg_globals,\n    timeout=5\n)\nassert result.status == ExecutionStatus.SUCCESS\nassert \"15\" in result.stdout\n\n# Test 2: Verify workspace persistence\nresult2 = run_structured_in_subprocess(\n    code_snippet=\"z = y * 2\",  # Uses y from previous execution\n    arg_globals=result.out_globals,  # Passed forward\n    timeout=5\n)\nassert result2.status == ExecutionStatus.SUCCESS\n```\n\nSources: [docs/python.design.md:1-98](), [docs/python.design.md:427-645]()\n\n---\n\n## Debugging with Observability\n\n### Log File Structure\n\nThe logging system (importance: 10.93) produces multiple specialized log files:\n\n```mermaid\ngraph TB\n    APP[\"Application Code\"]\n    DECORATOR[\"@log_function / @traceable\"]\n    \n    APP -->|\"Decorated with\"| DECORATOR\n    \n    subgraph \"Logger Instances\"\n        GLOBAL_LOGGER[\"global_logger\"]\n        TRACE_LOGGER[\"traceable_logger\"]\n        ALL_LOGGER[\"all_logger\"]\n    end\n    \n    DECORATOR --> GLOBAL_LOGGER\n    DECORATOR --> TRACE_LOGGER\n    DECORATOR --> ALL_LOGGER\n    \n    GLOBAL_LOGGER -->|\"System traces\"| GLOBAL_LOG[\"logs/global.log\"]\n    TRACE_LOGGER -->|\"Detailed execution\"| TRACE_LOG[\"logs/trace.log\"]\n    ALL_LOGGER -->|\"Combined\"| ALL_LOG[\"logs/all.log\"]\n    \n    subgraph \"Log Content\"\n        FUNC[\"Function calls & args\"]\n        TIME[\"Execution timing\"]\n        STACK[\"Stack traces on error\"]\n        STATE[\"Variable states\"]\n    end\n    \n    GLOBAL_LOG --> FUNC\n    GLOBAL_LOG --> TIME\n    TRACE_LOG --> STACK\n    TRACE_LOG --> STATE\n```\n\n**Debugging workflow:**\n\n1. **Start with `logs/utils.log`**: User-level operations and query processing\n2. **Check `logs/global.log`**: Function call traces with timing data\n3. **Deep dive `logs/trace.log`**: Full stack traces and parameter values\n4. **Correlate `logs/all.log`**: Combined view for complex issues\n\n**Common debugging patterns:**\n\n| Issue | Log File | What to Look For |\n|-------|----------|------------------|\n| Tool execution failure | `global.log` | `call_tools_safely` function call with tool arguments |\n| Timeout in execution | `trace.log` | Subprocess/thread execution duration |\n| Serialization error | `trace.log` | `filter_and_deepcopy_globals` exceptions (PickleError, UnicodeDecodeError) |\n| Task tree changes | `global.log` | `RecursivePlanTreeTodoTool.run()` calls and `_analyze_changes` output |\n| LLM response format | `global.log` | `generate_assistant_output_append` with response structure |\n\nSources: Architecture diagram (Diagram 4: Observability and Logging Architecture)\n\n---\n\n## Task Tree Version Tracking\n\n### Testing Task Management\n\nThe task tracking system maintains version history in `arg_todo_list` and `out_todo_list` at [src/memory/tree_todo/todo_track.py:6-18]():\n\n```mermaid\ngraph LR\n    V1[\"Version 1<br/>RecursivePlanTree\"]\n    V2[\"Version 2<br/>RecursivePlanTree\"]\n    V3[\"Version 3<br/>RecursivePlanTree\"]\n    \n    V1 -->|\"append to\"| ARG[\"arg_todo_list\"]\n    V2 -->|\"append to\"| ARG\n    V3 -->|\"append to\"| ARG\n    \n    DIFF[\"_analyze_changes()\"]\n    V1 -->|\"compare\"| DIFF\n    V2 -->|\"with\"| DIFF\n    DIFF -->|\"Detects\"| CHANGES[\"New tasks<br/>Status changes<br/>Level adjustments\"]\n    \n    TEST[\"Test Assertions\"]\n    TEST -->|\"Check history\"| ARG\n    TEST -->|\"Verify diff\"| CHANGES\n```\n\n**Testing change detection:**\n\n```python\n# Create initial task tree\ninitial_tree = RecursivePlanTree(\n    core_goal=\"Test Goal\",\n    tree_nodes=[\n        RecursivePlanTreeNode(\n            task_id=\"TASK-1\",\n            task_name=\"Setup\",\n            status=TaskStatus.PENDING\n        )\n    ]\n)\n\n# Run tool (saves to arg_todo_list)\nresult1 = run(initial_tree)\n\n# Modify tree\nupdated_tree = RecursivePlanTree(\n    core_goal=\"Test Goal\",\n    tree_nodes=[\n        RecursivePlanTreeNode(\n            task_id=\"TASK-1\",\n            task_name=\"Setup\",\n            status=TaskStatus.COMPLETED  # Changed status\n        ),\n        RecursivePlanTreeNode(\n            task_id=\"TASK-2\",\n            task_name=\"Execute\",  # New task\n            status=TaskStatus.PENDING\n        )\n    ]\n)\n\nresult2 = run(updated_tree)\n\n# Assert change detection\nassert \"ð ç¶æåæ´\" in result2[\"changes_summary\"]\nassert \"ð æ°å¢ä»»å¡\" in result2[\"changes_summary\"]\n```\n\nSources: [src/memory/tree_todo/todo_track.py:1-201](), [src/memory/tree_todo/schemas.py:1-81]()\n\n---\n\n## Common Development Workflows\n\n### Workflow 1: Adding Python Execution Features\n\n```mermaid\ngraph TB\n    START[\"Identify requirement\"]\n    DESIGN[\"Design namespace handling\"]\n    \n    START --> DESIGN\n    \n    IMPL[\"Implement in executor\"]\n    TEST_ISO[\"Test isolation\"]\n    TEST_SERIAL[\"Test serialization\"]\n    \n    DESIGN --> IMPL\n    IMPL --> TEST_ISO\n    TEST_ISO --> TEST_SERIAL\n    \n    subgraph \"Test Cases\"\n        T1[\"Subprocess: Full isolation\"]\n        T2[\"Subthread: Shared memory\"]\n        T3[\"Direct: In-process\"]\n    end\n    \n    TEST_ISO --> T1\n    TEST_ISO --> T2\n    TEST_ISO --> T3\n    \n    VERIFY[\"Verify globals persistence\"]\n    LOG_CHECK[\"Check trace logs\"]\n    \n    T1 --> VERIFY\n    T2 --> VERIFY\n    T3 --> VERIFY\n    VERIFY --> LOG_CHECK\n    \n    LOG_CHECK --> DONE[\"Complete\"]\n```\n\n**Key considerations:**\n- Document namespace behavior in [docs/python.design.md]()\n- Test all three execution strategies\n- Verify `filter_and_deepcopy_globals` handles new data types\n- Check logs for serialization errors (UnicodeDecodeError, PickleError)\n\n### Workflow 2: Extending Tool System\n\n1. **Define tool class** extending `BaseTool`\n2. **Add Pydantic fields** with descriptions\n3. **Implement `run()` method** with business logic\n4. **Test schema generation** via `get_tool_schema()`\n5. **Register in agent** (add to tool registry)\n6. **Test end-to-end** with agent query\n\nSee [Creating New Tools](#8.2) for detailed guide.\n\nSources: [src/agent/tool/base_tool.py:7-76]()\n\n---\n\n## Best Practices\n\n### 1. Execution Mode Selection\n\n| Use Case | Recommended Mode | Reason |\n|----------|-----------------|---------|\n| Production agent | Subprocess | Maximum isolation, crash safety |\n| Development/debugging | Subthread | Faster, shared memory easier to inspect |\n| Unit tests | Direct | Fast execution, deterministic |\n| Untrusted code | Subprocess | Process isolation prevents escape |\n\n### 2. Logging Strategy\n\n- **Always use decorators**: Apply `@traceable` to new functions for automatic logging\n- **Log before serialization**: Critical for debugging pickle errors\n- **Include context**: Log function arguments for reproducibility\n- **Filter sensitive data**: Don't log credentials or API keys\n\n### 3. State Management\n\n- **Explicit workspace passing**: Always pass `arg_globals` between executions\n- **Deep copy caution**: Understand `model_copy(deep=True)` semantics in Pydantic V2\n- **Validate serialization**: Test that custom objects can be pickled\n- **Document exclusions**: Update `filter_and_deepcopy_globals` for non-serializable types\n\n### 4. Error Handling\n\n```python\n# Pattern: Structured execution with explicit status\nresult = run_structured_in_subprocess(code, globals, timeout)\n\nmatch result.status:\n    case ExecutionStatus.SUCCESS:\n        # Process result.stdout and result.out_globals\n        pass\n    case ExecutionStatus.TIMEOUT:\n        # Handle timeout (log.warning, retry, or fail gracefully)\n        pass\n    case ExecutionStatus.CRASHED:\n        # Process died (log.error, investigate core dump)\n        pass\n    case ExecutionStatus.FAILURE:\n        # Exception occurred (check result.exception_type and result.exception_str)\n        pass\n```\n\n### 5. Testing Guidelines\n\n- **Isolate components**: Test tools independently before integration\n- **Mock LLM calls**: Don't hit real APIs in tests\n- **Verify logs**: Assert expected log entries for critical paths\n- **Test failure paths**: Intentionally trigger errors to verify handling\n- **Version task trees**: Save task state snapshots for regression testing\n\nSources: Architecture diagrams, [src/agent/tool/base_tool.py:7-76](), [docs/python.design.md:274-426]()\n\n---\n\n## Next Steps\n\nFor hands-on guidance:\n- [Testing the Execution System](#8.1) - Deep dive into executor testing and namespace debugging\n- [Creating New Tools](#8.2) - Step-by-step tool implementation guide\n- [Database Setup for Examples](#8.3) - Configure PostgreSQL/MySQL for data processing examples\n\nFor system internals:\n- [Execution Runtime](#5) - Detailed executor architecture\n- [Observability and Logging](#6) - Complete logging system reference\n- [Tool System](#4) - Tool architecture and registry\n\n---\n\n# Page: Testing the Execution System\n\n# Testing the Execution System\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/python.design.md](docs/python.design.md)\n- [src/runtime/python_executor.py](src/runtime/python_executor.py)\n- [tests/unit/runtime/test_exec_runner.py](tests/unit/runtime/test_exec_runner.py)\n- [tests/unit/runtime/test_python_executor.py](tests/unit/runtime/test_python_executor.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document provides guidance for developers who need to test and understand the execution system's behavior. It covers how to test the various execution modes, understand Python's `exec()` namespace semantics, and validate subprocess isolation. \n\nFor information about the execution strategies themselves (subprocess, subthread, direct), see [Execution Runtime](#5). For details on implementing new tools that use the execution system, see [Creating New Tools](#8.2).\n\n---\n\n## Understanding exec() Namespace Behavior\n\nBefore testing the execution system, it is critical to understand how Python's `exec()` function handles global and local namespaces, as this directly affects how code executes and how state is managed.\n\n### Key Namespace Principles\n\nWhen `exec()` is called with custom namespaces, Python applies specific rules:\n\n1. **Empty Dictionary Behavior**: When `exec(code, {}, {})` is called with two empty dictionaries, Python automatically injects `__builtins__` into the global namespace, but does **not** inject `__name__` or other typical module attributes\n2. **Namespace Priority**: Variable lookup follows the order: local namespace â global namespace â `__builtins__`\n3. **Assignment Target**: When both `globals` and `locals` are provided, module-level assignments (including function definitions) are written to `locals`, not `globals`\n\n### The __name__ Anomaly\n\n```mermaid\ngraph TB\n    subgraph \"exec() with Default Namespaces\"\n        A[\"exec(code)\"]\n        B[\"Uses current globals()\"]\n        C[\"__name__ = '__main__'\"]\n    end\n    \n    subgraph \"exec() with Empty Dicts\"\n        D[\"exec(code, {}, {})\"]\n        E[\"Python injects __builtins__\"]\n        F[\"__name__ resolves to builtins module\"]\n        G[\"No __name__ variable created\"]\n    end\n    \n    A --> B --> C\n    D --> E --> F\n    E --> G\n```\n\n**Sources**: [docs/python.design.md:1-98]()\n\n### globals() vs locals() Modification Support\n\nThe ability to dynamically modify these namespaces depends on the execution scope:\n\n| Scope | `globals()` Modifiable | `locals()` Modifiable | Notes |\n|-------|------------------------|----------------------|-------|\n| Module level | â Yes | â Yes | `locals()` is same as `globals()` |\n| Class body | â Yes | â Yes | Changes sync to class namespace |\n| Function body | â Yes | â No | `locals()` returns read-only snapshot |\n\n**Sources**: [docs/python.design.md:535-645]()\n\n---\n\n## Test Infrastructure Overview\n\nThe test suite for the execution system is located in `tests/unit/runtime/test_python_executor.py` and validates three key aspects: basic execution, structured results, and timeout handling.\n\n```mermaid\ngraph LR\n    subgraph \"Test Functions\"\n        T1[\"test_python_executor()\"]\n        T2[\"test_exception()\"]\n        T3[\"test_structured_executor()\"]\n    end\n    \n    subgraph \"Tested Functions\"\n        F1[\"run()\"]\n        F2[\"run_structured()\"]\n        F3[\"worker()\"]\n        F4[\"worker_with_globals_capture()\"]\n    end\n    \n    subgraph \"Execution Backend\"\n        E1[\"multiprocessing.Process\"]\n        E2[\"multiprocessing.Queue\"]\n        E3[\"ExecutionResult\"]\n        E4[\"ExecutionStatus\"]\n    end\n    \n    T1 --> F1\n    T2 --> F1\n    T3 --> F2\n    \n    F1 --> F3\n    F2 --> F4\n    \n    F3 --> E1\n    F4 --> E1\n    F3 --> E2\n    F4 --> E2\n    F4 --> E3\n    F4 --> E4\n```\n\n**Sources**: [tests/unit/runtime/test_python_executor.py:1-105](), [src/runtime/python_executor.py:1-164]()\n\n---\n\n## Testing Basic Execution with run()\n\nThe `run()` function executes code and returns string output. It demonstrates critical namespace behaviors that must be tested.\n\n### Test Case: Namespace Priority\n\nThe test at [tests/unit/runtime/test_python_executor.py:10-35]() validates how `exec()` handles conflicting values in `globals` and `locals`:\n\n```python\nmy_globals = {\"a\": 123, \"b\": [1, 2, 3]}\nmy_locals = {\"a\": 123, \"b\": [1, 2, 3]}\n\nrun(\"a+=100000\", my_globals, my_locals)  # Modifies locals[\"a\"]\nrun(\"print(a)\", my_globals, my_locals)   # Outputs: 100123 (from locals)\n```\n\n**Key Testing Insight**: When both `globals` and `locals` are provided, modifications target `locals` first. After execution:\n- `my_globals[\"a\"]` remains `123`\n- `my_locals[\"a\"]` becomes `100123`\n\n### Test Case: Exception Handling\n\nThe test at [tests/unit/runtime/test_python_executor.py:37-50]() validates error reporting:\n\n```python\nrun(\"print(c)\", my_globals, my_locals)  \n# Returns formatted error with traceback\n```\n\nThe `worker()` function at [src/runtime/python_executor.py:72-90]() uses `source_code.get_code_and_traceback()` to capture detailed error information including line numbers and exception context.\n\n**Sources**: [tests/unit/runtime/test_python_executor.py:10-50](), [src/runtime/python_executor.py:72-90]()\n\n---\n\n## Testing Structured Execution with run_structured()\n\nThe `run_structured()` function returns an `ExecutionResult` object instead of a string, enabling programmatic status checking and state capture.\n\n### Execution Result Schema\n\n```mermaid\ngraph TB\n    subgraph \"ExecutionResult Fields\"\n        ER[\"ExecutionResult\"]\n        CMD[\"command: str\"]\n        TO[\"timeout: Optional[int]\"]\n        GLOB[\"globals: Dict[str, Any]\"]\n        STAT[\"exit_status: ExecutionStatus\"]\n        OUT[\"stdout: str\"]\n        EXC_T[\"exception_type: Optional[str]\"]\n        EXC_V[\"exception_value: Optional[str]\"]\n        EXC_TB[\"exception_traceback: Optional[str]\"]\n    end\n    \n    subgraph \"ExecutionStatus Enum\"\n        S1[\"SUCCESS\"]\n        S2[\"FAILURE\"]\n        S3[\"TIMEOUT\"]\n        S4[\"CRASHED\"]\n    end\n    \n    ER --> CMD\n    ER --> TO\n    ER --> GLOB\n    ER --> STAT\n    ER --> OUT\n    ER --> EXC_T\n    ER --> EXC_V\n    ER --> EXC_TB\n    \n    STAT --> S1\n    STAT --> S2\n    STAT --> S3\n    STAT --> S4\n```\n\n**Sources**: [src/runtime/python_executor.py:8](), [src/runtime/schemas.py]()\n\n### Test Case: Success Path\n\n```python\nmy_globals = {\"a\": 123, \"b\": [1, 2, 3]}\nresult = run_structured(\"a += 100000\", my_globals, None)\n\nassert result.exit_status == ExecutionStatus.SUCCESS\nassert result.globals[\"a\"] == 100123\nassert result.exception_type is None\n```\n\nThe `worker_with_globals_capture()` function at [src/runtime/python_executor.py:28-71]() executes code and captures the modified globals dictionary in the `ExecutionResult`.\n\n### Test Case: Timeout Detection\n\nThe timeout test at [tests/unit/runtime/test_python_executor.py:79-86]() validates subprocess termination:\n\n```python\nresult = run_structured(\"\"\"\nimport time\nc = 10\ntime.sleep(5)\n\"\"\", my_globals, my_locals, timeout=1)\n\nassert result.exit_status == ExecutionStatus.TIMEOUT\nassert result.stdout == \"\"\n```\n\nThe timeout mechanism at [src/runtime/python_executor.py:114-124]() uses `Process.join(timeout)` followed by `Process.terminate()` if the process is still alive.\n\n**Sources**: [tests/unit/runtime/test_python_executor.py:52-97](), [src/runtime/python_executor.py:28-71](), [src/runtime/python_executor.py:92-130]()\n\n---\n\n## Testing Subprocess Isolation\n\nA critical aspect of the execution system is process-level isolation. Tests must validate that subprocess execution does **not** share memory with the parent process.\n\n### Process Communication Architecture\n\n```mermaid\nsequenceDiagram\n    participant P as \"Parent Process\"\n    participant Q as \"multiprocessing.Queue\"\n    participant C as \"Child Process\"\n    participant W as \"worker_with_globals_capture()\"\n    \n    P->>Q: Create Queue\n    P->>C: Spawn Process(target=worker_with_globals_capture)\n    P->>C: Start process\n    C->>W: Execute in isolated memory\n    W->>W: exec(code, globals, locals)\n    W->>W: Capture ExecutionResult\n    W->>Q: queue.put(exec_result)\n    C->>C: Process exits\n    P->>Q: result = queue.get()\n    P->>P: Return ExecutionResult\n```\n\n**Sources**: [src/runtime/python_executor.py:104-130]()\n\n### Testing Isolation Properties\n\nWhen testing subprocess execution, validate these isolation properties:\n\n1. **No Shared State**: Modifications in the child process do not affect parent process variables\n2. **Serialization Boundary**: Only picklable objects can be passed through the queue (see [Workspace State Management](#5.5) for filtering details)\n3. **Independent Environment**: Child process has its own `sys.stdout`, `sys.stderr`, and working directory\n\n### Test Case: Import Isolation\n\nThe test at [tests/unit/runtime/test_python_executor.py:89-97]() validates that imports in the subprocess work correctly:\n\n```python\nresult = run_structured(\"\"\"\nimport time\nc = 10\nimport scipy\n\"\"\", my_globals, None, timeout=20000)\n\nassert result.exit_status == ExecutionStatus.SUCCESS\n# scipy import does not pollute parent process\n```\n\n**Sources**: [tests/unit/runtime/test_python_executor.py:79-97](), [src/runtime/python_executor.py:106-116]()\n\n---\n\n## Understanding Function Definition Behavior\n\nA common point of confusion when testing `exec()` is where function definitions end up. This has significant implications for code that defines and calls functions.\n\n### The Function Placement Problem\n\n```mermaid\ngraph TB\n    subgraph \"Scenario 1: Only globals provided\"\n        S1A[\"exec(code, my_globals)\"]\n        S1B[\"locals defaults to globals\"]\n        S1C[\"Functions written to my_globals\"]\n    end\n    \n    subgraph \"Scenario 2: Both globals and locals provided\"\n        S2A[\"exec(code, my_globals, my_locals)\"]\n        S2B[\"Separate namespace objects\"]\n        S2C[\"Functions written to my_locals\"]\n        S2D[\"my_globals unchanged\"]\n    end\n    \n    S1A --> S1B --> S1C\n    S2A --> S2B --> S2C\n    S2B --> S2D\n```\n\n**Sources**: [docs/python.design.md:428-534]()\n\n### Test Case: Function Accessibility\n\nWhen testing code that defines functions, you must account for this behavior:\n\n```python\ncompiled_code = compile(\"\"\"\ndef myadd(a, b):\n    return a + b\n    \nresult = myadd(5, 3)\nprint(result)\n\"\"\", \"<string>\", \"exec\")\n\n# Scenario 1: Only globals\nmy_globals = {'__name__': '__main__'}\nexec(compiled_code, my_globals)\nassert 'myadd' in my_globals  # â True\n\n# Scenario 2: Separate locals\nmy_globals = {'__name__': '__main__'}\nmy_locals = {}\nexec(compiled_code, my_globals, my_locals)\nassert 'myadd' not in my_globals  # â True - functions in locals only\nassert 'myadd' in my_locals       # â True\n```\n\n### Recommended Testing Pattern\n\nFor tests that need functions in the global namespace (as the execution system does), pass only the `globals` parameter:\n\n```python\n# Recommended for tool execution\nresult = run_structured(code, my_globals)  # locals defaults to globals\n\n# Avoid for function definitions\nresult = run_structured(code, my_globals, {})  # Separate namespaces\n```\n\n**Sources**: [docs/python.design.md:441-534](), [src/runtime/python_executor.py:92-130]()\n\n---\n\n## Testing the exec_runner Module\n\nThe `exec_runner` module provides a higher-level interface for executing code with pre-injected modules. Tests for this system are in `tests/unit/runtime/test_exec_runner.py`.\n\n### Module Injection Test Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Test Configuration\"\n        TC[\"test_exec_runner.py:main()\"]\n        CONF[\"config.CODE_TO_EXEC\"]\n        FLAGS[\"Module inclusion flags\"]\n    end\n    \n    subgraph \"Execution Pipeline\"\n        ER[\"exec_runner.run_exec_with_modules()\"]\n        INJ[\"Module injection logic\"]\n        EXEC[\"exec() with enriched globals\"]\n    end\n    \n    subgraph \"Module Categories\"\n        BLT[\"include_builtin\"]\n        STD[\"include_stdlib\"]\n        TPP[\"include_third_party\"]\n        ALL[\"include_all_installed\"]\n        CST[\"include_custom\"]\n    end\n    \n    TC --> CONF\n    TC --> FLAGS\n    TC --> ER\n    ER --> INJ\n    INJ --> BLT\n    INJ --> STD\n    INJ --> TPP\n    INJ --> ALL\n    INJ --> CST\n    INJ --> EXEC\n```\n\n**Sources**: [tests/unit/runtime/test_exec_runner.py:1-29]()\n\n### Test Configuration\n\nThe test at [tests/unit/runtime/test_exec_runner.py:5-26]() demonstrates how to test module injection:\n\n```python\nexec_runner.run_exec_with_modules(\n    code=config.CODE_TO_EXEC,\n    include_builtin=True,      # __builtins__ injection\n    include_stdlib=True,        # os, sys, json, etc.\n    include_third_party=True,   # numpy, pandas, etc.\n    include_all_installed=False,\n    include_custom=config.CUSTOM_PACKAGES,\n    print_mod_names=config.PRINT_SOURCES  # Debug output\n)\n```\n\nThis tests that the execution environment has access to the specified module categories without requiring explicit import statements in the executed code.\n\n**Sources**: [tests/unit/runtime/test_exec_runner.py:5-26]()\n\n---\n\n## Common Testing Patterns\n\n### Pattern 1: Testing State Persistence\n\n```python\ndef test_state_persistence():\n    globals_dict = {}\n    \n    # First execution\n    result1 = run_structured(\"x = 42\", globals_dict)\n    assert result1.exit_status == ExecutionStatus.SUCCESS\n    \n    # Second execution uses modified globals\n    result2 = run_structured(\"print(x)\", globals_dict)\n    assert \"42\" in result2.stdout\n```\n\n### Pattern 2: Testing Exception Information\n\n```python\ndef test_exception_details():\n    result = run_structured(\"1 / 0\", {})\n    \n    assert result.exit_status == ExecutionStatus.FAILURE\n    assert result.exception_type == \"ZeroDivisionError\"\n    assert result.exception_value is not None\n    assert result.exception_traceback is not None\n```\n\n### Pattern 3: Testing Timeout with Buffer\n\n```python\ndef test_timeout_with_margin():\n    # Use timeout > expected duration to account for startup overhead\n    result = run_structured(\n        \"import time; time.sleep(0.1)\",\n        {},\n        timeout=5  # 50x the actual sleep time\n    )\n    \n    assert result.exit_status == ExecutionStatus.SUCCESS\n```\n\n### Pattern 4: Testing Output Capture\n\n```python\ndef test_stdout_capture():\n    result = run_structured(\"\"\"\nimport sys\nprint(\"Line 1\", file=sys.stdout)\nprint(\"Line 2\", file=sys.stdout)\n\"\"\", {})\n    \n    assert result.exit_status == ExecutionStatus.SUCCESS\n    assert \"Line 1\" in result.stdout\n    assert \"Line 2\" in result.stdout\n```\n\n**Sources**: [tests/unit/runtime/test_python_executor.py:10-105](), [src/runtime/python_executor.py:28-71]()\n\n---\n\n## Debugging Failed Tests\n\nWhen tests fail, the execution system provides multiple debugging mechanisms:\n\n| Information Source | Location | Content |\n|-------------------|----------|---------|\n| `ExecutionResult.exception_type` | Returned by `run_structured()` | Exception class name |\n| `ExecutionResult.exception_value` | Returned by `run_structured()` | Exception `repr()` output |\n| `ExecutionResult.exception_traceback` | Returned by `run_structured()` | Full traceback string |\n| `ExecutionResult.stdout` | Returned by `run_structured()` | Captured print output |\n| Log files | `logs/global.log`, `logs/trace.log` | Function call traces with `@traceable` decorator |\n\nThe traceback includes line numbers relative to the executed code string, making it easier to identify the exact location of failures.\n\n**Sources**: [src/runtime/python_executor.py:46-67](), [src/runtime/source_code.py]()\n\n---\n\n## Summary\n\nTesting the execution system requires understanding:\n\n1. **Namespace Semantics**: How `exec()` handles `globals` vs `locals` and when to use each\n2. **Process Isolation**: How subprocess execution prevents memory sharing and requires serialization\n3. **Result Structures**: Using `ExecutionResult` to programmatically validate execution outcomes\n4. **Timeout Handling**: How to test time-limited execution and detect hangs\n5. **Function Definitions**: Where functions are stored depends on whether separate `locals` is provided\n\nKey test files:\n- [tests/unit/runtime/test_python_executor.py](): Core execution system tests\n- [tests/unit/runtime/test_exec_runner.py](): Module injection tests\n- [docs/python.design.md](): Detailed namespace behavior documentation\n\n**Sources**: [tests/unit/runtime/test_python_executor.py:1-105](), [src/runtime/python_executor.py:1-164](), [docs/python.design.md:1-645]()\n\n---\n\n# Page: Creating New Tools\n\n# Creating New Tools\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/functino_call_err.design.md](docs/functino_call_err.design.md)\n- [src/agent/action.py](src/agent/action.py)\n- [src/agent/memory.py](src/agent/memory.py)\n- [src/agent/tool/base_tool.py](src/agent/tool/base_tool.py)\n- [src/agent/tool/python_tool.py](src/agent/tool/python_tool.py)\n- [src/agent/tool/todo_tool.py](src/agent/tool/todo_tool.py)\n- [src/memory/tree_todo/schemas.py](src/memory/tree_todo/schemas.py)\n- [src/memory/tree_todo/todo_track.py](src/memory/tree_todo/todo_track.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page provides a step-by-step guide for developers who want to extend the algo_agent system by implementing custom tools. You will learn how to:\n\n- Extend the `BaseTool` abstract class with proper Pydantic schemas\n- Implement tool execution logic in the `run()` method\n- Integrate your tool with the agent's action dispatcher\n- Follow best practices for parameter definitions and error handling\n\nFor information about the overall tool architecture and existing tools, see [Tool System](#4). For details on the BaseTool interface specification, see [BaseTool Interface](#4.1).\n\n---\n\n## Understanding the BaseTool Interface\n\nThe `BaseTool` class ([src/agent/tool/base_tool.py:7-76]()) defines the contract that all tools must implement. It uses Pydantic for automatic schema generation and validation.\n\n### Core Components\n\n```mermaid\nclassDiagram\n    class BaseTool {\n        <<abstract>>\n        +str tool_call_purpose\n        +tool_name() str\n        +tool_description() str\n        +get_parameter_schema() dict\n        +get_tool_schema() dict\n        +run() str*\n    }\n    \n    class ExecutePythonCodeTool {\n        +str python_code_snippet\n        +int timeout\n        +run() str\n    }\n    \n    class RecursivePlanTreeTodoTool {\n        +RecursivePlanTree recursive_plan_tree\n        +run() Dict~str,str~\n    }\n    \n    class YourCustomTool {\n        +CustomType your_parameter\n        +run() str\n    }\n    \n    BaseTool <|-- ExecutePythonCodeTool\n    BaseTool <|-- RecursivePlanTreeTodoTool\n    BaseTool <|-- YourCustomTool\n    \n    note for BaseTool \"All methods are classmethods except run()\\nExtends pydantic.BaseModel\"\n```\n\n**Sources:** [src/agent/tool/base_tool.py:7-76]()\n\n### Required Methods\n\n| Method | Type | Purpose | Implementation |\n|--------|------|---------|----------------|\n| `tool_name()` | `@classmethod` | Returns unique identifier | Auto-generated from class name |\n| `tool_description()` | `@classmethod` | Returns tool description | Extracted from class docstring |\n| `get_parameter_schema()` | `@classmethod` | Returns JSON schema | Auto-generated by Pydantic |\n| `get_tool_schema()` | `@classmethod` | Returns full tool schema | Combines above into OpenAI format |\n| `run()` | instance method | Executes tool logic | **Must be implemented** |\n\n**Sources:** [src/agent/tool/base_tool.py:13-76]()\n\n---\n\n## Step-by-Step Implementation Guide\n\n### Step 1: Create Tool Class with Pydantic Fields\n\nCreate a new file in `src/agent/tool/` directory. Your tool must extend `BaseTool` and define parameters as Pydantic fields:\n\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Dict, Any\nfrom src.agent.tool.base_tool import BaseTool\n\nclass MyCustomTool(BaseTool):\n    \"\"\"\n    Brief description of what this tool does.\n    \n    **Functionality:**\n    - List key capabilities\n    - Explain when to use this tool\n    \n    **Important Rules:**\n    1. List any constraints or requirements\n    2. Specify expected input/output formats\n    \"\"\"\n    \n    # Define parameters as Pydantic fields\n    input_parameter: str = Field(\n        ...,  # Required field\n        description=\"Clear description for the LLM to understand\",\n        examples=[\"example value\"]\n    )\n    \n    optional_timeout: int = Field(\n        default=30,\n        description=\"Optional parameter with default value\"\n    )\n```\n\n**Sources:** [src/agent/tool/python_tool.py:13-40](), [src/agent/tool/todo_tool.py:10-26]()\n\n### Tool Naming Convention\n\nThe `tool_name()` classmethod automatically generates the tool identifier from your class name using the `inflection` library:\n\n```mermaid\ngraph LR\n    A[\"MyCustomTool\"] --> B[\"inflection.underscore()\"]\n    B --> C[\"Remove 'Tool' suffix\"]\n    C --> D[\"my_custom\"]\n    \n    E[\"ExecutePythonCodeTool\"] --> F[\"inflection.underscore()\"]\n    F --> G[\"Remove 'Tool' suffix\"]\n    G --> H[\"execute_python_code\"]\n```\n\n**Sources:** [src/agent/tool/base_tool.py:13-18]()\n\n### Step 2: Implement the run() Method\n\nThe `run()` method contains your tool's core logic. It receives parameters via `self` (Pydantic model instance) and must return a string result:\n\n```python\ndef run(self) -> str:\n    \"\"\"Execute the tool logic\"\"\"\n    try:\n        # Access parameters via self\n        user_input = self.input_parameter\n        timeout = self.optional_timeout\n        \n        # Perform your tool's logic\n        result = perform_operation(user_input, timeout)\n        \n        # Return formatted string for LLM\n        return f\"Operation completed: {result}\"\n        \n    except Exception as e:\n        # Return error information\n        return f\"Tool execution failed: {str(e)}\"\n```\n\n**Return Value Guidelines:**\n- Always return a string (LLM will read this)\n- Include relevant execution results\n- For errors, provide diagnostic information\n- Format output to be LLM-readable\n\n**Sources:** [src/agent/tool/python_tool.py:41-51](), [src/agent/tool/todo_tool.py:28-36]()\n\n---\n\n## Schema Generation and LLM Integration\n\n### How Schema Generation Works\n\nThe `get_tool_schema()` method combines class metadata into OpenAI function calling format:\n\n```mermaid\nflowchart TB\n    subgraph \"Class Definition\"\n        A[\"MyCustomTool class\"]\n        B[\"Docstring\"]\n        C[\"Pydantic Fields\"]\n    end\n    \n    subgraph \"Schema Generation\"\n        D[\"tool_name()\"]\n        E[\"tool_description()\"]\n        F[\"get_parameter_schema()\"]\n    end\n    \n    subgraph \"Output Schema\"\n        G[\"type: function\"]\n        H[\"function.name\"]\n        I[\"function.description\"]\n        J[\"function.parameters\"]\n    end\n    \n    A --> D\n    B --> E\n    C --> F\n    \n    D --> H\n    E --> I\n    F --> J\n    \n    H --> G\n    I --> G\n    J --> G\n```\n\n**Sources:** [src/agent/tool/base_tool.py:31-71]()\n\n### Generated Schema Example\n\nFor the `ExecutePythonCodeTool`, the schema becomes:\n\n| Schema Field | Source | Example Value |\n|--------------|--------|---------------|\n| `function.name` | Class name â `tool_name()` | `\"execute_python_code\"` |\n| `function.description` | Docstring â `tool_description()` | Chinese description from [src/agent/tool/python_tool.py:14-28]() |\n| `function.parameters.properties.python_code_snippet` | Field definition | `{\"type\": \"string\", \"description\": \"...\", \"examples\": [...]}` |\n| `function.parameters.properties.timeout` | Field definition | `{\"type\": \"integer\", \"default\": 30, \"description\": \"...\"}` |\n\n**Sources:** [src/agent/tool/python_tool.py:13-40](), [src/agent/tool/base_tool.py:31-71]()\n\n---\n\n## Tool Registration and Dispatch\n\n### Step 3: Register Tool in Action Dispatcher\n\nTo make your tool available to the agent, add it to the `call_tools_safely()` function in `src/agent/action.py`:\n\n```mermaid\nsequenceDiagram\n    participant Agent as \"Deep Research Agent\"\n    participant Dispatcher as \"call_tools_safely()\"\n    participant YourTool as \"YourCustomTool\"\n    \n    Agent->>Dispatcher: tool_info dict\n    Note over Dispatcher: tool_call_name<br/>tool_call_arguments\n    \n    Dispatcher->>Dispatcher: Parse function_name<br/>and arguments\n    \n    alt function_name == \"your_custom\"\n        Dispatcher->>YourTool: Instantiate(**arguments)\n        YourTool->>YourTool: run()\n        YourTool-->>Dispatcher: result string\n    end\n    \n    Dispatcher->>Dispatcher: Set tool_info[\"content\"]\n    Dispatcher-->>Agent: Updated tool_info\n```\n\n**Implementation:**\n\nAdd your tool to the dispatch chain in [src/agent/action.py:10-48]():\n\n```python\nfrom src.agent.tool.your_module import YourCustomTool\n\n@traceable\ndef call_tools_safely(tool_info: dict):\n    def call_tools(tool_info: dict):\n        function_name = tool_info[\"tool_call_name\"]\n        arguments = json.loads(tool_info[\"tool_call_arguments\"])\n        \n        if False: pass\n        elif function_name == ExecutePythonCodeTool.tool_name():\n            execute_python_code_tool = ExecutePythonCodeTool(**arguments)\n            tool_info[\"content\"] = execute_python_code_tool.run()\n        elif function_name == RecursivePlanTreeTodoTool.tool_name():\n            recursive_plan_tree_todo_tool = RecursivePlanTreeTodoTool(**arguments)\n            tool_info[\"content\"] = recursive_plan_tree_todo_tool.run()\n        # Add your tool here\n        elif function_name == YourCustomTool.tool_name():\n            your_custom_tool = YourCustomTool(**arguments)\n            tool_info[\"content\"] = your_custom_tool.run()\n        \n        return tool_info\n```\n\n**Sources:** [src/agent/action.py:10-48]()\n\n### Step 4: Register Tool Schema for LLM\n\nEnsure the tool schema is included when calling the LLM. This typically happens in the agent's query processing loop where tools are gathered:\n\n```python\n# In your agent initialization or query loop\ntools_list = [\n    ExecutePythonCodeTool.get_tool_schema(),\n    RecursivePlanTreeTodoTool.get_tool_schema(),\n    YourCustomTool.get_tool_schema(),  # Add your tool\n]\n```\n\n**Sources:** [src/agent/tool/base_tool.py:31-71]()\n\n---\n\n## Best Practices\n\n### Field Definitions\n\n| Practice | Example | Rationale |\n|----------|---------|-----------|\n| **Use descriptive names** | `python_code_snippet` instead of `code` | LLM needs clear parameter names |\n| **Provide detailed descriptions** | Include purpose, format, constraints | Helps LLM construct valid arguments |\n| **Add examples** | `examples=[\"print('Hello')\"]` | Guides LLM parameter generation |\n| **Use appropriate defaults** | `timeout: int = Field(default=30)` | Makes parameters optional where sensible |\n| **Specify types precisely** | Use `List[str]`, `Dict[str, Any]`, etc. | Enables validation and better schemas |\n\n**Sources:** [src/agent/tool/python_tool.py:29-39]()\n\n### Docstring Guidelines\n\nThe class docstring becomes the tool's description for the LLM. Write it in a clear, instructional format:\n\n```python\nclass MyTool(BaseTool):\n    \"\"\"\n    [One-line summary of purpose]\n    \n    **Functionality:**\n    - Key feature 1\n    - Key feature 2\n    \n    **Important Rules:**\n    1. Constraint or requirement\n    2. Expected behavior\n    3. Common pitfalls to avoid\n    \"\"\"\n```\n\n**Sources:** [src/agent/tool/python_tool.py:14-28](), [src/agent/tool/todo_tool.py:11-17]()\n\n### Error Handling\n\nAlways catch and format exceptions appropriately:\n\n```python\ndef run(self) -> str:\n    try:\n        # Tool logic\n        result = self.execute_logic()\n        return f\"Success: {result}\"\n    except ValidationError as e:\n        return f\"Validation failed: {e}\"\n    except Exception as e:\n        return f\"Execution error: {str(e)}\"\n```\n\nThe `call_tools_safely()` function provides an outer error handler, but internal error handling improves user experience.\n\n**Sources:** [src/agent/action.py:40-47]()\n\n### State Management Considerations\n\nIf your tool needs to maintain state across invocations:\n\n1. **Use workspace module** - Like `ExecutePythonCodeTool` uses `workspace.get_arg_globals()` and `workspace.append_out_globals()` ([src/agent/tool/python_tool.py:42-49]())\n2. **Use module-level variables** - Like `RecursivePlanTreeTodoTool` uses `arg_todo_list` in `todo_track.py` ([src/memory/tree_todo/todo_track.py:6-18]())\n3. **Document state behavior** - Clearly explain in docstring how state persists\n\n**Sources:** [src/agent/tool/python_tool.py:42-49](), [src/memory/tree_todo/todo_track.py:6-18]()\n\n---\n\n## Complete Example: File Reader Tool\n\nHere's a complete example of a simple tool that reads and summarizes files:\n\n```python\n# src/agent/tool/file_reader_tool.py\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\nimport os\nfrom src.agent.tool.base_tool import BaseTool\nfrom src.utils import global_logger\n\nclass FileReaderTool(BaseTool):\n    \"\"\"\n    Reads text files and returns their contents or statistics.\n    \n    **Functionality:**\n    - Read file contents with encoding support\n    - Return file statistics (size, line count)\n    - Handle common file reading errors\n    \n    **Important Rules:**\n    1. File paths must be relative to working directory\n    2. Only text files are supported\n    3. Large files (>1MB) return statistics instead of full content\n    \"\"\"\n    \n    file_path: str = Field(\n        ...,\n        description=\"Path to the file to read, relative to working directory\",\n        examples=[\"data/input.txt\", \"logs/output.log\"]\n    )\n    \n    encoding: str = Field(\n        default=\"utf-8\",\n        description=\"Character encoding of the file\"\n    )\n    \n    max_size_mb: int = Field(\n        default=1,\n        description=\"Maximum file size in MB to read fully\"\n    )\n    \n    def run(self) -> str:\n        try:\n            # Validate file exists\n            if not os.path.exists(self.file_path):\n                return f\"Error: File '{self.file_path}' does not exist\"\n            \n            # Check file size\n            file_size = os.path.getsize(self.file_path)\n            file_size_mb = file_size / (1024 * 1024)\n            \n            if file_size_mb > self.max_size_mb:\n                # Return statistics for large files\n                with open(self.file_path, 'r', encoding=self.encoding) as f:\n                    line_count = sum(1 for _ in f)\n                return (\n                    f\"File '{self.file_path}' is too large ({file_size_mb:.2f}MB).\\n\"\n                    f\"Statistics: {line_count} lines, {file_size} bytes\"\n                )\n            \n            # Read file contents\n            with open(self.file_path, 'r', encoding=self.encoding) as f:\n                content = f.read()\n            \n            global_logger.info(f\"Read file: {self.file_path}\")\n            return f\"File contents ({len(content)} chars):\\n{content}\"\n            \n        except UnicodeDecodeError as e:\n            return f\"Encoding error: Cannot decode file with {self.encoding}. Try a different encoding.\"\n        except Exception as e:\n            global_logger.error(f\"FileReaderTool failed: {e}\", exc_info=True)\n            return f\"Error reading file: {str(e)}\"\n```\n\n### Integration Steps\n\n```mermaid\ngraph TB\n    subgraph \"1. Tool Definition\"\n        A[\"Define FileReaderTool class\"]\n        B[\"Add Pydantic fields\"]\n        C[\"Implement run() method\"]\n    end\n    \n    subgraph \"2. Action Registration\"\n        D[\"Import in action.py\"]\n        E[\"Add elif branch\"]\n        F[\"Instantiate and call\"]\n    end\n    \n    subgraph \"3. Schema Registration\"\n        G[\"Add to tools_list\"]\n        H[\"Pass to LLM\"]\n    end\n    \n    subgraph \"4. Testing\"\n        I[\"Test standalone\"]\n        J[\"Test via agent\"]\n    end\n    \n    A --> B --> C --> D\n    D --> E --> F --> G\n    G --> H --> I\n    I --> J\n```\n\nThen register in `action.py`:\n\n```python\nfrom src.agent.tool.file_reader_tool import FileReaderTool\n\nelif function_name == FileReaderTool.tool_name():\n    file_reader_tool = FileReaderTool(**arguments)\n    tool_info[\"content\"] = file_reader_tool.run()\n```\n\n**Sources:** [src/agent/tool/python_tool.py:13-51](), [src/agent/tool/base_tool.py:7-76](), [src/agent/action.py:10-48]()\n\n---\n\n## Debugging and Testing\n\n### Testing Your Tool Standalone\n\nBefore integrating with the agent, test your tool directly:\n\n```python\nif __name__ == \"__main__\":\n    # Test with valid input\n    tool = FileReaderTool(\n        tool_call_purpose=\"Testing file reader\",\n        file_path=\"test.txt\",\n        encoding=\"utf-8\"\n    )\n    result = tool.run()\n    print(result)\n    \n    # Test error handling\n    tool_invalid = FileReaderTool(\n        tool_call_purpose=\"Testing error case\",\n        file_path=\"nonexistent.txt\"\n    )\n    error_result = tool_invalid.run()\n    print(error_result)\n```\n\n### Validating Schema Generation\n\nCheck that your tool generates the expected schema:\n\n```python\nimport json\n\nschema = FileReaderTool.get_tool_schema()\nprint(json.dumps(schema, indent=2))\n```\n\nThis should output the OpenAI function calling format with all your parameters correctly specified.\n\n**Sources:** [src/agent/tool/todo_tool.py:41-133](), [src/agent/tool/base_tool.py:31-71]()\n\n---\n\n## Common Patterns and Anti-Patterns\n\n### â Good Patterns\n\n| Pattern | Example Tool | Benefit |\n|---------|--------------|---------|\n| **Stateless when possible** | Simple calculations | Easier to reason about |\n| **Clear parameter validation** | Check file paths exist | Fail fast with good errors |\n| **Descriptive return values** | Include context in results | LLM can understand outcomes |\n| **Leverage Pydantic validation** | Use `Field` constraints | Automatic validation |\n| **Log important operations** | `global_logger.info()` | Debugging and tracing |\n\n**Sources:** [src/agent/tool/python_tool.py:41-51]()\n\n### â Anti-Patterns to Avoid\n\n| Anti-Pattern | Problem | Solution |\n|--------------|---------|----------|\n| **Non-string return values** | LLM expects strings | Always return `str` from `run()` |\n| **Unclear error messages** | \"Error occurred\" | Include details: \"File not found: path.txt\" |\n| **Missing docstrings** | LLM can't understand tool | Write clear, structured docstrings |\n| **Mutable default arguments** | State leakage | Use `Field(default_factory=list)` |\n| **Side effects without logging** | Hard to debug | Log state changes with `global_logger` |\n\n**Sources:** [src/agent/tool/base_tool.py:73-75](), [src/agent/action.py:40-47]()\n\n---\n\n## Tool Integration Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Tool Development\"\n        DEV[\"Developer\"]\n        TOOL[\"MyCustomTool class\"]\n        TEST[\"Standalone Tests\"]\n    end\n    \n    subgraph \"Registration\"\n        ACTION[\"action.py\"]\n        DISPATCH[\"call_tools_safely()\"]\n    end\n    \n    subgraph \"Agent Runtime\"\n        AGENT[\"Deep Research Agent\"]\n        LLM[\"LLM Service\"]\n        SCHEMA[\"Tool Schema List\"]\n    end\n    \n    subgraph \"Execution\"\n        INSTANTIATE[\"Tool Instance\"]\n        RUN[\"run() method\"]\n        RESULT[\"String Result\"]\n    end\n    \n    DEV --> TOOL\n    TOOL --> TEST\n    TEST --> ACTION\n    \n    ACTION --> DISPATCH\n    TOOL --> SCHEMA\n    \n    AGENT --> LLM\n    SCHEMA --> LLM\n    LLM --> DISPATCH\n    \n    DISPATCH --> INSTANTIATE\n    INSTANTIATE --> RUN\n    RUN --> RESULT\n    RESULT --> AGENT\n```\n\n**Sources:** [src/agent/action.py:10-48](), [src/agent/tool/base_tool.py:7-76]()\n\n---\n\n## Summary Checklist\n\nWhen creating a new tool, ensure you have:\n\n- [ ] Extended `BaseTool` class\n- [ ] Written clear docstring with functionality and rules\n- [ ] Defined all parameters as Pydantic fields with descriptions\n- [ ] Implemented `run()` method returning a string\n- [ ] Added error handling in `run()`\n- [ ] Registered tool in `call_tools_safely()` dispatcher\n- [ ] Added tool schema to LLM tools list\n- [ ] Written standalone tests\n- [ ] Tested integration with agent\n- [ ] Added logging for important operations\n- [ ] Documented any state management behavior\n\n**Sources:** [src/agent/tool/base_tool.py:7-76](), [src/agent/action.py:10-48](), [src/agent/tool/python_tool.py:13-51](), [src/agent/tool/todo_tool.py:10-36]()\n\n---\n\n# Page: Database Setup for Examples\n\n# Database Setup for Examples\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.vscode/settings.json](.vscode/settings.json)\n- [docs/gen.md](docs/gen.md)\n- [docs/pg.md](docs/pg.md)\n- [docs/sql.md](docs/sql.md)\n- [pyproject.toml](pyproject.toml)\n- [uv.lock](uv.lock)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document provides instructions for setting up database environments required to run data processing examples in the algo_agent system. The examples in sections [7.2](#7.2) and [7.3](#7.3) demonstrate data processing capabilities using relational databases (PostgreSQL, MySQL) and vector databases (pgvector).\n\nThis page covers Docker-based database installation, configuration, and verification. For information about the actual data processing examples and use cases, see [Use Cases and Examples](#7). For general testing strategies, see [Testing the Execution System](#8.1).\n\n---\n\n## Database Requirements Overview\n\nThe algo_agent system's data processing examples require database backends for:\n\n| Database Type | Use Case | Required For |\n|--------------|----------|--------------|\n| **PostgreSQL 16** | Relational data storage and SQL query examples | Geographic data processing, structured data analysis |\n| **MySQL 8.0** | Cross-database compatibility testing | SQL portability examples, continuous login analysis |\n| **pgvector (PostgreSQL 17)** | Vector similarity search and embeddings storage | AI/ML feature storage, semantic search examples |\n\nAll databases are deployed via Docker containers to ensure consistent, reproducible development environments without local installation conflicts.\n\n**Sources:** [docs/pg.md:1-257]()\n\n---\n\n## Docker Environment Prerequisites\n\n### System Requirements\n\n```mermaid\ngraph LR\n    DEV[\"Developer Machine\"] --> DOCKER[\"Docker Desktop/Engine\"]\n    DOCKER --> PG16[\"PostgreSQL 16 Container\"]\n    DOCKER --> MYSQL[\"MySQL 8.0 Container\"]\n    DOCKER --> PGVEC[\"pgvector:pg17 Container\"]\n    \n    PG16 --> PORT1[\"Port 5432\"]\n    MYSQL --> PORT2[\"Port 3306\"]\n    PGVEC --> PORT3[\"Port 5433\"]\n    \n    style DOCKER fill:#f9f9f9\n    style PG16 fill:#f9f9f9\n    style MYSQL fill:#f9f9f9\n    style PGVEC fill:#f9f9f9\n```\n\n**Diagram: Database Container Architecture**\n\n### Prerequisites Checklist\n\n1. **Docker Installation**: Docker Desktop (Windows/Mac) or Docker Engine (Linux) must be running\n2. **Port Availability**: Ensure ports 3306, 5432, 5433 are not in use by existing services\n3. **Disk Space**: At least 2GB free for container images and data volumes\n4. **Network Access**: Internet connection required for initial image download\n\n**Verification Commands:**\n\n```bash\n# Check Docker is running\ndocker --version\n\n# Check port availability (Linux/Mac)\nnetstat -an | grep -E '3306|5432|5433'\n\n# Check port availability (Windows PowerShell)\nnetstat -an | findstr \"3306 5432 5433\"\n```\n\n**Sources:** [docs/pg.md:258-262]()\n\n---\n\n## PostgreSQL 16 Setup\n\n### Container Deployment\n\nPostgreSQL 16 serves as the primary relational database for structured data examples.\n\n```bash\n# Create data persistence directory\nmkdir -p /docker/postgres16/data\n\n# Start PostgreSQL 16 container with data volume\ndocker run -d \\\n  --name postgres16 \\\n  --restart always \\\n  -p 5432:5432 \\\n  -e POSTGRES_PASSWORD=123456 \\\n  -e POSTGRES_USER=postgres \\\n  -e POSTGRES_DB=test_db \\\n  -v /docker/postgres16/data:/var/lib/postgresql/data \\\n  postgres:16\n\n# Verify container is running\ndocker ps | grep postgres16\n```\n\n**Sources:** [docs/pg.md:19-32]()\n\n### Configuration Parameters\n\n| Parameter | Value | Description |\n|-----------|-------|-------------|\n| `--name` | `postgres16` | Container identifier for management commands |\n| `-p` | `5432:5432` | Port mapping: host 5432 â container 5432 |\n| `-e POSTGRES_PASSWORD` | `123456` | Superuser `postgres` password |\n| `-e POSTGRES_DB` | `test_db` | Auto-created default database |\n| `-v` | `/docker/postgres16/data:/var/lib/postgresql/data` | Data persistence volume mount |\n| `--restart` | `always` | Auto-restart on system reboot |\n\n**Sources:** [docs/pg.md:7-32]()\n\n### Connection Verification\n\n```mermaid\nsequenceDiagram\n    participant DEV as \"Developer Terminal\"\n    participant DOCKER as \"Docker Engine\"\n    participant PG16 as \"postgres16 Container\"\n    participant PSQL as \"psql Client\"\n    participant DB as \"test_db Database\"\n    \n    DEV->>DOCKER: \"docker exec -it postgres16 psql\"\n    DOCKER->>PG16: \"Execute psql command\"\n    PG16->>PSQL: \"Start psql client\"\n    PSQL->>DB: \"Connect to test_db\"\n    DB-->>PSQL: \"Connection established\"\n    PSQL-->>DEV: \"postgres=# prompt\"\n    DEV->>PSQL: \"CREATE TABLE / SELECT queries\"\n    PSQL->>DB: \"Execute SQL\"\n    DB-->>PSQL: \"Query results\"\n    PSQL-->>DEV: \"Display results\"\n```\n\n**Diagram: PostgreSQL Connection Flow**\n\n```bash\n# Connect to PostgreSQL via container\ndocker exec -it postgres16 psql -U postgres -d test_db\n\n# Test SQL operations\nCREATE TABLE test_connection (\n    id SERIAL PRIMARY KEY,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nINSERT INTO test_connection DEFAULT VALUES;\nSELECT * FROM test_connection;\n\n# Exit psql\n\\q\n```\n\n**Sources:** [docs/pg.md:35-48]()\n\n### External Client Connection\n\nFor connecting via external tools (Navicat, DBeaver, DataGrip):\n\n| Connection Parameter | Value |\n|---------------------|-------|\n| Host | `127.0.0.1` (or server IP) |\n| Port | `5432` |\n| Database | `test_db` |\n| Username | `postgres` |\n| Password | `123456` |\n| SSL Mode | `prefer` (or `disable` for local) |\n\n**Sources:** [docs/pg.md:50-56]()\n\n---\n\n## MySQL 8.0 Setup\n\n### Container Deployment\n\nMySQL 8.0 is used for cross-database SQL compatibility testing and specific MySQL examples.\n\n```bash\n# Start MySQL 8.0 container\ndocker run -d \\\n  --name mysql-test \\\n  --restart always \\\n  -p 3306:3306 \\\n  -e MYSQL_ROOT_PASSWORD=123456 \\\n  -e MYSQL_DATABASE=test_db \\\n  mysql:8.0\n\n# Verify container is running\ndocker ps | grep mysql-test\n```\n\n**Sources:** [docs/pg.md:266-275]()\n\n### Configuration Parameters\n\n| Parameter | Value | Description |\n|-----------|-------|-------------|\n| `--name` | `mysql-test` | Container identifier |\n| `-p` | `3306:3306` | Port mapping: host 3306 â container 3306 |\n| `-e MYSQL_ROOT_PASSWORD` | `123456` | Root user password |\n| `-e MYSQL_DATABASE` | `test_db` | Auto-created database |\n\n**Note:** For production environments with data persistence, add volume mount:\n```bash\n-v /docker/mysql8/data:/var/lib/mysql\n```\n\n**Sources:** [docs/pg.md:266-280]()\n\n### Connection Verification\n\n```bash\n# Connect to MySQL via container\ndocker exec -it mysql-test mysql -uroot -p123456 test_db\n\n# Test SQL operations\nCREATE TABLE test_connection (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nINSERT INTO test_connection () VALUES ();\nSELECT * FROM test_connection;\n\n# Exit MySQL\nexit;\n```\n\n**Sources:** [docs/pg.md:283-299]()\n\n### External Client Connection\n\n| Connection Parameter | Value |\n|---------------------|-------|\n| Host | `127.0.0.1` |\n| Port | `3306` |\n| Database | `test_db` |\n| Username | `root` |\n| Password | `123456` |\n\n**Sources:** [docs/pg.md:266-280]()\n\n---\n\n## pgvector Setup for Vector Storage\n\n### Container Deployment\n\nThe `pgvector` extension adds vector similarity search capabilities to PostgreSQL, required for AI/ML examples with embeddings storage.\n\n```bash\n# Create data persistence directory\nmkdir -p /docker/pgvector17/data\n\n# Start pgvector container (PostgreSQL 17 with vector extension)\ndocker run -d \\\n  --name pgvector17 \\\n  --restart always \\\n  -p 5433:5432 \\\n  -e POSTGRES_PASSWORD=123456 \\\n  -v /docker/pgvector17/data:/var/lib/postgresql/data \\\n  pgvector/pgvector:pg17\n\n# Verify container is running\ndocker ps | grep pgvector17\n```\n\n**Important:** Port 5433 is used on the host to avoid conflicts with the standard PostgreSQL 16 container on port 5432.\n\n**Sources:** [docs/pg.md:61-74]()\n\n### Vector Extension Activation\n\nThe `vector` extension must be explicitly enabled in each database:\n\n```bash\n# Connect to pgvector container\ndocker exec -it pgvector17 psql -U postgres\n\n# Create database and enable vector extension\nCREATE DATABASE vector_db;\n\\c vector_db;\n\nCREATE EXTENSION vector;\n\n# Verify extension installation\n\\dx;\n```\n\nExpected output should show the `vector` extension listed.\n\n**Sources:** [docs/pg.md:77-93]()\n\n### Vector Data Type Usage\n\n```mermaid\ngraph TB\n    subgraph \"Vector Storage Architecture\"\n        TABLE[\"embeddings Table\"]\n        VEC[\"vec vector(N)\"]\n        DATA[\"content TEXT\"]\n        \n        TABLE --> VEC\n        TABLE --> DATA\n    end\n    \n    subgraph \"Vector Operations\"\n        INSERT[\"INSERT vector data\"]\n        SEARCH[\"Similarity search\"]\n        DISTANCE[\"Distance calculation\"]\n        \n        VEC --> INSERT\n        VEC --> SEARCH\n        VEC --> DISTANCE\n    end\n    \n    INSERT --> OP1[\"L2 distance: <->\"]\n    SEARCH --> OP1\n    INSERT --> OP2[\"Cosine distance: <=>\"]\n    SEARCH --> OP2\n    INSERT --> OP3[\"Inner product: <#>\"]\n    SEARCH --> OP3\n    \n    style TABLE fill:#f9f9f9\n    style VEC fill:#f9f9f9\n```\n\n**Diagram: pgvector Data Type and Operations**\n\n```sql\n-- Create table with vector column (example: 3-dimensional vectors)\nCREATE TABLE embeddings (\n  id SERIAL PRIMARY KEY,\n  vec vector(3),        -- vector(N) where N is dimension count\n  content TEXT\n);\n\n-- Insert vector data (array notation)\nINSERT INTO embeddings (vec, content)\nVALUES \n  ('[1.1, 2.2, 3.3]', 'Sample text 1'),\n  ('[4.4, 5.5, 6.6]', 'Sample text 2'),\n  ('[0.9, 2.1, 3.2]', 'Sample text 3');\n\n-- Similarity search using L2 distance (Euclidean)\nSELECT \n  id, \n  content, \n  vec <-> '[1.0, 2.0, 3.0]' AS distance\nFROM embeddings\nORDER BY distance ASC\nLIMIT 5;\n\n-- Alternative distance metrics\n-- Cosine distance: vec <=> '[1.0, 2.0, 3.0]'\n-- Inner product: vec <#> '[1.0, 2.0, 3.0]'\n```\n\n**Sources:** [docs/pg.md:95-116]()\n\n### Vector Dimension Configuration\n\n| Use Case | Typical Dimensions | Example Model |\n|----------|-------------------|---------------|\n| Small text embeddings | 384 | `sentence-transformers/all-MiniLM-L6-v2` |\n| Standard embeddings | 768 | `bert-base-uncased` |\n| Large embeddings | 1536 | `text-embedding-ada-002` (OpenAI) |\n| Custom embeddings | Variable | Domain-specific models |\n\n**Dimension Declaration Syntax:**\n- 3D vector: `vector(3)`\n- 768D vector: `vector(768)`\n- 1536D vector: `vector(1536)`\n\n**Sources:** [docs/pg.md:95-116]()\n\n---\n\n## Database Connection from Python Code\n\n### Connection Architecture\n\n```mermaid\ngraph LR\n    subgraph \"algo_agent Runtime\"\n        AGENT[\"Agent Execution\"]\n        PYTOOL[\"ExecutePythonCodeTool\"]\n        EXEC[\"Subprocess Executor\"]\n    end\n    \n    subgraph \"Python Code Snippet\"\n        CODE[\"User SQL Code\"]\n        LIB1[\"psycopg2\"]\n        LIB2[\"mysql-connector\"]\n        LIB3[\"sqlalchemy\"]\n    end\n    \n    subgraph \"Docker Containers\"\n        PG[\"postgres16:5432\"]\n        MYSQL[\"mysql-test:3306\"]\n        PGVEC[\"pgvector17:5433\"]\n    end\n    \n    AGENT --> PYTOOL\n    PYTOOL --> EXEC\n    EXEC --> CODE\n    \n    CODE --> LIB1\n    CODE --> LIB2\n    CODE --> LIB3\n    \n    LIB1 --> PG\n    LIB1 --> PGVEC\n    LIB2 --> MYSQL\n    LIB3 --> PG\n    LIB3 --> MYSQL\n    LIB3 --> PGVEC\n    \n    style AGENT fill:#f9f9f9\n    style PYTOOL fill:#f9f9f9\n    style EXEC fill:#f9f9f9\n```\n\n**Diagram: Database Connection Flow from Agent Code Execution**\n\n### PostgreSQL Connection Example\n\n```python\n# Example code that can be executed via ExecutePythonCodeTool\nimport psycopg2\n\n# Connect to PostgreSQL 16 container\nconn = psycopg2.connect(\n    host=\"localhost\",\n    port=5432,\n    database=\"test_db\",\n    user=\"postgres\",\n    password=\"123456\"\n)\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT version();\")\nversion = cursor.fetchone()\nprint(f\"PostgreSQL version: {version[0]}\")\n\ncursor.close()\nconn.close()\n```\n\n### MySQL Connection Example\n\n```python\n# Example code that can be executed via ExecutePythonCodeTool\nimport mysql.connector\n\n# Connect to MySQL 8.0 container\nconn = mysql.connector.connect(\n    host=\"localhost\",\n    port=3306,\n    database=\"test_db\",\n    user=\"root\",\n    password=\"123456\"\n)\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT VERSION();\")\nversion = cursor.fetchone()\nprint(f\"MySQL version: {version[0]}\")\n\ncursor.close()\nconn.close()\n```\n\n### pgvector Connection Example\n\n```python\n# Example code that can be executed via ExecutePythonCodeTool\nimport psycopg2\n\n# Connect to pgvector container\nconn = psycopg2.connect(\n    host=\"localhost\",\n    port=5433,  # Note: Different port than standard PostgreSQL\n    database=\"vector_db\",\n    user=\"postgres\",\n    password=\"123456\"\n)\n\ncursor = conn.cursor()\n\n# Create table with vector column\ncursor.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS doc_embeddings (\n        id SERIAL PRIMARY KEY,\n        doc_text TEXT,\n        embedding vector(768)\n    )\n\"\"\")\n\n# Insert sample vector (768 dimensions)\nsample_vector = [0.1] * 768  # Placeholder embedding\ncursor.execute(\n    \"INSERT INTO doc_embeddings (doc_text, embedding) VALUES (%s, %s)\",\n    (\"Sample document\", sample_vector)\n)\n\nconn.commit()\ncursor.close()\nconn.close()\n```\n\n**Note:** Database client libraries (`psycopg2`, `mysql-connector-python`) are not listed in [pyproject.toml:7-20]() and must be installed separately or added to project dependencies for these examples to work.\n\n**Sources:** [pyproject.toml:1-21](), [docs/pg.md:283-299]()\n\n---\n\n## SQL Example: Continuous Login Analysis\n\nThis example demonstrates window function usage for analyzing user login patterns, runnable in both PostgreSQL and MySQL.\n\n### Test Data Setup\n\n```sql\n-- Create test table (PostgreSQL/MySQL compatible)\nCREATE TABLE user_login (\n    id SERIAL PRIMARY KEY,        -- PostgreSQL\n    -- id INT AUTO_INCREMENT PRIMARY KEY,  -- MySQL alternative\n    user_id VARCHAR(50) NOT NULL,\n    login_date DATE NOT NULL\n);\n\n-- Insert test data\nINSERT INTO user_login (user_id, login_date) VALUES\n('u001', '2025-01-01'), ('u001', '2025-01-01'),  -- Duplicate login same day\n('u001', '2025-01-02'), ('u001', '2025-01-03'), ('u001', '2025-01-04'),\n('u002', '2025-01-01'), ('u002', '2025-01-03'), ('u002', '2025-01-04'),\n('u003', '2025-01-01'), ('u003', '2025-01-02'), ('u003', '2025-01-03');\n```\n\n**Sources:** [docs/pg.md:144-158](), [docs/pg.md:288-299]()\n\n### Continuous Login Query (PostgreSQL)\n\n```sql\nWITH \n-- Step 1: Deduplicate login records (one record per user per day)\ndistinct_login AS (\n    SELECT DISTINCT user_id, login_date \n    FROM user_login\n),\n-- Step 2: Calculate grouping key using window function\nlogin_rn AS (\n    SELECT \n        user_id,\n        login_date,\n        -- Consecutive dates will have same group_key\n        login_date - ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY login_date) * INTERVAL '1 day' AS group_key\n    FROM distinct_login\n),\n-- Step 3: Aggregate continuous login periods\ncontinuous_login AS (\n    SELECT \n        user_id,\n        MIN(login_date) AS start_date,\n        MAX(login_date) AS end_date,\n        COUNT(*) AS continuous_days\n    FROM login_rn\n    GROUP BY user_id, group_key\n)\n-- Step 4: Filter for 3+ consecutive days\nSELECT \n    user_id,\n    start_date,\n    end_date,\n    continuous_days\nFROM continuous_login\nWHERE continuous_days >= 3\nORDER BY user_id, start_date;\n```\n\n**Expected Result:**\n\n| user_id | start_date | end_date   | continuous_days |\n|---------|------------|------------|-----------------|\n| u001    | 2025-01-01 | 2025-01-04 | 4               |\n| u003    | 2025-01-01 | 2025-01-03 | 3               |\n\n**Sources:** [docs/pg.md:208-242](), [docs/pg.md:372-397]()\n\n### Continuous Login Query (MySQL)\n\n```sql\nWITH \ndistinct_login AS (\n    SELECT DISTINCT user_id, login_date \n    FROM user_login\n),\nlogin_rn AS (\n    SELECT \n        user_id,\n        login_date,\n        -- MySQL date arithmetic\n        DATE_SUB(login_date, INTERVAL ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY login_date) DAY) AS group_key\n    FROM distinct_login\n),\ncontinuous_login AS (\n    SELECT \n        user_id,\n        MIN(login_date) AS start_date,\n        MAX(login_date) AS end_date,\n        COUNT(*) AS continuous_days\n    FROM login_rn\n    GROUP BY user_id, group_key\n)\nSELECT \n    user_id,\n    start_date,\n    end_date,\n    continuous_days\nFROM continuous_login\nWHERE continuous_days >= 3\nORDER BY user_id, start_date;\n```\n\n**Key Difference:** MySQL uses `DATE_SUB(date, INTERVAL n DAY)` while PostgreSQL uses `date - n * INTERVAL '1 day'`.\n\n**Sources:** [docs/pg.md:161-196](), [docs/pg.md:300-326]()\n\n---\n\n## Container Management Commands\n\n### Lifecycle Operations\n\n```bash\n# Start containers\ndocker start postgres16\ndocker start mysql-test\ndocker start pgvector17\n\n# Stop containers\ndocker stop postgres16\ndocker stop mysql-test\ndocker stop pgvector17\n\n# Restart containers\ndocker restart postgres16\ndocker restart mysql-test\ndocker restart pgvector17\n\n# View container logs\ndocker logs postgres16\ndocker logs -f mysql-test     # Follow mode (real-time)\ndocker logs --tail 100 pgvector17  # Last 100 lines\n```\n\n**Sources:** [docs/pg.md:122-129]()\n\n### Data Persistence Management\n\n```bash\n# Remove container (data preserved in volume)\ndocker stop postgres16\ndocker rm postgres16\n\n# Remove container and data\ndocker stop postgres16\ndocker rm postgres16\nrm -rf /docker/postgres16/data\n\n# Backup database (PostgreSQL example)\ndocker exec postgres16 pg_dump -U postgres test_db > backup.sql\n\n# Restore database (PostgreSQL example)\ndocker exec -i postgres16 psql -U postgres test_db < backup.sql\n```\n\n**Sources:** [docs/pg.md:420-427]()\n\n### Port Conflict Resolution\n\nIf default ports are occupied:\n\n```bash\n# Use alternative ports (host:container mapping)\ndocker run -d \\\n  --name postgres16 \\\n  -p 5434:5432 \\    # Map to host port 5434\n  -e POSTGRES_PASSWORD=123456 \\\n  postgres:16\n\ndocker run -d \\\n  --name mysql-test \\\n  -p 3307:3306 \\    # Map to host port 3307\n  -e MYSQL_ROOT_PASSWORD=123456 \\\n  mysql:8.0\n```\n\nConnection parameters must be updated accordingly:\n- PostgreSQL: `host=localhost, port=5434`\n- MySQL: `host=localhost, port=3307`\n\n**Sources:** [docs/pg.md:119-120](), [docs/pg.md:429]()\n\n---\n\n## Troubleshooting\n\n### Common Issues and Solutions\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| \"Port already in use\" | Local database service running | Stop local service or use different port mapping (`-p 5433:5432`) |\n| \"Permission denied\" on volume mount | Directory ownership mismatch | `chmod 777 /docker/postgres16/data` or use named volumes |\n| Container exits immediately | Invalid configuration | Check logs: `docker logs postgres16` |\n| \"Connection refused\" | Container not started or wrong port | Verify: `docker ps` and check port mapping |\n| Vector extension not available | Extension not enabled | Run `CREATE EXTENSION vector;` in target database |\n\n**Sources:** [docs/pg.md:130-134](), [docs/pg.md:429-432]()\n\n### Container Health Verification\n\n```bash\n# Check container status\ndocker ps -a | grep -E 'postgres16|mysql-test|pgvector17'\n\n# Inspect container configuration\ndocker inspect postgres16\n\n# Test network connectivity\ndocker exec postgres16 pg_isready -U postgres\n\n# Check resource usage\ndocker stats postgres16 mysql-test pgvector17\n```\n\n### Database Connection Testing\n\n```bash\n# PostgreSQL connection test\ndocker exec postgres16 psql -U postgres -c \"SELECT 1\"\n\n# MySQL connection test\ndocker exec mysql-test mysql -uroot -p123456 -e \"SELECT 1\"\n\n# pgvector extension test\ndocker exec pgvector17 psql -U postgres -c \"SELECT extname FROM pg_extension WHERE extname='vector'\"\n```\n\n**Sources:** [docs/pg.md:407-418]()\n\n---\n\n## Integration with Example Data Generation\n\nThe database setups work in conjunction with the data generation utilities for creating test datasets.\n\n```mermaid\ngraph TB\n    subgraph \"Data Generation\"\n        GEN[\"docs/gen.md\"]\n        PYDANTIC[\"Pydantic Models\"]\n        RANDOMDATA[\"generate_random_data()\"]\n    end\n    \n    subgraph \"Database Storage\"\n        PG[\"PostgreSQL 16\"]\n        MYSQL[\"MySQL 8.0\"]\n        PGVEC[\"pgvector:pg17\"]\n    end\n    \n    subgraph \"Agent Execution\"\n        AGENT[\"user_query()\"]\n        TOOL[\"ExecutePythonCodeTool\"]\n        SQL[\"SQL INSERT/SELECT\"]\n    end\n    \n    GEN --> PYDANTIC\n    PYDANTIC --> RANDOMDATA\n    RANDOMDATA --> JSON[\"JSON Test Data\"]\n    \n    JSON --> AGENT\n    AGENT --> TOOL\n    TOOL --> SQL\n    \n    SQL --> PG\n    SQL --> MYSQL\n    SQL --> PGVEC\n    \n    style GEN fill:#f9f9f9\n    style PYDANTIC fill:#f9f9f9\n    style AGENT fill:#f9f9f9\n```\n\n**Diagram: Data Generation to Database Storage Flow**\n\nThe [docs/gen.md:1-137]() example demonstrates Pydantic-based data structure generation for emergency response planning scenarios. These generated structures can be stored in PostgreSQL for:\n\n1. **Structured Query Testing**: Geographic point/edge data in relational tables\n2. **Vector Storage**: Embedding representations of materials/locations in pgvector\n3. **Cross-Database Validation**: Comparing query results across PostgreSQL and MySQL\n\nExample integration:\n```python\n# From docs/gen.md example - generate test data\nfrom docs.gen import generate_all_random_data\nimport json\nimport psycopg2\n\n# Generate random emergency response data\ndata = generate_all_random_data()\n\n# Store in PostgreSQL\nconn = psycopg2.connect(\n    host=\"localhost\", port=5432,\n    database=\"test_db\", user=\"postgres\", password=\"123456\"\n)\ncursor = conn.cursor()\n\n# Store map points\nfor point in data.map_data.points:\n    cursor.execute(\n        \"INSERT INTO map_points (x, y) VALUES (%s, %s)\",\n        (point.x, point.y)\n    )\n\nconn.commit()\n```\n\n**Sources:** [docs/gen.md:1-137](), [docs/pg.md:283-299]()\n\n---\n\n## Summary\n\nThis page documented the Docker-based database setup for running algo_agent data processing examples:\n\n- **PostgreSQL 16**: Primary relational database on port 5432\n- **MySQL 8.0**: Cross-database testing on port 3306  \n- **pgvector (PostgreSQL 17)**: Vector storage with similarity search on port 5433\n\nAll containers use persistent data volumes and can be managed via standard Docker commands. Database connections from Python code executed via `ExecutePythonCodeTool` use standard client libraries (`psycopg2`, `mysql-connector-python`).\n\nFor actual implementation of data processing algorithms and examples, see [Data Processing and Visualization](#7.2) and [Geographic Data Processing](#7.3).\n\n**Sources:** [docs/pg.md:1-432](), [docs/gen.md:1-187](), [pyproject.toml:1-21]()\n\n---\n\n# Page: Architecture Reference\n\n# Architecture Reference\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [docs/crashed.design.md](docs/crashed.design.md)\n- [docs/error correction.design.md](docs/error correction.design.md)\n- [docs/log.md](docs/log.md)\n- [src/agent/tool/base_tool.py](src/agent/tool/base_tool.py)\n- [src/memory/tree_todo/schemas.py](src/memory/tree_todo/schemas.py)\n- [src/memory/tree_todo/todo_track.py](src/memory/tree_todo/todo_track.py)\n- [src/runtime/schemas.py](src/runtime/schemas.py)\n- [src/runtime/workspace.py](src/runtime/workspace.py)\n- [src/utils/__pycache__/__init__.cpython-312.pyc](src/utils/__pycache__/__init__.cpython-312.pyc)\n- [tests/playground/crashed.py](tests/playground/crashed.py)\n- [tests/playground/subprocess_output.py](tests/playground/subprocess_output.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document provides a comprehensive technical reference for the core architectural components, data models, and schemas in the algo_agent system. It focuses on the structure and relationships of the fundamental data types that flow through the system, including execution results, task management structures, and tool interfaces.\n\nThis reference is intended for developers who need to understand:\n- The data models used for code execution results and state management\n- The schemas for hierarchical task planning and tracking\n- The tool system's interface contracts and schema generation\n- How data flows between components and is serialized/deserialized\n\nFor implementation details of specific subsystems, see:\n- Code execution strategies: [Execution Runtime](#5)\n- Tool implementations: [Tool System](#4)\n- State persistence mechanisms: [Workspace State Management](#5.5)\n\n---\n\n## Core Data Model Overview\n\nThe system is built around three primary schema families that enable structured communication between the agent, execution runtime, and task management subsystems:\n\n```mermaid\ngraph TB\n    subgraph \"Execution Schemas (runtime/schemas.py)\"\n        ES[ExecutionStatus enum]\n        ER[ExecutionResult model]\n    end\n    \n    subgraph \"Task Management Schemas (memory/tree_todo/schemas.py)\"\n        TS[TaskStatus enum]\n        TN[RecursivePlanTreeNode model]\n        TR[RecursivePlanTree model]\n    end\n    \n    subgraph \"Tool Interface (agent/tool/base_tool.py)\"\n        BT[BaseTool abstract class]\n        TSCHEMA[\"Tool schema dict\"]\n    end\n    \n    subgraph \"Workspace State (runtime/workspace.py)\"\n        ARGLIST[\"arg_globals_list: list[dict]\"]\n        OUTLIST[\"out_globals_list: list[dict]\"]\n    end\n    \n    ER --> ES\n    ER --> ARGLIST\n    ER --> OUTLIST\n    \n    TR --> TN\n    TN --> TS\n    TN --> TN\n    \n    BT --> TSCHEMA\n    \n    TSCHEMA -..\"consumed by\".-> Agent[\"Agent Decision Loop\"]\n    ER -..\"returned to\".-> Agent\n    TR -..\"tracked by\".-> Agent\n    \n    style ES fill:#f9f9f9\n    style TS fill:#f9f9f9\n    style ER fill:#e8f4f8\n    style TR fill:#e8f4f8\n    style BT fill:#f0f8e8\n```\n\n**Sources:** [src/runtime/schemas.py:1-111](), [src/memory/tree_todo/schemas.py:1-81](), [src/agent/tool/base_tool.py:1-76](), [src/runtime/workspace.py:1-108]()\n\n---\n\n## Execution Result Schema Architecture\n\nThe `ExecutionResult` model is the primary data structure for capturing code execution outcomes. It encapsulates both input parameters and output results, supporting multiple execution states and providing structured error information.\n\n### ExecutionStatus Enum\n\n```mermaid\ngraph LR\n    START[\"Code Execution\"] --> SUCCESS\n    START --> FAILURE\n    START --> TIMEOUT\n    START --> CRASHED\n    \n    SUCCESS[\"ExecutionStatus.SUCCESS<br/>exit_code=0<br/>No exceptions\"]\n    FAILURE[\"ExecutionStatus.FAILURE<br/>exit_code=0<br/>Python exception caught\"]\n    TIMEOUT[\"ExecutionStatus.TIMEOUT<br/>exit_code=-15 (SIGTERM)<br/>Exceeded timeout limit\"]\n    CRASHED[\"ExecutionStatus.CRASHED<br/>exit_code varies<br/>Process terminated abnormally\"]\n    \n    SUCCESS --> RET[\"ret_stdout captured<br/>arg_globals preserved\"]\n    FAILURE --> RET\n    TIMEOUT --> RET\n    CRASHED --> RET\n```\n\nThe `ExecutionStatus` enum defines four mutually exclusive execution outcomes:\n\n| Status | Value | Exit Code | Description | Typical Causes |\n|--------|-------|-----------|-------------|----------------|\n| `SUCCESS` | `\"success\"` | 0 | Code executed without errors | Normal completion |\n| `FAILURE` | `\"failure\"` | 0 | Python exception raised | Syntax errors, runtime exceptions, logic errors |\n| `TIMEOUT` | `\"timeout\"` | -15 | Execution exceeded time limit | Infinite loops, blocking operations |\n| `CRASHED` | `\"crashed\"` | varies | Process terminated abnormally | SegFault, OOM, signal termination |\n\n**Sources:** [src/runtime/schemas.py:10-16]()\n\n### ExecutionStatus.get_return_llm Method\n\nThis class method generates human-readable descriptions for the LLM based on execution status:\n\n```mermaid\ngraph TB\n    STATUS[\"ExecutionStatus + ExecutionResult\"] --> METHOD[\"get_return_llm(status, result)\"]\n    \n    METHOD --> SUCCESS_DESC[\"SUCCESS:<br/>ä»£ç æ§è¡æåï¼è¾åºç»æå®æ´<br/>+ ret_stdout\"]\n    METHOD --> FAILURE_DESC[\"FAILURE:<br/>ä»£ç æ§è¡å¤±è´¥ï¼æ ¹æ®æ¥éä¿¡æ¯è°è¯<br/>+ ret_stdout + arg_command with line numbers<br/>+ exception_traceback\"]\n    METHOD --> TIMEOUT_DESC[\"TIMEOUT:<br/>ä»£ç æ§è¡è¶æ¶ï¼è°æ´è¶æ¶æ¶é´<br/>+ ret_stdout + arg_timeout\"]\n    METHOD --> CRASHED_DESC[\"CRASHED:<br/>è¿ç¨å¼å¸¸éåº<br/>+ ret_stdout + exit_code\"]\n    \n    SUCCESS_DESC --> LLMINPUT[\"ret_tool2llm field\"]\n    FAILURE_DESC --> LLMINPUT\n    TIMEOUT_DESC --> LLMINPUT\n    CRASHED_DESC --> LLMINPUT\n```\n\n**Sources:** [src/runtime/schemas.py:18-47]()\n\n### ExecutionResult Model Fields\n\nThe `ExecutionResult` Pydantic model contains three categories of fields:\n\n**Input Parameters (set by caller):**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `arg_command` | `str` | The Python code string executed |\n| `arg_timeout` | `int` | Timeout limit in seconds |\n| `arg_globals` | `Dict[str, Any]` | Filtered and deep-copied global variables (validated by `field_validate_globals`) |\n\n**Execution Results (set by subprocess):**\n\n| Field | Type | Optional | Description |\n|-------|------|----------|-------------|\n| `exit_status` | `ExecutionStatus` | No | Execution outcome enum |\n| `exit_code` | `int` | Yes | Process exit code (set by parent process) |\n| `exception_repr` | `str` | Yes | Exception repr string (FAILURE only) |\n| `exception_type` | `str` | Yes | Exception class name (FAILURE only) |\n| `exception_value` | `str` | Yes | Exception message (FAILURE only) |\n| `exception_traceback` | `str` | Yes | Full traceback string (FAILURE only) |\n\n**Post-Processing Fields (set by parent process):**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `ret_stdout` | `str` | Captured stdout/stderr from execution |\n| `ret_tool2llm` | `str` | Formatted message for LLM (generated by `get_return_llm`) |\n\n**Sources:** [src/runtime/schemas.py:56-84]()\n\n### ExecutionResult Validation\n\nThe `arg_globals` field uses a Pydantic validator that automatically filters and deep-copies globals:\n\n```python\n@field_validator('arg_globals')\n@classmethod        \ndef field_validate_globals(cls, value: Dict[str, Any]) -> Dict[str, Any]:\n    if value is None:\n        return {}\n    return workspace.filter_and_deepcopy_globals(value)\n```\n\nThis validator calls `workspace.filter_and_deepcopy_globals()` which:\n1. Removes `__builtins__` and module objects\n2. Tests pickle serializability to ensure IPC compatibility\n3. Creates deep copies to prevent reference sharing\n\n**Sources:** [src/runtime/schemas.py:77-83](), [src/runtime/workspace.py:38-78]()\n\n---\n\n## Subprocess Execution Implementation\n\nThe `run_structured_in_subprocess` function implements the subprocess-based execution strategy, demonstrating how `ExecutionResult` is constructed:\n\n```mermaid\nsequenceDiagram\n    participant Parent as \"Parent Process\"\n    participant Worker as \"_worker_with_pipe<br/>(subprocess)\"\n    participant Reader as \"_reader<br/>(thread)\"\n    participant Pipe as \"multiprocessing.Pipe\"\n    \n    Parent->>+Pipe: Create pipe (parent_conn, child_conn)\n    Parent->>+Worker: Start subprocess with child_conn\n    Parent->>Reader: Start reader thread\n    \n    Worker->>Worker: Redirect stdout/stderr to PipeWriter\n    Worker->>Worker: exec(command, _globals, _locals)\n    \n    alt Execution Success\n        Worker->>Pipe: Send (_PipeType.STDOUT, output)\n        Worker->>Pipe: Send (_PipeType.RESULT, ExecutionResult<br/>exit_status=SUCCESS)\n        Pipe->>Reader: Receive messages\n    else Execution Failure\n        Worker->>Worker: Catch exception\n        Worker->>Pipe: Send (_PipeType.RESULT, ExecutionResult<br/>exit_status=FAILURE<br/>+ exception details)\n        Pipe->>Reader: Receive messages\n    end\n    \n    Worker->>-Pipe: Close child_conn\n    \n    alt Timeout\n        Parent->>Worker: p.join(timeout) returns with process alive\n        Parent->>Worker: p.terminate()\n        Parent->>Parent: Build ExecutionResult<br/>exit_status=TIMEOUT\n    else Normal/Abnormal Exit\n        Reader->>-Reader: EOFError (pipe closed)\n        Parent->>Parent: Check subprocess_result_container\n        alt Result Received\n            Parent->>Parent: Use ExecutionResult from subprocess\n        else No Result (Crashed)\n            Parent->>Parent: Build ExecutionResult<br/>exit_status=CRASHED\n        end\n    end\n    \n    Parent->>Parent: Set exit_code = p.exitcode\n    Parent->>Parent: Set ret_stdout = joined buffer\n    Parent->>Parent: Set ret_tool2llm = get_return_llm()\n    Parent->>-Parent: Return final ExecutionResult\n```\n\n**Key implementation details:**\n\n1. **Pipe Communication Protocol:** Two message types distinguished by `_PipeType` enum\n   - `STDOUT`: Real-time output strings\n   - `RESULT`: Final `ExecutionResult` object\n\n2. **Timeout Handling:** Parent process checks `p.is_alive()` after `p.join(timeout)`. If alive, terminates and builds TIMEOUT result.\n\n3. **Crash Detection:** If subprocess exits without sending RESULT message, parent detects empty `subprocess_result_container` and builds CRASHED result.\n\n4. **Exit Code Interpretation:**\n   - `0`: Normal exit\n   - `> 0`: Error exit (caught by Python)\n   - `< 0`: Signal termination (`-15` = SIGTERM, `139` = SegFault)\n\n**Sources:** [tests/playground/subprocess_output.py:68-156](), [tests/playground/subprocess_output.py:18-66]()\n\n---\n\n## Task Management Schema Architecture\n\nThe task management system uses a recursive tree structure to represent hierarchical task plans with status tracking.\n\n### TaskStatus Enum\n\n```mermaid\ngraph LR\n    PENDING[\"TaskStatus.PENDING<br/>â³ å¾æ§è¡\"]\n    PROCESSING[\"TaskStatus.PROCESSING<br/>â¡ï¸ æ­£å¨æ§è¡\"]\n    COMPLETED[\"TaskStatus.COMPLETED<br/>â æ§è¡æå\"]\n    FAILED[\"TaskStatus.FAILED<br/>â æ§è¡å¤±è´¥\"]\n    RETRY[\"TaskStatus.RETRY<br/>â»ï¸ éè¯\"]\n    SKIPPED[\"TaskStatus.SKIPPED<br/>â å·²è·³è¿\"]\n    \n    PENDING --> PROCESSING\n    PROCESSING --> COMPLETED\n    PROCESSING --> FAILED\n    FAILED --> RETRY\n    RETRY --> PROCESSING\n    PENDING --> SKIPPED\n```\n\nEach status has associated properties:\n\n| Status | Value | Symbol | Display Description | Usage |\n|--------|-------|--------|---------------------|-------|\n| `PENDING` | `\"pending\"` | `[â³]` | å¾æ§è¡ | Task not yet started |\n| `PROCESSING` | `\"processing\"` | `[â¡ï¸]` | æ­£å¨æ§è¡ | Task currently running |\n| `COMPLETED` | `\"completed\"` | `[â]` | æ§è¡æå | Task finished successfully |\n| `FAILED` | `\"failed\"` | `[â]` | æ§è¡å¤±è´¥ | Task encountered error |\n| `RETRY` | `\"retry\"` | `[â»ï¸]` | éè¯ | Task will be retried |\n| `SKIPPED` | `\"skipped\"` | `[â]` | å·²è·³è¿ | Task skipped due to changed conditions |\n\n**Sources:** [src/memory/tree_todo/schemas.py:6-41]()\n\n### RecursivePlanTreeNode Model\n\nThe `RecursivePlanTreeNode` represents a single task in the hierarchical plan:\n\n```mermaid\ngraph TB\n    NODE[\"RecursivePlanTreeNode\"]\n    \n    NODE --> ID[\"task_id: str<br/>auto-generated UUID\"]\n    NODE --> NAME[\"task_name: str<br/>globally unique name\"]\n    NODE --> DESC[\"description: str<br/>detailed requirements\"]\n    NODE --> STATUS[\"status: TaskStatus<br/>current state\"]\n    NODE --> OUTPUT[\"output: str<br/>execution result\"]\n    NODE --> DEPS[\"dependencies: Optional[List[str]]<br/>task_name references\"]\n    NODE --> RESEARCH[\"research_directions: Optional[List[str]]<br/>complex task analysis\"]\n    NODE --> CHILDREN[\"children: Optional[List[RecursivePlanTreeNode]]<br/>recursive subtasks\"]\n    \n    CHILDREN -.-> NODE\n    \n    style CHILDREN fill:#f9f9f9\n```\n\n**Field specifications:**\n\n| Field | Type | Default | Description |\n|-------|------|---------|-------------|\n| `task_id` | `str` | auto-generated | Unique ID in format `TASK-{uuid4()}` |\n| `task_name` | `str` | required | Unique name for dependency references |\n| `description` | `str` | `\"\"` | Optional detailed task requirements |\n| `status` | `TaskStatus` | `PENDING` | Current execution state |\n| `output` | `str` | `\"\"` | Result when completed/failed |\n| `dependencies` | `Optional[List[str]]` | `None` | List of prerequisite `task_name` values |\n| `research_directions` | `Optional[List[str]]` | `None` | Deep research topics for complex tasks |\n| `children` | `Optional[List[RecursivePlanTreeNode]]` | `None` | Nested subtasks (recursive) |\n\n**Validation:** The `empty_children_to_none` validator ensures empty child lists are normalized to `None`.\n\n**Sources:** [src/memory/tree_todo/schemas.py:43-64]()\n\n### RecursivePlanTree Model\n\nThe `RecursivePlanTree` is the top-level container for the complete task hierarchy:\n\n```mermaid\ngraph TB\n    TREE[\"RecursivePlanTree\"]\n    \n    TREE --> GOAL[\"core_goal: str<br/>ultimate objective\"]\n    TREE --> NODES[\"tree_nodes: List[RecursivePlanTreeNode]<br/>root task list\"]\n    TREE --> ACTION[\"next_action: Dict[str, Any]<br/>suggested next step\"]\n    TREE --> REFS[\"references: Optional[List[str]]<br/>resource links\"]\n    \n    NODES --> NODE1[\"RecursivePlanTreeNode\"]\n    NODES --> NODE2[\"RecursivePlanTreeNode\"]\n    NODES --> NODE3[\"RecursivePlanTreeNode\"]\n    \n    NODE1 --> CHILD1[\"children\"]\n    NODE2 --> CHILD2[\"children\"]\n```\n\n**Field specifications:**\n\n| Field | Type | Default | Description |\n|-------|------|---------|-------------|\n| `core_goal` | `str` | required | The ultimate objective of the plan |\n| `tree_nodes` | `List[RecursivePlanTreeNode]` | `[]` | Root-level tasks (may have nested children) |\n| `next_action` | `Dict[str, Any]` | `{}` | Agent's suggested next execution step |\n| `references` | `Optional[List[str]]` | `None` | Documentation links, data sources |\n\n**Sources:** [src/memory/tree_todo/schemas.py:67-81]()\n\n---\n\n## Task Tree Version Tracking\n\nThe `todo_track.py` module maintains historical versions of task trees and analyzes changes:\n\n```mermaid\ngraph TB\n    subgraph \"Global State\"\n        ARGLIST[\"arg_todo_list: List[RecursivePlanTree]<br/>Historical versions\"]\n        OUTLIST[\"out_todo_list: List[RecursivePlanTree]<br/>(unused in current impl)\"]\n        DIFFLIST[\"track_diff_result_list: List[str]<br/>Change summaries\"]\n    end\n    \n    subgraph \"Change Analysis\"\n        RUN[\"run(current_plan_tree)\"]\n        RUN --> SAVE[\"Append to arg_todo_list\"]\n        RUN --> ANALYZE[\"_analyze_changes(last, current)\"]\n        RUN --> RENDER[\"_render_plan_tree_markdown(nodes)\"]\n        RUN --> STATS[\"_calculate_status_statistics(tree)\"]\n        \n        ANALYZE --> NEWTASK[\"ð New tasks\"]\n        ANALYZE --> DELTASK[\"ðï¸ Deleted tasks\"]\n        ANALYZE --> STATUSCHG[\"ð Status changes\"]\n        ANALYZE --> LEVELCHG[\"ð Level adjustments\"]\n    end\n    \n    subgraph \"Return Value\"\n        RESULT[\"Dict[str, str]\"]\n        RESULT --> CHANGES[\"changes_summary: str\"]\n        RESULT --> MARKDOWN[\"markdown_todo_list: str\"]\n        RESULT --> STATUSSTATS[\"status_statistics: dict\"]\n    end\n    \n    SAVE --> ARGLIST\n    ANALYZE --> CHANGES\n    RENDER --> MARKDOWN\n    STATS --> STATUSSTATS\n```\n\n**Key functions:**\n\n- `run(current_plan_tree)`: Main entry point that saves, analyzes, and renders\n- `_analyze_changes(last_plan, current_plan)`: Identifies new/deleted/modified tasks\n- `_render_plan_tree_markdown(nodes, indent_level)`: Recursive Markdown generation\n- `_calculate_status_statistics(tree)`: Counts tasks by status with completion rate\n\n**Sources:** [src/memory/tree_todo/todo_track.py:1-201]()\n\n---\n\n## Tool Schema Format\n\nThe `BaseTool` abstract class defines the interface all tools must implement, including schema generation for LLM function calling.\n\n### BaseTool Interface\n\n```mermaid\nclassDiagram\n    class BaseTool {\n        +str tool_call_purpose\n        +tool_name() str$\n        +tool_description() str$\n        +get_parameter_schema() dict$\n        +get_tool_schema() dict$\n        +run() str\n    }\n    \n    class ExecutePythonCodeTool {\n        +str code\n        +int timeout\n        +run() str\n    }\n    \n    class RecursivePlanTreeTodoTool {\n        +RecursivePlanTree current_plan_tree\n        +run() str\n    }\n    \n    BaseTool <|-- ExecutePythonCodeTool\n    BaseTool <|-- RecursivePlanTreeTodoTool\n    \n    note for BaseTool \"All class methods are auto-generated<br/>from Pydantic model metadata\"\n```\n\n**Key methods:**\n\n| Method | Type | Returns | Description |\n|--------|------|---------|-------------|\n| `tool_name()` | classmethod | `str` | Underscore name from class (e.g., `\"execute_python_code\"`) |\n| `tool_description()` | classmethod | `str` | Extracted from class docstring |\n| `get_parameter_schema()` | classmethod | `dict` | Pydantic JSON Schema for fields |\n| `get_tool_schema()` | classmethod | `dict` | OpenAI function calling format |\n| `run()` | instance | `str` | Execute tool logic (must override) |\n\n**Sources:** [src/agent/tool/base_tool.py:6-76]()\n\n### Tool Schema Structure\n\nThe `get_tool_schema()` method returns a dictionary in OpenAI function calling format:\n\n```python\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"type\": \"function\",\n        \"name\": \"<tool_name>\",\n        \"description\": \"<tool_description>\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"field_name\": {\n                    \"type\": \"string\",\n                    \"description\": \"Field description from Pydantic Field\"\n                },\n                ...\n            },\n            \"required\": [\"field1\", \"field2\"]\n        },\n        \"strict\": True\n    }\n}\n```\n\nThe `parameters` section is automatically generated from the Pydantic model's `model_json_schema()` output, which includes:\n- Field types (`string`, `integer`, `array`, etc.)\n- Field descriptions from `Field(..., description=\"...\")`\n- Required field list from `Field(...)` vs `Field(default=...)`\n- Nested object schemas for complex types\n\n**Sources:** [src/agent/tool/base_tool.py:31-71]()\n\n---\n\n## Workspace State Management\n\nThe `workspace.py` module manages global variable persistence across multiple code executions.\n\n### Global State Lists\n\n```mermaid\ngraph LR\n    EXEC1[\"Execution 1\"] --> ARG1[\"arg_globals_list[0]<br/>Initial workspace\"]\n    EXEC1 --> OUT1[\"out_globals_list[0]<br/>After exec 1\"]\n    \n    EXEC2[\"Execution 2\"] --> ARG2[\"arg_globals_list[1]<br/>= filtered copy of out_globals_list[0]\"]\n    EXEC2 --> OUT2[\"out_globals_list[1]<br/>After exec 2\"]\n    \n    EXEC3[\"Execution 3\"] --> ARG3[\"arg_globals_list[2]<br/>= filtered copy of out_globals_list[1]\"]\n    EXEC3 --> OUT3[\"out_globals_list[2]<br/>After exec 3\"]\n    \n    OUT1 --> ARG2\n    OUT2 --> ARG3\n    \n    style ARG1 fill:#e8f4f8\n    style ARG2 fill:#e8f4f8\n    style ARG3 fill:#e8f4f8\n    style OUT1 fill:#f0f8e8\n    style OUT2 fill:#f0f8e8\n    style OUT3 fill:#f0f8e8\n```\n\n**Key data structures:**\n\n| Variable | Type | Purpose |\n|----------|------|---------|\n| `arg_globals_list` | `list[dict]` | Input globals for each execution (filtered) |\n| `out_globals_list` | `list[dict]` | Output globals after each execution (filtered) |\n\n**Sources:** [src/runtime/workspace.py:10-11]()\n\n### Filtering and Serialization\n\nThe `filter_and_deepcopy_globals()` function ensures only valid, serializable objects are preserved:\n\n```mermaid\ngraph TB\n    INPUT[\"original_globals dict\"] --> FILTER[\"filter_and_deepcopy_globals()\"]\n    \n    FILTER --> CHECK1[\"Exclude __builtins__\"]\n    FILTER --> CHECK2[\"Exclude module objects<br/>(isinstance(value, type(sys)))\"]\n    FILTER --> CHECK3[\"Test pickle.dumps(value)<br/>Catch PicklingError, TypeError, etc.\"]\n    FILTER --> COPY[\"Deep copy valid objects<br/>copy.deepcopy(value)\"]\n    \n    CHECK1 --> FILTERED[\"filtered_dict\"]\n    CHECK2 --> FILTERED\n    CHECK3 --> FILTERED\n    COPY --> FILTERED\n    \n    FILTERED --> OUTPUT[\"Dict[str, Any]<br/>Safe for IPC/serialization\"]\n```\n\n**Filtering rules:**\n\n1. **Exclude `__builtins__`**: Standard library globals are not persisted\n2. **Exclude modules**: Module objects (e.g., `import numpy`) cannot be pickled\n3. **Test serializability**: Uses `pickle.dumps()` to validate picklability\n4. **Deep copy**: Creates independent copies to prevent reference sharing\n\n**Exception types caught during serialization test:**\n- `pickle.PicklingError`\n- `TypeError`\n- `AttributeError`\n- `RecursionError`\n- `MemoryError`\n\n**Sources:** [src/runtime/workspace.py:38-78]()\n\n### Workspace Initialization\n\n```python\ndef initialize_workspace() -> dict:\n    workspace: dict = __create_workspace()\n    instance = workspace.update({'__name__': '__main__'})\n    return workspace\n```\n\nThe `initialize_workspace()` function creates a new execution namespace by:\n1. Calling `exec(\"\", workspace)` to populate default builtins\n2. Setting `__name__` to `'__main__'` for script-like execution context\n\n**Sources:** [src/runtime/workspace.py:14-23]()\n\n---\n\n## Data Flow Relationships\n\nThe following diagram shows how schemas and data structures relate across the system:\n\n```mermaid\ngraph TB\n    subgraph \"Agent Layer\"\n        USER[\"User Query\"]\n        MESSAGES[\"messages list<br/>(OpenAI format)\"]\n        LLM[\"LLM Response<br/>tool_calls\"]\n    end\n    \n    subgraph \"Tool Invocation\"\n        TOOLSCHEMA[\"BaseTool.get_tool_schema()<br/>Function signature\"]\n        TOOLCALL[\"Tool instantiation<br/>with parsed arguments\"]\n    end\n    \n    subgraph \"Python Execution Tool\"\n        PYTOOL[\"ExecutePythonCodeTool.run()\"]\n        GETGLOBALS[\"workspace.get_arg_globals()\"]\n        SUBPROCESS[\"run_structured_in_subprocess()\"]\n        APPENDGLOBALS[\"workspace.append_out_globals()\"]\n    end\n    \n    subgraph \"Execution Runtime\"\n        WORKER[\"_worker_with_pipe\"]\n        EXECRESULT[\"ExecutionResult construction\"]\n        FILTER[\"filter_and_deepcopy_globals\"]\n    end\n    \n    subgraph \"Task Management Tool\"\n        TODOTOOL[\"RecursivePlanTreeTodoTool.run()\"]\n        TRACK[\"todo_track.run()\"]\n        ARGLIST[\"arg_todo_list\"]\n        ANALYZE[\"_analyze_changes\"]\n    end\n    \n    USER --> MESSAGES\n    MESSAGES --> LLM\n    LLM --> TOOLSCHEMA\n    TOOLSCHEMA --> TOOLCALL\n    \n    TOOLCALL --> PYTOOL\n    TOOLCALL --> TODOTOOL\n    \n    PYTOOL --> GETGLOBALS\n    GETGLOBALS --> SUBPROCESS\n    SUBPROCESS --> WORKER\n    WORKER --> EXECRESULT\n    EXECRESULT --> FILTER\n    FILTER --> APPENDGLOBALS\n    EXECRESULT --> MESSAGES\n    \n    TODOTOOL --> TRACK\n    TRACK --> ARGLIST\n    TRACK --> ANALYZE\n    ANALYZE --> MESSAGES\n```\n\n**Sources:** [src/runtime/schemas.py:1-111](), [src/memory/tree_todo/schemas.py:1-81](), [src/agent/tool/base_tool.py:1-76](), [src/runtime/workspace.py:1-108](), [tests/playground/subprocess_output.py:1-706]()\n\n---\n\n## Summary of Key Architectural Patterns\n\n### 1. Structured Error Handling\n\nAll execution outcomes are captured in strongly-typed enums (`ExecutionStatus`, `TaskStatus`) with associated metadata. This enables:\n- Predictable error handling logic\n- Clear state transitions\n- Informative LLM feedback generation\n\n### 2. Recursive Data Structures\n\nBoth `RecursivePlanTreeNode` and nested children support arbitrary depth hierarchies, enabling:\n- Complex task decomposition\n- Dependency graph representation\n- Incremental status tracking\n\n### 3. Serialization-First Design\n\nAll data models use Pydantic validation with explicit serialization checks (`filter_and_deepcopy_globals`), ensuring:\n- IPC compatibility for multiprocessing\n- State persistence across executions\n- Type safety at runtime\n\n### 4. Schema Auto-Generation\n\nTool schemas are derived from Pydantic models via `model_json_schema()`, providing:\n- Single source of truth for parameters\n- Automatic OpenAI function calling format\n- Type-safe tool invocations\n\n**Sources:** [src/runtime/schemas.py:1-111](), [src/memory/tree_todo/schemas.py:1-81](), [src/agent/tool/base_tool.py:1-76](), [src/runtime/workspace.py:1-108]()\n\n---\n\n# Page: ExecutionResult Schema Reference\n\n# ExecutionResult Schema Reference\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [docs/crashed.design.md](docs/crashed.design.md)\n- [src/runtime/cwd.py](src/runtime/cwd.py)\n- [src/runtime/schemas.py](src/runtime/schemas.py)\n- [src/runtime/subprocess_python_executor.py](src/runtime/subprocess_python_executor.py)\n- [tests/playground/crashed.py](tests/playground/crashed.py)\n- [tests/playground/subprocess_output.py](tests/playground/subprocess_output.py)\n\n</details>\n\n\n\nThis page provides a complete technical reference for the `ExecutionResult` and `ExecutionStatus` data structures that represent the outcome of Python code execution across all execution strategies (subprocess, subthread, and direct execution).\n\nFor information about how ExecutionResult is used in practice during code execution, see [ExecutionResult and Status Handling](#5.4). For implementation details of the subprocess executor that produces these results, see [Subprocess Execution](#5.1).\n\n---\n\n## Overview\n\nThe `ExecutionResult` schema is a Pydantic model that captures comprehensive information about Python code execution outcomes. It serves as the unified return type for all execution strategies, providing structured data about success, failure, timeout, or crash scenarios along with associated metadata.\n\nThe schema is designed to:\n- Provide consistent execution results across different executor implementations\n- Capture detailed error information for debugging\n- Preserve global variable state for workspace persistence\n- Generate formatted feedback for the LLM agent\n- Support timeout and crash detection\n\nSources: [src/runtime/schemas.py:56-70]()\n\n---\n\n## ExecutionStatus Enum\n\n### Status Values\n\nThe `ExecutionStatus` enum defines four possible execution outcomes:\n\n| Status | String Value | Description | Exit Code Range |\n|--------|-------------|-------------|-----------------|\n| `SUCCESS` | `\"success\"` | Code executed without exceptions | `0` |\n| `FAILURE` | `\"failure\"` | Code raised an exception (e.g., ZeroDivisionError, TypeError) | `0` (exception caught) |\n| `TIMEOUT` | `\"timeout\"` | Execution exceeded timeout limit and was terminated | `-15` (SIGTERM) |\n| `CRASHED` | `\"crashed\"` | Process crashed abnormally (e.g., SegFault, OOM) | `139` (SIGSEGV), `1` (other errors) |\n\nSources: [src/runtime/schemas.py:11-16]()\n\n### Status Determination Flow\n\n```mermaid\ngraph TD\n    START[\"Process Execution Begins\"]\n    EXEC[\"exec(command, globals, locals)\"]\n    WAIT[\"p.join(timeout)\"]\n    \n    START --> EXEC\n    EXEC --> WAIT\n    \n    WAIT --> TIMEOUT_CHECK{\"p.is_alive()?\"}\n    TIMEOUT_CHECK -->|Yes| TIMEOUT[\"Status: TIMEOUT<br/>exit_code: -15\"]\n    TIMEOUT_CHECK -->|No| NORMAL[\"Process Exited\"]\n    \n    NORMAL --> RESULT_CHECK{\"ExecutionResult<br/>received from<br/>subprocess?\"}\n    \n    RESULT_CHECK -->|Yes| EXEC_RESULT{\"Exception<br/>occurred?\"}\n    EXEC_RESULT -->|No| SUCCESS[\"Status: SUCCESS<br/>exit_code: 0\"]\n    EXEC_RESULT -->|Yes| FAILURE[\"Status: FAILURE<br/>exit_code: 0\"]\n    \n    RESULT_CHECK -->|No| CRASHED[\"Status: CRASHED<br/>exit_code: 139 or 1\"]\n    \n    style SUCCESS fill:#f0fff0\n    style FAILURE fill:#fff0f0\n    style TIMEOUT fill:#fff8f0\n    style CRASHED fill:#f8f0ff\n```\n\n**Status Determination Logic:**\n1. **Parent process waits** with timeout via `p.join(timeout)`\n2. **If process still alive** after timeout â `TIMEOUT`\n3. **If process exited normally:**\n   - **If ExecutionResult received** from pipe â check exception fields\n     - No exception â `SUCCESS`\n     - Exception present â `FAILURE`\n   - **If no ExecutionResult received** â `CRASHED`\n\nSources: [src/runtime/subprocess_python_executor.py:128-159](), [tests/playground/subprocess_output.py:120-155]()\n\n---\n\n## ExecutionResult Schema\n\n### Schema Structure\n\n```mermaid\nclassDiagram\n    class ExecutionResult {\n        +arg_command: str\n        +arg_timeout: int\n        +arg_globals: Dict[str, Any]\n        +exit_status: ExecutionStatus\n        +exit_code: Optional[int]\n        +exception_repr: Optional[str]\n        +exception_type: Optional[str]\n        +exception_value: Optional[str]\n        +exception_traceback: Optional[str]\n        +ret_stdout: str\n        +ret_tool2llm: Optional[str]\n        +field_validate_globals(value) Dict\n    }\n    \n    class ExecutionStatus {\n        <<enumeration>>\n        SUCCESS\n        FAILURE\n        TIMEOUT\n        CRASHED\n        +get_return_llm(status, result) str\n    }\n    \n    ExecutionResult --> ExecutionStatus : exit_status\n    ExecutionResult ..> \"workspace.filter_and_deepcopy_globals\" : validates arg_globals\n    ExecutionStatus ..> ExecutionResult : formats ret_tool2llm\n```\n\nSources: [src/runtime/schemas.py:56-83]()\n\n### Field Reference\n\n#### Input Arguments (Populated by Caller)\n\n| Field | Type | Required | Description | Population Point |\n|-------|------|----------|-------------|------------------|\n| `arg_command` | `str` | Yes | The Python code snippet that was executed | Caller passes to executor |\n| `arg_timeout` | `int` | Yes | Maximum allowed execution time in seconds | Caller passes to executor |\n| `arg_globals` | `Dict[str, Any]` | Yes | Global variables after execution, filtered and deep-copied | Subprocess filters during construction |\n\n**`arg_globals` Validation:**\nThe `field_validate_globals` validator automatically processes this field by calling `workspace.filter_and_deepcopy_globals()`, which:\n- Removes built-in objects (`__builtins__`)\n- Removes module references\n- Excludes non-picklable objects\n- Deep copies remaining values to prevent mutation\n\nSources: [src/runtime/schemas.py:77-83](), [src/runtime/schemas.py:58-60]()\n\n#### Execution Results (Populated by Subprocess or Parent)\n\n| Field | Type | Required | Description | Population Point |\n|-------|------|----------|-------------|------------------|\n| `exit_status` | `ExecutionStatus` | Yes | Execution outcome: SUCCESS, FAILURE, TIMEOUT, or CRASHED | Set in subprocess (SUCCESS/FAILURE) or parent (TIMEOUT/CRASHED) |\n| `exit_code` | `Optional[int]` | No | Process exit code from OS | Parent process via `p.exitcode` |\n\nSources: [src/runtime/schemas.py:62-63]()\n\n#### Exception Details (Populated on FAILURE)\n\n| Field | Type | Required | Description | Example |\n|-------|------|----------|-------------|---------|\n| `exception_repr` | `Optional[str]` | No | String representation of exception | `\"ZeroDivisionError('division by zero')\"` |\n| `exception_type` | `Optional[str]` | No | Exception class name | `\"ZeroDivisionError\"` |\n| `exception_value` | `Optional[str]` | No | Exception message | `\"division by zero\"` |\n| `exception_traceback` | `Optional[str]` | No | Full traceback from `traceback.format_exc()` | Multi-line traceback string |\n\nThese fields are populated only when `exit_status == FAILURE`, captured in the subprocess via `except Exception as e:` block.\n\nSources: [src/runtime/schemas.py:64-67](), [src/runtime/subprocess_python_executor.py:58-69]()\n\n#### Generated Output (Populated by Parent Process)\n\n| Field | Type | Required | Description | Population Point |\n|-------|------|----------|-------------|------------------|\n| `ret_stdout` | `str` | No (defaults to `\"\"`) | Captured stdout/stderr output | Parent accumulates from pipe reader thread |\n| `ret_tool2llm` | `Optional[str]` | No | Formatted message for LLM based on status | Parent calls `ExecutionStatus.get_return_llm()` |\n\nSources: [src/runtime/schemas.py:69-70](), [src/runtime/subprocess_python_executor.py:161-162]()\n\n---\n\n## Field Population Lifecycle\n\n### Execution Timeline\n\n```mermaid\nsequenceDiagram\n    participant Caller\n    participant Parent as \"Parent Process\"\n    participant Subprocess as \"Child Process\"\n    participant Pipe as \"Pipe Communication\"\n    \n    Caller->>Parent: run_structured_in_subprocess(command, globals, timeout)\n    Parent->>Parent: Create multiprocessing.Pipe()\n    Parent->>Subprocess: spawn/fork with _worker_with_pipe()\n    Parent->>Parent: Start reader thread\n    \n    Subprocess->>Subprocess: Redirect stdout to _PipeWriter\n    \n    alt Normal Execution\n        Subprocess->>Subprocess: exec(command, globals, locals)\n        Subprocess->>Subprocess: Create ExecutionResult(SUCCESS)\n        Subprocess->>Pipe: send(('result', ExecutionResult))\n        Pipe->>Parent: subprocess_result_container[0] = result\n    end\n    \n    alt Exception During Execution\n        Subprocess->>Subprocess: exec() raises exception\n        Subprocess->>Subprocess: Capture traceback.format_exc()\n        Subprocess->>Subprocess: Create ExecutionResult(FAILURE)\n        Subprocess->>Pipe: send(('result', ExecutionResult))\n        Pipe->>Parent: subprocess_result_container[0] = result\n    end\n    \n    alt Timeout\n        Parent->>Parent: p.join(timeout) - still alive\n        Parent->>Subprocess: p.terminate() [SIGTERM]\n        Parent->>Parent: Create ExecutionResult(TIMEOUT)\n    end\n    \n    alt Crash\n        Subprocess->>Subprocess: SegFault / OOM / os._exit()\n        Subprocess--xPipe: No result sent\n        Parent->>Parent: subprocess_result_container is empty\n        Parent->>Parent: Create ExecutionResult(CRASHED)\n    end\n    \n    Parent->>Parent: final_res.exit_code = p.exitcode\n    Parent->>Parent: final_res.ret_stdout = \"\".join(buffer)\n    Parent->>Parent: final_res.ret_tool2llm = get_return_llm()\n    Parent->>Caller: return final_res\n```\n\n**Lifecycle Stages:**\n\n1. **Initialization** (Caller â Parent):\n   - `arg_command`, `arg_timeout`, `arg_globals` provided\n   \n2. **Subprocess Execution** (Child Process):\n   - Redirects stdout/stderr to pipe\n   - Executes code with `exec()`\n   - **On success:** Creates `ExecutionResult(SUCCESS)` with filtered globals\n   - **On exception:** Creates `ExecutionResult(FAILURE)` with exception details\n   - Sends result through pipe\n   \n3. **Parent Process Monitoring**:\n   - Reader thread accumulates stdout messages\n   - **If timeout:** Creates `ExecutionResult(TIMEOUT)` without subprocess data\n   - **If crashed:** Creates `ExecutionResult(CRASHED)` when no result received\n   \n4. **Finalization** (Parent):\n   - Sets `exit_code` from `p.exitcode`\n   - Joins `ret_stdout` from accumulated buffer\n   - Generates `ret_tool2llm` via `get_return_llm()`\n\nSources: [src/runtime/subprocess_python_executor.py:76-163](), [tests/playground/subprocess_output.py:68-155]()\n\n---\n\n## Status-Based LLM Formatting\n\n### get_return_llm Method\n\nThe `ExecutionStatus.get_return_llm()` static method generates formatted messages for the LLM agent based on execution status. This method is called by the parent process after all fields are populated.\n\n```mermaid\ngraph LR\n    RESULT[\"ExecutionResult<br/>with exit_status\"]\n    METHOD[\"ExecutionStatus.get_return_llm(status, result)\"]\n    \n    SUCCESS_MSG[\"## ä»£ç æ§è¡æåï¼è¾åºç»æå®æ´ï¼ä»»å¡å®æ<br/>### ç»ç«¯è¾åºï¼<br/>{stdout}\"]\n    FAILURE_MSG[\"## ä»£ç æ§è¡å¤±è´¥ï¼ä»£ç æåºå¼å¸¸ï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯<br/>### ç»ç«¯è¾åºï¼<br/>{stdout}<br/>### åå§ä»£ç ï¼<br/>{numbered_code}<br/>### æ¥éä¿¡æ¯ï¼<br/>{traceback}\"]\n    TIMEOUT_MSG[\"## ä»£ç æ§è¡è¶æ¶ï¼å¼ºå¶éåºæ§è¡ï¼è°æ´è¶æ¶æ¶é´åéè¯<br/>### ç»ç«¯è¾åºï¼<br/>{stdout}<br/>### è¶åºéå¶çæ¶é´ï¼{timeout} ç§\"]\n    CRASHED_MSG[\"## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯<br/>### ç»ç«¯è¾åºï¼<br/>{stdout}<br/>### éåºç¶æç ï¼{exit_code}\"]\n    \n    RESULT --> METHOD\n    METHOD -->|SUCCESS| SUCCESS_MSG\n    METHOD -->|FAILURE| FAILURE_MSG\n    METHOD -->|TIMEOUT| TIMEOUT_MSG\n    METHOD -->|CRASHED| CRASHED_MSG\n```\n\nSources: [src/runtime/schemas.py:18-47]()\n\n### Format Templates\n\n#### SUCCESS Format\n```markdown\n## ä»£ç æ§è¡æåï¼è¾åºç»æå®æ´ï¼ä»»å¡å®æ\n### ç»ç«¯è¾åºï¼\n{result.ret_stdout}\n```\n\n#### FAILURE Format\n```markdown\n## ä»£ç æ§è¡å¤±è´¥ï¼ä»£ç æåºå¼å¸¸ï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\n{result.ret_stdout}\n### åå§ä»£ç ï¼\n{source_code.add_line_numbers(result.arg_command)}\n### æ¥éä¿¡æ¯ï¼\n{result.exception_traceback}\n```\n\n#### TIMEOUT Format\n```markdown\n## ä»£ç æ§è¡è¶æ¶ï¼å¼ºå¶éåºæ§è¡ï¼è°æ´è¶æ¶æ¶é´åéè¯\n### ç»ç«¯è¾åºï¼\n{result.ret_stdout}\n### è¶åºéå¶çæ¶é´ï¼{result.arg_timeout} ç§\n```\n\n#### CRASHED Format\n```markdown\n## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\n{result.ret_stdout}\n### éåºç¶æç ï¼{result.exit_code}\n```\n\nSources: [src/runtime/schemas.py:23-46]()\n\n---\n\n## Exit Code Interpretation\n\n### Unix Exit Code Semantics\n\nThe `exit_code` field follows standard Unix conventions:\n\n| Exit Code | Meaning | Example Scenario |\n|-----------|---------|------------------|\n| `0` | Normal exit | Successful execution or caught exception |\n| `> 0` | Error exit | Logic errors, caught and re-raised exceptions |\n| `< 0` | Signal termination | `-15` (SIGTERM), `-9` (SIGKILL) |\n| `139` (`128+11`) | SIGSEGV | Segmentation fault |\n\n**Code Comment Reference:**\n```python\n\"\"\"\nexitcode = 0ï¼è¿ç¨æ­£å¸¸éåºï¼\nexitcode > 0ï¼è¿ç¨å éè¯¯éåºï¼å¦ä»£ç é»è¾éè¯¯ãå½ä»¤æ§è¡å¤±è´¥ï¼ï¼\nexitcode < 0ï¼è¿ç¨è¢«ä¿¡å·ç»æ­¢ï¼-ä¿¡å·ç¼å·ï¼å¦ -15 å¯¹åº SIGTERMï¼-9 å¯¹åº SIGKILL å¼ºå¶ç»æ­¢ï¼\n             -11=139=128+11 å¯¹åº SIGSEGV segmentation faultï¼ã\n\"\"\"\n```\n\nSources: [src/runtime/schemas.py:50-54]()\n\n---\n\n## Example Outputs by Status\n\n### SUCCESS Example\n\n```python\n{\n    'arg_command': 'import time\\nc = 10\\nimport scipy\\nprint(\"scipy imported\")\\n',\n    'arg_globals': {'a': 123, 'b': [1, 2, 3], 'c': 10},\n    'arg_timeout': 20000,\n    'exception_repr': None,\n    'exception_traceback': None,\n    'exception_type': None,\n    'exception_value': None,\n    'exit_code': 0,\n    'exit_status': <ExecutionStatus.SUCCESS: 'success'>,\n    'ret_stdout': 'scipy imported\\n',\n    'ret_tool2llm': '## ä»£ç æ§è¡æåï¼è¾åºç»æå®æ´ï¼ä»»å¡å®æ\\n### ç»ç«¯è¾åºï¼\\nscipy imported\\n'\n}\n```\n\nSources: [tests/playground/subprocess_output.py:362-372]()\n\n### FAILURE Example\n\n```python\n{\n    'arg_command': '\\na = 123\\nb = 0\\nc = a/b\\n',\n    'arg_globals': {'a': 123, 'b': 0},\n    'arg_timeout': 20000,\n    'exception_repr': \"ZeroDivisionError('division by zero')\",\n    'exception_traceback': 'Traceback (most recent call last):\\n'\n                           '  File \"...subprocess_output.py\", line 42, in _worker_with_pipe\\n'\n                           '    exec(command, _globals, _locals)\\n'\n                           '  File \"<string>\", line 4, in <module>\\n'\n                           'ZeroDivisionError: division by zero\\n',\n    'exception_type': 'ZeroDivisionError',\n    'exception_value': 'division by zero',\n    'exit_code': 0,\n    'exit_status': <ExecutionStatus.FAILURE: 'failure'>,\n    'ret_stdout': '',\n    'ret_tool2llm': '## ä»£ç æ§è¡å¤±è´¥ï¼ä»£ç æåºå¼å¸¸ï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\\n...'\n}\n```\n\nSources: [tests/playground/subprocess_output.py:397-427]()\n\n### TIMEOUT Example\n\n```python\n{\n    'arg_command': '\\nimport time\\nprint(\"Start sleeping...\", flush=True)\\ntime.sleep(10)\\n...',\n    'arg_globals': {'a': 123, 'b': [1, 2, 3]},\n    'arg_timeout': 3,\n    'exception_repr': None,\n    'exception_traceback': None,\n    'exception_type': None,\n    'exception_value': None,\n    'exit_code': -15,  # SIGTERM\n    'exit_status': <ExecutionStatus.TIMEOUT: 'timeout'>,\n    'ret_stdout': 'Start sleeping...\\n',\n    'ret_tool2llm': '## ä»£ç æ§è¡è¶æ¶ï¼å¼ºå¶éåºæ§è¡ï¼è°æ´è¶æ¶æ¶é´åéè¯\\n...'\n}\n```\n\nSources: [tests/playground/subprocess_output.py:320-337]()\n\n### CRASHED Example (SegFault)\n\n```python\n{\n    'arg_command': '\\nimport os\\nos._exit(139)  # Trigger SegFault\\n',\n    'arg_globals': {'a': 123, 'b': [1, 2, 3]},\n    'arg_timeout': 3,\n    'exception_repr': None,\n    'exception_traceback': None,\n    'exception_type': None,\n    'exception_value': None,\n    'exit_code': 139,  # SIGSEGV\n    'exit_status': <ExecutionStatus.CRASHED: 'crashed'>,\n    'ret_stdout': '',\n    'ret_tool2llm': '## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\\n...'\n}\n```\n\nSources: [tests/playground/subprocess_output.py:451-468]()\n\n### CRASHED Example (RecursionError with Pickle Failure)\n\n```python\n{\n    'arg_command': '\\ndef recursive_crash(depth=0):\\n    recursive_crash(depth + 1)\\nrecursive_crash()\\n',\n    'arg_globals': {'a': 123, 'b': [1, 2, 3]},\n    'arg_timeout': 3,\n    'exception_repr': None,\n    'exception_traceback': None,\n    'exception_type': None,\n    'exception_value': None,\n    'exit_code': 1,\n    'exit_status': <ExecutionStatus.CRASHED: 'crashed'>,\n    'ret_stdout': 'å½åéå½æ·±åº¦ï¼0\\n...\\n_pickle.PicklingError: Can\\'t pickle <function recursive_crash>...\\n',\n    'ret_tool2llm': '## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\\n...'\n}\n```\n\n**Note:** This crashes because the subprocess attempts to send the `ExecutionResult` containing a local function through the pipe, but the function cannot be pickled. The process exits before successfully transmitting the result, triggering `CRASHED` status.\n\nSources: [tests/playground/subprocess_output.py:493-593]()\n\n---\n\n## Pydantic Configuration\n\nThe `ExecutionResult` model uses the following Pydantic configuration:\n\n```python\nmodel_config = ConfigDict(\n    # use_enum_values=True,  # Commented out - keeps enum objects rather than strings\n    arbitrary_types_allowed=True  # Allows Dict[str, Any] with non-JSON types\n)\n```\n\n**Configuration Details:**\n- `arbitrary_types_allowed=True`: Required to support arbitrary Python objects in `arg_globals` (functions, custom classes, etc.)\n- `use_enum_values` is disabled: `exit_status` remains as `ExecutionStatus` enum object rather than converting to string value\n\nSources: [src/runtime/schemas.py:72-75]()\n\n---\n\n## Integration with Workspace Management\n\nThe `arg_globals` field integrates with the workspace system to enable state persistence across multiple code executions:\n\n```mermaid\ngraph LR\n    EXEC1[\"Execution 1:<br/>ExecutionResult\"]\n    FILTER1[\"workspace.filter_and_deepcopy_globals()\"]\n    GLOBALS1[\"arg_globals filtered\"]\n    \n    WORKSPACE[\"workspace.append_out_globals()\"]\n    \n    EXEC2[\"Execution 2:<br/>receives globals\"]\n    \n    EXEC1 --> FILTER1\n    FILTER1 --> GLOBALS1\n    GLOBALS1 --> WORKSPACE\n    WORKSPACE --> EXEC2\n```\n\n**Filtering Process:**\n1. **Automatic validation**: Triggered by `@field_validator('arg_globals')` decorator\n2. **Filter operation**: `workspace.filter_and_deepcopy_globals(value)` removes:\n   - `__builtins__` objects\n   - Module references (identified by `type(v).__name__ == 'module'`)\n   - Non-picklable objects (tested via `pickle.dumps()`)\n3. **Deep copy**: Prevents mutations to original objects\n4. **Workspace append**: Filtered globals added to persistent workspace state\n\nFor more details on workspace management, see [Workspace State Management](#5.5).\n\nSources: [src/runtime/schemas.py:77-83](), [tests/playground/subprocess_output.py:88-99]()\n\n---\n\n## Common Patterns and Edge Cases\n\n### Pattern: Checking for Successful Execution\n\n```python\nif result.exit_status == ExecutionStatus.SUCCESS:\n    # Safe to use arg_globals for subsequent executions\n    workspace.append_out_globals(result.arg_globals)\nelse:\n    # Log error details\n    logger.error(f\"Execution failed: {result.ret_tool2llm}\")\n```\n\n### Edge Case: Empty Exception Fields on CRASHED\n\nWhen status is `CRASHED`, exception fields (`exception_type`, `exception_value`, etc.) are `None` because the subprocess terminated before sending the result. The crash details may be available in `ret_stdout` (stderr output before crash).\n\n### Edge Case: Exit Code 0 with FAILURE Status\n\nWhen code raises an exception that is caught by the subprocess wrapper, the process exits normally (`exit_code=0`) but `exit_status=FAILURE` because the exception details are captured.\n\n### Edge Case: Pickle Errors During Result Transmission\n\nIf `arg_globals` contains unpicklable objects (functions defined in `__main__`, certain class instances), the subprocess may fail to send the result through the pipe, resulting in `CRASHED` status with pickle-related errors in `ret_stdout`.\n\nSources: [tests/playground/subprocess_output.py:215-260](), [tests/playground/subprocess_output.py:469-593]()\n\n---\n\n# Page: Task Tree Schema Reference\n\n# Task Tree Schema Reference\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [src/agent/tool/base_tool.py](src/agent/tool/base_tool.py)\n- [src/agent/tool/python_tool.py](src/agent/tool/python_tool.py)\n- [src/agent/tool/todo_tool.py](src/agent/tool/todo_tool.py)\n- [src/memory/tree_todo/schemas.py](src/memory/tree_todo/schemas.py)\n- [src/memory/tree_todo/todo_track.py](src/memory/tree_todo/todo_track.py)\n\n</details>\n\n\n\nThis document provides a complete technical reference for the task tree data structures used by the `RecursivePlanTreeTodoTool`. These schemas define hierarchical task planning, status tracking, and change analysis capabilities in the algo_agent system.\n\nFor information about how these schemas are used in practice, see [Recursive Task Planning Tool](#4.3). For the execution result schemas, see [ExecutionResult Schema Reference](#9.1).\n\n---\n\n## Overview\n\nThe task tree system provides structured hierarchical task planning through three primary components:\n\n| Component | Type | Purpose | File Location |\n|-----------|------|---------|---------------|\n| `TaskStatus` | Enum | Task lifecycle state enumeration | [src/memory/tree_todo/schemas.py:8-40]() |\n| `RecursivePlanTreeNode` | Pydantic Model | Individual task unit with recursive children | [src/memory/tree_todo/schemas.py:44-64]() |\n| `RecursivePlanTree` | Pydantic Model | Complete task tree with metadata | [src/memory/tree_todo/schemas.py:68-80]() |\n\nAll schemas use Pydantic V2 for validation and serialization. The tree structure supports unlimited nesting depth through recursive `children` fields.\n\n**Sources:** [src/memory/tree_todo/schemas.py:1-80]()\n\n---\n\n## Schema Architecture\n\n### Task Tree Structure Diagram\n\n```mermaid\ngraph TB\n    RecursivePlanTree[\"RecursivePlanTree<br/>(Root Container)\"]\n    Node1[\"RecursivePlanTreeNode<br/>(task_id, task_name, status)\"]\n    Node2[\"RecursivePlanTreeNode<br/>(task_id, task_name, status)\"]\n    Node3[\"RecursivePlanTreeNode<br/>(task_id, task_name, status)\"]\n    Node4[\"RecursivePlanTreeNode<br/>(task_id, task_name, status)\"]\n    \n    RecursivePlanTree -->|\"tree_nodes: List\"| Node1\n    RecursivePlanTree -->|\"tree_nodes: List\"| Node2\n    Node1 -->|\"children: Optional[List]\"| Node3\n    Node1 -->|\"children: Optional[List]\"| Node4\n    Node3 -->|\"children: Optional[List]\"| Node5[\"RecursivePlanTreeNode<br/>(recursive)\"]\n    \n    RecursivePlanTree -.->|\"core_goal: str\"| Goal[\"'Complete Python project'\"]\n    Node1 -.->|\"status: TaskStatus\"| Status1[\"PROCESSING\"]\n    Node2 -.->|\"status: TaskStatus\"| Status2[\"PENDING\"]\n    Node3 -.->|\"status: TaskStatus\"| Status3[\"COMPLETED\"]\n    \n    RecursivePlanTree -.->|\"references: Optional[List[str]]\"| Refs[\"['doc_url', 'data_source']\"]\n    RecursivePlanTree -.->|\"next_action: Dict[str, Any]\"| NextAction[\"{'priority': 'high'}\"]\n```\n\n**Sources:** [src/memory/tree_todo/schemas.py:44-80]()\n\n---\n\n## TaskStatus Enum\n\nThe `TaskStatus` enum defines six lifecycle states for tasks. Each status has an associated display symbol and Chinese description used for rendering.\n\n### Enum Definition\n\n```python\nclass TaskStatus(str, Enum):\n    PENDING = \"pending\"        # Waiting to execute\n    PROCESSING = \"processing\"  # Currently executing\n    COMPLETED = \"completed\"    # Successfully completed\n    FAILED = \"failed\"          # Execution failed\n    RETRY = \"retry\"            # Retry required\n    SKIPPED = \"skipped\"        # Intentionally skipped\n```\n\n### Status Properties\n\n| Status | Value | Symbol | Description (Chinese) | Use Case |\n|--------|-------|--------|----------------------|----------|\n| `PENDING` | `\"pending\"` | `[â³]` | å¾æ§è¡ | Task not yet started |\n| `PROCESSING` | `\"processing\"` | `[â¡ï¸]` | æ­£å¨æ§è¡ | Task currently executing |\n| `COMPLETED` | `\"completed\"` | `[â]` | æ§è¡æå | Task finished successfully |\n| `FAILED` | `\"failed\"` | `[â]` | æ§è¡å¤±è´¥ | Task encountered error |\n| `RETRY` | `\"retry\"` | `[â»ï¸]` | éè¯ | Task requires retry |\n| `SKIPPED` | `\"skipped\"` | `[â]` | å·²è·³è¿ | Task intentionally bypassed |\n\n### Property Methods\n\nThe enum provides two property methods for display purposes:\n\n- **`display_symbol`** - Returns the emoji/symbol for visual rendering (e.g., `[â]`)\n- **`display_desc`** - Returns the Chinese language description (e.g., `\"æ§è¡æå\"`)\n\n**Implementation:**\n```python\n@property\ndef display_symbol(self) -> str:\n    \"\"\"ç¶æå¯¹åºçå¯è§åç¬¦å·\"\"\"\n    symbol_map = {\n        self.PENDING: \"[â³]\",\n        self.PROCESSING: \"[â¡ï¸]\",\n        # ... (additional mappings)\n    }\n    return symbol_map[self]\n```\n\n**Sources:** [src/memory/tree_todo/schemas.py:8-40]()\n\n---\n\n## RecursivePlanTreeNode Schema\n\n`RecursivePlanTreeNode` represents an individual task unit with support for hierarchical nesting through the `children` field.\n\n### Field Reference\n\n| Field Name | Type | Required | Default | Description |\n|------------|------|----------|---------|-------------|\n| `task_id` | `str` | No | `TASK-{uuid4()}` | Unique task identifier, auto-generated if not provided |\n| `task_name` | `str` | **Yes** | - | Task name (must be globally unique, used in dependencies) |\n| `description` | `str` | No | `\"\"` | Detailed task explanation or requirements |\n| `status` | `TaskStatus` | No | `PENDING` | Current task status (enum value) |\n| `output` | `str` | No | `\"\"` | Execution result or completion notes |\n| `dependencies` | `Optional[List[str]]` | No | `None` | List of prerequisite task names (references `task_name`) |\n| `research_directions` | `Optional[List[str]]` | No | `None` | Deep research directions for complex tasks |\n| `children` | `Optional[List[RecursivePlanTreeNode]]` | No | `None` | Nested child tasks (recursive structure) |\n\n### Field Validation\n\nThe schema includes a validator for the `children` field that converts empty lists to `None`:\n\n```python\n@field_validator(\"children\")\ndef empty_children_to_none(cls, v: Optional[List[\"RecursivePlanTreeNode\"]]) -> Optional[List[\"RecursivePlanTreeNode\"]]:\n    return v if v and len(v) > 0 else None\n```\n\nThis ensures that tasks without children have `children=None` rather than `children=[]`.\n\n**Sources:** [src/memory/tree_todo/schemas.py:44-64]()\n\n### Task ID Generation\n\nTask IDs are auto-generated using the pattern `TASK-{uuid4()}` if not explicitly provided:\n\n```python\ntask_id: str = Field(\n    default_factory=lambda: f\"TASK-{str(uuid.uuid4())}\", \n    description=\"ä»»å¡å¯ä¸IDï¼æ¬æ¬¡å·¥å·è°ç¨å¿é¡»å¯ä¸ï¼åç»­æ¨çå¦ææ¯æä»£åä¸ä¸ªä»»å¡ç´æ¥å¼ç¨ï¼\"\n)\n```\n\n**Sources:** [src/memory/tree_todo/schemas.py:46]()\n\n### Dependency System\n\nThe `dependencies` field allows tasks to declare prerequisites using task names:\n\n```python\ndependencies: Optional[List[str]] = Field(\n    default=None, \n    description=\"ä¾èµçä»»å¡åç§°çåè¡¨ï¼ä»»å¡åç§°å¿é¡»æ¯task_name\"\n)\n```\n\n**Important:** Dependencies reference `task_name` values, not `task_id` values. Task names must be globally unique to support this reference mechanism.\n\n**Sources:** [src/memory/tree_todo/schemas.py:51]()\n\n### Self-Reference Resolution\n\nThe schema uses self-referential typing for the `children` field. Pydantic V2 requires explicit model rebuilding to resolve this forward reference:\n\n```python\nRecursivePlanTreeNode.model_rebuild()\n```\n\n**Sources:** [src/memory/tree_todo/schemas.py:64]()\n\n---\n\n## RecursivePlanTree Schema\n\n`RecursivePlanTree` is the root container for the entire task hierarchy, including metadata and context.\n\n### Field Reference\n\n| Field Name | Type | Required | Default | Description |\n|------------|------|----------|---------|-------------|\n| `core_goal` | `str` | **Yes** | - | Primary objective of the entire plan tree |\n| `tree_nodes` | `List[RecursivePlanTreeNode]` | No | `[]` | Root-level task list |\n| `next_action` | `Dict[str, Any]` | No | `{}` | Suggested next steps or metadata |\n| `references` | `Optional[List[str]]` | No | `None` | External references (URLs, data sources) |\n\n### Schema Definition\n\n```python\nclass RecursivePlanTree(BaseModel):\n    \"\"\"å®æ´éå½è®¡åæ ï¼åå«å±çº§ä»»å¡æ ãæ ¸å¿ç®æ ãç¶æç»è®¡ç­\"\"\"\n    core_goal: str = Field(..., description=\"æ ¸å¿ç®æ ï¼è®¡åæ è¦è¾¾æçæç»ç®çï¼\")\n    tree_nodes: List[RecursivePlanTreeNode] = Field(default_factory=list, description=\"è®¡åæ æ ¹ä»»å¡åè¡¨\")\n    next_action: Dict[str, Any] = Field(default_factory=dict, description=\"ä¸ä¸æ­¥å»ºè®®å¨ä½ï¼å¯éï¼\")\n    references: Optional[List[str]] = Field(default=None, description=\"åèèµæºåè¡¨ï¼å¯éï¼å¦ææ¡£é¾æ¥ãæ°æ®æ¥æºï¼\")\n```\n\n**Sources:** [src/memory/tree_todo/schemas.py:68-80]()\n\n### Metadata Fields\n\nThe `RecursivePlanTree` contains two optional metadata fields:\n\n1. **`next_action`** - A flexible dictionary for storing suggested next steps, priorities, or other action metadata\n2. **`references`** - A list of URLs, file paths, or data source identifiers for documentation\n\nThese fields are not processed by the tracking system but are available for tool-specific logic.\n\n**Sources:** [src/memory/tree_todo/schemas.py:74-75]()\n\n---\n\n## Schema Integration with Tools\n\n### Tool Usage Pattern\n\n```mermaid\nsequenceDiagram\n    participant LLM[\"LLM (qwen-plus)\"]\n    participant Tool[\"RecursivePlanTreeTodoTool\"]\n    participant Validator[\"Pydantic Validator\"]\n    participant Tracker[\"todo_track.run()\"]\n    participant Storage[\"arg_todo_list\"]\n    \n    LLM->>Tool: \"tool_calls with recursive_plan_tree\"\n    Tool->>Validator: \"Validate RecursivePlanTree schema\"\n    Validator->>Validator: \"Validate each RecursivePlanTreeNode\"\n    Validator->>Validator: \"Check TaskStatus enum values\"\n    Validator->>Validator: \"Apply empty_children_to_none validator\"\n    Validator-->>Tool: \"Validated RecursivePlanTree instance\"\n    Tool->>Tracker: \"run(recursive_plan_tree)\"\n    Tracker->>Storage: \"Append to arg_todo_list\"\n    Tracker->>Tracker: \"Compare with last version\"\n    Tracker-->>Tool: \"changes_summary, markdown_todo_list\"\n    Tool-->>LLM: \"Return formatted result string\"\n```\n\n**Sources:** [src/agent/tool/todo_tool.py:10-36](), [src/memory/tree_todo/todo_track.py:21-49]()\n\n### Tool Parameter Schema\n\nThe `RecursivePlanTreeTodoTool` declares the tree schema as a required parameter:\n\n```python\nclass RecursivePlanTreeTodoTool(BaseTool):\n    recursive_plan_tree: RecursivePlanTree = Field(\n        ..., \n        description=\"è¦ç®¡ççéå½è®¡åæ å¯¹è±¡ï¼åå«ä»»å¡èç¹ãç¶æåå­ä»»å¡ãåªå¢å ãä¸ä¿®æ¹ãä¸å é¤ä»»å¡èç¹ã\"\n    )\n```\n\nWhen the LLM invokes this tool, it must provide a complete `RecursivePlanTree` object serialized as JSON in the `tool_calls` parameters.\n\n**Sources:** [src/agent/tool/todo_tool.py:19-25]()\n\n---\n\n## Version Tracking and Change Analysis\n\n### Version Storage\n\nThe tracking system maintains a history of all plan tree versions in the `arg_todo_list`:\n\n```python\narg_todo_list: List[RecursivePlanTree] = [\n    RecursivePlanTree(\n        core_goal=\"ç©ºè®¡åæ ç­å¾åå§å\",\n    )\n]\n```\n\nEach time the tool runs, the current tree is appended:\n\n```python\nlast_plan = arg_todo_list[-1] if arg_todo_list else None\narg_todo_list.append(current_plan_tree.model_copy(deep=True))\n```\n\n**Sources:** [src/memory/tree_todo/todo_track.py:6-32]()\n\n### Change Detection\n\nThe `_analyze_changes()` function compares consecutive versions to detect:\n\n| Change Type | Detection Method | Example Output |\n|-------------|------------------|----------------|\n| New tasks | `set(current_ids) - set(last_ids)` | `\"ð æ°å¢ä»»å¡ï¼æ¡æ¶å¯¹æ¯, é¡¹ç®åå§å\"` |\n| Deleted tasks | `set(last_ids) - set(current_ids)` | `\"ðï¸ å é¤ä»»å¡ï¼æ§éæ±\"` |\n| Status changes | Compare status of common task IDs | `\"ð ç¶æåæ´ï¼éæ±åæï¼å¾æ§è¡ â æ§è¡æåï¼\"` |\n| Hierarchy changes | Compare parent tasks | `\"ð å±çº§è°æ´ï¼æ¡æ¶å¯¹æ¯ï¼ç¶ä»»å¡ï¼æ ¹èç¹ â ææ¯éåï¼\"` |\n\n**Implementation:**\n```python\ndef _analyze_changes(last_plan: RecursivePlanTree, current_plan: RecursivePlanTree) -> str:\n    \"\"\"å¯¹æ¯ä¸¤ä¸ªè®¡åæ ï¼åæåæ´åå®¹\"\"\"\n    changes = []\n    # Collect all task IDs recursively\n    last_task_ids = collect_all_task_ids(last_plan.tree_nodes)\n    current_task_ids = collect_all_task_ids(current_plan.tree_nodes)\n    # Detect new, deleted, status-changed, and level-changed tasks\n    # ...\n    return \"\\n\".join(changes) if changes else \"â¹ï¸ è®¡åæ æ ææ¾åæ´\"\n```\n\n**Sources:** [src/memory/tree_todo/todo_track.py:62-121]()\n\n---\n\n## Markdown Rendering\n\n### Rendering Algorithm\n\nThe `_render_plan_tree_markdown()` function recursively generates a hierarchical Todo list:\n\n```python\ndef _render_plan_tree_markdown(nodes: List[RecursivePlanTreeNode], indent_level: int = 0) -> str:\n    \"\"\"éå½æ¸²æè®¡åæ ä¸ºMarkdown Todoåè¡¨\"\"\"\n    markdown_lines = []\n    indent = \"  \" * indent_level  # 2 spaces per level\n    \n    for node in nodes:\n        status_symbol = node.status.display_symbol\n        task_line = f\"{indent}- {status_symbol} **{node.task_name}**ï¼IDï¼{node.task_id}ï¼\"\n        \n        if node.description:\n            task_line += f\"\\n{indent}  > è¯´æï¼{node.description}\"\n        \n        if node.output:\n            task_line += f\"\\n{indent}  > ç»æï¼{node.output}\"\n        \n        markdown_lines.append(task_line)\n        \n        if node.children:\n            child_lines = _render_plan_tree_markdown(node.children, indent_level + 1)\n            markdown_lines.append(child_lines)\n    \n    return \"\\n\".join(markdown_lines)\n```\n\n**Sources:** [src/memory/tree_todo/todo_track.py:137-170]()\n\n### Rendering Example\n\nFor a node with children:\n```\n- [â¡ï¸] **éæ±åæ**ï¼IDï¼TASK-abc123ï¼\n  > è¯´æï¼æ¢³çæ ¸å¿åè½åéåè½éæ±\n  - [â] **æ¶éç¨æ·éæ±**ï¼IDï¼TASK-def456ï¼\n    > ç»æï¼å·²æ¶é3ç±»æ ¸å¿éæ±\n  - [â³] **æ°åéæ±ææ¡£**ï¼IDï¼TASK-ghi789ï¼\n```\n\n**Sources:** [src/memory/tree_todo/todo_track.py:137-170]()\n\n---\n\n## Status Statistics\n\n### Calculation Logic\n\nThe `_calculate_status_statistics()` function aggregates task counts by status:\n\n```python\ndef _calculate_status_statistics(recursive_plan_tree: RecursivePlanTree) -> Dict[str, int]:\n    \"\"\"æ ¹æ®ææä»»å¡ç¶æèªå¨çæç»è®¡ä¿¡æ¯\"\"\"\n    status_count = {status.value: 0 for status in TaskStatus}\n    \n    def count_status(nodes: List[RecursivePlanTreeNode]):\n        for node in nodes:\n            status_count[node.status.value] += 1\n            if node.children:\n                count_status(node.children)\n    \n    count_status(recursive_plan_tree.tree_nodes)\n    \n    total_tasks = sum(status_count.values())\n    statistics = {\n        \"__total\": total_tasks,\n        \"__completion_rate\": round(status_count[TaskStatus.COMPLETED.value] / total_tasks, 2) if total_tasks > 0 else 0.0,\n        \"__pending_rate\": round(status_count[TaskStatus.PENDING.value] / total_tasks, 2) if total_tasks > 0 else 0.0,\n    }\n    status_count.update(statistics)\n    return status_count\n```\n\n**Sources:** [src/memory/tree_todo/todo_track.py:173-200]()\n\n### Statistics Output Format\n\nThe function returns a dictionary with the following structure:\n\n| Key | Type | Description |\n|-----|------|-------------|\n| `\"pending\"` | `int` | Count of PENDING tasks |\n| `\"processing\"` | `int` | Count of PROCESSING tasks |\n| `\"completed\"` | `int` | Count of COMPLETED tasks |\n| `\"failed\"` | `int` | Count of FAILED tasks |\n| `\"retry\"` | `int` | Count of RETRY tasks |\n| `\"skipped\"` | `int` | Count of SKIPPED tasks |\n| `\"__total\"` | `int` | Total task count |\n| `\"__completion_rate\"` | `float` | Completion rate (0.0-1.0) |\n| `\"__pending_rate\"` | `float` | Pending task rate (0.0-1.0) |\n\n**Sources:** [src/memory/tree_todo/todo_track.py:173-200]()\n\n---\n\n## Configuration and Validation\n\n### Pydantic Configuration\n\nBoth models use the following configuration:\n\n```python\nclass Config:\n    arbitrary_types_allowed = True  # Allow arbitrary nested types\n```\n\nThe commented-out `use_enum_values = True` option would serialize enums as their string values rather than enum objects. The current implementation preserves enum objects for type safety.\n\n**Sources:** [src/memory/tree_todo/schemas.py:59-61](), [src/memory/tree_todo/schemas.py:77-79]()\n\n### Serialization Methods\n\nPydantic V2 model serialization:\n\n| Method | Purpose | Example |\n|--------|---------|---------|\n| `model_dump()` | Serialize to dictionary | `tree.model_dump()` |\n| `model_dump_json()` | Serialize to JSON string | `tree.model_dump_json()` |\n| `model_copy(deep=True)` | Deep copy the model | `tree.model_copy(deep=True)` |\n\n**Note:** Pydantic V1's `dict()` and `copy()` methods are replaced by `model_dump()` and `model_copy()` in V2.\n\n**Sources:** [src/memory/tree_todo/todo_track.py:32](), [src/agent/tool/todo_tool.py:70]()\n\n---\n\n## Complete Schema Diagram\n\n```mermaid\nclassDiagram\n    class TaskStatus {\n        <<enumeration>>\n        +PENDING: \"pending\"\n        +PROCESSING: \"processing\"\n        +COMPLETED: \"completed\"\n        +FAILED: \"failed\"\n        +RETRY: \"retry\"\n        +SKIPPED: \"skipped\"\n        +display_symbol() str\n        +display_desc() str\n    }\n    \n    class RecursivePlanTreeNode {\n        +task_id: str\n        +task_name: str\n        +description: str\n        +status: TaskStatus\n        +output: str\n        +dependencies: Optional~List~str~~\n        +research_directions: Optional~List~str~~\n        +children: Optional~List~RecursivePlanTreeNode~~\n        +empty_children_to_none() validator\n    }\n    \n    class RecursivePlanTree {\n        +core_goal: str\n        +tree_nodes: List~RecursivePlanTreeNode~\n        +next_action: Dict~str,Any~\n        +references: Optional~List~str~~\n    }\n    \n    class RecursivePlanTreeTodoTool {\n        +tool_call_purpose: str\n        +recursive_plan_tree: RecursivePlanTree\n        +run() Dict~str,str~\n    }\n    \n    class todo_track {\n        +arg_todo_list: List~RecursivePlanTree~\n        +run(current_plan_tree) Dict\n        +_analyze_changes(last, current) str\n        +_render_plan_tree_markdown(nodes) str\n        +_calculate_status_statistics(tree) Dict\n    }\n    \n    RecursivePlanTreeNode --> TaskStatus : uses\n    RecursivePlanTreeNode --> RecursivePlanTreeNode : recursive children\n    RecursivePlanTree --> RecursivePlanTreeNode : contains\n    RecursivePlanTreeTodoTool --> RecursivePlanTree : parameter\n    RecursivePlanTreeTodoTool --> todo_track : calls\n    todo_track --> RecursivePlanTree : stores history\n```\n\n**Sources:** [src/memory/tree_todo/schemas.py:1-80](), [src/agent/tool/todo_tool.py:10-37](), [src/memory/tree_todo/todo_track.py:1-201]()\n\n---\n\n## Usage Example Structure\n\nA minimal valid `RecursivePlanTree` requires only the `core_goal` and at least one root task:\n\n```json\n{\n  \"core_goal\": \"Complete Python project development\",\n  \"tree_nodes\": [\n    {\n      \"task_name\": \"Requirements analysis\",\n      \"status\": \"processing\",\n      \"children\": [\n        {\n          \"task_name\": \"Collect user requirements\",\n          \"status\": \"completed\",\n          \"output\": \"Collected 3 core requirements\"\n        },\n        {\n          \"task_name\": \"Write requirements document\",\n          \"status\": \"pending\"\n        }\n      ]\n    }\n  ]\n}\n```\n\nThe LLM generates this structure in the `tool_calls` parameters when invoking `RecursivePlanTreeTodoTool`.\n\n**Sources:** [src/agent/tool/todo_tool.py:42-69]()\n\n---\n\n# Page: Tool Schema Format\n\n# Tool Schema Format\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [src/agent/llm.py](src/agent/llm.py)\n- [src/agent/tool/base_tool.py](src/agent/tool/base_tool.py)\n- [src/memory/tree_todo/schemas.py](src/memory/tree_todo/schemas.py)\n- [src/memory/tree_todo/todo_track.py](src/memory/tree_todo/todo_track.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis page documents the JSON schema format used to describe tools to the LLM, including the OpenAI function calling convention, parameter definitions, and how schemas are automatically generated from Pydantic models. This reference focuses on the **schema structure itself** rather than tool implementation.\n\nFor information about implementing new tools, see [Creating New Tools](#8.2). For the base tool interface that generates these schemas, see [BaseTool Interface](#4.1). For how the agent dispatches tool calls, see [Action Coordination](#3.4).\n\n---\n\n## Schema Generation Pipeline\n\nThe following diagram shows how tool schemas are generated from Python classes and consumed by the LLM:\n\n**Tool Schema Generation Flow**\n\n```mermaid\ngraph TB\n    subgraph \"Tool Definition\"\n        CLASS[\"Tool Class<br/>(extends BaseTool)\"]\n        DOCSTRING[\"Class Docstring\"]\n        PYDANTIC[\"Pydantic Field Definitions\"]\n    end\n    \n    subgraph \"Schema Generation Methods\"\n        TOOLNAME[\"tool_name()<br/>inflection.underscore()\"]\n        TOOLDESC[\"tool_description()<br/>inspect.getdoc()\"]\n        PARAMSCHEMA[\"get_parameter_schema()<br/>model_json_schema()\"]\n        TOOLSCHEMA[\"get_tool_schema()\"]\n    end\n    \n    subgraph \"Generated Schema\"\n        JSONSCHEMA[\"JSON Schema<br/>OpenAI Function Format\"]\n        NAME[\"function.name\"]\n        DESC[\"function.description\"]\n        PARAMS[\"function.parameters\"]\n        STRICT[\"function.strict: True\"]\n    end\n    \n    subgraph \"LLM Integration\"\n        TOOLSLIST[\"tools_schema_list\"]\n        LLMCALL[\"_generate_chat_completion()\"]\n        RESPONSE[\"assistant_output.tool_calls\"]\n    end\n    \n    CLASS --> TOOLNAME\n    DOCSTRING --> TOOLDESC\n    PYDANTIC --> PARAMSCHEMA\n    \n    TOOLNAME --> TOOLSCHEMA\n    TOOLDESC --> TOOLSCHEMA\n    PARAMSCHEMA --> TOOLSCHEMA\n    \n    TOOLSCHEMA --> JSONSCHEMA\n    JSONSCHEMA --> NAME\n    JSONSCHEMA --> DESC\n    JSONSCHEMA --> PARAMS\n    JSONSCHEMA --> STRICT\n    \n    JSONSCHEMA --> TOOLSLIST\n    TOOLSLIST --> LLMCALL\n    LLMCALL --> RESPONSE\n```\n\n**Sources:** [src/agent/tool/base_tool.py:13-71](), [src/agent/llm.py:14-24]()\n\n---\n\n## OpenAI Function Calling Format\n\nTool schemas follow the OpenAI function calling format, which structures tools as function objects with typed parameters. The schema is a nested dictionary with the following structure:\n\n### Schema Structure\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `type` | `\"function\"` | Top-level type identifier |\n| `function` | `object` | Function definition container |\n| `function.type` | `\"function\"` | Redundant type field |\n| `function.name` | `string` | Unique tool identifier |\n| `function.description` | `string` | Tool purpose and usage |\n| `function.parameters` | `object` | JSON Schema for parameters |\n| `function.strict` | `boolean` | Always `True` for strict validation |\n\n### Example Schema Structure\n\n```json\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"type\": \"function\",\n        \"name\": \"execute_python_code\",\n        \"description\": \"Executes Python code with state persistence\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {...},\n            \"required\": [...],\n            \"additionalProperties\": false\n        },\n        \"strict\": true\n    }\n}\n```\n\nThe `strict: true` flag enforces that the LLM must provide exactly the parameters defined in the schema with correct types.\n\n**Sources:** [src/agent/tool/base_tool.py:31-71]()\n\n---\n\n## Tool Naming Convention\n\nTool names are automatically generated from class names using a consistent convention that removes \"Tool\" suffixes and converts to snake_case.\n\n### Name Generation Process\n\n**Tool Name Generation Logic**\n\n```mermaid\ngraph LR\n    CLASSNAME[\"Class Name<br/>'ExecutePythonCodeTool'\"]\n    REMOVE[\"Remove 'Tool' / 'tool'<br/>'ExecutePythonCode'\"]\n    UNDERSCORE[\"inflection.underscore()<br/>'execute_python_code'\"]\n    \n    CLASSNAME --> REMOVE\n    REMOVE --> UNDERSCORE\n```\n\nThe `tool_name()` class method implements this at [src/agent/tool/base_tool.py:13-18]():\n\n```python\n@classmethod\ndef tool_name(cls) -> str:\n    \"\"\"å·¥å·å¯ä¸æ è¯åï¼ç¨äºè·¯ç±å¹éï¼å¦ \"weatherquery\"ï¼\"\"\"\n    return inflection.underscore(\n        cls.__name__.replace(\"Tool\", \"\").replace(\"tool\", \"\")\n    )\n```\n\n### Naming Examples\n\n| Class Name | Generated Tool Name |\n|------------|---------------------|\n| `ExecutePythonCodeTool` | `execute_python_code` |\n| `RecursivePlanTreeTodoTool` | `recursive_plan_tree_todo` |\n| `WeatherQueryTool` | `weather_query` |\n| `DataProcessingTool` | `data_processing` |\n\nTool names must be unique across all available tools since they are used for routing tool calls from the LLM.\n\n**Sources:** [src/agent/tool/base_tool.py:13-18]()\n\n---\n\n## Tool Description\n\nTool descriptions are extracted from the class docstring and provide natural language guidance to the LLM about when and how to use the tool.\n\n### Description Extraction\n\nThe `tool_description()` method uses Python's `inspect.getdoc()` to extract the docstring at [src/agent/tool/base_tool.py:20-23]():\n\n```python\n@classmethod\ndef tool_description(cls) -> str:\n    \"\"\"å·¥å·æè¿°ï¼ä¾ Agent çè§£ç¨éï¼\"\"\"\n    return inspect.getdoc(cls) or \"æ å·¥å·æè¿°\"\n```\n\n### Description Best Practices\n\n| Guideline | Rationale |\n|-----------|-----------|\n| Start with a verb phrase | Describes what the tool does |\n| Mention key capabilities | Helps LLM understand when to use it |\n| Include limitations | Prevents inappropriate usage |\n| Keep under 200 characters | Concise for LLM token efficiency |\n\n### Example Descriptions\n\n```python\nclass ExecutePythonCodeTool(BaseTool):\n    \"\"\"\n    Executes arbitrary Python code in an isolated environment with \n    state persistence across calls. Supports timeout control and \n    error handling. Use for data processing, calculations, and \n    algorithm implementation.\n    \"\"\"\n```\n\nThe description appears in the generated schema as `function.description` and influences the LLM's decision to invoke the tool.\n\n**Sources:** [src/agent/tool/base_tool.py:20-23]()\n\n---\n\n## Parameter Schema Generation\n\nParameter schemas are automatically generated from Pydantic model definitions using `model_json_schema()`, which produces JSON Schema compatible with OpenAI's function calling format.\n\n### Schema Generation Method\n\nThe `get_parameter_schema()` method at [src/agent/tool/base_tool.py:26-28]() delegates to Pydantic:\n\n```python\n@classmethod\ndef get_parameter_schema(cls) -> dict:\n    \"\"\"è·ååæ° JSON Schemaï¼ä¾ Agent æé åæ°ï¼\"\"\"\n    return cls.model_json_schema()\n```\n\n### Pydantic to JSON Schema Mapping\n\n**Field Type Conversion**\n\n```mermaid\ngraph TB\n    subgraph \"Pydantic Field Types\"\n        STR[\"str\"]\n        INT[\"int\"]\n        FLOAT[\"float\"]\n        BOOL[\"bool\"]\n        LIST[\"List[T]\"]\n        DICT[\"Dict[K,V]\"]\n        ENUM[\"Enum\"]\n        OPTIONAL[\"Optional[T]\"]\n        NESTED[\"BaseModel\"]\n    end\n    \n    subgraph \"JSON Schema Types\"\n        JSTR[\"type: string\"]\n        JINT[\"type: integer\"]\n        JFLOAT[\"type: number\"]\n        JBOOL[\"type: boolean\"]\n        JARRAY[\"type: array<br/>items: T schema\"]\n        JOBJECT[\"type: object<br/>properties: {...}\"]\n        JENUM[\"type: string<br/>enum: [values]\"]\n        JOPTIONAL[\"Not in required[]\"]\n        JNESTED[\"type: object<br/>$ref or inline\"]\n    end\n    \n    STR --> JSTR\n    INT --> JINT\n    FLOAT --> JFLOAT\n    BOOL --> JBOOL\n    LIST --> JARRAY\n    DICT --> JOBJECT\n    ENUM --> JENUM\n    OPTIONAL --> JOPTIONAL\n    NESTED --> JNESTED\n```\n\n### Field Definition Components\n\nPydantic `Field()` objects provide schema metadata:\n\n| Field Parameter | JSON Schema Output | Example |\n|----------------|-------------------|---------|\n| `description` | `description` | `\"The Python code to execute\"` |\n| `default` | `default` value | `\"pending\"` |\n| `default_factory` | `default` computed | `lambda: str(uuid.uuid4())` |\n| `...` (Ellipsis) | Added to `required[]` | Mandatory field |\n\n### Example Parameter Schema\n\nGiven a Pydantic model definition from [src/memory/tree_todo/schemas.py:44-53]():\n\n```python\nclass RecursivePlanTreeNode(BaseModel):\n    task_id: str = Field(\n        default_factory=lambda: f\"TASK-{str(uuid.uuid4())}\", \n        description=\"ä»»å¡å¯ä¸IDï¼æ¬æ¬¡å·¥å·è°ç¨å¿é¡»å¯ä¸ï¼åç»­æ¨çå¦ææ¯æä»£åä¸ä¸ªä»»å¡ç´æ¥å¼ç¨ï¼\"\n    )\n    task_name: str = Field(\n        ..., \n        description=\"ä»»å¡åç§°ï¼ç®æ´æè¿°æ ¸å¿å¨ä½ï¼ï¼å¤§è¯­è¨æ¨¡åçæï¼å¿é¡»å¨å±å¯ä¸ï¼ä¼è¢«dependenciesåè¡¨å¼ç¨\"\n    )\n    status: TaskStatus = Field(\n        default=TaskStatus.PENDING, \n        description=f\"ä»»å¡ç¶ææä¸¾ï¼{[status.value for status in TaskStatus]}\"\n    )\n```\n\nThe generated parameter schema includes:\n\n```json\n{\n    \"type\": \"object\",\n    \"properties\": {\n        \"task_id\": {\n            \"type\": \"string\",\n            \"description\": \"ä»»å¡å¯ä¸IDï¼æ¬æ¬¡å·¥å·è°ç¨å¿é¡»å¯ä¸ï¼åç»­æ¨çå¦ææ¯æä»£åä¸ä¸ªä»»å¡ç´æ¥å¼ç¨ï¼\",\n            \"default\": \"TASK-<uuid>\"\n        },\n        \"task_name\": {\n            \"type\": \"string\",\n            \"description\": \"ä»»å¡åç§°ï¼ç®æ´æè¿°æ ¸å¿å¨ä½ï¼ï¼å¤§è¯­è¨æ¨¡åçæï¼å¿é¡»å¨å±å¯ä¸ï¼ä¼è¢«dependenciesåè¡¨å¼ç¨\"\n        },\n        \"status\": {\n            \"type\": \"string\",\n            \"enum\": [\"pending\", \"processing\", \"completed\", \"failed\", \"retry\", \"skipped\"],\n            \"default\": \"pending\",\n            \"description\": \"ä»»å¡ç¶ææä¸¾ï¼['pending', 'processing', 'completed', 'failed', 'retry', 'skipped']\"\n        }\n    },\n    \"required\": [\"task_name\"],\n    \"additionalProperties\": false\n}\n```\n\n**Sources:** [src/agent/tool/base_tool.py:26-28](), [src/memory/tree_todo/schemas.py:44-53]()\n\n---\n\n## Nested and Recursive Schemas\n\nPydantic models with nested or self-referential structures generate complex JSON schemas with `$ref` references or inline definitions.\n\n### Recursive Schema Example\n\nThe `RecursivePlanTreeNode` model demonstrates self-reference at [src/memory/tree_todo/schemas.py:53]():\n\n```python\nchildren: Optional[List[\"RecursivePlanTreeNode\"]] = Field(\n    default=None, \n    description=\"å­ä»»å¡åè¡¨ï¼å±çº§åµå¥ï¼\"\n)\n```\n\nAfter calling `RecursivePlanTreeNode.model_rebuild()` at [src/memory/tree_todo/schemas.py:64](), the generated schema includes:\n\n```json\n{\n    \"properties\": {\n        \"children\": {\n            \"anyOf\": [\n                {\n                    \"type\": \"array\",\n                    \"items\": {\"$ref\": \"#/$defs/RecursivePlanTreeNode\"}\n                },\n                {\"type\": \"null\"}\n            ],\n            \"default\": null,\n            \"description\": \"å­ä»»å¡åè¡¨ï¼å±çº§åµå¥ï¼\"\n        }\n    },\n    \"$defs\": {\n        \"RecursivePlanTreeNode\": {\n            \"type\": \"object\",\n            \"properties\": {...}\n        }\n    }\n}\n```\n\n### Handling Optional Types\n\nThe `Optional[T]` type generates an `anyOf` schema with null support:\n\n```json\n{\n    \"anyOf\": [\n        {\"type\": \"array\", \"items\": {...}},\n        {\"type\": \"null\"}\n    ]\n}\n```\n\nThis allows the LLM to either provide the value or omit it entirely.\n\n**Sources:** [src/memory/tree_todo/schemas.py:44-64]()\n\n---\n\n## Complete Schema Examples\n\n### Example 1: BaseTool with tool_call_purpose\n\nAll tools inherit a `tool_call_purpose` field from `BaseTool` at [src/agent/tool/base_tool.py:9-12]():\n\n```python\ntool_call_purpose: str = Field(\n    ..., \n    description=\"å·¥å·è°ç¨çç®çï¼å³è°ç¨è¯¥å·¥å·çå·ä½åºæ¯æé®é¢ã\"\n)\n```\n\nThis field appears in every tool's parameter schema:\n\n```json\n{\n    \"type\": \"object\",\n    \"properties\": {\n        \"tool_call_purpose\": {\n            \"type\": \"string\",\n            \"description\": \"å·¥å·è°ç¨çç®çï¼å³è°ç¨è¯¥å·¥å·çå·ä½åºæ¯æé®é¢ã\"\n        },\n        ...\n    },\n    \"required\": [\"tool_call_purpose\", ...]\n}\n```\n\n### Example 2: RecursivePlanTree Schema\n\nThe `RecursivePlanTree` model at [src/memory/tree_todo/schemas.py:68-79]() generates:\n\n```json\n{\n    \"type\": \"object\",\n    \"properties\": {\n        \"core_goal\": {\n            \"type\": \"string\",\n            \"description\": \"æ ¸å¿ç®æ ï¼è®¡åæ è¦è¾¾æçæç»ç®çï¼\"\n        },\n        \"tree_nodes\": {\n            \"type\": \"array\",\n            \"items\": {\"$ref\": \"#/$defs/RecursivePlanTreeNode\"},\n            \"default\": [],\n            \"description\": \"è®¡åæ æ ¹ä»»å¡åè¡¨\"\n        },\n        \"next_action\": {\n            \"type\": \"object\",\n            \"default\": {},\n            \"description\": \"ä¸ä¸æ­¥å»ºè®®å¨ä½ï¼å¯éï¼\"\n        },\n        \"references\": {\n            \"anyOf\": [\n                {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                {\"type\": \"null\"}\n            ],\n            \"default\": null,\n            \"description\": \"åèèµæºåè¡¨ï¼å¯éï¼å¦ææ¡£é¾æ¥ãæ°æ®æ¥æºï¼\"\n        }\n    },\n    \"required\": [\"core_goal\"]\n}\n```\n\n**Sources:** [src/agent/tool/base_tool.py:9-12](), [src/memory/tree_todo/schemas.py:68-79]()\n\n---\n\n## Integration with LLM\n\nThe generated schemas are passed to the LLM as a list through the `tools` parameter in the chat completion API.\n\n### Schema List Construction\n\n**Schema Flow to LLM**\n\n```mermaid\ngraph LR\n    subgraph \"Tool Classes\"\n        TOOL1[\"ExecutePythonCodeTool\"]\n        TOOL2[\"RecursivePlanTreeTodoTool\"]\n        TOOLN[\"...\"]\n    end\n    \n    subgraph \"Schema Generation\"\n        SCHEMA1[\"tool1.get_tool_schema()\"]\n        SCHEMA2[\"tool2.get_tool_schema()\"]\n        SCHEMAN[\"...\"]\n    end\n    \n    subgraph \"API Call\"\n        LIST[\"tools_schema_list:<br/>[schema1, schema2, ...]\"]\n        API[\"client.chat.completions.create()\"]\n        PARAM[\"tools=tools_schema_list\"]\n    end\n    \n    TOOL1 --> SCHEMA1\n    TOOL2 --> SCHEMA2\n    TOOLN --> SCHEMAN\n    \n    SCHEMA1 --> LIST\n    SCHEMA2 --> LIST\n    SCHEMAN --> LIST\n    \n    LIST --> PARAM\n    PARAM --> API\n```\n\n### LLM API Integration\n\nThe `_generate_chat_completion()` function at [src/agent/llm.py:14-24]() passes the schema list:\n\n```python\n@traceable\ndef _generate_chat_completion(messages: list[dict], tools_schema_list=None) -> ChatCompletion:\n    completion: ChatCompletion = client.chat.completions.create(\n        model=\"qwen-plus\",\n        messages=messages,\n        tools=tools_schema_list,\n        function_call=None,\n        parallel_tool_calls=True,\n    )\n    return completion\n```\n\n### API Parameters\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `tools` | `tools_schema_list` | List of available tool schemas |\n| `function_call` | `None` | Let LLM decide when to call tools |\n| `parallel_tool_calls` | `True` | Allow multiple tool calls in one response |\n\n### Tool Call Response Format\n\nWhen the LLM decides to invoke a tool, it returns `tool_calls` in the response at [src/agent/llm.py:29]():\n\n```python\nassistant_output: ChatCompletionMessage = completion.choices[0].message\n# assistant_output.tool_calls is a list of tool call objects\n```\n\nEach tool call object contains:\n- `id`: Unique identifier for tracking\n- `type`: Always `\"function\"`\n- `function.name`: Matches a tool's `tool_name()`\n- `function.arguments`: JSON string of parameters matching the schema\n\n**Sources:** [src/agent/llm.py:14-31](), [src/agent/llm.py:44-49]()\n\n---\n\n## Schema Validation and Strict Mode\n\nThe `strict: true` flag in tool schemas enforces strict validation of LLM-generated parameters.\n\n### Strict Mode Behavior\n\n| Validation | Without Strict | With Strict |\n|------------|---------------|-------------|\n| Missing required field | May use default | Error returned to LLM |\n| Extra field | Ignored | Error returned to LLM |\n| Wrong type | Coerced if possible | Error returned to LLM |\n| Invalid enum value | May accept | Error returned to LLM |\n\n### additionalProperties: false\n\nPydantic's `model_json_schema()` automatically sets `additionalProperties: false` in the generated schema, ensuring the LLM cannot add unexpected fields. This works in conjunction with `strict: true` to guarantee parameter conformance.\n\n### Validation Errors\n\nWhen the LLM provides parameters that violate the schema, the API returns an error that includes:\n- The specific validation failure\n- The field path that failed\n- The expected vs. actual value\n\nThe agent can then retry with corrected parameters or provide feedback to the user.\n\n**Sources:** [src/agent/tool/base_tool.py:68]()\n\n---\n\n## Summary\n\nThe tool schema format provides a bridge between Pydantic-based Python tool definitions and the OpenAI function calling format consumed by the LLM. Key aspects:\n\n1. **Automatic Generation**: Schemas are generated from class names, docstrings, and Pydantic field definitions\n2. **Strict Validation**: `strict: true` and `additionalProperties: false` ensure parameter conformance\n3. **Recursive Support**: Nested and self-referential models are supported via `$ref` definitions\n4. **Standard Format**: Follows OpenAI function calling conventions for compatibility\n5. **Type Safety**: Pydantic types map directly to JSON Schema types with full validation\n\nThis architecture allows tool developers to focus on Pydantic model definitions while the schema generation system handles the conversion to LLM-compatible formats automatically.\n\n**Sources:** [src/agent/tool/base_tool.py:1-76](), [src/agent/llm.py:14-24](), [src/memory/tree_todo/schemas.py:1-81]()\n\n---\n\n# Page: Troubleshooting\n\n# Troubleshooting\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [docs/crashed.design.md](docs/crashed.design.md)\n- [docs/error correction.design.md](docs/error correction.design.md)\n- [docs/log.md](docs/log.md)\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/runtime/schemas.py](src/runtime/schemas.py)\n- [src/runtime/workspace.py](src/runtime/workspace.py)\n- [src/utils/__pycache__/__init__.cpython-312.pyc](src/utils/__pycache__/__init__.cpython-312.pyc)\n- [tests/playground/crashed.py](tests/playground/crashed.py)\n- [tests/playground/subprocess_output.py](tests/playground/subprocess_output.py)\n\n</details>\n\n\n\nThis page provides a comprehensive guide to identifying, diagnosing, and resolving issues that occur when running the algo_agent system. It covers the error handling architecture, common failure patterns, and debugging strategies using the system's built-in observability features.\n\nFor specific error types and detailed solutions, see:\n- **Execution-related errors** (UnicodeDecodeError, PickleError, timeouts): [Common Execution Errors](#10.1)\n- **LLM and tool invocation issues**: [Tool Call Errors](#10.2)\n\nThis page focuses on understanding the overall error handling system and general debugging approaches that apply across different error categories.\n\n---\n\n## Error Handling Architecture\n\nThe algo_agent system implements a structured error handling mechanism centered around the `ExecutionResult` schema and `ExecutionStatus` enum. All code executions return standardized result objects that capture success states, failures, timeouts, and crashes.\n\n### Execution Status Flow\n\n```mermaid\nflowchart TD\n    Start[\"Code Execution Request\"] --> Exec[\"Execute in Subprocess/Thread\"]\n    \n    Exec --> Check{Execution Outcome}\n    \n    Check -->|\"Normal Completion\"| Success[\"ExecutionStatus.SUCCESS\"]\n    Check -->|\"Exception Caught\"| Failure[\"ExecutionStatus.FAILURE\"]\n    Check -->|\"Timeout Exceeded\"| Timeout[\"ExecutionStatus.TIMEOUT\"]\n    Check -->|\"Process Crashed\"| Crashed[\"ExecutionStatus.CRASHED\"]\n    \n    Success --> BuildResult[\"Build ExecutionResult\"]\n    Failure --> BuildResult\n    Timeout --> BuildResult\n    Crashed --> BuildResult\n    \n    BuildResult --> Validate[\"field_validate_globals\"]\n    Validate --> Filter[\"filter_and_deepcopy_globals\"]\n    Filter --> Return[\"Return to Agent\"]\n    \n    Return --> Format[\"get_return_llm\"]\n    Format --> LLM[\"Send to LLM with Context\"]\n```\n\n**Sources:** [src/runtime/schemas.py:10-47](), [src/runtime/workspace.py:38-78]()\n\nThe `ExecutionStatus` enum defines four possible outcomes, each with specific handling:\n\n| Status | Exit Code | Description | Captured Information |\n|--------|-----------|-------------|---------------------|\n| `SUCCESS` | 0 | Code executed without errors | stdout, modified globals |\n| `FAILURE` | 0 or 1 | Python exception raised | exception type, traceback, stdout |\n| `TIMEOUT` | -15 (SIGTERM) | Execution exceeded timeout limit | partial stdout, timeout duration |\n| `CRASHED` | 1, 139, or negative | Process terminated abnormally | stdout, exit code, crash signal |\n\n**Sources:** [src/runtime/schemas.py:10-47](), [tests/playground/subprocess_output.py:296-705]()\n\n### ExecutionResult Schema\n\nThe `ExecutionResult` data structure captures all execution metadata:\n\n```mermaid\nclassDiagram\n    class ExecutionResult {\n        +str arg_command\n        +int arg_timeout\n        +Dict arg_globals\n        +ExecutionStatus exit_status\n        +Optional[int] exit_code\n        +Optional[str] exception_repr\n        +Optional[str] exception_type\n        +Optional[str] exception_value\n        +Optional[str] exception_traceback\n        +str ret_stdout\n        +Optional[str] ret_tool2llm\n        +field_validate_globals()\n    }\n    \n    class ExecutionStatus {\n        <<enumeration>>\n        SUCCESS\n        FAILURE\n        TIMEOUT\n        CRASHED\n        +get_return_llm()\n    }\n    \n    class workspace {\n        +filter_and_deepcopy_globals()\n        +arg_globals_list\n        +out_globals_list\n    }\n    \n    ExecutionResult --> ExecutionStatus\n    ExecutionResult --> workspace : validates globals\n```\n\n**Sources:** [src/runtime/schemas.py:56-83](), [src/runtime/workspace.py:38-98]()\n\n---\n\n## Common Error Categories\n\nThe system encounters errors in three primary layers:\n\n### 1. Subprocess Execution Layer\n\nErrors occurring during isolated code execution in subprocess:\n\n```mermaid\ngraph TB\n    subgraph \"Subprocess Execution Flow\"\n        Worker[\"_worker_with_pipe\"] --> ExecCode[\"exec(command, _globals)\"]\n        \n        ExecCode -->|Success| BuildSuccess[\"ExecutionResult(SUCCESS)\"]\n        ExecCode -->|Exception| CatchExc[\"except Exception\"]\n        CatchExc --> BuildFailure[\"ExecutionResult(FAILURE)\"]\n        \n        BuildSuccess --> SendPipe[\"child_conn.send(RESULT)\"]\n        BuildFailure --> SendPipe\n        \n        SendPipe --> PickleAttempt{Picklable?}\n        PickleAttempt -->|Yes| ParentRecv[\"Parent Receives\"]\n        PickleAttempt -->|No| PickleError[\"PicklingError\"]\n        PickleError --> ProcessCrash[\"Process Exits with Error\"]\n        \n        ParentRecv --> ParentJoin[\"p.join(timeout)\"]\n        ProcessCrash --> ParentJoin\n        \n        ParentJoin -->|\"p.is_alive() == False\"| CheckContainer{Result Container Empty?}\n        ParentJoin -->|\"p.is_alive() == True\"| TimeoutHandle[\"Timeout Handler\"]\n        \n        CheckContainer -->|Has Result| FinalSuccess[\"Return SUCCESS/FAILURE\"]\n        CheckContainer -->|Empty| FinalCrash[\"Return CRASHED\"]\n        TimeoutHandle --> FinalTimeout[\"Return TIMEOUT\"]\n    end\n```\n\n**Sources:** [tests/playground/subprocess_output.py:18-155](), [src/runtime/subprocess_python_executor.py:18-155]()\n\n### 2. Serialization Layer\n\nThe workspace management system filters and deep-copies globals to ensure only serializable objects persist:\n\n**Common serialization failures:**\n- File handles (`TextIOWrapper`) - lines 166, 219, 272 in logs/utils.log\n- Module objects - filtered by [src/runtime/workspace.py:71-73]()\n- Lambda functions - cannot be pickled across processes\n- Thread locks - not serializable\n\n**Sources:** [src/runtime/workspace.py:38-78](), [logs/utils.log:139-287]()\n\n### 3. Encoding Layer\n\nFile I/O operations without explicit encoding specification:\n\n**Pattern:**\n```python\n# Problematic code (defaults to 'gbk' on Windows)\nwith open('data.json', 'r') as f:  \n    data = json.load(f)  # UnicodeDecodeError\n\n# Correct code\nwith open('data.json', 'r', encoding='utf-8') as f:\n    data = json.load(f)\n```\n\n**Sources:** [logs/utils.log:146-252](), [logs/utils.log:289-290]()\n\n---\n\n## Debugging Strategy\n\n### Using Log Files\n\nThe system provides four specialized log files for different debugging needs:\n\n| Log File | Purpose | Key Information |\n|----------|---------|-----------------|\n| `logs/global.log` | Function call traces | Stack paths, timing, arguments, return values |\n| `logs/utils.log` | User operations | Query inputs, tool outputs, LLM responses |\n| `logs/trace.log` | Detailed execution | Stack traces, performance metrics |\n| `logs/all.log` | Comprehensive | All system activity |\n\n**Example log entry pattern:**\n```\n[timestamp] ãè°ç¨å¼å§ã æ è·¯å¾ï¼ module.class.function | å¼å§æ¶é´ï¼ ... | ä½ç½®åæ°ï¼ (...) | å³é®å­åæ°ï¼ {}\n[timestamp] ãè°ç¨æåã æ è·¯å¾ï¼ module.class.function | èæ¶ï¼ Xms | è¿åå¼ï¼ ...\n```\n\n**Sources:** [src/utils/log_decorator.py](), [logs/global.log:1-700]()\n\n### Analyzing ExecutionResult\n\nWhen code execution fails, the `ExecutionResult.ret_tool2llm` field contains formatted error information:\n\n```mermaid\nflowchart LR\n    subgraph \"Error Information Flow\"\n        ExecResult[\"ExecutionResult\"] --> GetReturnLLM[\"ExecutionStatus.get_return_llm()\"]\n        \n        GetReturnLLM --> FormatSuccess[\"## ä»£ç æ§è¡æå\\n### ç»ç«¯è¾åº:\\nstdout\"]\n        GetReturnLLM --> FormatFailure[\"## ä»£ç æ§è¡å¤±è´¥\\n### ç»ç«¯è¾åº\\n### åå§ä»£ç \\n### æ¥éä¿¡æ¯:\\ntraceback\"]\n        GetReturnLLM --> FormatTimeout[\"## ä»£ç æ§è¡è¶æ¶\\n### ç»ç«¯è¾åº\\n### è¶åºéå¶çæ¶é´\"]\n        GetReturnLLM --> FormatCrashed[\"## ä»£ç æ§è¡å´©æº\\n### ç»ç«¯è¾åº\\n### éåºç¶æç \"]\n        \n        FormatSuccess --> AgentMemory[\"Appended to messages\"]\n        FormatFailure --> AgentMemory\n        FormatTimeout --> AgentMemory\n        FormatCrashed --> AgentMemory\n        \n        AgentMemory --> LLMContext[\"LLM sees full context\"]\n    end\n```\n\n**Sources:** [src/runtime/schemas.py:19-47](), [logs/utils.log:136-287]()\n\n### Process Exit Code Interpretation\n\nExit codes provide clues about crash causes:\n\n| Exit Code | Signal | Meaning |\n|-----------|--------|---------|\n| 0 | - | Normal termination (may still have FAILURE status if exception was caught) |\n| 1 | - | General error (uncaught exception, PicklingError) |\n| -15 | SIGTERM | Timeout termination by parent process |\n| 139 | SIGSEGV (11) | Segmentation fault (memory access violation) |\n| -11 | SIGSEGV | Segmentation fault (alternative representation) |\n\n**Exit code formula:** Negative exit code = -(signal number), e.g., -15 = SIGTERM (15)\n\n**Sources:** [src/runtime/schemas.py:50-54](), [tests/playground/subprocess_output.py:428-702]()\n\n---\n\n## Diagnostic Checklist\n\nWhen encountering an error, follow this systematic approach:\n\n### Step 1: Identify Error Category\n\n```mermaid\nflowchart TD\n    Start{Error Occurred} --> CheckLogs[\"Check logs/utils.log\"]\n    \n    CheckLogs --> Category{Error Type?}\n    \n    Category -->|\"UnicodeDecodeError\"| Encoding[\"Encoding Issue\"]\n    Category -->|\"PicklingError\\nTypeError: cannot pickle\"| Pickle[\"Serialization Issue\"]\n    Category -->|\"TimeoutError\\nexit_code=-15\"| Timeout[\"Timeout Issue\"]\n    Category -->|\"exit_code=139\\nexit_code=1\"| Crash[\"Process Crash\"]\n    Category -->|\"Tool call format\"| ToolError[\"Tool Call Issue\"]\n    \n    Encoding --> Page101[\"See section 10.1\"]\n    Pickle --> Page101\n    Timeout --> Page101\n    Crash --> Page101\n    ToolError --> Page102[\"See section 10.2\"]\n```\n\n**Sources:** [logs/utils.log:136-287](), [src/runtime/schemas.py:10-47]()\n\n### Step 2: Examine ExecutionResult\n\nFor execution errors, the `ExecutionResult` object contains:\n- `exit_status`: Which of the four states occurred\n- `exception_traceback`: Full Python traceback (FAILURE only)\n- `ret_stdout`: Captured output before error\n- `exit_code`: Process exit code\n- `arg_command`: The code that was executed\n\n**Sources:** [src/runtime/schemas.py:56-70]()\n\n### Step 3: Check Workspace State\n\nIf serialization errors occur, inspect what globals are being persisted:\n\n**Filtering logic:**\n1. Excludes `__builtins__` key\n2. Excludes module type objects\n3. Tests picklability with `pickle.dumps()`\n4. Deep copies remaining objects\n\n**Sources:** [src/runtime/workspace.py:38-78]()\n\n---\n\n## Quick Reference: Error to Solution Map\n\n| Error Pattern | Root Cause | Solution | Page Reference |\n|---------------|------------|----------|----------------|\n| `'gbk' codec can't decode` | File opened without UTF-8 encoding | Add `encoding='utf-8'` to `open()` | [#10.1](#10.1) |\n| `cannot pickle 'TextIOWrapper'` | File handle in globals | Close file in `with` block, don't expose | [#10.1](#10.1) |\n| `UnboundLocalError: cannot access local variable 'res'` | Nested exception during error handling | Subprocess crash during pickle attempt | [#10.1](#10.1) |\n| `exit_status=TIMEOUT, exit_code=-15` | Execution exceeded timeout | Increase timeout or optimize code | [#10.1](#10.1) |\n| `exit_status=CRASHED, exit_code=139` | Segmentation fault | Memory access violation, check C extensions | [#10.1](#10.1) |\n| `exit_status=CRASHED, exit_code=1` | Process terminated during error handling | Often PicklingError in result transmission | [#10.1](#10.1) |\n| `function_call is not None but tool_calls is None` | LLM response format mismatch | Model returned old function_call format | [#10.2](#10.2) |\n| Missing tool schema | Tool not registered | Check `get_tools_schema()` includes tool | [#10.2](#10.2) |\n\n**Sources:** [logs/utils.log:136-287](), [src/runtime/schemas.py:10-54]()\n\n---\n\n## Recovery Patterns\n\nThe agent implements automatic recovery for certain error types:\n\n### LLM Self-Correction Loop\n\n```mermaid\nsequenceDiagram\n    participant Agent\n    participant LLM\n    participant Executor\n    participant Memory\n    \n    Agent->>LLM: Generate code\n    LLM->>Executor: execute_python_code\n    Executor->>Executor: Run in subprocess\n    \n    alt Execution Failed\n        Executor->>Memory: Store ExecutionResult(FAILURE)\n        Memory->>LLM: Send error context with traceback\n        LLM->>LLM: Analyze error, adjust code\n        LLM->>Executor: execute_python_code (retry)\n    else Execution Timeout\n        Executor->>Memory: Store ExecutionResult(TIMEOUT)\n        Memory->>LLM: Send timeout info\n        LLM->>LLM: Optimize or increase timeout\n        LLM->>Executor: execute_python_code (retry)\n    else Execution Crashed\n        Executor->>Memory: Store ExecutionResult(CRASHED)\n        Memory->>LLM: Send crash info with stdout\n        LLM->>LLM: Diagnose crash cause\n        Note over LLM: May need human intervention\n    end\n```\n\n**Sources:** [src/agent/deep_research.py](), [src/runtime/schemas.py:19-47]()\n\n---\n\n## System Error Message Format\n\nThe system generates structured error messages that the LLM can interpret:\n\n**SUCCESS format:**\n```\n## ä»£ç æ§è¡æåï¼è¾åºç»æå®æ´ï¼ä»»å¡å®æ\n### ç»ç«¯è¾åºï¼\n{stdout}\n```\n\n**FAILURE format:**\n```\n## ä»£ç æ§è¡å¤±è´¥ï¼ä»£ç æåºå¼å¸¸ï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\n{stdout}\n### åå§ä»£ç ï¼\n{numbered_code}\n### æ¥éä¿¡æ¯ï¼\n{exception_traceback}\n```\n\n**TIMEOUT format:**\n```\n## ä»£ç æ§è¡è¶æ¶ï¼å¼ºå¶éåºæ§è¡ï¼è°æ´è¶æ¶æ¶é´åéè¯\n### ç»ç«¯è¾åºï¼\n{stdout}\n### è¶åºéå¶çæ¶é´ï¼{timeout} ç§\n```\n\n**CRASHED format:**\n```\n## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\n{stdout}\n### éåºç¶æç ï¼{exit_code}\n```\n\n**Sources:** [src/runtime/schemas.py:23-47]()\n\n---\n\n## Next Steps\n\nFor detailed solutions to specific error types:\n- **UnicodeDecodeError, PickleError, timeout issues**: See [Common Execution Errors](#10.1)\n- **Tool call format errors, LLM integration problems**: See [Tool Call Errors](#10.2)\n\nFor understanding the logging system architecture: See [Observability and Logging](#6)\n\nFor details on execution strategies and isolation: See [Execution Runtime](#5)\n\n---\n\n# Page: Common Execution Errors\n\n# Common Execution Errors\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/runtime/cwd.py](src/runtime/cwd.py)\n- [src/runtime/subprocess_python_executor.py](src/runtime/subprocess_python_executor.py)\n\n</details>\n\n\n\n## Purpose and Scope\n\nThis document catalogs the most frequently encountered errors during Python code execution in the algo_agent system, their root causes, and detailed resolution strategies. It focuses on errors that occur within the execution runtime layer when the system attempts to run user-provided or LLM-generated Python code snippets.\n\nFor errors related to tool calling and LLM integration (such as malformed tool schemas or function call format issues), see [Tool Call Errors](#10.2). For general system architecture and execution flow, see [Execution Runtime](#5).\n\n---\n\n## Overview of Execution Error Types\n\nThe execution system can produce several distinct error categories, each represented by a specific `ExecutionStatus`:\n\n| Status | Description | Recovery Difficulty |\n|--------|-------------|---------------------|\n| `SUCCESS` | Code executed without exceptions | N/A |\n| `FAILURE` | Python exception caught during execution | Easy - retry with fixes |\n| `TIMEOUT` | Execution exceeded time limit | Medium - optimize code |\n| `CRASHED` | Process terminated abnormally (SegFault, unhandled exceptions in result construction) | Hard - system-level issue |\n\nSources: [src/runtime/schemas.py:20-30](), [logs/utils.log:136-180]()\n\n---\n\n## Error Flow Architecture\n\n```mermaid\ngraph TB\n    Code[\"User/LLM Code Snippet\"]\n    Subprocess[\"Subprocess Worker<br/>_worker_with_pipe()\"]\n    Exec[\"exec(command, globals, locals)\"]\n    Result[\"ExecutionResult Construction\"]\n    Pipe[\"Pipe Communication<br/>child_conn.send()\"]\n    Parent[\"Parent Process<br/>run_structured_in_subprocess()\"]\n    \n    Code --> Subprocess\n    Subprocess --> Exec\n    \n    Exec -->|Success| Result\n    Exec -->|Python Exception| ResultFail[\"ExecutionResult<br/>status=FAILURE\"]\n    \n    Result -->|deepcopy globals| Serialize[\"filter_and_deepcopy_globals()\"]\n    ResultFail --> Serialize\n    \n    Serialize -->|Success| Pipe\n    Serialize -->|PickleError| Crash[\"Process Crash<br/>UnboundLocalError\"]\n    \n    Pipe -->|Success| Parent\n    Crash -->|EOFError| ParentCrashed[\"Parent detects CRASHED\"]\n    \n    Parent -->|p.join(timeout)| TimeoutCheck{Timeout?}\n    TimeoutCheck -->|Yes| Terminate[\"p.terminate()<br/>status=TIMEOUT\"]\n    TimeoutCheck -->|No| CheckResult{Result received?}\n    CheckResult -->|Yes| ReturnResult[\"Return ExecutionResult\"]\n    CheckResult -->|No| ReturnCrashed[\"Return CRASHED result\"]\n    \n    style Crash fill:#fee\n    style ParentCrashed fill:#fee\n    style Terminate fill:#ffe\n    style Serialize fill:#eff\n```\n\n**Diagram: Execution Error Flow and Status Determination**\n\nThis diagram shows how different error conditions lead to different ExecutionStatus values. Critical failure points include serialization (PickleError) and timeout detection.\n\nSources: [src/runtime/subprocess_python_executor.py:19-163](), [src/runtime/schemas.py:15-40]()\n\n---\n\n## 1. UnicodeDecodeError: Encoding Mismatch\n\n### Error Signature\n\n```\nUnicodeDecodeError: 'gbk' codec can't decode byte 0x94 in position 1265: illegal multibyte sequence\n```\n\n### Root Cause\n\n**Platform-specific default encoding mismatch**: \n- JSON files are typically saved with UTF-8 encoding\n- Windows systems default to locale-specific encodings (e.g., `gbk` in Chinese locales, `cp1252` in Western locales)\n- Python's `open()` without explicit encoding uses the system default\n\n**Code pattern causing the error**:\n```python\n# â WRONG: Uses system default encoding (gbk on Windows)\nwith open('schema.json', 'r') as file:\n    schema = json.load(file)\n```\n\nSources: [logs/utils.log:143-146](), [logs/utils.log:196-199](), [logs/utils.log:249-252]()\n\n### Resolution Strategy\n\n**Always specify UTF-8 encoding explicitly**:\n\n```python\n# â CORRECT: Explicit UTF-8 encoding\nwith open('schema.json', 'r', encoding='utf-8') as file:\n    schema = json.load(file)\n```\n\n### Detection in Logs\n\nLook for these patterns in [logs/utils.log]():\n- `UnicodeDecodeError: 'gbk' codec can't decode byte`\n- `illegal multibyte sequence`\n- Accompanied by file I/O operations (lines 143-146, 196-199, 249-252)\n\n### System Response\n\nWhen UnicodeDecodeError occurs:\n1. Python raises the exception during `file.read()` or `json.load()`\n2. Subprocess catches it in the `try-except` block at [src/runtime/subprocess_python_executor.py:58-69]()\n3. System attempts to construct `ExecutionResult` with `status=FAILURE`\n4. **Secondary failure**: If the exception leaves unpicklable objects in `_globals`, see next section\n\nSources: [logs/utils.log:139-180](), [src/runtime/subprocess_python_executor.py:58-69]()\n\n---\n\n## 2. PickleError: Unpicklable Objects in Globals\n\n### Error Signature\n\n```\nTypeError: cannot pickle 'TextIOWrapper' instances\n```\n\n### Root Cause Cascade\n\n```mermaid\ngraph LR\n    FileOpen[\"open('file.json', 'r')\"]\n    GlobalVar[\"file handle stored<br/>in _globals namespace\"]\n    Exception[\"UnicodeDecodeError<br/>during json.load()\"]\n    ResultConstruct[\"ExecutionResult(...,<br/>arg_globals=_globals)\"]\n    Validate[\"Pydantic validator<br/>field_validate_globals()\"]\n    Filter[\"filter_and_deepcopy_globals()\"]\n    Deepcopy[\"copy.deepcopy(value)\"]\n    PickleErr[\"TypeError: cannot pickle<br/>'TextIOWrapper'\"]\n    Crash[\"UnboundLocalError:<br/>res not defined\"]\n    \n    FileOpen --> GlobalVar\n    GlobalVar --> Exception\n    Exception --> ResultConstruct\n    ResultConstruct --> Validate\n    Validate --> Filter\n    Filter --> Deepcopy\n    Deepcopy --> PickleErr\n    PickleErr --> Crash\n    \n    style PickleErr fill:#fee\n    style Crash fill:#fcc\n```\n\n**Diagram: PickleError Cascade Leading to Process Crash**\n\nSources: [logs/utils.log:148-178](), [src/runtime/workspace.py:55](), [src/runtime/schemas.py:83]()\n\n### Detailed Explanation\n\n**The chain of events**:\n\n1. **File handle creation**: `open('file.json', 'r')` creates a `TextIOWrapper` object\n2. **Implicit global storage**: The `file` variable enters the `_globals` namespace due to `exec()` semantics\n3. **Primary exception**: `json.load(file)` raises `UnicodeDecodeError`\n4. **Secondary failure attempt**: Code at [src/runtime/subprocess_python_executor.py:58-69]() tries to construct `ExecutionResult` with `arg_globals=_globals`\n5. **Pydantic validation**: [src/runtime/schemas.py:83]() invokes `filter_and_deepcopy_globals()`\n6. **Serialization failure**: [src/runtime/workspace.py:55]() attempts `copy.deepcopy(value)` on the file handle\n7. **Pickle error**: `TextIOWrapper` is not serializable\n8. **Unhandled exception**: The except block at [src/runtime/subprocess_python_executor.py:66]() tries to send undefined `res`\n9. **Process crash**: `UnboundLocalError: cannot access local variable 'res'`\n\n### Objects That Cannot Be Pickled\n\n| Object Type | Example | Common Source |\n|-------------|---------|---------------|\n| File handles | `open('file')` | File I/O without context manager |\n| Thread locks | `threading.Lock()` | Concurrent programming |\n| Database connections | `sqlite3.connect()` | Database operations |\n| Socket objects | `socket.socket()` | Network programming |\n| Lambda functions with closures | `lambda x: nonlocal_var + x` | Functional programming |\n| Module-level imports | `import numpy as np` | Import statements (these are filtered) |\n\nSources: [src/runtime/workspace.py:40-68]()\n\n### Resolution Strategies\n\n#### Strategy 1: Use Context Managers (Recommended)\n\n```python\n# â CORRECT: File handle automatically closed, not stored in globals\nwith open('schema.json', 'r', encoding='utf-8') as f:\n    schema = json.load(f)\n# f is out of scope here, not in _globals\n```\n\n#### Strategy 2: Explicit Cleanup\n\n```python\n# â CORRECT: Explicit cleanup before execution ends\nfile = open('schema.json', 'r', encoding='utf-8')\nschema = json.load(file)\nfile.close()\ndel file  # Remove from namespace\n```\n\n#### Strategy 3: Local Scope Isolation\n\n```python\n# â CORRECT: Use a function to isolate file handle\ndef load_schema():\n    with open('schema.json', 'r', encoding='utf-8') as f:\n        return json.load(f)\n\nschema = load_schema()\n# File handle never enters global namespace\n```\n\n### System Filtering Mechanisms\n\nThe system attempts to filter out unpicklable objects at [src/runtime/workspace.py:40-68]():\n\n```python\n# Simplified filtering logic\nfiltered_dict = {}\nfor key, value in _globals.items():\n    if key.startswith('_'):  # Skip private variables\n        continue\n    if isinstance(value, type(sys)):  # Skip modules\n        continue\n    try:\n        filtered_dict[key] = copy.deepcopy(value)  # Test serializability\n    except Exception:\n        # Skip unpicklable objects\n        continue\n```\n\n**However**, this filtering runs **during ExecutionResult construction**, which is **after** the primary exception. If the primary exception handler itself fails due to unpicklable objects, the process crashes.\n\nSources: [src/runtime/workspace.py:40-68](), [src/runtime/schemas.py:80-85]()\n\n---\n\n## 3. Process Crashes and UnboundLocalError\n\n### Error Signature\n\n```\nUnboundLocalError: cannot access local variable 'res' where it is not associated with a value\n```\n\n### Root Cause\n\nThis error occurs in [src/runtime/subprocess_python_executor.py:66]() when:\n1. The code in the `except Exception as e:` block (lines 58-69) raises a **secondary exception**\n2. The `finally` block (line 72) executes and tries to send `res` via pipe\n3. Variable `res` was never successfully assigned due to the secondary exception\n\n### Code Analysis\n\n```python\n# src/runtime/subprocess_python_executor.py:49-73\ntry:\n    exec(command, _globals, _locals)\n    res = ExecutionResult(...)  # Line 52: Success path\nexcept Exception as e:\n    res = ExecutionResult(...)  # Line 60: Failure path - may raise!\nfinally:\n    child_conn.send((_PipeType.RESULT, res))  # Line 72: res may be undefined\n    child_conn.close()\n```\n\n**The problem**: If line 60's `ExecutionResult(...)` constructor raises an exception (e.g., during Pydantic validation or `filter_and_deepcopy_globals`), `res` is never assigned, causing the `finally` block to fail.\n\nSources: [src/runtime/subprocess_python_executor.py:49-73](), [logs/utils.log:170-178]()\n\n### Parent Process Detection\n\nWhen the subprocess crashes:\n1. Pipe reader receives `EOFError` at [src/runtime/subprocess_python_executor.py:108]()\n2. `subprocess_result_container` remains empty\n3. Parent process detects missing result at [src/runtime/subprocess_python_executor.py:149-159]()\n4. Returns `ExecutionResult` with `status=CRASHED`\n\n```python\n# src/runtime/subprocess_python_executor.py:149-159\nif subprocess_result_container:\n    final_res: ExecutionResult = subprocess_result_container[0]\nelse:\n    # No result received - process crashed\n    final_res = ExecutionResult(\n        arg_command=command,\n        arg_timeout=timeout,\n        arg_globals=_globals or {},\n        exit_status=ExecutionStatus.CRASHED,\n    )\n```\n\nSources: [src/runtime/subprocess_python_executor.py:149-159](), [logs/utils.log:133]()\n\n### Error Message to User\n\nWhen `CRASHED` status is returned, the tool output shows:\n\n```\n## ä»£ç æ§è¡å´©æºï¼è¿ç¨å¼å¸¸éåºï¼æ ¹æ®æ¥éä¿¡æ¯è¿è¡è°è¯\n### ç»ç«¯è¾åºï¼\nProcess Process-1:\nTraceback (most recent call last):\n  ...\nUnboundLocalError: cannot access local variable 'res' where it is not associated with a value\n### éåºç¶æç ï¼1\n```\n\nSources: [logs/utils.log:136-180]()\n\n---\n\n## 4. Timeout Errors\n\n### Error Signature\n\n```\nExecutionStatus.TIMEOUT\n```\n\n### Root Cause\n\nCode execution exceeds the specified timeout duration (default 30 seconds). Common causes:\n\n1. **Infinite loops**:\n```python\nwhile True:  # Never terminates\n    pass\n```\n\n2. **Long-running computations**:\n```python\n# Processing millions of records\nfor i in range(10_000_000):\n    complex_calculation(i)\n```\n\n3. **Blocking I/O operations**:\n```python\nimport requests\nresponse = requests.get('http://slow-server.com')  # Hangs indefinitely\n```\n\n### Detection Mechanism\n\n```python\n# src/runtime/subprocess_python_executor.py:129-144\np.join(timeout)  # Wait for subprocess with timeout\nif p.is_alive():\n    # Timeout occurred\n    p.terminate()\n    p.join()\n    parent_conn.close()\n    t.join()\n    final_res = ExecutionResult(\n        arg_command=command,\n        arg_timeout=timeout,\n        arg_globals=_globals or {},\n        exit_status=ExecutionStatus.TIMEOUT,\n    )\n```\n\nThe parent process uses `multiprocessing.Process.join(timeout)` to wait. If the process is still alive after timeout seconds, it forcibly terminates the subprocess using `p.terminate()`.\n\nSources: [src/runtime/subprocess_python_executor.py:129-144]()\n\n### Resolution Strategies\n\n#### Strategy 1: Increase Timeout\n\n```python\n# Pass larger timeout value to the tool\n{\n    \"tool_call_purpose\": \"Long-running analysis\",\n    \"python_code_snippet\": \"...\",\n    \"timeout\": 300  # 5 minutes instead of default 30 seconds\n}\n```\n\n#### Strategy 2: Optimize Code\n\n- Add early termination conditions to loops\n- Use generators and lazy evaluation\n- Process data in chunks\n- Add progress indicators to verify execution is proceeding\n\n#### Strategy 3: Asynchronous Execution\n\nFor truly long-running tasks, consider:\n- Breaking into smaller sub-tasks\n- Implementing checkpoints to resume interrupted work\n- Moving computation outside the agent loop\n\nSources: [src/tool/python_tool.py]() (tool parameter definition)\n\n---\n\n## 5. Debugging Strategies\n\n### Log File Analysis\n\n```mermaid\ngraph TB\n    Error[\"Execution Error Occurs\"]\n    \n    CheckLog{\"Which log file?\"}\n    \n    Error --> CheckLog\n    \n    CheckLog -->|Global| GlobalLog[\"logs/global.log<br/>- Function call traces<br/>- Execution timing<br/>- System-level events\"]\n    \n    CheckLog -->|Utils| UtilsLog[\"logs/utils.log<br/>- User queries<br/>- Tool outputs<br/>- Error messages<br/>- LLM responses\"]\n    \n    CheckLog -->|Trace| TraceLog[\"logs/trace.log<br/>- Detailed stack traces<br/>- Parameter values<br/>- Exception details\"]\n    \n    GlobalLog --> Pattern1[\"Search for:<br/>'å­è¿ç¨å¼å¸¸ç»æ'<br/>'å­è¿ç¨å´©æºéåº'<br/>'Pipe reader error'\"]\n    \n    UtilsLog --> Pattern2[\"Search for:<br/>'ä»£ç æ§è¡å´©æº'<br/>'UnicodeDecodeError'<br/>'TypeError: cannot pickle'\"]\n    \n    TraceLog --> Pattern3[\"Search for:<br/>Full tracebacks<br/>Variable states<br/>Call chains\"]\n    \n    Pattern1 --> Action1[\"Identify crash location<br/>Check exitcode\"]\n    Pattern2 --> Action2[\"Extract full error message<br/>Identify error type\"]\n    Pattern3 --> Action3[\"Reconstruct execution state<br/>Trace root cause\"]\n```\n\n**Diagram: Log File Navigation for Error Debugging**\n\nSources: [logs/global.log:1-620](), [logs/utils.log:1-296]()\n\n### Error Pattern Recognition\n\n| Log Pattern | Error Type | Next Step |\n|-------------|------------|-----------|\n| `'gbk' codec can't decode` | UnicodeDecodeError | Add `encoding='utf-8'` |\n| `cannot pickle 'TextIOWrapper'` | PickleError | Use context managers |\n| `UnboundLocalError: cannot access local variable 'res'` | Process crash | Fix unpicklable objects |\n| `Pipe reader error: type=EOFError` | Subprocess crash | Check subprocess_result_container |\n| `å­è¿ç¨è¶æ¶éåºï¼è¶æ¶ X ç§` | Timeout | Increase timeout or optimize code |\n\n### Subprocess Working Directory\n\nThe system changes the working directory at [src/runtime/cwd.py:9-28]() before executing code. Check log messages:\n\n```\nå­è¿ç¨ PID: 11336 è¦å°å·¥ä½ç®å½æ´æ¹ä¸º: D:\\zyt\\git_ln\\algo_agent\\wsm\\1\\g4-1\nå­è¿ç¨ PID: 11336 å·²å°å·¥ä½ç®å½æ´æ¹ä¸º: D:\\zyt\\git_ln\\algo_agent\\wsm\\1\\g4-1\n```\n\nFile paths in errors are relative to this working directory, not the project root.\n\nSources: [src/runtime/cwd.py:9-28](), [logs/utils.log:130-131]()\n\n### Step-by-Step Debugging Process\n\n1. **Identify the error type** from exit status:\n   - Check `exit_status` field in `ExecutionResult`\n   - Look for corresponding log patterns\n\n2. **Locate the error in logs**:\n   - Search for the timestamp or subprocess PID\n   - Find the full traceback in `exception_traceback` field\n\n3. **Analyze the code snippet**:\n   - Review `arg_command` field\n   - Identify problematic constructs (file I/O, imports, etc.)\n\n4. **Check global state**:\n   - Examine `arg_globals` for unpicklable objects\n   - Verify `out_globals` was successfully serialized\n\n5. **Apply the fix**:\n   - Follow resolution strategies for specific error type\n   - Re-run with corrected code\n\nSources: [src/runtime/schemas.py:40-100]()\n\n---\n\n## 6. Prevention Best Practices\n\n### Code Generation Guidelines for LLMs\n\nWhen generating code snippets, the LLM should follow these patterns:\n\n```python\n# â SAFE PATTERN: Context managers + explicit encoding\ndef safe_file_operation():\n    with open('data.json', 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    return data\n\n# â SAFE PATTERN: Minimal global scope pollution\nresult = safe_file_operation()\nprint(result)\n\n# â UNSAFE PATTERN: File handle in global scope\nf = open('data.json', 'r')  # No encoding specified\ndata = json.load(f)  # May crash on non-ASCII\n# f remains in _globals, cannot be pickled\n```\n\n### Error Recovery in Agent Loop\n\nThe agent system should implement retry logic at [src/agent/deep_research.py]():\n\n1. **Detect error type** from `ExecutionResult.exit_status`\n2. **Analyze error message** from `exception_traceback`\n3. **Generate corrected code** based on error type\n4. **Retry execution** with fix applied\n5. **Update task tree** to record retry attempts\n\nThis prevents the agent from repeatedly making the same mistake.\n\nSources: [src/agent/deep_research.py]() (agent orchestration)\n\n### System-Level Protections\n\nThe execution system provides multiple safety layers:\n\n| Layer | Location | Protection |\n|-------|----------|------------|\n| Process isolation | [subprocess_python_executor.py]() | SegFault doesn't crash agent |\n| Timeout enforcement | [subprocess_python_executor.py:129]() | Infinite loops auto-terminate |\n| Global filtering | [workspace.py:40-68]() | Removes unpicklable objects |\n| Exception capture | [subprocess_python_executor.py:58]() | All exceptions logged |\n\nSources: [src/runtime/subprocess_python_executor.py:1-205](), [src/runtime/workspace.py:40-68]()\n\n---\n\n## Summary Table: Quick Error Reference\n\n| Error | Log Indicator | Primary Cause | Quick Fix | See Section |\n|-------|---------------|---------------|-----------|-------------|\n| UnicodeDecodeError | `'gbk' codec can't decode` | Missing UTF-8 encoding | Add `encoding='utf-8'` to `open()` | Â§1 |\n| PickleError | `cannot pickle 'TextIOWrapper'` | File handle in globals | Use `with` statements | Â§2 |\n| Process Crash | `UnboundLocalError` + `EOFError` | Secondary exception during error handling | Fix picklability issues | Â§3 |\n| Timeout | `å­è¿ç¨è¶æ¶éåº` | Code runs too long | Increase timeout or optimize | Â§4 |\n| CRASHED status | `subprocess_result_container` empty | Various system-level failures | Check subprocess logs | Â§3 |\n\nSources: All sections above\n\n---\n\n# Page: Tool Call Errors\n\n# Tool Call Errors\n\n<details>\n<summary>Relevant source files</summary>\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/functino_call_err.design.md](docs/functino_call_err.design.md)\n- [logs/global.log](logs/global.log)\n- [logs/utils.log](logs/utils.log)\n- [src/agent/action.py](src/agent/action.py)\n- [src/agent/llm.py](src/agent/llm.py)\n- [src/agent/memory.py](src/agent/memory.py)\n\n</details>\n\n\n\nThis document covers errors that occur during LLM tool calling and function invocation, including format mismatches, message structure issues, and argument parsing failures. These errors typically manifest when the LLM attempts to invoke tools but the request or response format is invalid.\n\nFor errors occurring during code execution within tools (like timeout or pickle errors), see [Common Execution Errors](#10.1). For general tool system architecture, see [Tool System](#4).\n\n---\n\n## Overview of Tool Call Error Types\n\nTool call errors occur at the interface between the LLM and the tool execution system. The system uses OpenAI's function calling protocol, which requires strict adherence to message formats and structure.\n\n**Common Error Categories:**\n\n| Error Type | Trigger Point | Impact |\n|------------|---------------|--------|\n| Format Mismatch (`function_call` vs `tool_calls`) | LLM response parsing | Agent loop terminates |\n| Message Structure Validation | LLM API request | BadRequestError from API |\n| Argument Parsing | Tool dispatch | Tool execution skipped |\n| Tool Execution Failure | Inside tool `run()` method | Graceful error message returned |\n\nSources: [src/agent/llm.py:1-51](), [src/agent/action.py:1-49](), [logs/utils.log:1-296]()\n\n---\n\n## Function Call Format Mismatch\n\n### Problem Description\n\nThe most common tool call error occurs when the LLM returns a deprecated `function_call` field instead of the modern `tool_calls` array. This happens despite explicitly setting `function_call=None` in the API request.\n\n```mermaid\nsequenceDiagram\n    participant Agent as \"deep_research.user_query\"\n    participant LLM as \"llm.generate_assistant_output_append\"\n    participant API as \"DashScope API\"\n    participant Checker as \"has_function_call()\"\n    \n    Agent->>LLM: \"messages + tools_schema_list\"\n    Note over LLM: \"function_call=None<br/>parallel_tool_calls=True\"\n    LLM->>API: \"POST /chat/completions\"\n    API-->>LLM: \"ChatCompletionMessage with function_call\"\n    Note over LLM: \"Deprecated format returned!\"\n    LLM-->>Agent: \"assistant_output\"\n    Agent->>Checker: \"has_function_call(assistant_output)\"\n    Checker-->>Agent: \"True\"\n    Note over Agent: \"Error: function_call not supported\"\n    Agent->>Agent: \"Append error message to messages\"\n```\n\n**Example Error from Logs:**\n\n```\nChatCompletionMessage(\n    content='', \n    role='assistant',\n    function_call=FunctionCall(\n        arguments='{\"file_path\": \"schema.json\"}', \n        name='file_read'\n    ), \n    tool_calls=None\n)\n```\n\n### Detection Logic\n\nThe system detects this error using helper functions in [src/agent/llm.py:44-49]():\n\n```python\ndef has_tool_call(assistant_output: ChatCompletionMessage) -> bool:\n    return assistant_output.tool_calls is not None\n\ndef has_function_call(assistant_output: ChatCompletionMessage) -> bool:\n    return assistant_output.function_call is not None\n```\n\nWhen `has_function_call()` returns `True`, the system rejects the response because no handler is registered for `function_call`.\n\n### Recovery Strategy\n\nThe system appends an error message to the conversation and retries:\n\n```\næ²¡æå®ä¹function_callå·¥å·è°ç¨ï¼æ æ³æ§è¡function_callï¼è¯·ä½¿ç¨tool_callsè°ç¨å·¥å·ã\n```\n\nThis feedback message is added to the message history, prompting the LLM to retry with the correct `tool_calls` format.\n\nSources: [src/agent/llm.py:44-49](), [logs/utils.log:62-64](), [docs/functino_call_err.design.md:1-3]()\n\n---\n\n## Message Structure Validation Errors\n\n### The tool_call_id Response Requirement\n\nWhen the LLM generates a message with `tool_calls`, the OpenAI API requires that each `tool_call_id` in the assistant message must have a corresponding tool response message. Failure to provide responses causes a BadRequestError.\n\n**Error Structure:**\n\n```mermaid\ngraph TB\n    AssistantMsg[\"Assistant Message<br/>(role: assistant)\"]\n    ToolCall1[\"tool_calls[0]<br/>id: call_abc123\"]\n    ToolCall2[\"tool_calls[1]<br/>id: call_def456\"]\n    ToolCall3[\"tool_calls[2]<br/>id: call_ghi789\"]\n    \n    ToolResp1[\"Tool Message<br/>(role: tool)<br/>tool_call_id: call_abc123\"]\n    ToolResp2[\"Tool Message<br/>(role: tool)<br/>tool_call_id: call_def456\"]\n    Missing[\"â Missing Response<br/>for call_ghi789\"]\n    \n    AssistantMsg --> ToolCall1\n    AssistantMsg --> ToolCall2\n    AssistantMsg --> ToolCall3\n    \n    ToolCall1 --> ToolResp1\n    ToolCall2 --> ToolResp2\n    ToolCall3 --> Missing\n    \n    style Missing fill:#fee,stroke:#f00\n```\n\n**Actual Error from Logs:**\n\n```\nBadRequestError: Error code: 400 - {\n  'error': {\n    'message': '<400> InternalError.Algo.InvalidParameter: \n                An assistant message with \"tool_calls\" must be followed \n                by tool messages responding to each \"tool_call_id\". \n                The following tool_call_ids did not have response messages: \n                message[7].role',\n    'type': 'invalid_request_error',\n    'code': 'invalid_parameter_error'\n  }\n}\n```\n\n### Message Array Structure\n\nThe conversation messages array must maintain this strict ordering:\n\n```mermaid\ngraph LR\n    System[\"message[0]<br/>role: system\"]\n    User1[\"message[1]<br/>role: user\"]\n    Asst1[\"message[2]<br/>role: assistant<br/>tool_calls: [...]\"]\n    Tool1[\"message[3]<br/>role: tool<br/>tool_call_id: ...\"]\n    Tool2[\"message[4]<br/>role: tool<br/>tool_call_id: ...\"]\n    Asst2[\"message[5]<br/>role: assistant\"]\n    \n    System --> User1\n    User1 --> Asst1\n    Asst1 --> Tool1\n    Asst1 --> Tool2\n    Tool1 --> Asst2\n    Tool2 --> Asst2\n    \n    style Asst1 fill:#fef,stroke:#333\n    style Tool1 fill:#efe,stroke:#333\n    style Tool2 fill:#efe,stroke:#333\n```\n\n### Root Causes\n\nThis error occurs when:\n\n1. **Missing tool response messages**: Some `tool_call_id` values are not followed by corresponding tool messages\n2. **Incorrect message role**: Using `role: \"user\"` instead of `role: \"tool\"` for tool responses\n3. **Malformed tool_call_id**: The `tool_call_id` in the tool message doesn't match any `tool_calls[i].id`\n\nSources: [docs/functino_call_err.design.md:1-51](), [logs/global.log:1-10000]()\n\n---\n\n## Parallel Tool Calls and Response Mapping\n\n### Configuration\n\nThe system enables parallel tool calls in [src/agent/llm.py:22]():\n\n```python\nparallel_tool_calls=True\n```\n\nThis allows the LLM to request multiple tool executions in a single response.\n\n### Tool Call Processing Flow\n\n```mermaid\ngraph TB\n    LLMResponse[\"LLM Response<br/>assistant_output.tool_calls\"]\n    \n    subgraph \"Tool Call Array\"\n        TC1[\"tool_calls[0]<br/>id: call_3085f1f75d534390a7c2b7<br/>name: execute_python_code<br/>arguments: {...}\"]\n        TC2[\"tool_calls[1]<br/>id: call_ab025bb2bb3e430aaefd92<br/>name: execute_python_code<br/>arguments: {...}\"]\n        TC3[\"tool_calls[2]<br/>id: call_92fd80e050c34c78a18ea9<br/>name: execute_python_code<br/>arguments: {...}\"]\n    end\n    \n    subgraph \"Tool Execution\"\n        Exec1[\"call_tools_safely(tool_info)<br/>Parse JSON arguments<br/>Dispatch to tool\"]\n        Exec2[\"call_tools_safely(tool_info)<br/>Parse JSON arguments<br/>Dispatch to tool\"]\n        Exec3[\"call_tools_safely(tool_info)<br/>Parse JSON arguments<br/>Dispatch to tool\"]\n    end\n    \n    subgraph \"Tool Response Messages\"\n        TM1[\"role: tool<br/>tool_call_id: call_3085...<br/>content: execution result\"]\n        TM2[\"role: tool<br/>tool_call_id: call_ab02...<br/>content: execution result\"]\n        TM3[\"role: tool<br/>tool_call_id: call_92fd...<br/>content: execution result\"]\n    end\n    \n    LLMResponse --> TC1\n    LLMResponse --> TC2\n    LLMResponse --> TC3\n    \n    TC1 --> Exec1\n    TC2 --> Exec2\n    TC3 --> Exec3\n    \n    Exec1 --> TM1\n    Exec2 --> TM2\n    Exec3 --> TM3\n```\n\n**Example from Logs:**\n\nThe system correctly processes three parallel tool calls:\n\n```python\ntool_calls=[\n    ChatCompletionMessageFunctionToolCall(\n        id='call_3085f1f75d534390a7c2b7',\n        function=Function(\n            arguments='{\"tool_call_purpose\": \"...\", \"python_code_snippet\": \"...\"}',\n            name='execute_python_code'\n        ),\n        type='function',\n        index=0\n    ),\n    ChatCompletionMessageFunctionToolCall(id='call_ab025bb2bb3e430aaefd92', ...),\n    ChatCompletionMessageFunctionToolCall(id='call_92fd80e050c34c78a18ea9', ...)\n]\n```\n\nSources: [logs/utils.log:128-129](), [src/agent/llm.py:14-24]()\n\n---\n\n## Argument Parsing Errors\n\n### JSON Deserialization\n\nTool arguments are passed as JSON strings that must be parsed in [src/agent/action.py:13]():\n\n```python\narguments = json.loads(tool_info[\"tool_call_arguments\"])\n```\n\n### Common Parsing Failures\n\n| Error Type | Cause | Example |\n|------------|-------|---------|\n| `JSONDecodeError` | Malformed JSON syntax | Missing quotes, trailing commas |\n| `KeyError` | Missing required field | Tool schema requires `python_code_snippet` but not provided |\n| `TypeError` | Wrong type for parameter | String provided for integer `timeout` parameter |\n| `ValidationError` | Pydantic validation failure | Invalid enum value for `TaskStatus` |\n\n### Error Handling\n\nThe `call_tools_safely` wrapper catches parsing errors in [src/agent/action.py:40-47]():\n\n```python\ntry:\n    return call_tools(tool_info)\nexcept Exception as e:\n    error_msg = traceback.format_exc()\n    global_logger.error(f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\", exc_info=True)\n    tool_info[\"content\"] = f\"å·¥å·å½æ°è°ç¨å¤±è´¥{tool_info['content']}, éè¯¯ä¿¡æ¯: {error_msg}\"\n    return tool_info\n```\n\nWhen parsing fails, the error message is:\n1. Logged to `logs/global.log` with full traceback\n2. Returned as the tool's \"content\" field\n3. Appended to the message history for LLM to see\n\nThis allows the LLM to adjust its approach based on the specific parsing error.\n\nSources: [src/agent/action.py:10-47]()\n\n---\n\n## Tool Dispatch Errors\n\n### Tool Name Resolution\n\nThe system maps tool names to implementations using a series of if-elif checks in [src/agent/action.py:14-20]():\n\n```mermaid\ngraph TD\n    Start[\"tool_info['tool_call_name']\"]\n    \n    Check1{\"name ==<br/>'execute_python_code'?\"}\n    Check2{\"name ==<br/>'recursive_plan_tree_todo'?\"}\n    Fallback[\"No handler found<br/>(implicit else)\"]\n    \n    Tool1[\"ExecutePythonCodeTool(**arguments)<br/>tool.run()\"]\n    Tool2[\"RecursivePlanTreeTodoTool(**arguments)<br/>tool.run()\"]\n    \n    Start --> Check1\n    Check1 -->|Yes| Tool1\n    Check1 -->|No| Check2\n    Check2 -->|Yes| Tool2\n    Check2 -->|No| Fallback\n    \n    style Fallback fill:#fee,stroke:#f00\n```\n\n### Unrecognized Tool Names\n\nIf the LLM requests a tool name that doesn't match any registered tool:\n- The dispatch logic falls through all checks\n- No tool is executed\n- The function returns without setting `tool_info[\"content\"]`\n- This leaves the content as the original (likely empty or error state)\n\n**Prevention:** The tool schema provided to the LLM should only include available tools. The schema is generated by [tool/schema.py:get_tools_schema]() and includes only:\n- `ExecutePythonCodeTool`\n- `RecursivePlanTreeTodoTool`\n\nSources: [src/agent/action.py:11-21]()\n\n---\n\n## Tool Execution Failures\n\n### Execution Errors vs Tool Call Errors\n\nTool execution errors occur **after** successful tool call parsing and dispatch. These are distinct from tool call errors:\n\n- **Tool Call Error**: Format/parsing issues before tool execution begins\n- **Tool Execution Error**: Issues during the tool's `run()` method (covered in [Common Execution Errors](#10.1))\n\n### Error Propagation\n\nWhen a tool's `run()` method fails:\n\n```mermaid\nsequenceDiagram\n    participant Action as \"call_tools_safely\"\n    participant Tool as \"ExecutePythonCodeTool.run\"\n    participant Executor as \"subprocess_python_executor\"\n    \n    Action->>Tool: \"tool.run()\"\n    Tool->>Executor: \"run_structured_in_subprocess(...)\"\n    Executor-->>Tool: \"ExecutionResult(status=CRASHED, ...)\"\n    Tool-->>Action: \"Formatted error message\"\n    Note over Action: \"Error caught by try-except\"\n    Action->>Action: \"tool_info['content'] = error_msg\"\n    Action-->>Agent: \"Return tool_info with error\"\n    Agent->>Messages: \"Append as tool message\"\n    Agent->>LLM: \"Send updated messages\"\n```\n\nThe error is wrapped and returned as a string in `tool_info[\"content\"]`, which becomes the tool response message content. The LLM can then read this error and adjust its strategy.\n\nSources: [src/agent/action.py:40-47]()\n\n---\n\n## Debugging Tool Call Errors\n\n### Log File Analysis\n\nTool call errors are logged to multiple files:\n\n| Log File | Contents | Location |\n|----------|----------|----------|\n| `logs/utils.log` | User inputs, LLM outputs, tool results | [logs/utils.log:1-296]() |\n| `logs/global.log` | Function call traces with arguments | [logs/global.log:1-10000]() |\n| `logs/trace.log` | Detailed execution traces | (Not provided in files) |\n\n### Key Log Patterns\n\n**1. Function Call Format Error:**\n```\nChatCompletionMessage(..., function_call=FunctionCall(...), tool_calls=None)\n```\n\n**2. Missing Tool Response:**\n```\nBadRequestError: ... tool_call_ids did not have response messages: message[7].role\n```\n\n**3. Argument Parsing Failure:**\n```\nå·¥å·å½æ°è°ç¨å¤±è´¥..., éè¯¯ä¿¡æ¯: JSONDecodeError: ...\n```\n\n### Inspection Points\n\n```mermaid\ngraph LR\n    P1[\"llm.py:36<br/>assistant_output inspection\"]\n    P2[\"llm.py:44-49<br/>has_tool_call() check\"]\n    P3[\"action.py:13<br/>JSON argument parsing\"]\n    P4[\"action.py:40<br/>Exception catching\"]\n    \n    P1 --> P2\n    P2 --> P3\n    P3 --> P4\n    \n    style P1 fill:#eff,stroke:#333\n    style P2 fill:#eff,stroke:#333\n    style P3 fill:#eff,stroke:#333\n    style P4 fill:#eff,stroke:#333\n```\n\nSources: [logs/utils.log:1-296](), [logs/global.log:1-10000]()\n\n---\n\n## Recovery Patterns\n\n### Pattern 1: Format Correction Feedback\n\nWhen `function_call` is detected instead of `tool_calls`:\n\n1. **Detect:** [src/agent/llm.py:48]() returns `True`\n2. **Feedback:** Append correction message to conversation\n3. **Retry:** LLM generates new response with `tool_calls` format\n\n### Pattern 2: Message Structure Fix\n\nWhen message validation fails:\n\n1. **Identify:** Parse error message to find missing `tool_call_id`\n2. **Reconstruct:** Ensure all tool calls have response messages\n3. **Validate:** Check message array has correct role sequence\n\n### Pattern 3: Argument Adjustment\n\nWhen argument parsing fails:\n\n1. **Extract:** Error details from exception traceback\n2. **Explain:** Return detailed error to LLM as tool response\n3. **Adjust:** LLM modifies arguments based on error feedback\n\n### Pattern 4: Tool Fallback\n\nWhen tool execution fails repeatedly:\n\n1. **Track:** Count consecutive failures for same tool\n2. **Escalate:** After threshold, suggest alternative approaches\n3. **Skip:** Mark problematic subtask as \"skipped\" in task tree\n\nSources: [src/agent/action.py:40-47](), [docs/functino_call_err.design.md:1-51]()\n\n---\n\n## System Prompt Guidance\n\nThe system prompt in [prompt.py:react_system_prompt]() includes error handling instructions:\n\n```\nå¦æéå°å·¥å·è°ç¨éè¯¯ï¼\n1. è®°å½éè¯¯ä¿¡æ¯ï¼åæéè¯¯åå ã\n2. å°è¯è°æ´éè¯ï¼æèè°æ´åæè®¡åæ ï¼éæ°è§åè·¯å¾ã\n3. è®°å½éè¯æ¬¡æ°ï¼é¿åæ éå¾ªç¯ã\n4. å¦æå¤æ¬¡éè¯ä»ç¶å¤±è´¥ï¼èèæ¾å¼è¯¥è·¯å¾ï¼è½¬åå¶ä»åææ¹åã\n```\n\nThis guidance helps the LLM:\n- Recognize when tool calls fail\n- Analyze error messages in tool responses\n- Adjust tool arguments or approach\n- Prevent infinite retry loops\n- Gracefully abandon failing paths\n\nSources: [src/agent/memory.py:1-17](), [logs/global.log:115-126]()\n\n---\n\n## Common Error Messages Reference\n\n| Error Message | Root Cause | Fix |\n|---------------|------------|-----|\n| \"æ²¡æå®ä¹function_callå·¥å·è°ç¨\" | LLM used deprecated `function_call` | Wait for LLM to retry with `tool_calls` |\n| \"tool_call_ids did not have response messages\" | Missing tool response in message array | Ensure each `tool_call_id` has response |\n| \"JSONDecodeError: Expecting value\" | Malformed JSON in arguments | Check for syntax errors in LLM output |\n| \"KeyError: 'python_code_snippet'\" | Missing required tool parameter | Review tool schema and ensure LLM provides all required fields |\n| \"å·¥å·å½æ°è°ç¨å¤±è´¥\" | Exception during tool execution | Check tool execution logs for details |\n\nSources: [logs/utils.log:64](), [docs/functino_call_err.design.md:1-51]()"
        },
        "isError": false
    }
}