import asyncio
import os
import sys
import json
from unittest.mock import Base
from pydantic import BaseModel, Field

from src.retrieval.arXiv.config import (
    MAX_REQUESTS_PER_SECOND, 
    DOWNLOAD_DIR, 
    DOWNLOAD_CONCURRENCY, 
    PAPERS_RESULT_PATH,
    )
from src.retrieval.arXiv.core.rate_limiter import TokenBucketLimiter
from src.retrieval.arXiv.core.network import RateLimitedClient
from src.retrieval.arXiv.services.search_service import SearchService
from src.retrieval.arXiv.services.download_service import DownloadService
from src.retrieval.arXiv.utils.logger import logger
from src.retrieval.arXiv import arxiv_pydantic

# ä½ çš„æŸ¥è¯¢åˆ—è¡¨
QUERIES = [
    'ti:"agent"',
]

class AllPapers(BaseModel):
    papers: list[arxiv_pydantic.Result] = Field(..., description="List of all unique papers found.")

async def main():
    # 1. åˆå§‹åŒ–åŸºç¡€è®¾æ–½
    logger.info("ğŸ› ï¸  Initializing system...")
    
    # æ ¸å¿ƒï¼šå…¨å±€é™æµå™¨ (1s 2ä¸ªä»¤ç‰Œ)
    global_limiter = TokenBucketLimiter(rate_per_second=MAX_REQUESTS_PER_SECOND)
    
    # ç½‘ç»œå®¢æˆ·ç«¯ (æ³¨å…¥é™æµå™¨)
    network_client = RateLimitedClient(global_limiter)
    await network_client.start()
    
    # ä¸šåŠ¡æœåŠ¡
    search_service = SearchService(global_limiter) # æœç´¢ä¹Ÿå…±äº«åŒä¸€ä¸ªé™æµå™¨
    download_service = DownloadService(network_client, DOWNLOAD_DIR)

    try:
        # 2. æ‰§è¡Œæœç´¢é˜¶æ®µ
        search_tasks = [search_service.search(q, max_results=1) for q in QUERIES]
        logger.info("ğŸ” Starting search phase...")
        results_list: list[list[arxiv_pydantic.Result]] = await asyncio.gather(*search_tasks)
        
        # å»é‡
        all_papers = AllPapers(papers=[])
        seen_ids = set()
        for res in results_list:
            for paper in res:
                if paper.get_short_id() not in seen_ids:
                    all_papers.papers.append(paper)
                    seen_ids.add(paper.get_short_id())
        logger.info(f"ğŸ“Š Total unique papers to process: {len(all_papers.papers)}")
        with open(PAPERS_RESULT_PATH, "w", encoding="utf-8") as f:
            all_papers_json_str = all_papers.model_dump_json(indent=4)
            json.dump(all_papers_json_str, f, ensure_ascii=False, indent=4)

        # 3. æ‰§è¡Œä¸‹è½½é˜¶æ®µ
        # ä½¿ç”¨Semaphoreæ§åˆ¶æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°ï¼ˆè™½ç„¶æœ‰ä»¤ç‰Œæ¡¶å…œåº•ï¼Œä½†Semaphoreå¯ä»¥é˜²æ­¢åˆ›å»ºè¿‡å¤šTaskå¯¹è±¡å ç”¨å†…å­˜ï¼‰
        sem = asyncio.Semaphore(DOWNLOAD_CONCURRENCY)
        async def bounded_process(paper: arxiv_pydantic.Result):
            async with sem:
                await download_service.process_paper(paper)

        download_tasks = [bounded_process(p) for p in all_papers.papers]
        if download_tasks:
            logger.info("ğŸ”¥ Starting download phase...")
            await asyncio.gather(*download_tasks)
        else:
            logger.warning("âš ï¸  No papers found.")

    finally:
        await network_client.close()
        logger.info("âœ¨ Mission Complete.")

if __name__ == "__main__":
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Stopped by user.")